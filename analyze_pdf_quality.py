import asyncio
import os
import sys
import logging
import re
from typing import List, Dict, Any

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(os.path.join(os.path.dirname(__file__), 'Chatbot-backend-main'))

from modules.gemini_question_analyzer import GeminiQuestionAnalyzer
import psycopg2
from psycopg2.extras import RealDictCursor

# ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def analyze_pdf_quality():
    """WALLIOR PCæ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«.pdfã®å“è³ªå•é¡Œã‚’åˆ†æ"""
    print("ğŸ” WALLIOR PCæ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«.pdfã®å“è³ªå•é¡Œã‚’åˆ†æã—ã¾ã™...")
    
    try:
        analyzer = GeminiQuestionAnalyzer()
        
        # 1. WALLIOR PCæ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«.pdfã®ãƒãƒ£ãƒ³ã‚¯å†…å®¹ã‚’è©³ç´°ç¢ºèª
        print("\nğŸ“„ 1. WALLIOR PCæ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«.pdfã®ãƒãƒ£ãƒ³ã‚¯å†…å®¹åˆ†æ...")
        with psycopg2.connect(analyzer.db_url, cursor_factory=RealDictCursor) as conn:
            with conn.cursor() as cur:
                # æ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã‚’å–å¾—
                cur.execute("""
                    SELECT id, name, type, uploaded_at
                    FROM document_sources
                    WHERE name LIKE '%æ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«%'
                    ORDER BY uploaded_at DESC;
                """)
                
                manual_docs = cur.fetchall()
                if not manual_docs:
                    print("âŒ æ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    return
                
                print(f"ğŸ“š è¦‹ã¤ã‹ã£ãŸæ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«: {len(manual_docs)}ä»¶")
                for doc in manual_docs:
                    print(f"  ğŸ“ {doc['name']} (ID: {doc['id'][:8]}...)")
                
                # æœ€æ–°ã®æ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚’åˆ†æå¯¾è±¡ã¨ã™ã‚‹
                target_doc = manual_docs[0]
                doc_id = target_doc['id']
                doc_name = target_doc['name']
                
                print(f"\nğŸ¯ åˆ†æå¯¾è±¡: {doc_name}")
                
                # ãƒãƒ£ãƒ³ã‚¯æ•°ã¨åŸºæœ¬çµ±è¨ˆã‚’ç¢ºèª
                cur.execute("""
                    SELECT 
                        COUNT(*) as total_chunks,
                        AVG(LENGTH(content)) as avg_length,
                        MIN(LENGTH(content)) as min_length,
                        MAX(LENGTH(content)) as max_length,
                        MIN(chunk_index) as min_index,
                        MAX(chunk_index) as max_index
                    FROM chunks
                    WHERE doc_id = %s AND content IS NOT NULL;
                """, (doc_id,))
                
                stats = cur.fetchone()
                print(f"ğŸ“Š ãƒãƒ£ãƒ³ã‚¯çµ±è¨ˆ:")
                print(f"  ğŸ§© ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {stats['total_chunks']}ä»¶")
                print(f"  ğŸ“ å¹³å‡æ–‡å­—æ•°: {stats['avg_length']:.1f}æ–‡å­—")
                print(f"  ğŸ“ æ–‡å­—æ•°ç¯„å›²: {stats['min_length']} - {stats['max_length']}æ–‡å­—")
                print(f"  ğŸ“‘ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²: {stats['min_index']} - {stats['max_index']}")
                
                # ã‚µãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒ³ã‚¯ã®å†…å®¹ã‚’ç¢ºèª
                print(f"\nğŸ“ ãƒãƒ£ãƒ³ã‚¯ã‚µãƒ³ãƒ—ãƒ«åˆ†æ:")
                cur.execute("""
                    SELECT 
                        id,
                        chunk_index,
                        content,
                        LENGTH(content) as content_length
                    FROM chunks
                    WHERE doc_id = %s AND content IS NOT NULL
                    ORDER BY chunk_index
                    LIMIT 10;
                """, (doc_id,))
                
                sample_chunks = cur.fetchall()
                
                for i, chunk in enumerate(sample_chunks, 1):
                    print(f"\n  ğŸ“„ ãƒãƒ£ãƒ³ã‚¯#{chunk['chunk_index']} ({chunk['content_length']}æ–‡å­—)")
                    content = chunk['content']
                    
                    # æ–‡å­—åŒ–ã‘ã‚„ç•°å¸¸ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
                    issues = []
                    
                    # æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
                    if re.search(r'[?]{3,}', content):
                        issues.append("é€£ç¶šã—ãŸ?ãƒãƒ¼ã‚¯ï¼ˆæ–‡å­—åŒ–ã‘ã®å¯èƒ½æ€§ï¼‰")
                    
                    # ç•°å¸¸ã«çŸ­ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
                    if len(content.strip()) < 20:
                        issues.append("å†…å®¹ãŒçŸ­ã™ãã‚‹")
                    
                    # æ„å‘³ã®ã‚ã‚‹æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    japanese_chars = len(re.findall(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', content))
                    if japanese_chars / len(content) < 0.1:
                        issues.append("æ—¥æœ¬èªã®å‰²åˆãŒä½ã„")
                    
                    # ç‰¹æ®Šæ–‡å­—ãŒå¤šã™ãã‚‹
                    special_chars = len(re.findall(r'[^\w\s\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', content))
                    if special_chars / len(content) > 0.3:
                        issues.append("ç‰¹æ®Šæ–‡å­—ãŒå¤šã™ãã‚‹")
                    
                    # ãƒšãƒ¼ã‚¸å¢ƒç•Œãƒãƒ¼ã‚«ãƒ¼ã®ç¢ºèª
                    if "===" in content and "ãƒšãƒ¼ã‚¸" in content:
                        issues.append("ãƒšãƒ¼ã‚¸å¢ƒç•Œãƒãƒ¼ã‚«ãƒ¼ãŒå«ã¾ã‚Œã¦ã„ã‚‹")
                    
                    if issues:
                        print(f"    âš ï¸ å•é¡Œ: {', '.join(issues)}")
                    else:
                        print(f"    âœ… æ­£å¸¸")
                    
                    # å†…å®¹ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆæœ€åˆã®200æ–‡å­—ï¼‰
                    preview = content.replace('\n', ' ').replace('\r', ' ')[:200]
                    print(f"    ğŸ“– å†…å®¹: {preview}...")
        
        # 2. æ¤œç´¢ã§ã®å•é¡Œã‚’å…·ä½“çš„ã«ç¢ºèª
        print(f"\nğŸ” 2. æ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«é–¢é€£ã®æ¤œç´¢ãƒ†ã‚¹ãƒˆ...")
        
        # æ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã«å«ã¾ã‚Œã‚‹ã§ã‚ã‚ã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ†ã‚¹ãƒˆ
        manual_keywords = [
            "WALLIOR PC",
            "æ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«", 
            "ãƒ¬ãƒ³ã‚¿ãƒ«",
            "å¥‘ç´„",
            "ç”³è¾¼",
            "æ•…éšœ",
            "æ’¤å»",
            "å†ãƒ¬ãƒ³ã‚¿ãƒ«",
            "è§£ç´„"
        ]
        
        for keyword in manual_keywords:
            print(f"\n  ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: '{keyword}'")
            
            # ç›´æ¥SQLæ¤œç´¢ã§PDFã‹ã‚‰ã®çµæœã‚’ç¢ºèª
            with psycopg2.connect(analyzer.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        SELECT 
                            c.id,
                            c.chunk_index,
                            c.content,
                            ds.name as document_name
                        FROM chunks c
                        LEFT JOIN document_sources ds ON ds.id = c.doc_id
                        WHERE c.content ILIKE %s
                          AND ds.name LIKE '%æ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«%'
                        ORDER BY c.chunk_index
                        LIMIT 3;
                    """, (f"%{keyword}%",))
                    
                    results = cur.fetchall()
                    print(f"    ğŸ“Š æ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‹ã‚‰ã®æ¤œç´¢çµæœ: {len(results)}ä»¶")
                    
                    for result in results:
                        content = result['content']
                        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‰å¾Œ50æ–‡å­—ã‚’æŠ½å‡º
                        keyword_pos = content.lower().find(keyword.lower())
                        if keyword_pos >= 0:
                            start = max(0, keyword_pos - 50)
                            end = min(len(content), keyword_pos + len(keyword) + 50)
                            context = content[start:end].replace('\n', ' ')
                            print(f"      ğŸ“„ ãƒãƒ£ãƒ³ã‚¯#{result['chunk_index']}: ...{context}...")
                        else:
                            print(f"      ğŸ“„ ãƒãƒ£ãƒ³ã‚¯#{result['chunk_index']}: {content[:100]}...")
            
            # Geminiæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã§ã®çµæœã‚‚ç¢ºèª
            analysis = await analyzer.analyze_question(f"{keyword}ã«ã¤ã„ã¦æ•™ãˆã¦")
            gemini_results = await analyzer.execute_sql_search(analysis, limit=10)
            
            manual_results = [r for r in gemini_results if 'æ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«' in r.document_name]
            print(f"    ğŸ¤– Geminiæ¤œç´¢ã§ã®æ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«çµæœ: {len(manual_results)}ä»¶")
            
            if manual_results:
                for result in manual_results[:2]:
                    print(f"      ğŸ¯ ã‚¹ã‚³ã‚¢: {result.score:.3f}")
                    print(f"      ğŸ“ å†…å®¹: {result.content[:100]}...")
        
        # 3. ä»–ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã¨ã®æ¯”è¼ƒ
        print(f"\nğŸ“Š 3. ä»–ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã¨ã®å“è³ªæ¯”è¼ƒ...")
        with psycopg2.connect(analyzer.db_url, cursor_factory=RealDictCursor) as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT 
                        ds.name as document_name,
                        COUNT(c.id) as chunk_count,
                        AVG(LENGTH(c.content)) as avg_length,
                        AVG(CASE 
                            WHEN c.content ~ '[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]' THEN 1 
                            ELSE 0 
                        END) as japanese_ratio
                    FROM document_sources ds
                    LEFT JOIN chunks c ON ds.id = c.doc_id
                    WHERE ds.type = 'pdf' AND c.content IS NOT NULL
                    GROUP BY ds.id, ds.name
                    ORDER BY chunk_count DESC;
                """)
                
                pdf_comparison = cur.fetchall()
                print("ğŸ“‹ PDFãƒ•ã‚¡ã‚¤ãƒ«å“è³ªæ¯”è¼ƒ:")
                
                for pdf in pdf_comparison:
                    print(f"  ğŸ“ {pdf['document_name']}")
                    print(f"    ğŸ§© ãƒãƒ£ãƒ³ã‚¯æ•°: {pdf['chunk_count']}ä»¶")
                    print(f"    ğŸ“ å¹³å‡æ–‡å­—æ•°: {pdf['avg_length']:.1f}æ–‡å­—")
                    print(f"    ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªå«æœ‰ç‡: {pdf['japanese_ratio']:.1%}")
                    
                    # æ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‹ã©ã†ã‹ã‚’åˆ¤å®š
                    if 'æ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«' in pdf['document_name']:
                        if pdf['avg_length'] < 100:
                            print(f"    âš ï¸ å¹³å‡æ–‡å­—æ•°ãŒå°‘ãªã™ãã‚‹å¯èƒ½æ€§")
                        if pdf['japanese_ratio'] < 0.5:
                            print(f"    âš ï¸ æ—¥æœ¬èªå«æœ‰ç‡ãŒä½ã™ãã‚‹å¯èƒ½æ€§")
                    print()
        
    except Exception as e:
        print(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nâœ… PDFå“è³ªåˆ†æå®Œäº†")

if __name__ == "__main__":
    asyncio.run(analyze_pdf_quality()) 
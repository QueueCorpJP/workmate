"""
PDFå…¨ä½“ã‚«ãƒãƒ¬ãƒƒã‚¸æ¤œç´¢ãƒ†ã‚¹ãƒˆ
PDFã®å¾ŒåŠã«ã‚ã‚‹é‡è¦æƒ…å ±ã‚‚ç¢ºå®Ÿã«å–å¾—ã§ãã¦ã„ã‚‹ã‹ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ãƒ†ã‚¹ãƒˆé …ç›®:
1. æ–‡æ›¸ã®ä½ç½®åˆ†å¸ƒï¼ˆå‰åŠãƒ»ä¸­ç›¤ãƒ»å¾ŒåŠï¼‰
2. åŒä¸€æ–‡æ›¸ã‹ã‚‰ã®è¤‡æ•°ãƒãƒ£ãƒ³ã‚¯å–å¾—
3. æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆ¥ã®æ€§èƒ½æ¯”è¼ƒ
4. å‹•çš„LIMITèª¿æ•´ã®åŠ¹æœæ¸¬å®š
"""

import asyncio
import sys
import os
from typing import List, Dict, Any
import logging

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from modules.comprehensive_search_system import comprehensive_search
from modules.chat_rag_enhanced import enhanced_rag_search
from modules.chat_rag import adaptive_rag_search
from modules.enhanced_postgresql_search import enhanced_search_chunks

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PDFCoverageTestSuite:
    """PDFå…¨ä½“ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ"""
    
    def __init__(self):
        self.test_queries = [
            # ä¸€èˆ¬çš„ãªæ¥­å‹™è³ªå•
            "ä¼šè­°ã®é€²è¡Œæ–¹æ³•ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
            "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æœ€çµ‚æ®µéšã§æ³¨æ„ã™ã¹ãç‚¹ã¯ä½•ã§ã™ã‹",
            "å“è³ªç®¡ç†ã®å…·ä½“çš„ãªæ‰‹é †ã‚’èª¬æ˜ã—ã¦ãã ã•ã„",
            
            # è©³ç´°ãªæŠ€è¡“è³ªå•
            "ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ã«ãŠã„ã¦å¾ŒåŠãƒ•ã‚§ãƒ¼ã‚ºã§ç™ºç”Ÿã—ã‚„ã™ã„å•é¡Œã¨ãã®å¯¾ç­–æ–¹æ³•ã‚’è©³ã—ãæ•™ãˆã¦ãã ã•ã„",
            "æ–‡æ›¸ã®æœ€å¾Œã®ç« ã‚„çµè«–éƒ¨åˆ†ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã¯ã‚ã‚Šã¾ã™ã‹",
            
            # åˆ†æãƒ»æ¯”è¼ƒè³ªå•
            "å°å…¥å‰ã¨å°å…¥å¾Œã®æ¯”è¼ƒçµæœã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„ã§ã™",
            "æœ€çµ‚çš„ãªè©•ä¾¡ã‚„æˆæœã«ã¤ã„ã¦ã¾ã¨ã‚ã¦æ•™ãˆã¦ãã ã•ã„"
        ]
    
    async def run_comprehensive_test(self, company_id: str = None):
        """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        logger.info("ğŸ§ª PDFå…¨ä½“ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        results = {
            'enhanced_rag': [],
            'comprehensive_search': [],
            'adaptive_rag': [],
            'enhanced_postgresql': []
        }
        
        for i, query in enumerate(self.test_queries, 1):
            logger.info(f"\nğŸ“ ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª {i}/{len(self.test_queries)}: '{query}'")
            
            # å„æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            enhanced_result = await self._test_enhanced_rag(query, company_id)
            comprehensive_result = await self._test_comprehensive_search(query, company_id)
            adaptive_result = await self._test_adaptive_rag(query, company_id)
            postgresql_result = await self._test_enhanced_postgresql(query, company_id)
            
            results['enhanced_rag'].append({
                'query': query,
                'result': enhanced_result
            })
            results['comprehensive_search'].append({
                'query': query,
                'result': comprehensive_result
            })
            results['adaptive_rag'].append({
                'query': query,
                'result': adaptive_result
            })
            results['enhanced_postgresql'].append({
                'query': query,
                'result': postgresql_result
            })
        
        # çµæœåˆ†æã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        await self._generate_coverage_report(results)
        
        logger.info("âœ… PDFå…¨ä½“ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ†ã‚¹ãƒˆå®Œäº†")
    
    async def _test_enhanced_rag(self, query: str, company_id: str = None) -> Dict[str, Any]:
        """æ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
        try:
            logger.info("  ğŸ” æ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆä¸­...")
            results = await enhanced_rag_search(
                query=query,
                context="",
                company_id=company_id,
                adaptive_limits=True
            )
            
            analysis = self._analyze_search_results(results, "enhanced_rag")
            logger.info(f"    çµæœ: {len(results)}ä»¶, æ–‡æ›¸æ•°: {analysis['document_count']}, "
                       f"å¾ŒåŠæ¯”ç‡: {analysis['rear_percentage']:.1f}%")
            
            return {
                'results': results,
                'analysis': analysis,
                'status': 'success'
            }
            
        except Exception as e:
            logger.error(f"    æ‹¡å¼µRAGãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'results': [],
                'analysis': {},
                'status': 'error',
                'error': str(e)
            }
    
    async def _test_comprehensive_search(self, query: str, company_id: str = None) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
        try:
            logger.info("  ğŸ” åŒ…æ‹¬çš„æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆä¸­...")
            results = await comprehensive_search(
                query=query,
                company_id=company_id,
                initial_limit=50,
                final_limit=15
            )
            
            analysis = self._analyze_search_results(results, "comprehensive_search")
            logger.info(f"    çµæœ: {len(results)}ä»¶, æ–‡æ›¸æ•°: {analysis['document_count']}, "
                       f"å¾ŒåŠæ¯”ç‡: {analysis['rear_percentage']:.1f}%")
            
            return {
                'results': results,
                'analysis': analysis,
                'status': 'success'
            }
            
        except Exception as e:
            logger.error(f"    åŒ…æ‹¬çš„æ¤œç´¢ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'results': [],
                'analysis': {},
                'status': 'error',
                'error': str(e)
            }
    
    async def _test_adaptive_rag(self, query: str, company_id: str = None) -> Dict[str, Any]:
        """å¾“æ¥ã®é©å¿œçš„RAGã®ãƒ†ã‚¹ãƒˆ"""
        try:
            logger.info("  ğŸ” å¾“æ¥é©å¿œçš„RAGãƒ†ã‚¹ãƒˆä¸­...")
            results = await adaptive_rag_search(query, limit=10)
            
            analysis = self._analyze_search_results(results, "adaptive_rag")
            logger.info(f"    çµæœ: {len(results)}ä»¶, æ–‡æ›¸æ•°: {analysis['document_count']}, "
                       f"å¾ŒåŠæ¯”ç‡: {analysis['rear_percentage']:.1f}%")
            
            return {
                'results': results,
                'analysis': analysis,
                'status': 'success'
            }
            
        except Exception as e:
            logger.error(f"    é©å¿œçš„RAGãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'results': [],
                'analysis': {},
                'status': 'error',
                'error': str(e)
            }
    
    async def _test_enhanced_postgresql(self, query: str, company_id: str = None) -> Dict[str, Any]:
        """Enhanced PostgreSQLæ¤œç´¢ã®ãƒ†ã‚¹ãƒˆ"""
        try:
            logger.info("  ğŸ” Enhanced PostgreSQLãƒ†ã‚¹ãƒˆä¸­...")
            results = await enhanced_search_chunks(
                query=query,
                company_id=company_id,
                limit=10,
                threshold=0.2
            )
            
            analysis = self._analyze_search_results(results, "enhanced_postgresql")
            logger.info(f"    çµæœ: {len(results)}ä»¶, æ–‡æ›¸æ•°: {analysis['document_count']}, "
                       f"å¾ŒåŠæ¯”ç‡: {analysis['rear_percentage']:.1f}%")
            
            return {
                'results': results,
                'analysis': analysis,
                'status': 'success'
            }
            
        except Exception as e:
            logger.error(f"    Enhanced PostgreSQLãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'results': [],
                'analysis': {},
                'status': 'error',
                'error': str(e)
            }
    
    def _analyze_search_results(self, results: List[Dict[str, Any]], system_name: str) -> Dict[str, Any]:
        """æ¤œç´¢çµæœã®åˆ†æ"""
        if not results:
            return {
                'total_results': 0,
                'document_count': 0,
                'position_distribution': {'å‰åŠ': 0, 'ä¸­ç›¤': 0, 'å¾ŒåŠ': 0},
                'rear_percentage': 0.0,
                'avg_score': 0.0,
                'document_diversity_score': 0.0
            }
        
        # åŸºæœ¬çµ±è¨ˆ
        total_results = len(results)
        unique_documents = set()
        position_distribution = {'å‰åŠ': 0, 'ä¸­ç›¤': 0, 'å¾ŒåŠ': 0}
        total_score = 0
        
        for result in results:
            # æ–‡æ›¸åã®å–å¾—
            doc_name = result.get('document_name', result.get('file_name', 'Unknown'))
            unique_documents.add(doc_name)
            
            # ã‚¹ã‚³ã‚¢ã®ç´¯è¨ˆ
            score = result.get('score', 0)
            total_score += score
            
            # ä½ç½®åˆ†å¸ƒã®è¨ˆç®—
            coverage = result.get('document_coverage', 0)
            if coverage is None:
                # document_coverageãŒãªã„å ´åˆã¯chunk_indexã‹ã‚‰æ¨å®š
                chunk_index = result.get('chunk_index', 0)
                if chunk_index < 5:
                    position_distribution['å‰åŠ'] += 1
                elif chunk_index < 15:
                    position_distribution['ä¸­ç›¤'] += 1
                else:
                    position_distribution['å¾ŒåŠ'] += 1
            else:
                if coverage <= 0.33:
                    position_distribution['å‰åŠ'] += 1
                elif coverage <= 0.66:
                    position_distribution['ä¸­ç›¤'] += 1
                else:
                    position_distribution['å¾ŒåŠ'] += 1
        
        # å¾ŒåŠæ¯”ç‡ã®è¨ˆç®—
        rear_percentage = (position_distribution['å¾ŒåŠ'] / total_results) * 100 if total_results > 0 else 0
        
        # å¹³å‡ã‚¹ã‚³ã‚¢
        avg_score = total_score / total_results if total_results > 0 else 0
        
        # æ–‡æ›¸å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢ï¼ˆæ–‡æ›¸æ•°/çµæœæ•°ï¼‰
        document_diversity_score = len(unique_documents) / total_results if total_results > 0 else 0
        
        return {
            'total_results': total_results,
            'document_count': len(unique_documents),
            'position_distribution': position_distribution,
            'rear_percentage': rear_percentage,
            'avg_score': avg_score,
            'document_diversity_score': document_diversity_score,
            'unique_documents': list(unique_documents)
        }
    
    async def _generate_coverage_report(self, results: Dict[str, List[Dict]]):
        """ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        logger.info("\nğŸ“Š PDFå…¨ä½“ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ†ã‚¹ãƒˆ ãƒ¬ãƒãƒ¼ãƒˆ")
        logger.info("=" * 60)
        
        systems = ['enhanced_rag', 'comprehensive_search', 'adaptive_rag', 'enhanced_postgresql']
        system_names = {
            'enhanced_rag': 'æ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ ',
            'comprehensive_search': 'åŒ…æ‹¬çš„æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ',
            'adaptive_rag': 'å¾“æ¥é©å¿œçš„RAG',
            'enhanced_postgresql': 'Enhanced PostgreSQL'
        }
        
        # ã‚·ã‚¹ãƒ†ãƒ åˆ¥é›†è¨ˆ
        system_stats = {}
        
        for system in systems:
            system_results = results[system]
            
            total_results = 0
            total_documents = 0
            total_rear_percentage = 0
            total_avg_score = 0
            total_diversity_score = 0
            success_count = 0
            
            for query_result in system_results:
                if query_result['result']['status'] == 'success':
                    analysis = query_result['result']['analysis']
                    total_results += analysis.get('total_results', 0)
                    total_documents += analysis.get('document_count', 0)
                    total_rear_percentage += analysis.get('rear_percentage', 0)
                    total_avg_score += analysis.get('avg_score', 0)
                    total_diversity_score += analysis.get('document_diversity_score', 0)
                    success_count += 1
            
            if success_count > 0:
                system_stats[system] = {
                    'avg_results_per_query': total_results / success_count,
                    'avg_documents_per_query': total_documents / success_count,
                    'avg_rear_percentage': total_rear_percentage / success_count,
                    'avg_score': total_avg_score / success_count,
                    'avg_diversity_score': total_diversity_score / success_count,
                    'success_rate': (success_count / len(system_results)) * 100
                }
            else:
                system_stats[system] = {
                    'avg_results_per_query': 0,
                    'avg_documents_per_query': 0,
                    'avg_rear_percentage': 0,
                    'avg_score': 0,
                    'avg_diversity_score': 0,
                    'success_rate': 0
                }
        
        # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
        for system, stats in system_stats.items():
            logger.info(f"\nğŸ“ˆ {system_names[system]}:")
            logger.info(f"  å¹³å‡çµæœæ•°/ã‚¯ã‚¨ãƒª: {stats['avg_results_per_query']:.1f}ä»¶")
            logger.info(f"  å¹³å‡æ–‡æ›¸æ•°/ã‚¯ã‚¨ãƒª: {stats['avg_documents_per_query']:.1f}æ–‡æ›¸")
            logger.info(f"  å¹³å‡å¾ŒåŠæ¯”ç‡: {stats['avg_rear_percentage']:.1f}%")
            logger.info(f"  å¹³å‡é–¢é€£ã‚¹ã‚³ã‚¢: {stats['avg_score']:.3f}")
            logger.info(f"  å¹³å‡æ–‡æ›¸å¤šæ§˜æ€§: {stats['avg_diversity_score']:.3f}")
            logger.info(f"  æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        
        # æ¯”è¼ƒåˆ†æ
        logger.info(f"\nğŸ† æ€§èƒ½æ¯”è¼ƒ:")
        
        # å¾ŒåŠæ¯”ç‡ã®æ¯”è¼ƒ
        rear_ratios = {system: stats['avg_rear_percentage'] for system, stats in system_stats.items()}
        best_rear = max(rear_ratios.items(), key=lambda x: x[1])
        logger.info(f"  ğŸ“ å¾ŒåŠãƒãƒ£ãƒ³ã‚¯å–å¾—ç‡ãŒæœ€é«˜: {system_names[best_rear[0]]} ({best_rear[1]:.1f}%)")
        
        # æ–‡æ›¸å¤šæ§˜æ€§ã®æ¯”è¼ƒ
        diversity_scores = {system: stats['avg_diversity_score'] for system, stats in system_stats.items()}
        best_diversity = max(diversity_scores.items(), key=lambda x: x[1])
        logger.info(f"  ğŸ“š æ–‡æ›¸å¤šæ§˜æ€§ãŒæœ€é«˜: {system_names[best_diversity[0]]} ({best_diversity[1]:.3f})")
        
        # çµæœæ•°ã®æ¯”è¼ƒ
        result_counts = {system: stats['avg_results_per_query'] for system, stats in system_stats.items()}
        best_results = max(result_counts.items(), key=lambda x: x[1])
        logger.info(f"  ğŸ“Š å¹³å‡çµæœæ•°ãŒæœ€å¤š: {system_names[best_results[0]]} ({best_results[1]:.1f}ä»¶)")
        
        logger.info("\nâœ… PDFå…¨ä½“ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ†ã‚¹ãƒˆå®Œäº†")

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("PDFå…¨ä½“ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    test_suite = PDFCoverageTestSuite()
    await test_suite.run_comprehensive_test()

if __name__ == "__main__":
    asyncio.run(main()) 
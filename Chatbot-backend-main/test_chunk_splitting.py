#!/usr/bin/env python3
"""
ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ
ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã®ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãŒæ­£ã—ãå‹•ä½œã™ã‚‹ã‹ã‚’æ¤œè¨¼
"""

import asyncio
import sys
import logging
from datetime import datetime

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def test_chunk_splitting_function():
    """ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ©Ÿèƒ½ã®å˜ä½“ãƒ†ã‚¹ãƒˆ"""
    logger.info("ğŸ§ª ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        from modules.document_processor import DocumentProcessor
        
        # DocumentProcessoråˆæœŸåŒ–
        processor = DocumentProcessor()
        logger.info("âœ… DocumentProcessoråˆæœŸåŒ–æˆåŠŸ")
        
        # ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        test_cases = [
            {
                "name": "çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆ",
                "text": "ã“ã‚Œã¯çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã®ãƒ†ã‚¹ãƒˆã‚’è¡Œã„ã¾ã™ã€‚",
                "expected_chunks": 1
            },
            {
                "name": "ä¸­ç¨‹åº¦ã®ãƒ†ã‚­ã‚¹ãƒˆ",
                "text": "ã“ã‚Œã¯ä¸­ç¨‹åº¦ã®é•·ã•ã®ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚" * 20 + "\n\n" + "æ®µè½ã‚’åˆ†ã‘ãŸãƒ†ã‚­ã‚¹ãƒˆã‚‚å«ã‚ã¾ã™ã€‚" * 20,
                "expected_chunks": 1
            },
            {
                "name": "é•·ã„ãƒ†ã‚­ã‚¹ãƒˆ",
                "text": "ã“ã‚Œã¯é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚" * 100 + "\n\n" + "è¤‡æ•°ã®æ®µè½ã«åˆ†ã‹ã‚Œã¦ã„ã¾ã™ã€‚" * 100 + "\n\n" + "ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã®ãƒ†ã‚¹ãƒˆã‚’è¡Œã„ã¾ã™ã€‚" * 100,
                "expected_chunks": 3
            },
            {
                "name": "éå¸¸ã«é•·ã„ãƒ†ã‚­ã‚¹ãƒˆ",
                "text": "é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã®ä¾‹ã§ã™ã€‚" * 200 + "\n\n" + "æ®µè½1: " + "å†…å®¹ãŒç¶šãã¾ã™ã€‚" * 150 + "\n\n" + "æ®µè½2: " + "ã•ã‚‰ã«å†…å®¹ãŒç¶šãã¾ã™ã€‚" * 150 + "\n\n" + "æ®µè½3: " + "æœ€å¾Œã®å†…å®¹ã§ã™ã€‚" * 150,
                "expected_chunks": 5
            }
        ]
        
        for test_case in test_cases:
            logger.info(f"\nğŸ” ãƒ†ã‚¹ãƒˆ: {test_case['name']}")
            logger.info(f"   ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(test_case['text'])} æ–‡å­—")
            
            # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å®Ÿè¡Œ
            chunks = processor._split_text_into_chunks(test_case['text'], test_case['name'])
            
            logger.info(f"   ç”Ÿæˆã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")
            
            # å„ãƒãƒ£ãƒ³ã‚¯ã®è©³ç´°ç¢ºèª
            for i, chunk in enumerate(chunks):
                token_count = chunk.get('token_count', 0)
                content_length = len(chunk['content'])
                logger.info(f"   ãƒãƒ£ãƒ³ã‚¯ {i+1}: {token_count} ãƒˆãƒ¼ã‚¯ãƒ³, {content_length} æ–‡å­—")
                logger.info(f"      å†…å®¹: {chunk['content'][:100]}...")
                
                # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒç¯„å›²å†…ã‹ãƒã‚§ãƒƒã‚¯
                if token_count > 500:
                    logger.warning(f"   âš ï¸ ãƒãƒ£ãƒ³ã‚¯ {i+1}: ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒä¸Šé™(500)ã‚’è¶…ãˆã¦ã„ã¾ã™: {token_count}")
                elif token_count < 50:
                    logger.warning(f"   âš ï¸ ãƒãƒ£ãƒ³ã‚¯ {i+1}: ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå°‘ãªã™ãã¾ã™: {token_count}")
                else:
                    logger.info(f"   âœ… ãƒãƒ£ãƒ³ã‚¯ {i+1}: ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒé©åˆ‡ã§ã™: {token_count}")
        
        logger.info("âœ… ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆå®Œäº†")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

async def test_database_chunks():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®å®Ÿéš›ã®ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’èª¿æŸ»"""
    logger.info("ğŸ§ª ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿èª¿æŸ»é–‹å§‹")
    
    try:
        from supabase_adapter import get_supabase_client
        
        supabase = get_supabase_client()
        logger.info("âœ… Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå–å¾—æˆåŠŸ")
        
        # æœ€æ–°ã®ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        logger.info("ğŸ” æœ€æ–°ã®ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        result = supabase.table("chunks").select("id, doc_id, chunk_index, content, created_at").order("created_at", desc=True).limit(10).execute()
        
        if not result.data:
            logger.info("ğŸ“Š ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return True
        
        chunks = result.data
        logger.info(f"ğŸ“Š å–å¾—ã—ãŸãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")
        
        # å„ãƒãƒ£ãƒ³ã‚¯ã®è©³ç´°åˆ†æ
        for i, chunk in enumerate(chunks):
            logger.info(f"\nğŸ“„ ãƒãƒ£ãƒ³ã‚¯ {i+1}:")
            logger.info(f"   ID: {chunk['id']}")
            logger.info(f"   DOC_ID: {chunk['doc_id']}")
            logger.info(f"   ãƒãƒ£ãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {chunk['chunk_index']}")
            logger.info(f"   ä½œæˆæ—¥æ™‚: {chunk['created_at']}")
            logger.info(f"   å†…å®¹é•·: {len(chunk['content'])} æ–‡å­—")
            logger.info(f"   å†…å®¹: {chunk['content'][:200]}...")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
            from modules.document_processor import DocumentProcessor
            processor = DocumentProcessor()
            token_count = processor._count_tokens(chunk['content'])
            logger.info(f"   æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°: {token_count}")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            if token_count > 500:
                logger.warning(f"   âš ï¸ ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒä¸Šé™(500)ã‚’è¶…ãˆã¦ã„ã¾ã™: {token_count}")
            elif token_count < 50:
                logger.warning(f"   âš ï¸ ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå°‘ãªã™ãã¾ã™: {token_count}")
            else:
                logger.info(f"   âœ… ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒé©åˆ‡ã§ã™: {token_count}")
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã®ãƒãƒ£ãƒ³ã‚¯çµ±è¨ˆ
        logger.info("\nğŸ“Š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã®ãƒãƒ£ãƒ³ã‚¯çµ±è¨ˆ:")
        doc_stats = {}
        for chunk in chunks:
            doc_id = chunk['doc_id']
            if doc_id not in doc_stats:
                doc_stats[doc_id] = []
            doc_stats[doc_id].append(chunk)
        
        for doc_id, doc_chunks in doc_stats.items():
            logger.info(f"   DOC_ID: {doc_id}")
            logger.info(f"   ãƒãƒ£ãƒ³ã‚¯æ•°: {len(doc_chunks)}")
            
            # ãƒãƒ£ãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®é€£ç¶šæ€§ãƒã‚§ãƒƒã‚¯
            indices = sorted([chunk['chunk_index'] for chunk in doc_chunks])
            if indices == list(range(len(indices))):
                logger.info(f"   âœ… ãƒãƒ£ãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒé€£ç¶šã—ã¦ã„ã¾ã™: {indices}")
            else:
                logger.warning(f"   âš ï¸ ãƒãƒ£ãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«æ¬ è½ãŒã‚ã‚Šã¾ã™: {indices}")
        
        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿èª¿æŸ»å®Œäº†")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿èª¿æŸ»ã‚¨ãƒ©ãƒ¼: {e}")
        return False

async def test_chunk_size_distribution():
    """ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®åˆ†å¸ƒã‚’èª¿æŸ»"""
    logger.info("ğŸ§ª ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºåˆ†å¸ƒèª¿æŸ»é–‹å§‹")
    
    try:
        from supabase_adapter import get_supabase_client
        from modules.document_processor import DocumentProcessor
        
        supabase = get_supabase_client()
        processor = DocumentProcessor()
        
        # å¤§é‡ã®ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        logger.info("ğŸ” ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        result = supabase.table("chunks").select("content").limit(100).execute()
        
        if not result.data:
            logger.info("ğŸ“Š ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return True
        
        chunks = result.data
        logger.info(f"ğŸ“Š åˆ†æå¯¾è±¡ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")
        
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¨ã‚µã‚¤ã‚ºã®çµ±è¨ˆ
        token_counts = []
        char_counts = []
        
        for chunk in chunks:
            content = chunk['content']
            token_count = processor._count_tokens(content)
            char_count = len(content)
            
            token_counts.append(token_count)
            char_counts.append(char_count)
        
        # çµ±è¨ˆæƒ…å ±ã®è¨ˆç®—
        def calculate_stats(data):
            return {
                'min': min(data),
                'max': max(data),
                'avg': sum(data) / len(data),
                'median': sorted(data)[len(data) // 2]
            }
        
        token_stats = calculate_stats(token_counts)
        char_stats = calculate_stats(char_counts)
        
        logger.info(f"\nğŸ“ˆ ãƒˆãƒ¼ã‚¯ãƒ³æ•°çµ±è¨ˆ:")
        logger.info(f"   æœ€å°: {token_stats['min']}")
        logger.info(f"   æœ€å¤§: {token_stats['max']}")
        logger.info(f"   å¹³å‡: {token_stats['avg']:.1f}")
        logger.info(f"   ä¸­å¤®å€¤: {token_stats['median']}")
        
        logger.info(f"\nğŸ“ˆ æ–‡å­—æ•°çµ±è¨ˆ:")
        logger.info(f"   æœ€å°: {char_stats['min']}")
        logger.info(f"   æœ€å¤§: {char_stats['max']}")
        logger.info(f"   å¹³å‡: {char_stats['avg']:.1f}")
        logger.info(f"   ä¸­å¤®å€¤: {char_stats['median']}")
        
        # ç¯„å›²å¤–ãƒãƒ£ãƒ³ã‚¯ã®æ¤œå‡º
        oversized_chunks = [t for t in token_counts if t > 500]
        undersized_chunks = [t for t in token_counts if t < 50]
        
        logger.info(f"\nâš ï¸ ç¯„å›²å¤–ãƒãƒ£ãƒ³ã‚¯:")
        logger.info(f"   ä¸Šé™è¶…é(>500): {len(oversized_chunks)}ä»¶")
        logger.info(f"   ä¸‹é™æœªæº€(<50): {len(undersized_chunks)}ä»¶")
        
        if oversized_chunks:
            logger.warning(f"   ä¸Šé™è¶…éã®è©³ç´°: {oversized_chunks}")
        if undersized_chunks:
            logger.warning(f"   ä¸‹é™æœªæº€ã®è©³ç´°: {undersized_chunks}")
        
        # åˆ†å¸ƒã®å¦¥å½“æ€§è©•ä¾¡
        optimal_chunks = [t for t in token_counts if 300 <= t <= 500]
        acceptable_chunks = [t for t in token_counts if 50 <= t <= 500]
        
        logger.info(f"\nâœ… åˆ†å¸ƒè©•ä¾¡:")
        logger.info(f"   æœ€é©ç¯„å›²(300-500): {len(optimal_chunks)}/{len(token_counts)} ({len(optimal_chunks)/len(token_counts)*100:.1f}%)")
        logger.info(f"   è¨±å®¹ç¯„å›²(50-500): {len(acceptable_chunks)}/{len(token_counts)} ({len(acceptable_chunks)/len(token_counts)*100:.1f}%)")
        
        logger.info("âœ… ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºåˆ†å¸ƒèª¿æŸ»å®Œäº†")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºåˆ†å¸ƒèª¿æŸ»ã‚¨ãƒ©ãƒ¼: {e}")
        return False

async def test_upload_simulation():
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
    logger.info("ğŸ§ª ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        from modules.document_processor import DocumentProcessor
        
        processor = DocumentProcessor()
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
        test_text = """
        ã“ã‚Œã¯æ–‡æ›¸å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆã§ã™ã€‚
        
        ç¬¬1ç« : ã¯ã˜ã‚ã«
        æœ¬ã‚·ã‚¹ãƒ†ãƒ ã¯ã€å¤§é‡ã®æ–‡æ›¸ã‚’åŠ¹ç‡çš„ã«å‡¦ç†ã—ã€æ¤œç´¢å¯èƒ½ãªå½¢å¼ã«å¤‰æ›ã™ã‚‹ãŸã‚ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚
        ä¸»ãªæ©Ÿèƒ½ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š
        - æ–‡æ›¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        - ãƒ†ã‚­ã‚¹ãƒˆã®æŠ½å‡º
        - ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        - ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
        - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
        
        ç¬¬2ç« : ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆ
        ã‚·ã‚¹ãƒ†ãƒ ã¯ä»¥ä¸‹ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‹ã‚‰æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ï¼š
        1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
        2. ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ³ã‚¸ãƒ³
        3. ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚¨ãƒ³ã‚¸ãƒ³
        4. ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³
        5. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
        
        ç¬¬3ç« : å‡¦ç†ãƒ•ãƒ­ãƒ¼
        æ–‡æ›¸å‡¦ç†ã¯ä»¥ä¸‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã§è¡Œã‚ã‚Œã¾ã™ï¼š
        ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ•ã‚¡ã‚¤ãƒ«ã®å—ä¿¡ã¨æ¤œè¨¼
        ã‚¹ãƒ†ãƒƒãƒ—2: å½¢å¼ã«å¿œã˜ãŸãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        ã‚¹ãƒ†ãƒƒãƒ—3: æ„å‘³å˜ä½ã§ã®ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        ã‚¹ãƒ†ãƒƒãƒ—4: å„ãƒãƒ£ãƒ³ã‚¯ã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
        ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜
        
        ç¬¬4ç« : å“è³ªä¿è¨¼
        ã‚·ã‚¹ãƒ†ãƒ ã®å“è³ªã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã€ä»¥ä¸‹ã®ä»•çµ„ã¿ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ï¼š
        - è‡ªå‹•ãƒ†ã‚¹ãƒˆ
        - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        - ãƒªãƒˆãƒ©ã‚¤ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 
        - ãƒ­ã‚®ãƒ³ã‚°æ©Ÿèƒ½
        
        ç¬¬5ç« : é‹ç”¨ãƒ»ä¿å®ˆ
        ã‚·ã‚¹ãƒ†ãƒ ã®å®‰å®šé‹ç”¨ã®ãŸã‚ã€ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¦ã„ã¾ã™ï¼š
        - ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
        - ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½
        - ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ©Ÿèƒ½
        - éšœå®³å¯¾å¿œæ‰‹é †
        
        ä»¥ä¸ŠãŒã‚·ã‚¹ãƒ†ãƒ ã®æ¦‚è¦ã¨ãªã‚Šã¾ã™ã€‚
        """ * 3  # 3å›ç¹°ã‚Šè¿”ã—ã¦ã‚ˆã‚Šé•·ã„ãƒ†ã‚­ã‚¹ãƒˆã«
        
        logger.info(f"ğŸ“ ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆé•·: {len(test_text)} æ–‡å­—")
        
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å®Ÿè¡Œ
        logger.info("ğŸ”ª ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å®Ÿè¡Œä¸­...")
        chunks = processor._split_text_into_chunks(test_text, "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
        
        logger.info(f"ğŸ“Š ç”Ÿæˆã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")
        
        # å„ãƒãƒ£ãƒ³ã‚¯ã®è©³ç´°ç¢ºèª
        for i, chunk in enumerate(chunks):
            token_count = chunk.get('token_count', 0)
            content_length = len(chunk['content'])
            
            logger.info(f"   ğŸ“„ ãƒãƒ£ãƒ³ã‚¯ {i+1}:")
            logger.info(f"      ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {chunk['chunk_index']}")
            logger.info(f"      ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {token_count}")
            logger.info(f"      æ–‡å­—æ•°: {content_length}")
            logger.info(f"      å†…å®¹: {chunk['content'][:150]}...")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            if token_count > 500:
                logger.warning(f"      âš ï¸ ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒä¸Šé™(500)ã‚’è¶…ãˆã¦ã„ã¾ã™")
            elif token_count < 50:
                logger.warning(f"      âš ï¸ ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå°‘ãªã™ãã¾ã™")
            else:
                logger.info(f"      âœ… ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒé©åˆ‡ã§ã™")
        
        # ãƒãƒ£ãƒ³ã‚¯ã®é€£ç¶šæ€§ãƒã‚§ãƒƒã‚¯
        expected_indices = list(range(len(chunks)))
        actual_indices = [chunk['chunk_index'] for chunk in chunks]
        
        if actual_indices == expected_indices:
            logger.info("âœ… ãƒãƒ£ãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒé€£ç¶šã—ã¦ã„ã¾ã™")
        else:
            logger.warning(f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å•é¡ŒãŒã‚ã‚Šã¾ã™: æœŸå¾…å€¤={expected_indices}, å®Ÿéš›å€¤={actual_indices}")
        
        # å…¨ä½“ã®å“è³ªè©•ä¾¡
        token_counts = [chunk.get('token_count', 0) for chunk in chunks]
        avg_tokens = sum(token_counts) / len(token_counts)
        max_tokens = max(token_counts)
        min_tokens = min(token_counts)
        
        logger.info(f"\nğŸ“ˆ å“è³ªè©•ä¾¡:")
        logger.info(f"   å¹³å‡ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {avg_tokens:.1f}")
        logger.info(f"   æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {max_tokens}")
        logger.info(f"   æœ€å°ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {min_tokens}")
        logger.info(f"   ç›®æ¨™ç¯„å›²(300-500)å†…: {len([t for t in token_counts if 300 <= t <= 500])}/{len(token_counts)}")
        
        logger.info("âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆå®Œäº†")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

async def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    logger.info("ğŸš€ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ¤œè¨¼ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    test_results = {}
    
    # å„ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    tests = [
        ("ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ©Ÿèƒ½", test_chunk_splitting_function),
        ("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿", test_database_chunks),
        ("ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºåˆ†å¸ƒ", test_chunk_size_distribution),
        ("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³", test_upload_simulation)
    ]
    
    for test_name, test_func in tests:
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ§ª {test_name} ãƒ†ã‚¹ãƒˆé–‹å§‹")
        logger.info(f"{'='*60}")
        
        try:
            result = await test_func()
            test_results[test_name] = result
            
            if result:
                logger.info(f"âœ… {test_name} ãƒ†ã‚¹ãƒˆæˆåŠŸ")
            else:
                logger.error(f"âŒ {test_name} ãƒ†ã‚¹ãƒˆå¤±æ•—")
                
        except Exception as e:
            logger.error(f"âŒ {test_name} ãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            test_results[test_name] = False
    
    # çµæœã‚µãƒãƒªãƒ¼
    logger.info(f"\n{'='*60}")
    logger.info("ğŸ“Š ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼")
    logger.info(f"{'='*60}")
    
    passed = sum(1 for result in test_results.values() if result)
    total = len(test_results)
    
    for test_name, result in test_results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        logger.info(f"{status} - {test_name}")
    
    logger.info(f"\nğŸ¯ ç·åˆçµæœ: {passed}/{total} ãƒ†ã‚¹ãƒˆæˆåŠŸ")
    
    if passed == total:
        logger.info("ğŸ‰ ã™ã¹ã¦ã®ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ¤œè¨¼ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")
    else:
        logger.error("âš ï¸ ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    
    return passed == total

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
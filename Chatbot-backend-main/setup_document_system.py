"""
ğŸš€ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ  ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å®Œå…¨ãªRAGå¯¾å¿œã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤

å®Ÿè¡Œå†…å®¹:
1ï¸âƒ£ chunksãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
2ï¸âƒ£ å¿…è¦ãªæ‹¡å¼µæ©Ÿèƒ½ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆpgvectorï¼‰
3ï¸âƒ£ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
4ï¸âƒ£ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œï¼ˆdocument_sources â†’ chunksï¼‰
5ï¸âƒ£ ã‚·ã‚¹ãƒ†ãƒ å‹•ä½œç¢ºèª
"""

import os
import sys
import asyncio
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
import psycopg2
from psycopg2.extras import RealDictCursor
from dotenv import load_dotenv

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('setup_document_system.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

class DocumentSystemSetup:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.db_connection = None
        self.setup_stats = {
            "start_time": None,
            "end_time": None,
            "tables_created": 0,
            "indexes_created": 0,
            "data_migrated": 0,
            "errors": []
        }
    
    def _get_database_url(self) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šURLå–å¾—"""
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # PostgreSQLæ¥ç¶šURLæ§‹ç¯‰
        if "supabase.co" in supabase_url:
            project_id = supabase_url.split("://")[1].split(".")[0]
            db_url = f"postgresql://postgres.{project_id}:{os.getenv('DB_PASSWORD', '')}@aws-0-ap-northeast-1.pooler.supabase.com:6543/postgres"
        else:
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                raise ValueError("DATABASE_URL ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        return db_url
    
    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šåˆæœŸåŒ–"""
        try:
            db_url = self._get_database_url()
            self.db_connection = psycopg2.connect(db_url, cursor_factory=RealDictCursor)
            self.db_connection.autocommit = False
            logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå®Œäº†")
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _execute_sql_file(self, file_path: str, description: str) -> bool:
        """SQLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®Ÿè¡Œ"""
        try:
            if not os.path.exists(file_path):
                logger.warning(f"âš ï¸ SQLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
                return False
            
            with open(file_path, 'r', encoding='utf-8') as f:
                sql_content = f.read()
            
            cursor = self.db_connection.cursor()
            cursor.execute(sql_content)
            self.db_connection.commit()
            
            logger.info(f"âœ… {description} å®Œäº†")
            return True
            
        except Exception as e:
            logger.error(f"âŒ {description} ã‚¨ãƒ©ãƒ¼: {e}")
            self.db_connection.rollback()
            self.setup_stats["errors"].append(f"{description}: {str(e)}")
            return False
        finally:
            if 'cursor' in locals():
                cursor.close()
    
    def _check_table_exists(self, table_name: str) -> bool:
        """ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ç¢ºèª"""
        try:
            cursor = self.db_connection.cursor()
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = %s
                );
            """, (table_name,))
            
            exists = cursor.fetchone()[0]
            return exists
            
        except Exception as e:
            logger.error(f"âŒ ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            return False
        finally:
            if 'cursor' in locals():
                cursor.close()
    
    def _check_extension_exists(self, extension_name: str) -> bool:
        """æ‹¡å¼µæ©Ÿèƒ½å­˜åœ¨ç¢ºèª"""
        try:
            cursor = self.db_connection.cursor()
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM pg_extension 
                    WHERE extname = %s
                );
            """, (extension_name,))
            
            exists = cursor.fetchone()[0]
            return exists
            
        except Exception as e:
            logger.error(f"âŒ æ‹¡å¼µæ©Ÿèƒ½ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            return False
        finally:
            if 'cursor' in locals():
                cursor.close()
    
    def _install_pgvector_extension(self) -> bool:
        """pgvectoræ‹¡å¼µæ©Ÿèƒ½ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
        try:
            if self._check_extension_exists("vector"):
                logger.info("âœ… pgvectoræ‹¡å¼µæ©Ÿèƒ½ã¯æ—¢ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã§ã™")
                return True
            
            cursor = self.db_connection.cursor()
            cursor.execute("CREATE EXTENSION IF NOT EXISTS vector;")
            self.db_connection.commit()
            
            logger.info("âœ… pgvectoræ‹¡å¼µæ©Ÿèƒ½ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†")
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ pgvectoræ‹¡å¼µæ©Ÿèƒ½ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
            logger.info("ğŸ’¡ Supabaseã§pgvectorãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ã€ç®¡ç†ç”»é¢ã‹ã‚‰æœ‰åŠ¹åŒ–ã—ã¦ãã ã•ã„")
            self.db_connection.rollback()
            return False
        finally:
            if 'cursor' in locals():
                cursor.close()
    
    def _create_chunks_table(self) -> bool:
        """chunksãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ"""
        try:
            if self._check_table_exists("chunks"):
                logger.info("âœ… chunksãƒ†ãƒ¼ãƒ–ãƒ«ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™")
                return True
            
            # chunksãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆSQL
            create_table_sql = """
                CREATE TABLE IF NOT EXISTS chunks (
                    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                    doc_id TEXT NOT NULL,
                    chunk_index INTEGER NOT NULL,
                    content TEXT NOT NULL,
                    embedding VECTOR(768),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    company_id TEXT,
                    active BOOLEAN DEFAULT true,
                    special TEXT,
                    
                    CONSTRAINT fk_chunks_doc_id FOREIGN KEY (doc_id) REFERENCES document_sources(id) ON DELETE CASCADE
                );
            """
            
            cursor = self.db_connection.cursor()
            cursor.execute(create_table_sql)
            self.db_connection.commit()
            
            self.setup_stats["tables_created"] += 1
            logger.info("âœ… chunksãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå®Œäº†")
            return True
            
        except Exception as e:
            logger.error(f"âŒ chunksãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            self.db_connection.rollback()
            self.setup_stats["errors"].append(f"chunksãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ: {str(e)}")
            return False
        finally:
            if 'cursor' in locals():
                cursor.close()
    
    def _create_indexes(self) -> bool:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ"""
        try:
            indexes = [
                ("idx_chunks_doc_id", "CREATE INDEX IF NOT EXISTS idx_chunks_doc_id ON chunks(doc_id);"),
                ("idx_chunks_company_id", "CREATE INDEX IF NOT EXISTS idx_chunks_company_id ON chunks(company_id);"),
                # Note: chunks table doesn't have active column - active status is managed in document_sources
                ("idx_chunks_doc_chunk_index", "CREATE INDEX IF NOT EXISTS idx_chunks_doc_chunk_index ON chunks(doc_id, chunk_index);"),
            ]
            
            cursor = self.db_connection.cursor()
            
            for index_name, index_sql in indexes:
                try:
                    cursor.execute(index_sql)
                    logger.info(f"âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ: {index_name}")
                    self.setup_stats["indexes_created"] += 1
                except Exception as e:
                    logger.warning(f"âš ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼ ({index_name}): {e}")
            
            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆpgvectorãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
            if self._check_extension_exists("vector"):
                try:
                    vector_index_sql = """
                        CREATE INDEX IF NOT EXISTS idx_chunks_embedding 
                        ON chunks USING ivfflat (embedding vector_cosine_ops) 
                        WITH (lists = 100);
                    """
                    cursor.execute(vector_index_sql)
                    logger.info("âœ… ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå®Œäº†")
                    self.setup_stats["indexes_created"] += 1
                except Exception as e:
                    logger.warning(f"âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            
            self.db_connection.commit()
            return True
            
        except Exception as e:
            logger.error(f"âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            self.db_connection.rollback()
            return False
        finally:
            if 'cursor' in locals():
                cursor.close()
    
    def _migrate_existing_data(self) -> bool:
        """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œï¼ˆdocument_sources â†’ chunksï¼‰"""
        try:
            cursor = self.db_connection.cursor()
            
            # contentã‚«ãƒ©ãƒ ãŒã‚ã‚‹document_sourcesãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèª
            cursor.execute("""
                SELECT COUNT(*) 
                FROM information_schema.columns 
                WHERE table_name = 'document_sources' 
                AND column_name = 'content'
            """)
            
            has_content_column = cursor.fetchone()[0] > 0
            
            if not has_content_column:
                logger.info("âœ… document_sourcesã«contentã‚«ãƒ©ãƒ ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆç§»è¡Œæ¸ˆã¿ï¼‰")
                return True
            
            # contentãŒã‚ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
            cursor.execute("""
                SELECT id, content, company_id, name
                FROM document_sources 
                WHERE content IS NOT NULL 
                AND content != ''
                AND id NOT IN (SELECT DISTINCT doc_id FROM chunks WHERE doc_id IS NOT NULL)
            """)
            
            documents_to_migrate = cursor.fetchall()
            
            if not documents_to_migrate:
                logger.info("âœ… ç§»è¡Œã™ã¹ããƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“")
                return True
            
            logger.info(f"ğŸ“‹ {len(documents_to_migrate)}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç§»è¡Œã—ã¾ã™")
            
            # å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦ç§»è¡Œ
            from .document_processor import document_processor
            
            migrated_count = 0
            for doc in documents_to_migrate:
                try:
                    # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
                    chunks = document_processor._split_text_into_chunks(
                        doc['content'], 
                        doc['name']
                    )
                    
                    # chunksãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
                    for chunk_data in chunks:
                        insert_sql = """
                            INSERT INTO chunks (doc_id, chunk_index, content, company_id)
                            VALUES (%s, %s, %s, %s)
                        """
                        cursor.execute(insert_sql, (
                            doc['id'],
                            chunk_data['chunk_index'],
                            chunk_data['content'],
                            doc['company_id']
                        ))
                    
                    migrated_count += 1
                    logger.info(f"âœ… ç§»è¡Œå®Œäº†: {doc['name']} ({len(chunks)}ãƒãƒ£ãƒ³ã‚¯)")
                    
                except Exception as e:
                    logger.error(f"âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç§»è¡Œã‚¨ãƒ©ãƒ¼ ({doc['name']}): {e}")
                    continue
            
            self.db_connection.commit()
            self.setup_stats["data_migrated"] = migrated_count
            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ç§»è¡Œå®Œäº†: {migrated_count}ä»¶")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ç§»è¡Œã‚¨ãƒ©ãƒ¼: {e}")
            self.db_connection.rollback()
            self.setup_stats["errors"].append(f"ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ: {str(e)}")
            return False
        finally:
            if 'cursor' in locals():
                cursor.close()
    
    def _verify_system(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ å‹•ä½œç¢ºèª"""
        try:
            cursor = self.db_connection.cursor()
            
            # chunksãƒ†ãƒ¼ãƒ–ãƒ«ã®çµ±è¨ˆæƒ…å ±å–å¾—
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_chunks,
                    COUNT(CASE WHEN embedding IS NOT NULL THEN 1 END) as chunks_with_embedding,
                    COUNT(CASE WHEN active = true THEN 1 END) as active_chunks,
                    COUNT(DISTINCT doc_id) as unique_documents
                FROM chunks
            """)
            
            stats = cursor.fetchone()
            
            logger.info("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ:")
            logger.info(f"  - ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {stats['total_chunks']}")
            logger.info(f"  - embeddingæ¸ˆã¿: {stats['chunks_with_embedding']}")
            logger.info(f"  - ã‚¢ã‚¯ãƒ†ã‚£ãƒ–: {stats['active_chunks']}")
            logger.info(f"  - ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {stats['unique_documents']}")
            
            # document_sourcesãƒ†ãƒ¼ãƒ–ãƒ«ã®çµ±è¨ˆ
            cursor.execute("SELECT COUNT(*) FROM document_sources")
            doc_count = cursor.fetchone()[0]
            logger.info(f"  - document_sources: {doc_count}ä»¶")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ã‚·ã‚¹ãƒ†ãƒ ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            return False
        finally:
            if 'cursor' in locals():
                cursor.close()
    
    def _print_final_report(self):
        """æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º"""
        total_time = self.setup_stats["end_time"] - self.setup_stats["start_time"]
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ  ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š ä½œæˆãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {self.setup_stats['tables_created']}")
        logger.info(f"ğŸ“Š ä½œæˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•°: {self.setup_stats['indexes_created']}")
        logger.info(f"ğŸ“Š ç§»è¡Œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {self.setup_stats['data_migrated']}")
        logger.info(f"â±ï¸ ç·å‡¦ç†æ™‚é–“: {total_time:.1f}ç§’")
        
        if self.setup_stats["errors"]:
            logger.warning("âš ï¸ ã‚¨ãƒ©ãƒ¼ä¸€è¦§:")
            for error in self.setup_stats["errors"]:
                logger.warning(f"  - {error}")
        else:
            logger.info("âœ… ã‚¨ãƒ©ãƒ¼ãªã—")
        
        logger.info("=" * 60)
        logger.info("ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        logger.info("1. python generate_embeddings_enhanced.py ã§embeddingç”Ÿæˆ")
        logger.info("2. APIã‚µãƒ¼ãƒãƒ¼èµ·å‹•ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½åˆ©ç”¨å¯èƒ½")
        logger.info("=" * 60)
    
    async def setup_system(self):
        """ã‚·ã‚¹ãƒ†ãƒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        try:
            self.setup_stats["start_time"] = time.time()
            logger.info("ğŸš€ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ  ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
            self._init_database()
            
            # 1. pgvectoræ‹¡å¼µæ©Ÿèƒ½ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
            logger.info("ğŸ“¦ pgvectoræ‹¡å¼µæ©Ÿèƒ½ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...")
            self._install_pgvector_extension()
            
            # 2. chunksãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
            logger.info("ğŸ—ƒï¸ chunksãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ...")
            if not self._create_chunks_table():
                raise Exception("chunksãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            # 3. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            logger.info("ğŸ“‡ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ...")
            self._create_indexes()
            
            # 4. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ
            logger.info("ğŸ“‹ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ...")
            self._migrate_existing_data()
            
            # 5. ã‚·ã‚¹ãƒ†ãƒ å‹•ä½œç¢ºèª
            logger.info("ğŸ” ã‚·ã‚¹ãƒ†ãƒ å‹•ä½œç¢ºèª...")
            self._verify_system()
            
            self.setup_stats["end_time"] = time.time()
            self._print_final_report()
            
        except Exception as e:
            logger.error(f"âŒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            raise
        
        finally:
            if self.db_connection:
                self.db_connection.close()
                logger.info("ğŸ”’ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã¾ã—ãŸ")

async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    logger.info("ğŸš€ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ  ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹")
    
    try:
        setup = DocumentSystemSetup()
        await setup.setup_system()
        logger.info("ğŸ‰ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
        
    except Exception as e:
        logger.error(f"ğŸ’¥ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
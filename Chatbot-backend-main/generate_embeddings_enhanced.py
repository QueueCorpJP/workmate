"""
ğŸ§  å¼·åŒ–ç‰ˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
chunksãƒ†ãƒ¼ãƒ–ãƒ«å¯¾å¿œãƒ»ãƒãƒƒãƒå‡¦ç†ãƒ»ã‚¨ãƒ©ãƒ¼å›å¾©æ©Ÿèƒ½ä»˜ã

æ©Ÿèƒ½:
- chunksãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰embeddingæœªç”Ÿæˆã®ãƒãƒ£ãƒ³ã‚¯ã‚’æ¤œç´¢
- Gemini Flash Embedding APIï¼ˆ768æ¬¡å…ƒï¼‰ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–
- ãƒãƒƒãƒå‡¦ç†ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š
- ã‚¨ãƒ©ãƒ¼å›å¾©ãƒ»ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½
- é€²æ—è¡¨ç¤ºãƒ»çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆ
"""

import os
import sys
import time
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging
from dotenv import load_dotenv
import google.generativeai as genai
import psycopg2
from psycopg2.extras import execute_values, RealDictCursor

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('embedding_generation.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

class EnhancedEmbeddingGenerator:
    """å¼·åŒ–ç‰ˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.gemini_client = None
        self.db_connection = None
        self.embedding_model = "gemini-embedding-exp-03-07"
        self.batch_size = 10  # ãƒãƒƒãƒã‚µã‚¤ã‚º
        self.max_retries = 3  # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
        self.retry_delay = 2  # ãƒªãƒˆãƒ©ã‚¤é–“éš”ï¼ˆç§’ï¼‰
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            "total_chunks": 0,
            "processed_chunks": 0,
            "successful_embeddings": 0,
            "failed_embeddings": 0,
            "skipped_chunks": 0,
            "start_time": None,
            "end_time": None
        }
    
    def _get_env_vars(self) -> Dict[str, str]:
        """ç’°å¢ƒå¤‰æ•°ã‚’å–å¾—ãƒ»æ¤œè¨¼"""
        api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY ã¾ãŸã¯ GEMINI_API_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # Supabaseæ¥ç¶šæƒ…å ±
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # PostgreSQLæ¥ç¶šURLæ§‹ç¯‰
        if "supabase.co" in supabase_url:
            project_id = supabase_url.split("://")[1].split(".")[0]
            db_url = f"postgresql://postgres.{project_id}:{os.getenv('DB_PASSWORD', '')}@aws-0-ap-northeast-1.pooler.supabase.com:6543/postgres"
        else:
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                raise ValueError("DATABASE_URL ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        return {
            "api_key": api_key,
            "db_url": db_url,
            "model": os.getenv("EMBEDDING_MODEL", self.embedding_model)
        }
    
    def _init_gemini_client(self, api_key: str):
        """Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–"""
        try:
            genai.configure(api_key=api_key)
            self.gemini_client = genai
            logger.info(f"ğŸ§  Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†: {self.embedding_model}")
        except Exception as e:
            logger.error(f"âŒ Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _init_database(self, db_url: str):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šåˆæœŸåŒ–"""
        try:
            self.db_connection = psycopg2.connect(db_url, cursor_factory=RealDictCursor)
            logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå®Œäº†")
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _get_pending_chunks(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """embeddingæœªç”Ÿæˆã®ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—"""
        try:
            cursor = self.db_connection.cursor()
            
            query = """
                SELECT 
                    c.id,
                    c.doc_id,
                    c.chunk_index,
                    c.content,
                    c.company_id,
                    ds.name as document_name
                FROM chunks c
                LEFT JOIN document_sources ds ON c.doc_id = ds.id
                WHERE c.content IS NOT NULL
                  AND c.content != ''
                  AND c.embedding IS NULL
                ORDER BY c.created_at ASC
            """
            
            if limit:
                query += f" LIMIT {limit}"
            
            cursor.execute(query)
            chunks = cursor.fetchall()
            
            logger.info(f"ğŸ“‹ embeddingæœªç”Ÿæˆãƒãƒ£ãƒ³ã‚¯: {len(chunks)}ä»¶")
            return [dict(chunk) for chunk in chunks]
            
        except Exception as e:
            logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            raise
        finally:
            if 'cursor' in locals():
                cursor.close()
    
    async def _generate_embedding_with_retry(self, text: str, chunk_id: str) -> Optional[List[float]]:
        """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãembeddingç”Ÿæˆ"""
        for attempt in range(self.max_retries):
            try:
                if not text or not text.strip():
                    logger.warning(f"âš ï¸ ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆ: {chunk_id}")
                    return None
                
                response = self.gemini_client.embed_content(
                    model=self.embedding_model,
                    content=text.strip()
                )
                
                if response and 'embedding' in response:
                    embedding_vector = response['embedding']
                    logger.debug(f"âœ… embeddingç”ŸæˆæˆåŠŸ: {chunk_id} (æ¬¡å…ƒ: {len(embedding_vector)})")
                    return embedding_vector
                else:
                    logger.warning(f"âš ï¸ embeddingç”Ÿæˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ç©º: {chunk_id}")
                    return None
                    
            except Exception as e:
                logger.warning(f"âš ï¸ embeddingç”Ÿæˆã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}/{self.max_retries}): {chunk_id} - {e}")
                
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(self.retry_delay * (attempt + 1))  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                else:
                    logger.error(f"âŒ embeddingç”Ÿæˆæœ€çµ‚å¤±æ•—: {chunk_id}")
                    return None
    
    def _update_chunk_embedding(self, chunk_id: str, embedding: List[float]) -> bool:
        """ãƒãƒ£ãƒ³ã‚¯ã®embeddingã‚’æ›´æ–°"""
        try:
            cursor = self.db_connection.cursor()
            
            update_query = """
                UPDATE chunks 
                SET embedding = %s, updated_at = CURRENT_TIMESTAMP
                WHERE id = %s
            """
            
            cursor.execute(update_query, (embedding, chunk_id))
            
            if cursor.rowcount > 0:
                logger.debug(f"âœ… embeddingæ›´æ–°æˆåŠŸ: {chunk_id}")
                return True
            else:
                logger.warning(f"âš ï¸ embeddingæ›´æ–°å¤±æ•—ï¼ˆè¡ŒãŒè¦‹ã¤ã‹ã‚‰ãªã„ï¼‰: {chunk_id}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ embeddingæ›´æ–°ã‚¨ãƒ©ãƒ¼: {chunk_id} - {e}")
            return False
        finally:
            if 'cursor' in locals():
                cursor.close()
    
    async def _process_chunk_batch(self, chunks: List[Dict[str, Any]]) -> Dict[str, int]:
        """ãƒãƒ£ãƒ³ã‚¯ãƒãƒƒãƒã‚’å‡¦ç†"""
        batch_stats = {"success": 0, "failed": 0, "skipped": 0}
        
        # ä¸¦è¡Œå‡¦ç†ã§embeddingç”Ÿæˆ
        embedding_tasks = []
        for chunk in chunks:
            task = self._generate_embedding_with_retry(chunk["content"], chunk["id"])
            embedding_tasks.append((chunk, task))
        
        # å…¨ã¦ã®embeddingç”Ÿæˆã‚’ä¸¦è¡Œå®Ÿè¡Œ
        for chunk, task in embedding_tasks:
            try:
                embedding = await task
                
                if embedding:
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
                    if self._update_chunk_embedding(chunk["id"], embedding):
                        batch_stats["success"] += 1
                        self.stats["successful_embeddings"] += 1
                    else:
                        batch_stats["failed"] += 1
                        self.stats["failed_embeddings"] += 1
                else:
                    batch_stats["skipped"] += 1
                    self.stats["skipped_chunks"] += 1
                
                self.stats["processed_chunks"] += 1
                
            except Exception as e:
                logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {chunk['id']} - {e}")
                batch_stats["failed"] += 1
                self.stats["failed_embeddings"] += 1
        
        return batch_stats
    
    def _print_progress(self, current: int, total: int, batch_stats: Dict[str, int]):
        """é€²æ—è¡¨ç¤º"""
        progress_percent = (current / total * 100) if total > 0 else 0
        elapsed_time = time.time() - self.stats["start_time"]
        
        logger.info(f"ğŸ“Š é€²æ—: {current}/{total} ({progress_percent:.1f}%) | "
                   f"æˆåŠŸ: {batch_stats['success']} | "
                   f"å¤±æ•—: {batch_stats['failed']} | "
                   f"ã‚¹ã‚­ãƒƒãƒ—: {batch_stats['skipped']} | "
                   f"çµŒéæ™‚é–“: {elapsed_time:.1f}ç§’")
    
    def _print_final_stats(self):
        """æœ€çµ‚çµ±è¨ˆè¡¨ç¤º"""
        total_time = self.stats["end_time"] - self.stats["start_time"]
        success_rate = (self.stats["successful_embeddings"] / self.stats["total_chunks"] * 100) if self.stats["total_chunks"] > 0 else 0
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆå®Œäº† - æœ€çµ‚çµ±è¨ˆ")
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {self.stats['total_chunks']}")
        logger.info(f"âœ… æˆåŠŸ: {self.stats['successful_embeddings']}")
        logger.info(f"âŒ å¤±æ•—: {self.stats['failed_embeddings']}")
        logger.info(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {self.stats['skipped_chunks']}")
        logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
        logger.info(f"â±ï¸ ç·å‡¦ç†æ™‚é–“: {total_time:.1f}ç§’")
        logger.info(f"âš¡ å¹³å‡å‡¦ç†é€Ÿåº¦: {self.stats['total_chunks'] / total_time:.2f} ãƒãƒ£ãƒ³ã‚¯/ç§’")
        logger.info("=" * 60)
    
    async def generate_embeddings(self, limit: Optional[int] = None):
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼šembeddingç”Ÿæˆ"""
        try:
            self.stats["start_time"] = time.time()
            
            # ç’°å¢ƒå¤‰æ•°å–å¾—
            env_vars = self._get_env_vars()
            
            # åˆæœŸåŒ–
            self._init_gemini_client(env_vars["api_key"])
            self._init_database(env_vars["db_url"])
            
            # å‡¦ç†å¯¾è±¡ãƒãƒ£ãƒ³ã‚¯å–å¾—
            pending_chunks = self._get_pending_chunks(limit)
            
            if not pending_chunks:
                logger.info("âœ… å‡¦ç†ã™ã¹ããƒãƒ£ãƒ³ã‚¯ã¯ã‚ã‚Šã¾ã›ã‚“")
                return
            
            self.stats["total_chunks"] = len(pending_chunks)
            logger.info(f"ğŸš€ embeddingç”Ÿæˆé–‹å§‹: {self.stats['total_chunks']}ãƒãƒ£ãƒ³ã‚¯")
            
            # ãƒãƒƒãƒå‡¦ç†
            for i in range(0, len(pending_chunks), self.batch_size):
                batch = pending_chunks[i:i + self.batch_size]
                batch_num = (i // self.batch_size) + 1
                total_batches = (len(pending_chunks) + self.batch_size - 1) // self.batch_size
                
                logger.info(f"ğŸ“¦ ãƒãƒƒãƒ {batch_num}/{total_batches} å‡¦ç†é–‹å§‹ ({len(batch)}ãƒãƒ£ãƒ³ã‚¯)")
                
                # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
                batch_stats = await self._process_chunk_batch(batch)
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚³ãƒŸãƒƒãƒˆ
                self.db_connection.commit()
                
                # é€²æ—è¡¨ç¤º
                self._print_progress(i + len(batch), len(pending_chunks), batch_stats)
                
                # ãƒãƒƒãƒé–“ã®å¾…æ©Ÿï¼ˆAPIåˆ¶é™å¯¾ç­–ï¼‰
                if i + self.batch_size < len(pending_chunks):
                    await asyncio.sleep(1)
            
            self.stats["end_time"] = time.time()
            self._print_final_stats()
            
        except Exception as e:
            logger.error(f"âŒ ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            if self.db_connection:
                self.db_connection.rollback()
            raise
        
        finally:
            # ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if self.db_connection:
                self.db_connection.close()
                logger.info("ğŸ”’ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã¾ã—ãŸ")

async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    logger.info("ğŸš€ å¼·åŒ–ç‰ˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹")
    
    try:
        generator = EnhancedEmbeddingGenerator()
        
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§åˆ¶é™æ•°ã‚’æŒ‡å®šå¯èƒ½
        limit = None
        if len(sys.argv) > 1:
            try:
                limit = int(sys.argv[1])
                logger.info(f"ğŸ“‹ å‡¦ç†åˆ¶é™: {limit}ãƒãƒ£ãƒ³ã‚¯")
            except ValueError:
                logger.warning("âš ï¸ ç„¡åŠ¹ãªåˆ¶é™æ•°ãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸã€‚å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†ã—ã¾ã™ã€‚")
        
        await generator.generate_embeddings(limit)
        logger.info("ğŸ‰ ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œå®Œäº†")
        
    except Exception as e:
        logger.error(f"ğŸ’¥ ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
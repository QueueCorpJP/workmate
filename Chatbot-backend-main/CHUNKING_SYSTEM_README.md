# チャンク化システム実装ガイド

## 概要

知識ベースが大きくなりすぎてAPI制限に引っかかる問題を解決するため、50万文字ごとにチャンク化して段階的に処理するシステムを実装しました。

## 主な機能

### 1. 自動チャンク化
- **チャンクサイズ**: 50万文字（設定可能）
- **境界調整**: 文の途中で切れないよう、改行やスペースで境界を調整
- **効率的処理**: 必要な部分のみを段階的に処理

### 2. 段階的API呼び出し
- 各チャンクを順次処理
- 関連情報がないチャンクは自動スキップ
- APIレート制限回避のため1秒間隔で処理

### 3. インテリジェント応答統合
- 複数チャンクからの回答を自動統合
- 重複除去と構造化表示
- 処理結果の詳細レポート

## 実装詳細

### コア関数

#### `chunk_knowledge_base(text: str, chunk_size: int = 500000) -> list[str]`
知識ベースを指定サイズでチャンク化

```python
def chunk_knowledge_base(text: str, chunk_size: int = 500000) -> list[str]:
    """
    知識ベースを指定されたサイズでチャンク化する
    
    Args:
        text: チャンク化するテキスト
        chunk_size: チャンクのサイズ（文字数）
    
    Returns:
        チャンク化されたテキストのリスト
    """
```

**特徴**:
- 改行優先で境界を調整
- 改行がない場合はスペースで調整
- 空のチャンクは自動除外

#### `process_chat_chunked(message: ChatMessage, db, current_user) -> dict`
チャンク化システムを使用したチャット処理

```python
async def process_chat_chunked(message: ChatMessage, db: Connection = Depends(get_db), current_user: dict = None):
    """
    チャンク化システムを使用したチャット処理
    知識ベースを50万文字ごとにチャンク化して段階的に処理
    """
```

**処理フロー**:
1. 利用制限チェック
2. アクティブリソース取得
3. 知識ベースチャンク化
4. 各チャンク順次処理
5. 応答統合
6. 結果返却

### APIエンドポイント

#### メインチャットエンドポイント
```
POST /chatbot/api/chat
```
- 従来のエンドポイントをチャンク化対応に更新
- レスポンス形式は既存と互換性維持

#### デバッグ用エンドポイント
```
POST /chatbot/api/chat-chunked-info
```
- チャンク化処理の詳細情報を取得
- 処理チャンク数、成功チャンク数等を返却

## 使用方法

### 1. 基本的な使用
```python
# 既存のチャット処理と同じインターフェース
message = ChatMessage(
    message="質問内容",
    user_id="user-id",
    employee_id="employee-id",
    employee_name="ユーザー名"
)

result = await process_chat_chunked(message, db, current_user)
```

### 2. テスト実行
```bash
# チャンク化関数の基本テスト
cd Chatbot-backend-main
python test_chunking_system.py

# 実際のチャット処理テスト（データベース接続必要）
python -c "import asyncio; from test_chunking_system import test_chunked_chat; asyncio.run(test_chunked_chat())"
```

## パフォーマンス特性

### 処理時間
- **小さなデータ** (50万文字以下): 従来と同等（3-5秒）
- **大きなデータ** (50万文字以上): チャンク数 × (3-5秒 + 1秒待機)

### メモリ使用量
- **従来**: 全データをメモリに保持
- **チャンク化**: 1チャンク分のみメモリに保持（大幅削減）

### API呼び出し数
- **従来**: 1回の大きなリクエスト
- **チャンク化**: チャンク数分のリクエスト（但し各々は制限内）

## 設定オプション

### チャンクサイズの調整
```python
# デフォルト: 50万文字
CHUNK_SIZE = 500000

# 小さくする場合（より細かく分割）
CHUNK_SIZE = 300000

# 大きくする場合（分割数を減らす）
CHUNK_SIZE = 700000
```

### API待機時間の調整
```python
# デフォルト: 1秒
await asyncio.sleep(1)

# より短い間隔（レート制限に注意）
await asyncio.sleep(0.5)

# より長い間隔（安全重視）
await asyncio.sleep(2)
```

## エラーハンドリング

### チャンク処理エラー
- 個別チャンクのエラーは処理を継続
- 成功したチャンクの結果のみを使用
- エラー詳細はログに記録

### API制限エラー
- 1秒待機でレート制限を回避
- 必要に応じて待機時間を調整可能

### メモリエラー
- チャンク化により大幅にメモリ使用量を削減
- 大容量データでも安定動作

## 監視とデバッグ

### ログ出力
```
🔄 チャンク化チャット処理開始 - ユーザーID: user-123
📚 知識ベース取得開始 - アクティブソース: 5個
📊 取得した知識ベース: 1,200,000 文字
🔪 チャンク化完了: 3個のチャンク
🔄 チャンク 1/3 処理開始 (500,000 文字)
✅ チャンク 1 処理成功
🔄 チャンク 2/3 処理開始 (500,000 文字)
✅ チャンク 2 処理成功
🔄 チャンク 3/3 処理開始 (200,000 文字)
ℹ️ チャンク 3 に該当情報なし
✅ チャンク化処理完了 - 成功チャンク: 2/3
```

### レスポンス情報
```json
{
  "response": "統合された回答内容",
  "chunks_processed": 3,
  "successful_chunks": 2,
  "remaining_questions": 8,
  "limit_reached": false
}
```

## 今後の拡張予定

### RAGシステム統合
- ベクトル検索による関連チャンクの自動選択
- 不要なチャンクの事前除外
- さらなる効率化

### 並列処理
- 複数チャンクの同時処理
- 処理時間の大幅短縮
- リソース使用量の最適化

### キャッシュシステム
- 処理済みチャンクの結果キャッシュ
- 同様の質問に対する高速応答
- ストレージ効率化

## トラブルシューティング

### よくある問題

#### 1. "チャンクに該当情報なし" が多発
**原因**: チャンクサイズが小さすぎる
**解決**: `CHUNK_SIZE` を大きくする（例: 700000文字）

#### 2. API制限エラー
**原因**: 待機時間が短すぎる
**解決**: `asyncio.sleep()` の時間を長くする

#### 3. 処理時間が長すぎる
**原因**: チャンク数が多すぎる
**解決**: チャンクサイズを大きくするか、事前フィルタリングを実装

#### 4. メモリ不足
**原因**: 大きなチャンクの同時処理
**解決**: チャンクサイズを小さくする

### デバッグ手順

1. **ログ確認**: 処理の各段階でのログを確認
2. **チャンク情報確認**: `/chatbot/api/chat-chunked-info` エンドポイントを使用
3. **テスト実行**: `test_chunking_system.py` でローカルテスト
4. **段階的確認**: 小さなデータから段階的にテスト

## まとめ

チャンク化システムにより以下を実現：

✅ **API制限回避**: 50万文字ごとの分割処理
✅ **メモリ効率化**: 大幅なメモリ使用量削減  
✅ **安定性向上**: エラー耐性と継続処理
✅ **スケーラビリティ**: 大容量データ対応
✅ **互換性維持**: 既存APIとの完全互換

大量の知識ベースを持つ企業でも、安定してAIチャットサービスをご利用いただけます。 
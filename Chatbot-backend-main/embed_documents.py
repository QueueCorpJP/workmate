"""
ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆãƒ»ç™»éŒ²ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Gemini Embedding APIã‚’ä½¿ç”¨ã—ã¦document_sourcesã‹ã‚‰document_embeddingsã«ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ ¼ç´ã™ã‚‹
"""

import os
import sys
import textwrap
from dotenv import load_dotenv
from google import genai
import psycopg2
from psycopg2.extras import execute_values
import logging

# ãƒ­ã‚®ãƒ³ã‚°ã®è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

def get_env_vars():
    """ç’°å¢ƒå¤‰æ•°ã‚’å–å¾—ã—ã¦æ¤œè¨¼ã™ã‚‹"""
    api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("GOOGLE_API_KEY ã¾ãŸã¯ GEMINI_API_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    # Supabaseæ¥ç¶šæƒ…å ±ã‚’æ§‹ç¯‰
    supabase_url = os.getenv("SUPABASE_URL")
    supabase_key = os.getenv("SUPABASE_KEY")
    
    if not supabase_url or not supabase_key:
        raise ValueError("SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    # PostgreSQLæ¥ç¶šURLã‚’æ§‹ç¯‰ï¼ˆSupabaseç”¨ï¼‰
    # Supabase URLã‹ã‚‰æ¥ç¶šæƒ…å ±ã‚’æŠ½å‡º
    if "supabase.co" in supabase_url:
        project_id = supabase_url.split("://")[1].split(".")[0]
        db_url = f"postgresql://postgres.{project_id}:{os.getenv('DB_PASSWORD', '')}@aws-0-ap-northeast-1.pooler.supabase.com:6543/postgres"
    else:
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã®å ´åˆ
        db_url = os.getenv("DATABASE_URL")
        if not db_url:
            raise ValueError("DATABASE_URL ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    model = os.getenv("EMBEDDING_MODEL", "gemini-embedding-exp-03-07")
    
    return api_key, db_url, model

def chunks(text, chunk_size=8000):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã™ã‚‹ï¼ˆç´„2000ãƒˆãƒ¼ã‚¯ãƒ³ç›¸å½“ï¼‰"""
    text = str(text) if text else ""
    for i in range(0, len(text), chunk_size):
        yield text[i:i+chunk_size]

def generate_embeddings():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼šã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç”Ÿæˆã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
    try:
        # ç’°å¢ƒå¤‰æ•°ã®å–å¾—
        api_key, db_url, model = get_env_vars()
        
        # Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        logger.info(f"Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ä¸­... ãƒ¢ãƒ‡ãƒ«: {model}")
        client = genai.Client(api_key=api_key)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
        logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶šä¸­...")
        conn = psycopg2.connect(db_url)
        cur = conn.cursor()
        
        # ã¾ã åŸ‹ã‚è¾¼ã¿ãŒç„¡ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
        logger.info("åŸ‹ã‚è¾¼ã¿ãŒæœªç”Ÿæˆã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢ä¸­...")
        cur.execute("""
            SELECT id, content, name
            FROM document_sources
            WHERE active = true
              AND content IS NOT NULL
              AND content != ''
              AND id NOT IN (
                  SELECT DISTINCT document_id 
                  FROM document_embeddings 
                  WHERE document_id IS NOT NULL
              );
        """)
        rows = cur.fetchall()
        
        if not rows:
            logger.info("âœ… æ–°ã—ãå‡¦ç†ã™ã¹ããƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“")
            return
        
        logger.info(f"ğŸ“„ {len(rows)}å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã—ã¾ã™")
        
        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆãƒ»ä¿å­˜
        records = []
        processed_count = 0
        
        for doc_id, content, name in rows:
            logger.info(f"ğŸ“‹ å‡¦ç†ä¸­: {name} (ID: {doc_id})")
            
            try:
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
                chunk_list = list(chunks(content, chunk_size=8000))
                logger.info(f"  - {len(chunk_list)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²")
                
                for i, chunk_content in enumerate(chunk_list):
                    if not chunk_content.strip():
                        continue
                    
                    # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
                    logger.info(f"  - ãƒãƒ£ãƒ³ã‚¯ {i+1}/{len(chunk_list)} ã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆä¸­...")
                    
                    try:
                        response = client.models.embed_content(
                            model=model, 
                            contents=chunk_content
                        )
                        
                        if response.embeddings and len(response.embeddings) > 0:
                            # 3072æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
                            full_embedding = response.embeddings[0].values
                            # MRLï¼ˆæ¬¡å…ƒå‰Šæ¸›ï¼‰: 3072 â†’ 1536æ¬¡å…ƒã«å‰Šæ¸›
                            embedding_vector = full_embedding[:1536]
                            snippet = chunk_content[:200] + "..." if len(chunk_content) > 200 else chunk_content
                            
                            # ãƒãƒ£ãƒ³ã‚¯ã®å ´åˆã¯ä¸€æ„ãªIDã‚’ç”Ÿæˆï¼ˆdocument_idã¨ã—ã¦ä½¿ç”¨ï¼‰
                            chunk_doc_id = f"{doc_id}_chunk_{i}" if len(chunk_list) > 1 else doc_id
                            
                            records.append((chunk_doc_id, embedding_vector, snippet))
                            logger.info(f"  - âœ… ãƒãƒ£ãƒ³ã‚¯ {i+1} å®Œäº† (æ¬¡å…ƒ: {len(embedding_vector)})")
                        else:
                            logger.warning(f"  - âš ï¸ ãƒãƒ£ãƒ³ã‚¯ {i+1} ã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã«å¤±æ•—")
                    
                    except Exception as e:
                        logger.error(f"  - âŒ ãƒãƒ£ãƒ³ã‚¯ {i+1} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                
                processed_count += 1
                logger.info(f"ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå®Œäº†: {name} ({processed_count}/{len(rows)})")
                
            except Exception as e:
                logger.error(f"âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ {name} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¸€æ‹¬æŒ¿å…¥
        if records:
            logger.info(f"ğŸ’¾ {len(records)}å€‹ã®åŸ‹ã‚è¾¼ã¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ä¸­...")
            
            # å®Ÿéš›ã®ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´
            # document_embeddings (document_id, embedding, snippet)
            execute_values(cur, """
                INSERT INTO document_embeddings (document_id, embedding, snippet)
                VALUES %s
                ON CONFLICT (document_id) DO UPDATE
                SET embedding = EXCLUDED.embedding,
                    snippet = EXCLUDED.snippet,
                    created_at = CURRENT_TIMESTAMP;
            """, records, template=None, page_size=100)
            
            conn.commit()
            logger.info(f"âœ… {len(records)}å€‹ã®åŸ‹ã‚è¾¼ã¿ã‚’æ­£å¸¸ã«ä¿å­˜ã—ã¾ã—ãŸ")
        else:
            logger.warning("âš ï¸ ä¿å­˜ã™ã¹ãåŸ‹ã‚è¾¼ã¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
    except Exception as e:
        logger.error(f"âŒ ãƒ¡ã‚¤ãƒ³å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
        if 'conn' in locals():
            conn.rollback()
        raise
    
    finally:
        # ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if 'cur' in locals():
            cur.close()
        if 'conn' in locals():
            conn.close()
        logger.info("ğŸ”’ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã¾ã—ãŸ")

if __name__ == "__main__":
    logger.info("ğŸš€ ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹")
    try:
        generate_embeddings()
        logger.info("ğŸ‰ ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆå®Œäº†")
    except Exception as e:
        logger.error(f"ğŸ’¥ ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1) 
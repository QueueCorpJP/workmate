#!/usr/bin/env python3
"""
ãƒãƒƒãƒæŒ¿å…¥ä¿®æ­£ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
50å€‹å˜ä½ã§ã®ãƒãƒ£ãƒ³ã‚¯+embeddingä¿å­˜ã‚’ãƒ†ã‚¹ãƒˆ
"""

import asyncio
import logging
import sys
import os
from datetime import datetime
from typing import List, Dict, Any

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from modules.document_processor import DocumentProcessor

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def test_batch_insert():
    """ãƒãƒƒãƒæŒ¿å…¥ã®ãƒ†ã‚¹ãƒˆ"""
    try:
        # DocumentProcessorã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
        processor = DocumentProcessor()
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆ120å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã§50å€‹å˜ä½ã®ãƒãƒƒãƒå‡¦ç†ã‚’ãƒ†ã‚¹ãƒˆï¼‰
        test_chunks = []
        for i in range(120):
            test_chunks.append({
                "chunk_index": i,
                "content": f"ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆç”¨ã®ãƒãƒ£ãƒ³ã‚¯å†…å®¹ã§ã™ã€‚ãƒãƒ£ãƒ³ã‚¯ç•ªå·: {i}ã€‚" +
                          "ã“ã®ãƒãƒ£ãƒ³ã‚¯ã«ã¯ååˆ†ãªé•·ã•ã®ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã¦ãŠã‚Šã€" +
                          ("embeddingã®ç”Ÿæˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚" * 3)
            })
        
        # ãƒ†ã‚¹ãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        doc_id = f"test_doc_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        company_id = "test_company_001"
        doc_name = "ãƒãƒƒãƒæŒ¿å…¥ãƒ†ã‚¹ãƒˆæ–‡æ›¸"
        
        logger.info(f"ğŸš€ ãƒãƒƒãƒæŒ¿å…¥ãƒ†ã‚¹ãƒˆé–‹å§‹")
        logger.info(f"ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯")
        logger.info(f"ğŸ“¦ ãƒãƒƒãƒã‚µã‚¤ã‚º: 50å€‹")
        logger.info(f"ğŸ¯ äºˆæƒ³ãƒãƒƒãƒæ•°: {(len(test_chunks) + 49) // 50}")
        
        # ãƒãƒƒãƒä¿å­˜ã®å®Ÿè¡Œ
        start_time = datetime.now()
        result = await processor._save_chunks_to_database(
            chunks=test_chunks,
            doc_id=doc_id,
            company_id=company_id,
            doc_name=doc_name
        )
        end_time = datetime.now()
        
        # çµæœã®è¡¨ç¤º
        processing_time = (end_time - start_time).total_seconds()
        logger.info(f"â±ï¸ å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
        logger.info(f"ğŸ“ˆ å‡¦ç†çµæœ:")
        logger.info(f"  - ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {result['total_chunks']}")
        logger.info(f"  - ä¿å­˜æˆåŠŸ: {result['saved_chunks']}")
        logger.info(f"  - embeddingæˆåŠŸ: {result['successful_embeddings']}")
        logger.info(f"  - embeddingå¤±æ•—: {result['failed_embeddings']}")
        logger.info(f"  - å†è©¦è¡Œå›æ•°: {result['retry_attempts']}")
        
        # æˆåŠŸç‡ã®è¨ˆç®—
        if result['total_chunks'] > 0:
            save_rate = (result['saved_chunks'] / result['total_chunks']) * 100
            embedding_rate = (result['successful_embeddings'] / result['total_chunks']) * 100
            logger.info(f"ğŸ“Š æˆåŠŸç‡:")
            logger.info(f"  - ä¿å­˜æˆåŠŸç‡: {save_rate:.1f}%")
            logger.info(f"  - embeddingæˆåŠŸç‡: {embedding_rate:.1f}%")
        
        # ãƒ†ã‚¹ãƒˆçµæœã®åˆ¤å®š
        if result['saved_chunks'] == result['total_chunks']:
            logger.info("ğŸ‰ ãƒ†ã‚¹ãƒˆæˆåŠŸ: å…¨ã¦ã®ãƒãƒ£ãƒ³ã‚¯ãŒæ­£å¸¸ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
            return True
        else:
            logger.warning(f"âš ï¸ ãƒ†ã‚¹ãƒˆéƒ¨åˆ†æˆåŠŸ: {result['saved_chunks']}/{result['total_chunks']} ãƒãƒ£ãƒ³ã‚¯ãŒä¿å­˜ã•ã‚Œã¾ã—ãŸ")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
        return False

async def test_small_batch():
    """å°ã•ãªãƒãƒƒãƒã§ã®ãƒ†ã‚¹ãƒˆï¼ˆ10å€‹ï¼‰"""
    try:
        processor = DocumentProcessor()
        
        # å°ã•ãªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        test_chunks = []
        for i in range(10):
            test_chunks.append({
                "chunk_index": i,
                "content": f"å°ã•ãªãƒãƒƒãƒãƒ†ã‚¹ãƒˆç”¨ãƒãƒ£ãƒ³ã‚¯ {i}: " + "ãƒ†ã‚¹ãƒˆå†…å®¹ " * 10
            })
        
        doc_id = f"small_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        company_id = "test_company_002"
        doc_name = "å°ãƒãƒƒãƒãƒ†ã‚¹ãƒˆ"
        
        logger.info(f"ğŸ”¬ å°ãƒãƒƒãƒãƒ†ã‚¹ãƒˆé–‹å§‹: {len(test_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯")
        
        result = await processor._save_chunks_to_database(
            chunks=test_chunks,
            doc_id=doc_id,
            company_id=company_id,
            doc_name=doc_name
        )
        
        logger.info(f"âœ… å°ãƒãƒƒãƒãƒ†ã‚¹ãƒˆçµæœ: {result['saved_chunks']}/{result['total_chunks']} ä¿å­˜æˆåŠŸ")
        return result['saved_chunks'] == result['total_chunks']
        
    except Exception as e:
        logger.error(f"âŒ å°ãƒãƒƒãƒãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return False

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("=" * 60)
    logger.info("ğŸ§ª ãƒãƒƒãƒæŒ¿å…¥ä¿®æ­£ãƒ†ã‚¹ãƒˆé–‹å§‹")
    logger.info("=" * 60)
    
    # å°ãƒãƒƒãƒãƒ†ã‚¹ãƒˆ
    logger.info("\n" + "=" * 40)
    logger.info("1ï¸âƒ£ å°ãƒãƒƒãƒãƒ†ã‚¹ãƒˆï¼ˆ10å€‹ï¼‰")
    logger.info("=" * 40)
    small_test_result = await test_small_batch()
    
    # å¤§ãƒãƒƒãƒãƒ†ã‚¹ãƒˆ
    logger.info("\n" + "=" * 40)
    logger.info("2ï¸âƒ£ å¤§ãƒãƒƒãƒãƒ†ã‚¹ãƒˆï¼ˆ120å€‹ â†’ 50å€‹å˜ä½ï¼‰")
    logger.info("=" * 40)
    large_test_result = await test_batch_insert()
    
    # æœ€çµ‚çµæœ
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    logger.info("=" * 60)
    logger.info(f"å°ãƒãƒƒãƒãƒ†ã‚¹ãƒˆ: {'âœ… æˆåŠŸ' if small_test_result else 'âŒ å¤±æ•—'}")
    logger.info(f"å¤§ãƒãƒƒãƒãƒ†ã‚¹ãƒˆ: {'âœ… æˆåŠŸ' if large_test_result else 'âŒ å¤±æ•—'}")
    
    if small_test_result and large_test_result:
        logger.info("ğŸ‰ å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸï¼ãƒãƒƒãƒæŒ¿å…¥ä¿®æ­£ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™")
    else:
        logger.warning("âš ï¸ ä¸€éƒ¨ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„")

if __name__ == "__main__":
    asyncio.run(main())
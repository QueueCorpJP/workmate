"""
ğŸ“Š Excel ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»æ§‹é€ åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
XLSãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œã€ç©ºç™½è¡Œãƒ»ç©ºç™½åˆ—é™¤å»ã€æ–‡å­—åŒ–ã‘ãƒ»è¨˜å·é™¤å»ã‚’å¼·åŒ–
ãƒ‡ãƒ¼ã‚¿æå¤±ã‚’æœ€å°é™ã«æŠ‘ãˆã¤ã¤ã€ä¸è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’é©åˆ‡ã«é™¤å»
"""

import pandas as pd
import numpy as np
import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from io import BytesIO
import unicodedata
import xlrd  # XLSãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œ

logger = logging.getLogger(__name__)

class ExcelDataCleanerEnhanced:
    """Excelãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ§‹é€ åŒ–ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    
    def __init__(self):
        self.max_cell_length = 3000     # ã‚»ãƒ«å†…å®¹ã®æœ€å¤§æ–‡å­—æ•°
        
        # é™¤å¤–å¯¾è±¡ã®ç„¡æ„å‘³ãªå€¤
        self.meaningless_values = {
            'nan', 'NaN', 'null', 'NULL', 'None', 'NONE', '',
            '#N/A', '#VALUE!', '#REF!', '#DIV/0!', '#NAME?', '#NUM!', '#NULL!',
            'naan', 'naaN', 'NAAN', 'NaT'
        }
        
        # é™¤å»å¯¾è±¡ã®è¨˜å·ãƒ»æ–‡å­—åŒ–ã‘æ–‡å­—
        self.unwanted_symbols = {
            'â—¯', 'â–³', 'Ã—', 'â—‹', 'â—', 'â–²', 'â– ', 'â–¡', 'â˜…', 'â˜†',
            'â€»', 'ï¼Š', 'â™ª', 'â™«', 'â™¬', 'â™­', 'â™¯', 'â™®',
            'â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', 'â‘¥', 'â‘¦', 'â‘§', 'â‘¨', 'â‘©',
            'ãŠ¤', 'ãŠ¥', 'ãŠ¦', 'ãŠ§', 'ãŠ¨', 'ãŠ™', 'ãŠš', 'ãŠ›', 'ãŠœ', 'ãŠ',
            'ã€’', 'ã€“', 'ã€”', 'ã€•', 'ã€–', 'ã€—', 'ã€˜', 'ã€™', 'ã€š', 'ã€›'
        }
        
        # ä¿æŒã™ã¹ãé‡è¦ãªè¨˜å·
        self.important_symbols = {
            '@', '#', '$', '%', '&', '*', '+', '-', '=', '/', '\\',
            '(', ')', '[', ']', '{', '}', '<', '>', '|', '~', '^',
            '!', '?', '.', ',', ';', ':', '"', "'", '`'
        }
        
    def clean_excel_data(self, content: bytes) -> str:
        """
        Excelãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
        XLS/XLSXä¸¡å¯¾å¿œã€æ–‡å­—åŒ–ã‘ãƒ»è¨˜å·é™¤å»å¼·åŒ–ç‰ˆ
        """
        try:
            # XLSãƒ•ã‚¡ã‚¤ãƒ«ã‹XLSXãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚’åˆ¤å®š
            if self._is_xls_file(content):
                logger.info("ğŸ“Š XLSãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡¦ç†é–‹å§‹")
                return self._process_xls_file(content)
            else:
                logger.info("ğŸ“Š XLSXãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡¦ç†é–‹å§‹")
                return self._process_xlsx_file(content)
                
        except Exception as e:
            logger.error(f"âŒ Excelå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
            try:
                logger.info("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†é–‹å§‹")
                return self._process_xlsx_file(content)
            except Exception as fallback_error:
                logger.error(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚‚å¤±æ•—: {fallback_error}")
                raise Exception(f"Excelãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    
    def _is_xls_file(self, content: bytes) -> bool:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ãŒXLSå½¢å¼ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        """
        try:
            # XLSãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
            if content[:8] == b'\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1':
                return True
            # xlrdã§èª­ã¿è¾¼ã¿å¯èƒ½ã‹ãƒ†ã‚¹ãƒˆ
            xlrd.open_workbook(file_contents=content)
            return True
        except:
            return False
    
    def _process_xls_file(self, content: bytes) -> str:
        """
        XLSãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        """
        try:
            workbook = xlrd.open_workbook(file_contents=content)
            cleaned_parts = []
            
            for sheet_name in workbook.sheet_names():
                try:
                    logger.info(f"ğŸ“Š XLSã‚·ãƒ¼ãƒˆå‡¦ç†é–‹å§‹: {sheet_name}")
                    
                    sheet = workbook.sheet_by_name(sheet_name)
                    
                    # XLSã‚·ãƒ¼ãƒˆã‚’DataFrameã«å¤‰æ›
                    df = self._xls_sheet_to_dataframe(sheet)
                    
                    if df is None or df.empty:
                        logger.warning(f"âš ï¸ ã‚·ãƒ¼ãƒˆ {sheet_name} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—")
                        continue
                    
                    # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                    cleaned_df = self._clean_dataframe_enhanced(df)
                    
                    if cleaned_df.empty:
                        logger.warning(f"âš ï¸ ã‚·ãƒ¼ãƒˆ {sheet_name} ã«ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                        continue
                    
                    # æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
                    structured_text = self._convert_to_structured_text_enhanced(cleaned_df, sheet_name)
                    
                    if structured_text.strip():
                        cleaned_parts.append(structured_text)
                        logger.info(f"âœ… ã‚·ãƒ¼ãƒˆ {sheet_name} å‡¦ç†å®Œäº†: {len(structured_text)} æ–‡å­—")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ ã‚·ãƒ¼ãƒˆ {sheet_name} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            if not cleaned_parts:
                logger.warning("âš ï¸ å‡¦ç†å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return "ã“ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯å‡¦ç†å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
            
            result = "\n\n".join(cleaned_parts)
            logger.info(f"ğŸ‰ XLSå‡¦ç†å®Œäº†: {len(cleaned_parts)}ã‚·ãƒ¼ãƒˆ, ç·æ–‡å­—æ•°: {len(result)}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ XLSå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _xls_sheet_to_dataframe(self, sheet) -> Optional[pd.DataFrame]:
        """
        XLSã‚·ãƒ¼ãƒˆã‚’DataFrameã«å¤‰æ›
        """
        try:
            data = []
            for row_idx in range(sheet.nrows):
                row_data = []
                for col_idx in range(sheet.ncols):
                    cell = sheet.cell(row_idx, col_idx)
                    
                    # ã‚»ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦å€¤ã‚’å–å¾—
                    if cell.ctype == xlrd.XL_CELL_EMPTY:
                        row_data.append("")
                    elif cell.ctype == xlrd.XL_CELL_TEXT:
                        row_data.append(cell.value)
                    elif cell.ctype == xlrd.XL_CELL_NUMBER:
                        row_data.append(cell.value)
                    elif cell.ctype == xlrd.XL_CELL_DATE:
                        # æ—¥ä»˜ã®å‡¦ç†
                        try:
                            date_tuple = xlrd.xldate_as_tuple(cell.value, sheet.book.datemode)
                            if date_tuple[:3] != (0, 0, 0):  # æœ‰åŠ¹ãªæ—¥ä»˜
                                row_data.append(f"{date_tuple[0]}-{date_tuple[1]:02d}-{date_tuple[2]:02d}")
                            else:
                                row_data.append(cell.value)
                        except:
                            row_data.append(cell.value)
                    elif cell.ctype == xlrd.XL_CELL_BOOLEAN:
                        row_data.append(bool(cell.value))
                    elif cell.ctype == xlrd.XL_CELL_ERROR:
                        row_data.append("#ERROR!")
                    else:
                        row_data.append(str(cell.value))
                
                data.append(row_data)
            
            if not data:
                return None
            
            # DataFrameã‚’ä½œæˆ
            df = pd.DataFrame(data)
            return df
            
        except Exception as e:
            logger.error(f"âŒ XLSã‚·ãƒ¼ãƒˆå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _process_xlsx_file(self, content: bytes) -> str:
        """
        XLSXãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        """
        try:
            excel_file = pd.ExcelFile(BytesIO(content))
            cleaned_parts = []
            
            for sheet_name in excel_file.sheet_names:
                try:
                    logger.info(f"ğŸ“Š XLSXã‚·ãƒ¼ãƒˆå‡¦ç†é–‹å§‹: {sheet_name}")
                    
                    # è¤‡æ•°ã®èª­ã¿è¾¼ã¿æ–¹æ³•ã‚’è©¦è¡Œ
                    df = self._read_excel_sheet_robust(excel_file, sheet_name)
                    
                    if df is None or df.empty:
                        logger.warning(f"âš ï¸ ã‚·ãƒ¼ãƒˆ {sheet_name} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—")
                        continue
                    
                    # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                    cleaned_df = self._clean_dataframe_enhanced(df)
                    
                    if cleaned_df.empty:
                        logger.warning(f"âš ï¸ ã‚·ãƒ¼ãƒˆ {sheet_name} ã«ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                        continue
                    
                    # æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
                    structured_text = self._convert_to_structured_text_enhanced(cleaned_df, sheet_name)
                    
                    if structured_text.strip():
                        cleaned_parts.append(structured_text)
                        logger.info(f"âœ… ã‚·ãƒ¼ãƒˆ {sheet_name} å‡¦ç†å®Œäº†: {len(structured_text)} æ–‡å­—")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ ã‚·ãƒ¼ãƒˆ {sheet_name} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            if not cleaned_parts:
                logger.warning("âš ï¸ å‡¦ç†å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return "ã“ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯å‡¦ç†å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
            
            result = "\n\n".join(cleaned_parts)
            logger.info(f"ğŸ‰ XLSXå‡¦ç†å®Œäº†: {len(cleaned_parts)}ã‚·ãƒ¼ãƒˆ, ç·æ–‡å­—æ•°: {len(result)}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ XLSXå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _read_excel_sheet_robust(self, excel_file, sheet_name: str) -> Optional[pd.DataFrame]:
        """
        Excelã‚·ãƒ¼ãƒˆã‚’å …ç‰¢ã«èª­ã¿è¾¼ã‚€ï¼ˆè¤‡æ•°ã®æ–¹æ³•ã‚’è©¦è¡Œï¼‰
        """
        read_methods = [
            # æ–¹æ³•1: ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã§èª­ã¿è¾¼ã¿
            lambda: pd.read_excel(excel_file, sheet_name=sheet_name, header=None),
            # æ–¹æ³•2: æœ€åˆã®è¡Œã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦èª­ã¿è¾¼ã¿
            lambda: pd.read_excel(excel_file, sheet_name=sheet_name, header=0),
            # æ–¹æ³•3: è¤‡æ•°è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦èª­ã¿è¾¼ã¿
            lambda: pd.read_excel(excel_file, sheet_name=sheet_name, header=None, skiprows=1),
            # æ–¹æ³•4: æ–‡å­—åˆ—ã¨ã—ã¦å…¨ã¦èª­ã¿è¾¼ã¿
            lambda: pd.read_excel(excel_file, sheet_name=sheet_name, header=None, dtype=str),
            # æ–¹æ³•5: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æŒ‡å®šã—ã¦èª­ã¿è¾¼ã¿
            lambda: pd.read_excel(excel_file, sheet_name=sheet_name, header=None, encoding='utf-8'),
            lambda: pd.read_excel(excel_file, sheet_name=sheet_name, header=None, encoding='shift_jis'),
            lambda: pd.read_excel(excel_file, sheet_name=sheet_name, header=None, encoding='cp932')
        ]
        
        for i, method in enumerate(read_methods):
            try:
                df = method()
                if df is not None and not df.empty:
                    logger.info(f"ğŸ“– ã‚·ãƒ¼ãƒˆ {sheet_name} èª­ã¿è¾¼ã¿æˆåŠŸï¼ˆæ–¹æ³•{i+1}ï¼‰")
                    return df
            except Exception as e:
                logger.debug(f"èª­ã¿è¾¼ã¿æ–¹æ³•{i+1}å¤±æ•—: {e}")
                continue
        
        logger.warning(f"âš ï¸ ã‚·ãƒ¼ãƒˆ {sheet_name} ã®å…¨ã¦ã®èª­ã¿è¾¼ã¿æ–¹æ³•ãŒå¤±æ•—")
        return None
    
    def _clean_dataframe_enhanced(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        DataFrameã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        """
        # 1. å®Œå…¨ã«ç©ºã®è¡Œãƒ»åˆ—ã‚’å‰Šé™¤
        df = df.dropna(how='all').dropna(axis=1, how='all')
        
        if df.empty:
            return df
        
        # 2. ã‚»ãƒ«å†…å®¹ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        for col in df.columns:
            df[col] = df[col].apply(self._clean_cell_content_enhanced)
        
        # 3. ç©ºç™½è¡Œã‚’å†åº¦å‰Šé™¤ï¼ˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œï¼‰
        df = df[df.apply(lambda row: any(str(cell).strip() for cell in row if pd.notna(cell)), axis=1)]
        
        # 4. æ„å‘³ã®ãªã„è¡Œã‚’å‰Šé™¤ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        df = df[df.apply(self._is_meaningful_row_enhanced, axis=1)]
        
        # 5. é‡è¤‡è¡Œã‚’å‰Šé™¤ï¼ˆãŸã ã—ã€é‡è¦ãªãƒ‡ãƒ¼ã‚¿ã¯ä¿æŒï¼‰
        df = self._remove_duplicates_smart(df)
        
        # 6. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ
        df = df.reset_index(drop=True)
        
        return df
    
    def _clean_cell_content_enhanced(self, cell_value) -> str:
        """
        ã‚»ãƒ«å†…å®¹ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        """
        if pd.isna(cell_value):
            return ""
        
        # æ–‡å­—åˆ—ã«å¤‰æ›
        text = str(cell_value).strip()
        
        # ç„¡æ„å‘³ãªå€¤ã‚’ãƒã‚§ãƒƒã‚¯
        if text.lower() in [v.lower() for v in self.meaningless_values]:
            return ""
        
        # é•·ã™ãã‚‹ã‚»ãƒ«ã¯åˆ‡ã‚Šè©°ã‚
        if len(text) > self.max_cell_length:
            text = text[:self.max_cell_length] + "..."
        
        # Unicodeæ­£è¦åŒ–
        text = unicodedata.normalize('NFKC', text)
        
        # æ–‡å­—åŒ–ã‘ãƒ»ä¸è¦ãªè¨˜å·ã‚’é™¤å»
        text = self._remove_unwanted_symbols(text)
        
        # ç‰¹æ®Šæ–‡å­—ã®å‡¦ç†
        text = text.replace('\x00', '')  # NULLæ–‡å­—å‰Šé™¤
        text = text.replace('\ufeff', '')  # BOMå‰Šé™¤
        text = text.replace('\u200b', '')  # ã‚¼ãƒ­å¹…ã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤
        text = text.replace('\u200c', '')  # ã‚¼ãƒ­å¹…éçµåˆå­å‰Šé™¤
        text = text.replace('\u200d', '')  # ã‚¼ãƒ­å¹…çµåˆå­å‰Šé™¤
        text = text.replace('\ufffe', '')  # ä¸æ­£ãªUnicodeæ–‡å­—å‰Šé™¤
        text = text.replace('\uffff', '')  # ä¸æ­£ãªUnicodeæ–‡å­—å‰Šé™¤
        
        # ç©ºç™½ã®æ•´ç†
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n+', '\n', text)
        
        return text.strip()
    
    def _remove_unwanted_symbols(self, text: str) -> str:
        """
        ä¸è¦ãªè¨˜å·ãƒ»æ–‡å­—åŒ–ã‘æ–‡å­—ã‚’é™¤å»
        """
        # ä¸è¦ãªè¨˜å·ã‚’é™¤å»
        for symbol in self.unwanted_symbols:
            text = text.replace(symbol, '')
        
        # åˆ¶å¾¡æ–‡å­—ã‚’é™¤å»ï¼ˆãŸã ã—æ”¹è¡Œãƒ»ã‚¿ãƒ–ã¯ä¿æŒï¼‰
        text = ''.join(char for char in text if ord(char) >= 32 or char in '\n\t\r')
        
        # é€£ç¶šã™ã‚‹ç‰¹æ®Šæ–‡å­—ã‚’æ•´ç†
        text = re.sub(r'[!@#$%^&*()_+\-=\[\]{};\':"\\|,.<>?/~`]{3,}', '', text)
        
        return text
    
    def _is_meaningful_row_enhanced(self, row: pd.Series) -> bool:
        """
        è¡ŒãŒæ„å‘³ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚“ã§ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        """
        meaningful_cells = 0
        total_content_length = 0
        
        for cell in row:
            if pd.notna(cell):
                cell_text = str(cell).strip()
                if cell_text and cell_text.lower() not in [v.lower() for v in self.meaningless_values]:
                    # 1æ–‡å­—ä»¥ä¸Šã§ç„¡æ„å‘³ãªå€¤ã§ãªã‘ã‚Œã°æ„å‘³ãŒã‚ã‚‹ã¨åˆ¤å®š
                    if len(cell_text) >= 1:
                        meaningful_cells += 1
                        total_content_length += len(cell_text)
        
        # æ„å‘³ã®ã‚ã‚‹ã‚»ãƒ«ãŒ1å€‹ä»¥ä¸Šã€ã¾ãŸã¯ç·æ–‡å­—æ•°ãŒ1æ–‡å­—ä»¥ä¸Š
        return meaningful_cells >= 1 or total_content_length >= 1
    
    def _remove_duplicates_smart(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        é‡è¤‡è¡Œã‚’ã‚¹ãƒãƒ¼ãƒˆã«å‰Šé™¤ï¼ˆé‡è¦ãªãƒ‡ãƒ¼ã‚¿ã¯ä¿æŒï¼‰
        """
        # å®Œå…¨ã«åŒä¸€ã®è¡Œã®ã¿ã‚’å‰Šé™¤
        df_deduplicated = df.drop_duplicates()
        
        # å‰Šé™¤ã•ã‚ŒãŸè¡Œæ•°ã‚’ãƒ­ã‚°å‡ºåŠ›
        removed_count = len(df) - len(df_deduplicated)
        if removed_count > 0:
            logger.info(f"ğŸ—‘ï¸ é‡è¤‡è¡Œã‚’{removed_count}è¡Œå‰Šé™¤")
        
        return df_deduplicated
    
    def _convert_to_structured_text_enhanced(self, df: pd.DataFrame, sheet_name: str) -> str:
        """
        ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸDataFrameã‚’æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        """
        if df.empty:
            return ""
        
        text_parts = [f"=== ã‚·ãƒ¼ãƒˆ: {sheet_name} ==="]
        
        # ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã‚’åˆ†æ
        structure_info = self._analyze_data_structure_enhanced(df)
        
        if structure_info["has_headers"]:
            # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒã‚ã‚‹å ´åˆã®å‡¦ç†
            text_parts.append(self._format_with_headers_enhanced(df, structure_info))
        else:
            # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒãªã„å ´åˆã®å‡¦ç†
            text_parts.append(self._format_without_headers_enhanced(df))
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ 
        stats = self._generate_data_statistics_enhanced(df)
        if stats:
            text_parts.append(f"\nã€ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã€‘\n{stats}")
        
        return "\n".join(text_parts)
    
    def _analyze_data_structure_enhanced(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã‚’åˆ†æï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        """
        structure = {
            "has_headers": False,
            "header_row": None,
            "data_types": {},
            "patterns": []
        }
        
        # æœ€åˆã®æ•°è¡Œã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ¢ã™
        for i in range(min(5, len(df))):
            row = df.iloc[i]
            if self._looks_like_header_enhanced(row):
                structure["has_headers"] = True
                structure["header_row"] = i
                break
        
        return structure
    
    def _looks_like_header_enhanced(self, row: pd.Series) -> bool:
        """
        è¡ŒãŒãƒ˜ãƒƒãƒ€ãƒ¼ã‚‰ã—ã„ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        """
        non_null_count = row.notna().sum()
        if non_null_count < 1:
            return False
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚‰ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå¤§å¹…æ‹¡å¼µï¼‰
        header_keywords = [
            'åå‰', 'name', 'ä¼šç¤¾', 'company', 'ä½æ‰€', 'address', 
            'é›»è©±', 'phone', 'tel', 'æ—¥ä»˜', 'date', 'id', 'no',
            'ç•ªå·', 'ç¨®é¡', 'type', 'çŠ¶æ…‹', 'status', 'é‡‘é¡', 'amount',
            'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹', 'é¡§å®¢', 'ç²å¾—', 'ç‰©ä»¶', 'å¥‘ç´„', 'æ›¸é¡',
            'ç™ºè¡Œ', 'è«‹æ±‚', 'mail', 'è§£ç´„', 'å‚™è€ƒ', '#', 'é …ç›®',
            'ãƒ—ãƒ­ãƒã‚¤ãƒ€', 'ISP', 'æ¡ˆä»¶', 'ä¸€è¦§', 'ç®¡ç†', 'æƒ…å ±',
            'æ‹…å½“', 'éƒ¨ç½²', 'æ”¯åº—', 'å–¶æ¥­', 'å£²ä¸Š', 'åˆ©ç›Š', 'è²»ç”¨',
            'é–‹å§‹', 'çµ‚äº†', 'æœŸé–“', 'æœŸé™', 'äºˆå®š', 'å®Ÿç¸¾', 'é€²æ—',
            'é¡§å®¢ç•ªå·', 'ç¤¾å', 'æ³•äºº', 'å€‹äºº', 'é€£çµ¡å…ˆ', 'ãƒ¡ãƒ¼ãƒ«',
            'éƒµä¾¿ç•ªå·', 'éƒ½é“åºœçœŒ', 'å¸‚åŒºç”ºæ‘', 'å»ºç‰©', 'ãƒ“ãƒ«',
            'å›ç·š', 'é€Ÿåº¦', 'æ–™é‡‘', 'ãƒ—ãƒ©ãƒ³', 'ã‚³ãƒ¼ã‚¹', 'ã‚ªãƒ—ã‚·ãƒ§ãƒ³',
            'SS', 'ISP', 'ç”³è¾¼', 'å·¥äº‹', 'é–‹é€š', 'è¨­ç½®', 'æ’¤å»'
        ]
        
        text_content = ' '.join([str(cell).lower() for cell in row if pd.notna(cell)])
        
        for keyword in header_keywords:
            if keyword.lower() in text_content:
                return True
        
        # æ•°å€¤ãŒå°‘ãªãã€ãƒ†ã‚­ã‚¹ãƒˆãŒå¤šã„å ´åˆã‚‚ãƒ˜ãƒƒãƒ€ãƒ¼ã®å¯èƒ½æ€§
        text_cells = 0
        numeric_cells = 0
        for cell in row:
            if pd.notna(cell):
                cell_str = str(cell).strip()
                if re.match(r'^-?\d+\.?\d*$', cell_str):
                    numeric_cells += 1
                else:
                    text_cells += 1
        
        # ãƒ†ã‚­ã‚¹ãƒˆãŒæ•°å€¤ä»¥ä¸Šã®å ´åˆã¯ãƒ˜ãƒƒãƒ€ãƒ¼ã®å¯èƒ½æ€§
        return text_cells >= numeric_cells and text_cells >= 2
    
    def _format_with_headers_enhanced(self, df: pd.DataFrame, structure_info: Dict) -> str:
        """
        ãƒ˜ãƒƒãƒ€ãƒ¼ã‚ã‚Šã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        """
        header_row = structure_info["header_row"]
        headers = df.iloc[header_row].tolist()
        data_rows = df.iloc[header_row + 1:]
        
        formatted_parts = []
        formatted_parts.append("ã€ãƒ‡ãƒ¼ã‚¿é …ç›®ã€‘")
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±
        valid_headers = []
        for i, header in enumerate(headers):
            if pd.notna(header):
                clean_header = str(header).strip()
                if clean_header:
                    valid_headers.append((i, clean_header))
                    formatted_parts.append(f"- {clean_header}")
                else:
                    valid_headers.append((i, f"åˆ—{i+1}"))
                    formatted_parts.append(f"- åˆ—{i+1}")
        
        formatted_parts.append("\nã€ãƒ‡ãƒ¼ã‚¿å†…å®¹ã€‘")
        
        # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’å‡¦ç†
        for idx, row in data_rows.iterrows():
            row_data = []
            for col_idx, header_name in valid_headers:
                if col_idx < len(row):
                    cell_value = row.iloc[col_idx]
                    if pd.notna(cell_value):
                        clean_value = str(cell_value).strip()
                        # ç„¡æ„å‘³ãªå€¤ä»¥å¤–ã¯ä¿æŒ
                        if clean_value and clean_value.lower() not in [v.lower() for v in self.meaningless_values]:
                            row_data.append(f"{header_name}: {clean_value}")
            
            if row_data:
                formatted_parts.append(f"â€¢ {' | '.join(row_data)}")
        
        return "\n".join(formatted_parts)
    
    def _format_without_headers_enhanced(self, df: pd.DataFrame) -> str:
        """
        ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        """
        formatted_parts = []
        formatted_parts.append("ã€ãƒ‡ãƒ¼ã‚¿å†…å®¹ã€‘")
        
        for idx, row in df.iterrows():
            row_data = []
            for col_idx, cell_value in enumerate(row):
                if pd.notna(cell_value):
                    clean_value = str(cell_value).strip()
                    # ç„¡æ„å‘³ãªå€¤ä»¥å¤–ã¯ä¿æŒ
                    if clean_value and clean_value.lower() not in [v.lower() for v in self.meaningless_values]:
                        row_data.append(clean_value)
            
            if row_data:
                formatted_parts.append(f"è¡Œ{idx + 1}: {' | '.join(row_data)}")
        
        return "\n".join(formatted_parts)
    
    def _generate_data_statistics_enhanced(self, df: pd.DataFrame) -> str:
        """
        ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆæƒ…å ±ã‚’ç”Ÿæˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        """
        stats_parts = []
        
        # åŸºæœ¬çµ±è¨ˆ
        stats_parts.append(f"ç·è¡Œæ•°: {len(df)}")
        stats_parts.append(f"ç·åˆ—æ•°: {len(df.columns)}")
        
        # éç©ºã‚»ãƒ«æ•°
        non_empty_cells = 0
        meaningful_cells = 0
        total_cells = len(df) * len(df.columns)
        
        for col in df.columns:
            for cell in df[col]:
                if pd.notna(cell):
                    cell_str = str(cell).strip()
                    if cell_str:
                        non_empty_cells += 1
                        if cell_str.lower() not in [v.lower() for v in self.meaningless_values]:
                            meaningful_cells += 1
        
        fill_rate = (non_empty_cells / total_cells * 100) if total_cells > 0 else 0
        meaningful_rate = (meaningful_cells / total_cells * 100) if total_cells > 0 else 0
        
        stats_parts.append(f"ãƒ‡ãƒ¼ã‚¿å……å¡«ç‡: {fill_rate:.1f}%")
        stats_parts.append(f"æœ‰æ„ç¾©ãƒ‡ãƒ¼ã‚¿ç‡: {meaningful_rate:.1f}%")
        
        return " | ".join(stats_parts)
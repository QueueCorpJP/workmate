"""
åˆ†æãƒ»ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½
queue@queueu-tech.jpç”¨ã®åˆ©ç”¨çŠ¶æ³åˆ†æã‚’æä¾›
"""

from datetime import datetime, timedelta, timezone
from typing import Dict, List, Any, Optional
from supabase_adapter import select_data, execute_query
import json

# å®‰å…¨ãªãƒ‡ãƒ¼ã‚¿å¤‰æ›é–¢æ•°
def safe_int(value, default=0):
    """å®‰å…¨ã«intã«å¤‰æ›ã™ã‚‹"""
    if value is None:
        return default
    if isinstance(value, (int, float)):
        return int(value)
    if isinstance(value, str):
        try:
            return int(value)
        except ValueError:
            return default
    if isinstance(value, dict):
        # Supabaseã‹ã‚‰è¾æ›¸å½¢å¼ã§è¿”ã•ã‚Œã‚‹å ´åˆã®å‡¦ç†
        if 'value' in value:
            return safe_int(value['value'], default)
        if len(value) == 1:
            return safe_int(list(value.values())[0], default)
    return default

def safe_float(value, default=0.0):
    """å®‰å…¨ã«floatã«å¤‰æ›ã™ã‚‹"""
    if value is None:
        return default
    if isinstance(value, (int, float)):
        return float(value)
    if isinstance(value, str):
        try:
            return float(value)
        except ValueError:
            return default
    if isinstance(value, dict):
        # Supabaseã‹ã‚‰è¾æ›¸å½¢å¼ã§è¿”ã•ã‚Œã‚‹å ´åˆã®å‡¦ç†
        if 'value' in value:
            return safe_float(value['value'], default)
        if len(value) == 1:
            return safe_float(list(value.values())[0], default)
    return default

def safe_str(value, default=""):
    """å®‰å…¨ã«strã«å¤‰æ›ã™ã‚‹"""
    if value is None:
        return default
    if isinstance(value, str):
        return value
    if isinstance(value, dict):
        # Supabaseã‹ã‚‰è¾æ›¸å½¢å¼ã§è¿”ã•ã‚Œã‚‹å ´åˆã®å‡¦ç†
        if 'value' in value:
            return safe_str(value['value'], default)
        if len(value) == 1:
            return safe_str(list(value.values())[0], default)
    return str(value)

def get_usage_analytics(db) -> Dict[str, Any]:
    """
    queue@queueu-tech.jpç”¨ã®åˆ©ç”¨çŠ¶æ³åˆ†æã‚’å–å¾—
    
    Returns:
        Dict containing:
        - company_usage_periods: ä¼šç¤¾å˜ä½ã®ç´¯è¨ˆåˆ©ç”¨æœŸé–“
        - user_usage_periods: ãƒ¦ãƒ¼ã‚¶ãƒ¼å˜ä½ã®ç´¯è¨ˆåˆ©ç”¨æœŸé–“
        - active_users: éå»1é€±é–“ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼
        - plan_continuity: ãƒ—ãƒ©ãƒ³ç¶™ç¶šæ•°ã®åˆ†æ
    """
    try:
        analytics = {
            "company_usage_periods": get_company_usage_periods(db),
            "user_usage_periods": get_user_usage_periods(db),
            "active_users": get_active_users(db),
            "plan_continuity": get_plan_continuity_analysis(db)
        }
        return analytics
    except Exception as e:
        print(f"åˆ†æãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {}

def get_company_usage_periods(db) -> List[Dict[str, Any]]:
    """ä¼šç¤¾å˜ä½ã®ç´¯è¨ˆåˆ©ç”¨æœŸé–“ã‚’è¨ˆç®—"""
    try:
        # å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ä¼šç¤¾æƒ…å ±ã‚’å–å¾—
        users_result = select_data("users", columns="id,email,company_id,created_at")
        if not users_result or not users_result.data:
            return []
        
        # å…¨ä¼šç¤¾æƒ…å ±ã‚’å–å¾—
        companies_result = select_data("companies", columns="id,name")
        company_name_map = {}
        if companies_result and companies_result.data:
            for company in companies_result.data:
                company_name_map[company["id"]] = company["name"]
        
        # ä¼šç¤¾ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        companies = {}
        for user in users_result.data:
            company_id = user.get("company_id")
            company_name = company_name_map.get(company_id) or "ä¸æ˜ãªä¼šç¤¾"
            if company_name not in companies:
                companies[company_name] = {
                    "company_name": company_name,
                    "users": [],
                    "total_usage_days": 0,
                    "earliest_start": None
                }
            companies[company_name]["users"].append(user)
        
        # å„ä¼šç¤¾ã®åˆ©ç”¨æœŸé–“ã‚’è¨ˆç®—
        company_analytics = []
        for company_name, company_data in companies.items():
            # æœ€ã‚‚æ—©ã„é–‹å§‹æ—¥ã‚’å–å¾—
            earliest_dates = [user.get("created_at") for user in company_data["users"] if user.get("created_at")]
            if earliest_dates:
                earliest_start = min(earliest_dates)
                start_date = datetime.fromisoformat(earliest_start.replace('Z', '+00:00'))
                usage_days = (datetime.now() - start_date).days
                
                company_analytics.append({
                    "company_name": company_name,
                    "user_count": len(company_data["users"]),
                    "usage_days": usage_days,
                    "start_date": earliest_start,
                    "usage_months": round(usage_days / 30.44, 1)  # å¹³å‡æœˆæ—¥æ•°
                })
        
        return sorted(company_analytics, key=lambda x: x["usage_days"], reverse=True)
    
    except Exception as e:
        print(f"ä¼šç¤¾åˆ¥åˆ©ç”¨æœŸé–“è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

def get_user_usage_periods(db) -> List[Dict[str, Any]]:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼å˜ä½ã®ç´¯è¨ˆåˆ©ç”¨æœŸé–“ã‚’è¨ˆç®—"""
    try:
        # å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å–å¾—
        users_result = select_data("users", columns="id,email,name,company_id,created_at")
        if not users_result or not users_result.data:
            return []
        
        # å…¨ä¼šç¤¾æƒ…å ±ã‚’å–å¾—
        companies_result = select_data("companies", columns="id,name")
        company_name_map = {}
        if companies_result and companies_result.data:
            for company in companies_result.data:
                company_name_map[company["id"]] = company["name"]
        
        user_analytics = []
        for user in users_result.data:
            if user.get("created_at"):
                start_date = datetime.fromisoformat(user["created_at"].replace('Z', '+00:00'))
                usage_days = (datetime.now() - start_date).days
                company_id = user.get("company_id")
                company_name = company_name_map.get(company_id) or "ä¸æ˜ãªä¼šç¤¾"
                
                user_analytics.append({
                    "user_id": user["id"],
                    "email": user["email"],
                    "name": user.get("name", ""),
                    "company_name": company_name,
                    "usage_days": usage_days,
                    "start_date": user["created_at"],
                    "usage_months": round(usage_days / 30.44, 1)
                })
        
        return sorted(user_analytics, key=lambda x: x["usage_days"], reverse=True)
    
    except Exception as e:
        print(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥åˆ©ç”¨æœŸé–“è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

def get_active_users(db) -> Dict[str, Any]:
    """éå»1é€±é–“ä»¥å†…ã«ãƒãƒ£ãƒƒãƒˆã‚’è¡Œã£ãŸã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è¨ˆç®—"""
    try:
        # 1é€±é–“å‰ã®æ—¥æ™‚ã‚’è¨ˆç®—
        one_week_ago = datetime.now() - timedelta(days=7)
        one_week_ago_str = one_week_ago.isoformat()
        
        # éå»1é€±é–“ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’å–å¾—
        chat_result = select_data(
            "chat_history", 
            columns="employee_id,employee_name,timestamp",
            filters={"timestamp": f"gte.{one_week_ago_str}"}
        )
        
        if not chat_result or not chat_result.data:
            return {
                "total_active_users": 0,
                "active_users_by_company": {},
                "active_users_list": []
            }
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’é›†è¨ˆ
        active_users = {}
        for chat in chat_result.data:
            user_id = chat.get("employee_id")
            if user_id and user_id not in active_users:
                active_users[user_id] = {
                    "user_id": user_id,
                    "name": chat.get("employee_name", ""),
                    "last_chat": chat.get("timestamp"),
                    "chat_count": 0
                }
            if user_id:
                active_users[user_id]["chat_count"] += 1
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—ã—ã¦ä¼šç¤¾åˆ¥ã«åˆ†é¡
        users_result = select_data("users", columns="id,email,company_id")
        companies_result = select_data("companies", columns="id,name")
        company_name_map = {}
        if companies_result and companies_result.data:
            for company in companies_result.data:
                company_name_map[company["id"]] = company["name"]
        
        user_company_map = {}
        if users_result and users_result.data:
            for user in users_result.data:
                company_id = user.get("company_id")
                company_name = company_name_map.get(company_id) or "ä¸æ˜ãªä¼šç¤¾"
                user_company_map[user["id"]] = company_name
        
        # ä¼šç¤¾åˆ¥ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ã‚’è¨ˆç®—
        active_by_company = {}
        active_users_list = []
        
        for user_id, user_data in active_users.items():
            company = user_company_map.get(user_id, "ä¸æ˜")
            if company not in active_by_company:
                active_by_company[company] = 0
            active_by_company[company] += 1
            
            user_data["company_name"] = company
            active_users_list.append(user_data)
        
        return {
            "total_active_users": len(active_users),
            "active_users_by_company": active_by_company,
            "active_users_list": sorted(active_users_list, key=lambda x: x["chat_count"], reverse=True),
            "analysis_period": f"{one_week_ago.strftime('%Y-%m-%d')} ã‹ã‚‰ {datetime.now().strftime('%Y-%m-%d')}"
        }
    
    except Exception as e:
        print(f"ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            "total_active_users": 0,
            "active_users_by_company": {},
            "active_users_list": []
        }

def get_plan_continuity_analysis(db) -> Dict[str, Any]:
    """ãƒ—ãƒ©ãƒ³ç¶™ç¶šæ•°ã®åˆ†æ"""
    try:
        # ãƒ—ãƒ©ãƒ³å±¥æ­´ã‚’å–å¾—
        from modules.database import get_plan_history
        user_histories = get_plan_history(db=db)
        
        if not user_histories:
            return {
                "total_users": 0,
                "continuity_stats": {},
                "plan_retention": {}
            }
        
        continuity_stats = {
            "never_changed": 0,  # ä¸€åº¦ã‚‚å¤‰æ›´ã—ã¦ã„ãªã„
            "changed_once": 0,   # 1å›å¤‰æ›´
            "changed_multiple": 0,  # è¤‡æ•°å›å¤‰æ›´
            "demo_to_prod_stayed": 0,  # ãƒ‡ãƒ¢â†’æœ¬ç•ªã§ç¶™ç¶š
            "prod_to_demo_returned": 0  # æœ¬ç•ªâ†’ãƒ‡ãƒ¢ã«æˆ»ã£ãŸ
        }
        
        plan_retention = {
            "demo_users": 0,
            "production_users": 0,
            "demo_avg_duration": 0,
            "production_avg_duration": 0
        }
        
        demo_durations = []
        prod_durations = []
        
        for user in user_histories:
            total_changes = user.get("total_changes", 0)
            current_plan = user.get("current_plan", "")
            changes = user.get("changes", [])
            
            # ç¶™ç¶šæ€§çµ±è¨ˆ
            if total_changes == 0:
                continuity_stats["never_changed"] += 1
            elif total_changes == 1:
                continuity_stats["changed_once"] += 1
            else:
                continuity_stats["changed_multiple"] += 1
            
            # ãƒ—ãƒ©ãƒ³ä¿æŒçµ±è¨ˆ
            if current_plan == "demo":
                plan_retention["demo_users"] += 1
            elif current_plan == "production":
                plan_retention["production_users"] += 1
            
            # æœŸé–“åˆ†æ
            for change in changes:
                duration = change.get("duration_days")
                if duration:
                    if change.get("from_plan") == "demo":
                        demo_durations.append(duration)
                    elif change.get("from_plan") == "production":
                        prod_durations.append(duration)
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            if len(changes) >= 1:
                first_change = changes[-1]  # æœ€åˆã®å¤‰æ›´ï¼ˆé…åˆ—ã¯æ–°ã—ã„é †ï¼‰
                if first_change.get("from_plan") == "demo" and first_change.get("to_plan") == "production":
                    # ãƒ‡ãƒ¢ã‹ã‚‰æœ¬ç•ªã«å¤‰æ›´ã—ã€ç¾åœ¨ã‚‚æœ¬ç•ªãªã‚‰ç¶™ç¶š
                    if current_plan == "production":
                        continuity_stats["demo_to_prod_stayed"] += 1
                elif first_change.get("from_plan") == "production" and first_change.get("to_plan") == "demo":
                    continuity_stats["prod_to_demo_returned"] += 1
        
        # å¹³å‡æœŸé–“è¨ˆç®—
        if demo_durations:
            plan_retention["demo_avg_duration"] = sum(demo_durations) / len(demo_durations)
        if prod_durations:
            plan_retention["production_avg_duration"] = sum(prod_durations) / len(prod_durations)
        
        return {
            "total_users": len(user_histories),
            "continuity_stats": continuity_stats,
            "plan_retention": plan_retention,
            "duration_analysis": {
                "demo_duration_samples": len(demo_durations),
                "production_duration_samples": len(prod_durations)
            }
        }
    
    except Exception as e:
        print(f"ãƒ—ãƒ©ãƒ³ç¶™ç¶šæ€§åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            "total_users": 0,
            "continuity_stats": {},
            "plan_retention": {}
        }

def get_enhanced_analytics(db, company_id: str = None) -> Dict[str, Any]:
    """
    å¼·åŒ–ã•ã‚ŒãŸåˆ†ææ©Ÿèƒ½ã‚’å–å¾—ï¼ˆsumry.mdã®è¦æ±‚é …ç›®ã«å¯¾å¿œï¼‰
    
    Args:
        db: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
        company_id: ä¼šç¤¾IDã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    
    Returns:
        Dict containing:
        - resource_reference_count: è³‡æ–™ã®å‚ç…§å›æ•°
        - category_distribution_analysis: è³ªå•ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒã¨åã‚Š
        - active_user_trends: ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¨ç§»
        - unresolved_and_repeat_analysis: æœªè§£æ±ºãƒ»å†è³ªå•ã®å‚¾å‘åˆ†æ
        - sentiment_analysis: æ„Ÿæƒ…åˆ†æ
    """
    try:
        analytics = {
            "resource_reference_count": get_resource_reference_analysis(db, company_id),
            "category_distribution_analysis": get_category_distribution_analysis(db, company_id),
            "active_user_trends": get_active_user_trends(db, company_id),
            "unresolved_and_repeat_analysis": get_unresolved_repeat_analysis(db, company_id),
            "sentiment_analysis": get_detailed_sentiment_analysis(db, company_id),
            "analysis_metadata": {
                "generated_at": datetime.now().isoformat(),
                "analysis_type": "enhanced",
                "company_id": company_id
            }
        }
        return analytics
    except Exception as e:
        print(f"å¼·åŒ–åˆ†æãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {}

def get_resource_reference_analysis(db, company_id: str = None) -> Dict[str, Any]:
    """è³‡æ–™ã®å‚ç…§å›æ•°åˆ†æ"""
    try:
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‹ã‚‰è³‡æ–™å‚ç…§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        base_query = """
        SELECT 
            ds.name as resource_name,
            ds.type as resource_type,
            COUNT(ch.id) as reference_count,
            COUNT(DISTINCT ch.employee_id) as unique_users,
            COUNT(DISTINCT DATE(ch.timestamp)) as unique_days,
            MAX(ch.timestamp) as last_referenced,
            AVG(CASE 
                WHEN ch.sentiment = 'positive' THEN 3
                WHEN ch.sentiment = 'neutral' THEN 2
                WHEN ch.sentiment = 'negative' THEN 1
                ELSE 2
            END) as avg_satisfaction
        FROM chat_history ch
        LEFT JOIN document_sources ds ON ds.id = ANY(
            SELECT unnest(string_to_array(ch.source_document, ','))::int
        )
        WHERE ds.name IS NOT NULL
        """
        
        if company_id:
            base_query += f" AND ch.company_id = '{company_id}'"
        
        base_query += """
        GROUP BY ds.name, ds.type
        ORDER BY reference_count DESC
        """
        
        result = execute_query(base_query)
        
        if not result:
            return {
                "resources": [],
                "total_references": 0,
                "most_referenced": None,
                "least_referenced": None,
                "summary": "ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™"
            }
        
        resources = []
        for row in result:
            resources.append({
                "name": safe_str(row.get("resource_name", "ä¸æ˜")),
                "type": safe_str(row.get("resource_type", "ä¸æ˜")),
                "reference_count": safe_int(row.get("reference_count", 0)),
                "unique_users": safe_int(row.get("unique_users", 0)),
                "unique_days": safe_int(row.get("unique_days", 0)),
                "last_referenced": safe_str(row.get("last_referenced")),
                "avg_satisfaction": round(safe_float(row.get("avg_satisfaction", 2.0)), 2) if row.get("avg_satisfaction") else 2.0,
                "usage_intensity": round(safe_int(row.get("reference_count", 0)) / max(safe_int(row.get("unique_users", 1)), 1), 2)
            })
        
        total_references = sum(r["reference_count"] for r in resources)
        most_referenced = resources[0] if resources else None
        least_referenced = resources[-1] if resources else None
        
        return {
            "resources": resources,
            "total_references": total_references,
            "most_referenced": most_referenced,
            "least_referenced": least_referenced,
            "active_resources": len(resources),
            "summary": f"åˆè¨ˆ{total_references}å›ã®è³‡æ–™å‚ç…§ãŒã‚ã‚Šã€{len(resources)}å€‹ã®ãƒªã‚½ãƒ¼ã‚¹ãŒåˆ©ç”¨ã•ã‚Œã¦ã„ã¾ã™"
        }
        
    except Exception as e:
        print(f"è³‡æ–™å‚ç…§åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return {
            "resources": [],
            "total_references": 0,
            "summary": f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
        }

def get_category_distribution_analysis(db, company_id: str = None) -> Dict[str, Any]:
    """è³ªå•ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒã¨åã‚Šåˆ†æ"""
    try:
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®è©³ç´°åˆ†æ
        base_query = """
        SELECT 
            category,
            COUNT(*) as count,
            COUNT(DISTINCT employee_id) as unique_users,
            COUNT(DISTINCT DATE(timestamp)) as unique_days,
            AVG(CASE 
                WHEN sentiment = 'positive' THEN 3
                WHEN sentiment = 'neutral' THEN 2
                WHEN sentiment = 'negative' THEN 1
                ELSE 2
            END) as avg_sentiment_score,
            SUM(CASE WHEN sentiment = 'positive' THEN 1 ELSE 0 END) as positive_count,
            SUM(CASE WHEN sentiment = 'neutral' THEN 1 ELSE 0 END) as neutral_count,
            SUM(CASE WHEN sentiment = 'negative' THEN 1 ELSE 0 END) as negative_count
        FROM chat_history
        WHERE category IS NOT NULL AND category != ''
        """
        
        if company_id:
            base_query += f" AND company_id = '{company_id}'"
        
        base_query += """
        GROUP BY category
        ORDER BY count DESC
        """
        
        result = execute_query(base_query)
        
        if not result:
            return {
                "categories": [],
                "distribution": {},
                "bias_analysis": {},
                "summary": "ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™"
            }
        
        categories = []
        total_questions = 0
        
        for row in result:
            # å®‰å…¨ãªãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚’ä½¿ç”¨
            count = safe_int(row.get("count", 0))
            total_questions += count
            
            categories.append({
                "category": safe_str(row.get("category", "ä¸æ˜")),
                "count": count,
                "unique_users": safe_int(row.get("unique_users", 0)),
                "unique_days": safe_int(row.get("unique_days", 0)),
                "avg_sentiment_score": round(safe_float(row.get("avg_sentiment_score", 2.0)), 2),
                "positive_count": safe_int(row.get("positive_count", 0)),
                "neutral_count": safe_int(row.get("neutral_count", 0)),
                "negative_count": safe_int(row.get("negative_count", 0))
            })
        
        # åˆ†å¸ƒã¨ãƒã‚¤ã‚¢ã‚¹åˆ†æ
        distribution = {}
        bias_analysis = {}
        
        for cat in categories:
            percentage = round((cat["count"] / total_questions) * 100, 2) if total_questions > 0 else 0
            distribution[cat["category"]] = {
                "count": cat["count"],
                "percentage": percentage
            }
            
            # ãƒã‚¤ã‚¢ã‚¹åˆ†æï¼ˆæœŸå¾…å€¤ã‹ã‚‰ã®åå·®ï¼‰
            expected_percentage = 100 / len(categories) if categories else 0
            bias = percentage - expected_percentage
            
            bias_analysis[cat["category"]] = {
                "bias_score": round(bias, 2),
                "is_over_represented": bias > 10,
                "is_under_represented": bias < -10,
                "sentiment_bias": "positive" if cat["avg_sentiment_score"] > 2.3 else "negative" if cat["avg_sentiment_score"] < 1.7 else "neutral"
            }
        
        # ãƒˆãƒƒãƒ—3ã¨ãƒœãƒˆãƒ 3ã‚’ç‰¹å®š
        top_categories = sorted(categories, key=lambda x: x["count"], reverse=True)[:3]
        bottom_categories = sorted(categories, key=lambda x: x["count"])[:3] if len(categories) > 3 else []
        
        return {
            "categories": categories,
            "distribution": distribution,
            "bias_analysis": bias_analysis,
            "top_categories": top_categories,
            "bottom_categories": bottom_categories,
            "total_questions": total_questions,
            "category_diversity": len(categories),
            "summary": f"åˆè¨ˆ{len(categories)}ã‚«ãƒ†ã‚´ãƒªã§{total_questions}ä»¶ã®è³ªå•ãŒã‚ã‚Šã€æœ€ã‚‚å¤šã„ã®ã¯'{top_categories[0]['category']}'({top_categories[0]['count']}ä»¶)ã§ã™" if top_categories else "ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™"
        }
        
    except Exception as e:
        print(f"ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return {
            "categories": [],
            "distribution": {},
            "bias_analysis": {},
            "summary": f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
        }

def get_active_user_trends(db, company_id: str = None, days: int = 30) -> Dict[str, Any]:
    """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¨ç§»åˆ†æ"""
    try:
        # æ—¥åˆ¥ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°
        base_query = """
        SELECT 
            DATE(timestamp) as date,
            COUNT(DISTINCT employee_id) as active_users,
            COUNT(*) as total_messages,
            COUNT(DISTINCT employee_name) as unique_names,
            AVG(CASE 
                WHEN sentiment = 'positive' THEN 1 
                ELSE 0 
            END) as positive_ratio
        FROM chat_history
        WHERE timestamp >= %s
        """
        
        if company_id:
            base_query += f" AND company_id = '{company_id}'"
        
        base_query += """
        GROUP BY DATE(timestamp)
        ORDER BY DATE(timestamp)
        """
        
        # éå»30æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        start_date = datetime.now() - timedelta(days=days)
        params = [start_date.strftime('%Y-%m-%d')]
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç½®æ›
        formatted_query = base_query.replace('%s', f"'{params[0]}'")
        result = execute_query(formatted_query)
        
        if not result:
            return {
                "daily_trends": [],
                "weekly_trends": [],
                "summary": "ãƒ¦ãƒ¼ã‚¶ãƒ¼æ´»å‹•ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™"
            }
        
        daily_trends = []
        for row in result:
            daily_trends.append({
                "date": safe_str(row.get("date")),
                "active_users": safe_int(row.get("active_users", 0)),
                "total_messages": safe_int(row.get("total_messages", 0)),
                "unique_names": safe_int(row.get("unique_names", 0)),
                "positive_ratio": round(safe_float(row.get("positive_ratio", 0)), 2)
            })
        
        # é€±æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰è¨ˆç®—
        weekly_trends = []
        if daily_trends:
            for i in range(0, len(daily_trends), 7):
                week_data = daily_trends[i:i+7]
                if week_data:
                    week_start = week_data[0]["date"]
                    week_end = week_data[-1]["date"]
                    total_active = sum(d["active_users"] for d in week_data)
                    avg_active = round(total_active / len(week_data), 1)
                    total_messages = sum(d["total_messages"] for d in week_data)
                    
                    weekly_trends.append({
                        "week_start": week_start,
                        "week_end": week_end,
                        "avg_active_users": avg_active,
                        "total_messages": total_messages,
                        "days_with_activity": len([d for d in week_data if d["active_users"] > 0])
                    })
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        if len(daily_trends) >= 7:
            recent_week = daily_trends[-7:]
            previous_week = daily_trends[-14:-7] if len(daily_trends) >= 14 else []
            
            recent_avg = sum(d["active_users"] for d in recent_week) / len(recent_week)
            previous_avg = sum(d["active_users"] for d in previous_week) / len(previous_week) if previous_week else recent_avg
            
            trend_direction = "increasing" if recent_avg > previous_avg else "decreasing" if recent_avg < previous_avg else "stable"
            trend_percentage = round(((recent_avg - previous_avg) / previous_avg * 100), 2) if previous_avg > 0 else 0
        else:
            trend_direction = "insufficient_data"
            trend_percentage = 0
        
        return {
            "daily_trends": daily_trends,
            "weekly_trends": weekly_trends,
            "trend_analysis": {
                "direction": trend_direction,
                "percentage_change": trend_percentage,
                "period": f"éå»{days}æ—¥é–“"
            },
            "peak_day": max(daily_trends, key=lambda x: x["active_users"]) if daily_trends else None,
            "total_unique_users": len(set(d["active_users"] for d in daily_trends)) if daily_trends else 0,
            "summary": f"éå»{days}æ—¥é–“ã§æœ€å¤§{max(d['active_users'] for d in daily_trends) if daily_trends else 0}äºº/æ—¥ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è¨˜éŒ²"
        }
        
    except Exception as e:
        print(f"ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¨ç§»åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return {
            "daily_trends": [],
            "weekly_trends": [],
            "summary": f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
        }

def get_unresolved_repeat_analysis(db, company_id: str = None) -> Dict[str, Any]:
    """æœªè§£æ±ºãƒ»å†è³ªå•ã®å‚¾å‘åˆ†æ"""
    try:
        # é¡ä¼¼è³ªå•ã®æ¤œå‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
        base_query = """
        SELECT 
            employee_id,
            employee_name,
            user_message,
            bot_response,
            timestamp,
            sentiment,
            category,
            LENGTH(bot_response) as response_length
        FROM chat_history
        WHERE employee_id IS NOT NULL
        """
        
        if company_id:
            base_query += f" AND company_id = '{company_id}'"
        
        base_query += " ORDER BY employee_id, timestamp"
        
        result = execute_query(base_query)
        
        if not result:
            return {
                "repeat_questions": [],
                "unresolved_patterns": [],
                "summary": "è³ªå•ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™"
            }
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥ã®è³ªå•å±¥æ­´ã‚’æ§‹ç¯‰
        user_questions = {}
        for row in result:
            employee_id = safe_str(row.get("employee_id"))
            if employee_id not in user_questions:
                user_questions[employee_id] = []
            
            user_questions[employee_id].append({
                "message": safe_str(row.get("user_message", "")),
                "response": safe_str(row.get("bot_response", "")),
                "timestamp": safe_str(row.get("timestamp")),
                "sentiment": safe_str(row.get("sentiment", "neutral")),
                "category": safe_str(row.get("category", "")),
                "response_length": safe_int(row.get("response_length", 0)),
                "employee_name": safe_str(row.get("employee_name", ""))
            })
        
        # å†è³ªå•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        repeat_questions = []
        unresolved_patterns = []
        
        for employee_id, questions in user_questions.items():
            if len(questions) < 2:
                continue
            
            for i in range(len(questions) - 1):
                current = questions[i]
                next_q = questions[i + 1]
                
                # ç°¡æ˜“çš„ãªé¡ä¼¼æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰
                current_words = set(current["message"].lower().split())
                next_words = set(next_q["message"].lower().split())
                
                if len(current_words) > 2 and len(next_words) > 2:
                    common_words = current_words.intersection(next_words)
                    similarity = len(common_words) / min(len(current_words), len(next_words))
                    
                    if similarity > 0.3:  # 30%ä»¥ä¸Šã®é¡ä¼¼æ€§
                        try:
                            time_diff = datetime.fromisoformat(next_q["timestamp"].replace('Z', '+00:00')) - \
                                       datetime.fromisoformat(current["timestamp"].replace('Z', '+00:00'))
                        except:
                            time_diff = "ä¸æ˜"
                        
                        repeat_questions.append({
                            "employee_id": employee_id,
                            "employee_name": current["employee_name"],
                            "first_question": current["message"][:100] + "..." if len(current["message"]) > 100 else current["message"],
                            "repeat_question": next_q["message"][:100] + "..." if len(next_q["message"]) > 100 else next_q["message"],
                            "time_between": str(time_diff),
                            "similarity_score": round(similarity, 2),
                            "first_sentiment": current["sentiment"],
                            "repeat_sentiment": next_q["sentiment"],
                            "category": current["category"]
                        })
                
                # æœªè§£æ±ºãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡ºï¼ˆçŸ­ã„å›ç­”ï¼‹ãƒã‚¬ãƒ†ã‚£ãƒ–æ„Ÿæƒ…ï¼‰
                if (current["response_length"] < 50 or 
                    current["sentiment"] == "negative" or
                    "ç”³ã—è¨³" in current["response"] or
                    "ã‚ã‹ã‚Šã¾ã›ã‚“" in current["response"]):
                    
                    unresolved_patterns.append({
                        "employee_id": employee_id,
                        "employee_name": current["employee_name"],
                        "question": current["message"][:100] + "..." if len(current["message"]) > 100 else current["message"],
                        "response": current["response"][:100] + "..." if len(current["response"]) > 100 else current["response"],
                        "timestamp": current["timestamp"],
                        "sentiment": current["sentiment"],
                        "category": current["category"],
                        "response_length": current["response_length"],
                        "issue_type": "short_response" if current["response_length"] < 50 else 
                                     "negative_sentiment" if current["sentiment"] == "negative" else
                                     "apologetic_response"
                    })
        
        # çµ±è¨ˆæƒ…å ±
        total_conversations = sum(len(questions) for questions in user_questions.values())
        repeat_rate = round((len(repeat_questions) / total_conversations * 100), 2) if total_conversations > 0 else 0
        unresolved_rate = round((len(unresolved_patterns) / total_conversations * 100), 2) if total_conversations > 0 else 0
        
        return {
            "repeat_questions": sorted(repeat_questions, key=lambda x: x["similarity_score"], reverse=True)[:20],
            "unresolved_patterns": sorted(unresolved_patterns, key=lambda x: x["timestamp"], reverse=True)[:20],
            "statistics": {
                "total_conversations": total_conversations,
                "repeat_questions_count": len(repeat_questions),
                "unresolved_patterns_count": len(unresolved_patterns),
                "repeat_rate": repeat_rate,
                "unresolved_rate": unresolved_rate
            },
            "summary": f"ç·ä¼šè©±{total_conversations}ä»¶ä¸­ã€å†è³ªå•{len(repeat_questions)}ä»¶({repeat_rate}%)ã€æœªè§£æ±ºãƒ‘ã‚¿ãƒ¼ãƒ³{len(unresolved_patterns)}ä»¶({unresolved_rate}%)ã‚’æ¤œå‡º"
        }
        
    except Exception as e:
        print(f"æœªè§£æ±ºãƒ»å†è³ªå•åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return {
            "repeat_questions": [],
            "unresolved_patterns": [],
            "summary": f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
        }

def get_detailed_sentiment_analysis(db, company_id: str = None) -> Dict[str, Any]:
    """è©³ç´°ãªæ„Ÿæƒ…åˆ†æ"""
    try:
        # æ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        base_query = """
        SELECT 
            sentiment,
            category,
            COUNT(*) as count,
            COUNT(DISTINCT employee_id) as unique_users,
            AVG(LENGTH(user_message)) as avg_question_length,
            AVG(LENGTH(bot_response)) as avg_response_length,
            DATE(timestamp) as date
        FROM chat_history
        WHERE sentiment IS NOT NULL
        """
        
        if company_id:
            base_query += f" AND company_id = '{company_id}'"
        
        base_query += """
        GROUP BY sentiment, category, DATE(timestamp)
        ORDER BY date DESC, count DESC
        """
        
        result = execute_query(base_query)
        
        if not result:
            return {
                "sentiment_distribution": {},
                "sentiment_by_category": {},
                "temporal_sentiment": [],
                "summary": "æ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™"
            }
        
        # æ„Ÿæƒ…åˆ†å¸ƒ
        sentiment_distribution = {}
        sentiment_by_category = {}
        temporal_sentiment = {}
        
        for row in result:
            sentiment = safe_str(row.get("sentiment", "neutral"))
            category = safe_str(row.get("category", "ãã®ä»–"))
            date = safe_str(row.get("date", ""))
            count = safe_int(row.get("count", 0))
            
            # å…¨ä½“ã®æ„Ÿæƒ…åˆ†å¸ƒ
            if sentiment not in sentiment_distribution:
                sentiment_distribution[sentiment] = 0
            sentiment_distribution[sentiment] += count
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥æ„Ÿæƒ…åˆ†å¸ƒ
            if category not in sentiment_by_category:
                sentiment_by_category[category] = {}
            if sentiment not in sentiment_by_category[category]:
                sentiment_by_category[category][sentiment] = 0
            sentiment_by_category[category][sentiment] += count
            
            # æ™‚ç³»åˆ—æ„Ÿæƒ…æ¨ç§»
            if date not in temporal_sentiment:
                temporal_sentiment[date] = {}
            if sentiment not in temporal_sentiment[date]:
                temporal_sentiment[date][sentiment] = 0
            temporal_sentiment[date][sentiment] += count
        
        # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢è¨ˆç®—
        total_responses = sum(sentiment_distribution.values())
        sentiment_score = 0
        if total_responses > 0:
            sentiment_score = (
                sentiment_distribution.get("positive", 0) * 1 +
                sentiment_distribution.get("neutral", 0) * 0.5 +
                sentiment_distribution.get("negative", 0) * 0
            ) / total_responses
        
        return {
            "sentiment_distribution": sentiment_distribution,
            "sentiment_by_category": sentiment_by_category,
            "temporal_sentiment": [
                {"date": date, "sentiments": sentiments}
                for date, sentiments in sorted(temporal_sentiment.items())
            ],
            "overall_sentiment_score": round(sentiment_score, 3),
            "total_responses": total_responses,
            "summary": f"ç·å›ç­”{total_responses}ä»¶ã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢: {round(sentiment_score * 100, 1)}ç‚¹/100ç‚¹"
        }
        
    except Exception as e:
        print(f"è©³ç´°æ„Ÿæƒ…åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return {
            "sentiment_distribution": {},
            "sentiment_by_category": {},
            "temporal_sentiment": [],
            "summary": f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
        }

async def generate_gemini_insights(analytics_data: Dict[str, Any], db, company_id: str = None) -> str:
    """Geminiã‚’ä½¿ç”¨ã—ã¦åˆ†æãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ´å¯Ÿã‚’ç”Ÿæˆï¼ˆå…¨ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’å«ã‚€ï¼‰"""
    try:
        # Geminiè¨­å®šã‚’ç¢ºèª
        from modules.config import setup_gemini
        model = setup_gemini()
        
        if not model:
            return "Gemini APIãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åŸºæœ¬çš„ãªçµ±è¨ˆåˆ†æã®ã¿è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚"
        
        # å…¨ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’å–å¾—ï¼ˆå¤§å¹…ã«å¢—é‡ï¼‰
        chat_history_query = """
        SELECT 
            employee_name,
            user_message,
            bot_response,
            timestamp,
            sentiment,
            category,
            company_name
        FROM chat_history
        WHERE 1=1
        """
        
        if company_id:
            chat_history_query += f" AND company_id = '{company_id}'"
        
        chat_history_query += " ORDER BY timestamp DESC LIMIT 2000"  # æœ€æ–°2000ä»¶ã‚’å–å¾—ï¼ˆå¤§å¹…å¢—é‡ï¼‰
        
        chat_history = execute_query(chat_history_query)
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã«å¤‰æ›ï¼ˆã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã‚‹ï¼‰
        chat_history_text = ""
        recent_patterns = []
        if chat_history:
            # æœ€æ–°300ä»¶ã®è©³ç´°ãªã‚„ã‚Šå–ã‚Šã‚’åˆ†æç”¨ã«æ•´å½¢
            for i, row in enumerate(chat_history[:300]):
                timestamp = safe_str(row.get('timestamp', ''))
                employee_name = safe_str(row.get('employee_name', 'åŒ¿å'))
                sentiment = safe_str(row.get('sentiment', 'neutral'))
                user_message = safe_str(row.get('user_message', ''))
                bot_response = safe_str(row.get('bot_response', ''))
                category = safe_str(row.get('category', 'ãªã—'))
                
                # ã‚ˆã‚Šè©³ç´°ãªãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
                recent_patterns.append({
                    'index': i + 1,
                    'employee': employee_name,
                    'sentiment': sentiment,
                    'question': user_message[:300],  # æ–‡å­—æ•°åˆ¶é™ã‚’ç·©å’Œ
                    'response': bot_response[:300],  # æ–‡å­—æ•°åˆ¶é™ã‚’ç·©å’Œ
                    'category': category,
                    'timestamp': timestamp[:10]  # æ—¥ä»˜ã®ã¿
                })
            
            # ã‚ˆã‚Šæ§‹é€ åŒ–ã•ã‚ŒãŸãƒãƒ£ãƒƒãƒˆå±¥æ­´ãƒ†ã‚­ã‚¹ãƒˆ
            chat_history_text = "### æœ€æ–°ã®ã‚„ã‚Šå–ã‚Šãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæœ€æ–°300ä»¶ï¼‰\n\n"
            for pattern in recent_patterns[:50]:  # ä»£è¡¨çš„ãª50ä»¶ã‚’è©³ç´°è¡¨ç¤º
                chat_history_text += f"**{pattern['index']}. [{pattern['timestamp']}] {pattern['employee']} ({pattern['sentiment']})**\n"
                chat_history_text += f"è³ªå•: {pattern['question']}\n"
                chat_history_text += f"å›ç­”: {pattern['response']}\n"
                chat_history_text += f"ã‚«ãƒ†ã‚´ãƒª: {pattern['category']}\n\n"
        
        # ã‚ˆã‚Šè©³ç´°ã§å®Ÿç”¨çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
        prompt = f"""
ã‚ãªãŸã¯ä¼æ¥­ã®AIãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆåˆ†æå°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰6ã¤ã®åˆ†æé …ç›®ã«åˆ†ã‘ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚

# ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦
- ç·ãƒãƒ£ãƒƒãƒˆå±¥æ­´æ•°: {len(chat_history) if chat_history else 0}ä»¶
- ç·å‚ç…§å›æ•°: {analytics_data.get('resource_reference_count', {}).get('total_references', 0)}å›
- ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹: {analytics_data.get('resource_reference_count', {}).get('active_resources', 0)}å€‹
- ç·è³ªå•æ•°: {analytics_data.get('category_distribution_analysis', {}).get('total_questions', 0)}ä»¶
- ã‚«ãƒ†ã‚´ãƒªæ•°: {analytics_data.get('category_distribution_analysis', {}).get('category_diversity', 0)}ç¨®é¡
- å†è³ªå•ç‡: {analytics_data.get('unresolved_and_repeat_analysis', {}).get('statistics', {}).get('repeat_rate', 0)}%
- æœªè§£æ±ºç‡: {analytics_data.get('unresolved_and_repeat_analysis', {}).get('statistics', {}).get('unresolved_rate', 0)}%
- æ„Ÿæƒ…ã‚¹ã‚³ã‚¢: {analytics_data.get('sentiment_analysis', {}).get('overall_sentiment_score', 0) * 100:.1f}ç‚¹/100ç‚¹

{chat_history_text}

# ğŸ¯ åˆ†æè¦æ±‚
ä»¥ä¸‹ã®6é …ç›®ã«ã¤ã„ã¦ã€å¿…ãšé …ç›®ç•ªå·ã¨è¦‹å‡ºã—ã‚’æ˜è¨˜ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š

**1. åˆ©ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ**
- æœ€ã‚‚å¤šã„è³ªå•ã‚«ãƒ†ã‚´ãƒªä¸Šä½3ã¤ã¨ãã®ä»¶æ•°ã‚’æ˜è¨˜
- åˆ©ç”¨é »åº¦ã®é«˜ã„æ™‚é–“å¸¯ã‚„æ›œæ—¥ãƒ‘ã‚¿ãƒ¼ãƒ³
- ãƒ¦ãƒ¼ã‚¶ãƒ¼1äººã‚ãŸã‚Šã®å¹³å‡è³ªå•æ•°

**2. å“è³ªèª²é¡Œã®ç‰¹å®š**
- å†è³ªå•ç‡{analytics_data.get('unresolved_and_repeat_analysis', {}).get('statistics', {}).get('repeat_rate', 0)}%ã®å…·ä½“çš„åŸå› 
- æœªè§£æ±ºç‡{analytics_data.get('unresolved_and_repeat_analysis', {}).get('statistics', {}).get('unresolved_rate', 0)}%ã®æ”¹å–„å¿…è¦é ˜åŸŸ
- ãƒã‚¬ãƒ†ã‚£ãƒ–æ„Ÿæƒ…ã®å…·ä½“çš„ãªä»¶æ•°ã¨åŸå› 

**3. å„ªå…ˆæ”¹å–„é …ç›®**
- æœ€å„ªå…ˆã§æ”¹å–„ã™ã¹ãé …ç›®ã‚’3ã¤æŒ™ã’ã€ãã‚Œãã‚Œã®æ”¹å–„ã«ã‚ˆã‚Šå‰Šæ¸›ã§ãã‚‹è³ªå•æ•°ã‚’æ¨å®š
- å„æ”¹å–„é …ç›®ã®å®Ÿè£…é›£æ˜“åº¦ï¼ˆé«˜/ä¸­/ä½ï¼‰
- ROIé †ã§ã®å„ªå…ˆé †ä½

**4. ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤è©•ä¾¡**
- ç¾åœ¨ã®æ™‚é–“å‰Šæ¸›åŠ¹æœã‚’æ™‚é–“æ•°ã§æ¨å®š
- æœˆé–“ã‚³ã‚¹ãƒˆå‰Šæ¸›åŠ¹æœã‚’é‡‘é¡ã§æ¨å®š
- æº€è¶³åº¦å‘ä¸Šã«ã‚ˆã‚‹å®šé‡çš„åŠ¹æœ

**5. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æˆ¦ç•¥**
- è¿½åŠ ã™ã¹ãè³‡æ–™ã®ç¨®é¡ã‚’å…·ä½“çš„ã«3ã¤
- FAQåŒ–ã™ã¹ãè³ªå•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å…·ä½“ä¾‹ã‚’ä»¶æ•°ä»˜ãã§
- æ—¢å­˜è³‡æ–™ã®æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ

**6. å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—**
- çŸ­æœŸï¼ˆ1ãƒ¶æœˆä»¥å†…ï¼‰ï¼šå®Ÿè¡Œå¯èƒ½ãªæ”¹å–„é …ç›®ã‚’3ã¤
- ä¸­æœŸï¼ˆ3-6ãƒ¶æœˆï¼‰ï¼šæ©Ÿèƒ½æ‹¡å¼µé …ç›®ã‚’2ã¤
- é•·æœŸï¼ˆ6ãƒ¶æœˆ-1å¹´ï¼‰ï¼šã‚·ã‚¹ãƒ†ãƒ ç™ºå±•é …ç›®ã‚’1ã¤

å„é …ç›®ã¯150æ–‡å­—ç¨‹åº¦ã§ã€å¿…ãšå…·ä½“çš„ãªæ•°å€¤ã‚’å«ã‚ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚é …ç›®ã”ã¨ã«æ˜ç¢ºã«åˆ†ã‘ã¦è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
"""
        
        # Geminiã§åˆ†æ
        response = model.generate_content(prompt)
        
        if response and hasattr(response, 'text') and response.text:
            return response.text.strip()
        else:
            return "AIåˆ†æã‚’å®Ÿè¡Œã—ã¾ã—ãŸãŒã€çµæœã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            
    except Exception as e:
        print(f"Geminiæ´å¯Ÿç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return f"AIåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

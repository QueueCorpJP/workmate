"""
ãƒãƒ£ãƒƒãƒˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
å„ç¨®æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ã‚’ç®¡ç†ã—ã¾ã™
"""
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from .chat_config import (
    safe_print, HTTPException, get_db_cursor,
    DIRECT_SEARCH_AVAILABLE, PARALLEL_SEARCH_AVAILABLE, 
    ENHANCED_JAPANESE_SEARCH_AVAILABLE, VECTOR_SEARCH_AVAILABLE,
    direct_search, parallel_search, enhanced_japanese_search, vector_search
)
from .elasticsearch_search import (
    get_elasticsearch_fuzzy_search, elasticsearch_available
)
from .postgresql_fuzzy_search import fuzzy_search_chunks

async def direct_search_system(query: str, limit: int = 10) -> List[Dict[str, Any]]:
    """
    ç›´æ¥æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
    """
    if not DIRECT_SEARCH_AVAILABLE:
        safe_print("Direct search is not available")
        return []
    
    try:
        results = await direct_search(query, limit)
        safe_print(f"Direct search returned {len(results)} results")
        return results
    except Exception as e:
        safe_print(f"Error in direct search: {e}")
        return []

async def parallel_search_system(query: str, limit: int = 10) -> List[Dict[str, Any]]:
    """
    ä¸¦åˆ—æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
    """
    if not PARALLEL_SEARCH_AVAILABLE:
        safe_print("Parallel search is not available")
        return []
    
    try:
        results = await parallel_search(query, limit)
        safe_print(f"Parallel search returned {len(results)} results")
        return results
    except Exception as e:
        safe_print(f"Error in parallel search: {e}")
        return []

async def enhanced_japanese_search_system(query: str, limit: int = 10) -> List[Dict[str, Any]]:
    """
    æ‹¡å¼µæ—¥æœ¬èªæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
    """
    if not ENHANCED_JAPANESE_SEARCH_AVAILABLE:
        safe_print("Enhanced Japanese search is not available")
        return []
    
    try:
        results = await enhanced_japanese_search(query, limit)
        safe_print(f"Enhanced Japanese search returned {len(results)} results")
        return results
    except Exception as e:
        safe_print(f"Error in enhanced Japanese search: {e}")
        return []

async def vector_search_system(query: str, limit: int = 10) -> List[Dict[str, Any]]:
    """
    ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
    """
    if not VECTOR_SEARCH_AVAILABLE:
        safe_print("Vector search is not available")
        return []
    
    try:
        results = await vector_search(query, limit)
        safe_print(f"Vector search returned {len(results)} results")
        return results
    except Exception as e:
        safe_print(f"Error in vector search: {e}")
        return []

async def elasticsearch_fuzzy_search_system(query: str, 
                                           company_id: str = None,
                                           fuzziness: str = "AUTO",
                                           limit: int = 10) -> List[Dict[str, Any]]:
    """
    Elasticsearch Fuzzyæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
    """
    if not elasticsearch_available():
        safe_print("Elasticsearch is not available")
        return []
    
    try:
        es_search = get_elasticsearch_fuzzy_search()
        if not es_search:
            safe_print("Elasticsearch fuzzy search not initialized")
            return []
        
        results = await es_search.fuzzy_search(
            query=query,
            company_id=company_id,
            fuzziness=fuzziness,
            limit=limit
        )
        safe_print(f"Elasticsearch fuzzy search returned {len(results)} results")
        return results
    except Exception as e:
        safe_print(f"Error in Elasticsearch fuzzy search: {e}")
        return []

async def elasticsearch_advanced_search_system(query: str,
                                              company_id: str = None,
                                              search_type: str = "multi_match",
                                              fuzziness: str = "AUTO",
                                              limit: int = 10) -> List[Dict[str, Any]]:
    """
    Elasticsearché«˜åº¦æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
    """
    if not elasticsearch_available():
        safe_print("Elasticsearch is not available")
        return []
    
    try:
        es_search = get_elasticsearch_fuzzy_search()
        if not es_search:
            safe_print("Elasticsearch fuzzy search not initialized")
            return []
        
        results = await es_search.advanced_search(
            query=query,
            company_id=company_id,
            search_type=search_type,
            fuzziness=fuzziness,
            limit=limit
        )
        safe_print(f"Elasticsearch advanced search returned {len(results)} results")
        return results
    except Exception as e:
        safe_print(f"Error in Elasticsearch advanced search: {e}")
        return []

async def postgresql_fuzzy_search_system(query: str, limit: int = 10) -> List[Dict[str, Any]]:
    """
    PostgreSQL Fuzzyæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ï¼ˆåˆ¥ã‚µãƒ¼ãƒãƒ¼ä¸è¦ï¼ï¼‰
    """
    try:
        results = await fuzzy_search_chunks(query, limit)
        formatted_results = []
        
        for result in results:
            formatted_results.append({
                'id': result['chunk_id'],
                'title': result['file_name'],
                'content': result['content'],
                'url': '',
                'similarity': result['score'],
                'metadata': {
                    'source': 'postgresql_fuzzy_search',
                    'search_types': result.get('search_types', []),
                    'highlight': result.get('highlight', '')
                }
            })
            
        safe_print(f"PostgreSQL Fuzzy search returned {len(formatted_results)} results")
        return formatted_results
        
    except Exception as e:
        safe_print(f"Error in PostgreSQL Fuzzy search: {e}")
        return []

async def fallback_search_system(query: str, limit: int = 10) -> List[Dict[str, Any]]:
    """
    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  - è¤‡æ•°ã®æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’é †æ¬¡è©¦è¡Œ
    """
    search_systems = [
        ("PostgreSQL Fuzzy Search", postgresql_fuzzy_search_system),
        ("Elasticsearch Fuzzy Search", lambda q, l: elasticsearch_fuzzy_search_system(q, None, "AUTO", l)),
        ("Enhanced Japanese Search", enhanced_japanese_search_system),
        ("Vector Search", vector_search_system),
        ("Parallel Search", parallel_search_system),
        ("Direct Search", direct_search_system),
        ("Database Fallback Search", database_search_fallback),
    ]
    
    for system_name, search_func in search_systems:
        try:
            safe_print(f"Trying {system_name}...")
            results = await search_func(query, limit)
            if results:
                safe_print(f"{system_name} succeeded with {len(results)} results")
                return results
            else:
                safe_print(f"{system_name} returned no results")
        except Exception as e:
            safe_print(f"{system_name} failed: {e}")
            continue
    
    safe_print("All search systems failed")
    return []

async def multi_system_search(query: str, limit: int = 10) -> List[Dict[str, Any]]:
    """
    è¤‡æ•°ã‚·ã‚¹ãƒ†ãƒ æ¤œç´¢ - è¤‡æ•°ã®æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’ä¸¦åˆ—å®Ÿè¡Œã—ã¦çµæœã‚’ãƒãƒ¼ã‚¸
    """
    search_tasks = []
    
    # PostgreSQL Fuzzy Searchï¼ˆå¸¸ã«åˆ©ç”¨å¯èƒ½ï¼‰
    search_tasks.append(postgresql_fuzzy_search_system(query, limit))
    
    if elasticsearch_available():
        search_tasks.append(elasticsearch_fuzzy_search_system(query, None, "AUTO", limit))
    if ENHANCED_JAPANESE_SEARCH_AVAILABLE:
        search_tasks.append(enhanced_japanese_search_system(query, limit))
    if VECTOR_SEARCH_AVAILABLE:
        search_tasks.append(vector_search_system(query, limit))
    if PARALLEL_SEARCH_AVAILABLE:
        search_tasks.append(parallel_search_system(query, limit))
    if DIRECT_SEARCH_AVAILABLE:
        search_tasks.append(direct_search_system(query, limit))
    
    if not search_tasks:
        safe_print("No search systems available")
        return []
    
    try:
        # ä¸¦åˆ—å®Ÿè¡Œ
        results_list = await asyncio.gather(*search_tasks, return_exceptions=True)
        
        # çµæœã‚’ãƒãƒ¼ã‚¸ï¼ˆé‡è¤‡é™¤å»ï¼‰
        merged_results = []
        seen_ids = set()
        
        for results in results_list:
            if isinstance(results, Exception):
                safe_print(f"Search system error: {results}")
                continue
            
            if isinstance(results, list):
                for result in results:
                    if isinstance(result, dict) and 'id' in result:
                        if result['id'] not in seen_ids:
                            seen_ids.add(result['id'])
                            merged_results.append(result)
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆï¼ˆã‚¹ã‚³ã‚¢ãŒã‚ã‚‹å ´åˆï¼‰
        merged_results.sort(key=lambda x: x.get('score', 0), reverse=True)
        
        # åˆ¶é™æ•°ã¾ã§åˆ‡ã‚Šè©°ã‚
        final_results = merged_results[:limit]
        safe_print(f"Multi-system search returned {len(final_results)} unique results")
        
        return final_results
        
    except Exception as e:
        safe_print(f"Error in multi-system search: {e}")
        return await fallback_search_system(query, limit)

async def database_search_fallback(query: str, limit: int = 10) -> List[Dict[str, Any]]:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç›´æ¥æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆé‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºå¯¾å¿œï¼‰
    """
    try:
        cursor = get_db_cursor()
        if not cursor:
            safe_print("Database cursor not available")
            return []
        
        # ğŸ”„ ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚’çµ„ã¿è¾¼ã¿
        try:
            from .question_variants_generator import generate_question_variants
            safe_print(f"Generating query variants for: {query}")
            variants_result = await generate_question_variants(query)
            search_terms = variants_result.all_variants
            safe_print(f"Generated {len(search_terms)} query variants")
        except Exception as e:
            safe_print(f"Variant generation failed, using original query: {e}")
            search_terms = [query]
        
        # ğŸ¯ é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º: é•·ã„æ–‡ç« ã‹ã‚‰é‡è¦ãªå˜èªã®ã¿ã‚’æŠ½å‡º
        important_keywords = []
        
        # å…ƒã®ã‚¯ã‚¨ãƒªã‹ã‚‰é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        query_keywords = extract_important_keywords(query)
        important_keywords.extend(query_keywords)
        
        # ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰ã‚‚é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆé•·ã„æ–‡ç« ã¯é™¤å¤–ï¼‰
        for term in search_terms:
            if len(term) <= 10:  # 10æ–‡å­—ä»¥ä¸‹ã®çŸ­ã„èªå¥ã®ã¿ä½¿ç”¨
                important_keywords.append(term)
            else:
                # é•·ã„æ–‡ç« ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
                term_keywords = extract_important_keywords(term)
                important_keywords.extend(term_keywords)
        
        # é‡è¤‡ã‚’é™¤å»ã—ã€æœ€å¤§10å€‹ã«åˆ¶é™
        important_keywords = list(set(important_keywords))[:10]
        safe_print(f"Extracted important keywords: {important_keywords}")
        
        if not important_keywords:
            important_keywords = [query]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        # ğŸ” é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚ˆã‚‹ã‚¹ãƒãƒ¼ãƒˆã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°æ¤œç´¢
        # å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ORæ¡ä»¶ã§æ¤œç´¢ã—ã€è¤‡æ•°ãƒãƒƒãƒã«ãƒœãƒ¼ãƒŠã‚¹ã‚’ä»˜ä¸
        keyword_conditions = []
        parameters = []
        
        for i, keyword in enumerate(important_keywords):
            keyword_conditions.append(f"c.content ILIKE %s")
            parameters.append(f"%{keyword}%")
        
        # SQLæ–‡ã‚’æ§‹ç¯‰
        sql_query = f"""
            SELECT 
                c.id,
                ds.name as title,
                c.content,
                '' as url,
                -- ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°: è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒã«ãƒœãƒ¼ãƒŠã‚¹
               CASE 
                    {' + '.join([f"WHEN c.content ILIKE %s THEN 1.0" for _ in important_keywords])}
                    ELSE 0.0
               END as rank
        FROM chunks c
        LEFT JOIN document_sources ds ON c.doc_id = ds.id
        WHERE c.content IS NOT NULL 
          AND LENGTH(c.content) > 10
              AND ({' OR '.join(keyword_conditions)})
        ORDER BY rank DESC, LENGTH(c.content) DESC
        LIMIT %s
        """
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰ï¼ˆã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ç”¨ + æ¡ä»¶ç”¨ + LIMITç”¨ï¼‰
        all_parameters = []
        # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        for keyword in important_keywords:
            all_parameters.append(f"%{keyword}%")
        # æ¡ä»¶ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        all_parameters.extend(parameters)
        # LIMITç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        all_parameters.append(limit)
        
        safe_print(f"Executing SQL search with {len(important_keywords)} keywords")
        cursor.execute(sql_query, all_parameters)
        
        results = []
        for row in cursor.fetchall():
            results.append({
                'id': row[0],
                'title': row[1] or 'Unknown Document',
                'content': row[2] or '',
                'url': row[3] or '',
                'similarity': float(row[4]) if row[4] else 0.0,
                'metadata': {
                    'source': 'database_search_fallback',
                    'keywords': important_keywords
                }
            })
        
        safe_print(f"Database search found {len(results)} results")
        return results
        
    except Exception as e:
        safe_print(f"Database search error: {e}")
        return []

def extract_important_keywords(text: str) -> List[str]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
    """
    import re
    
    # åŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    keywords = []
    
    # åè©çš„ãªå˜èªã‚’æŠ½å‡ºï¼ˆæ—¥æœ¬èªã®å ´åˆï¼‰
    # ã‚«ã‚¿ã‚«ãƒŠèªï¼ˆ3æ–‡å­—ä»¥ä¸Šï¼‰
    katakana_words = re.findall(r'[ã‚¡-ãƒ¶ãƒ¼]{3,}', text)
    keywords.extend(katakana_words)
    
    # æ¼¢å­—ã‚’å«ã‚€å˜èªï¼ˆ2æ–‡å­—ä»¥ä¸Šï¼‰
    kanji_words = re.findall(r'[ä¸€-é¾ ]{2,}', text)
    keywords.extend(kanji_words)
    
    # ã²ã‚‰ãŒãªï¼ˆç‰¹å®šã®é‡è¦èªï¼‰
    important_hiragana = ['ã‚„ã™ã„', 'ãŸã‹ã„', 'ãŠãŠãã„', 'ã¡ã„ã•ã„', 'ã‚ãŸã‚‰ã—ã„', 'ãµã‚‹ã„']
    for word in important_hiragana:
        if word in text:
            keywords.append(word)
    
    # ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆï¼ˆ2æ–‡å­—ä»¥ä¸Šï¼‰
    alphabet_words = re.findall(r'[a-zA-Z]{2,}', text)
    keywords.extend(alphabet_words)
    
    # æ•°å­—ã‚’å«ã‚€èª
    number_words = re.findall(r'[0-9]+[å††ä¸‡åƒç™¾åå„„å…†å°å€‹ä»¶åäºº]', text)
    keywords.extend(number_words)
    
    # ç‰¹åˆ¥ãªèªå½™
    special_words = ['å®‰ã„', 'ãƒ‘ã‚½ã‚³ãƒ³', 'PC', 'ä¾¡æ ¼', 'å€¤æ®µ', 'æ–™é‡‘', 'è²»ç”¨', 'ã‚³ã‚¹ãƒˆ']
    for word in special_words:
        if word in text:
            keywords.append(word)
    
    # é‡è¤‡ã‚’é™¤å»ã—ã€ç©ºæ–‡å­—åˆ—ã‚’é™¤å¤–
    keywords = list(set([k for k in keywords if k.strip()]))
    
    return keywords[:5]  # æœ€å¤§5å€‹

async def smart_search_system(query: str, limit: int = 10) -> List[Dict[str, Any]]:
    """
    ã‚¹ãƒãƒ¼ãƒˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  - ã‚¯ã‚¨ãƒªã®ç‰¹æ€§ã«å¿œã˜ã¦æœ€é©ãªæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’é¸æŠ
    """
    safe_print(f"Starting smart search for query: {query}")

    # æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    has_japanese = any('\u3040' <= char <= '\u309F' or '\u30A0' <= char <= '\u30FF' or '\u4E00' <= char <= '\u9FAF' for char in query)
    
    # ã‚¯ã‚¨ãƒªã®é•·ã•ã‚’ãƒã‚§ãƒƒã‚¯
    is_long_query = len(query) > 50
    
    # æŠ€è¡“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    technical_keywords = ['API', 'SQL', 'Python', 'JavaScript', 'HTML', 'CSS', 'JSON', 'XML', 'HTTP', 'HTTPS']
    has_technical_terms = any(keyword.lower() in query.lower() for keyword in technical_keywords)
    
    safe_print(f"Query analysis: Japanese={has_japanese}, Long={is_long_query}, Technical={has_technical_terms}")
    
    # æœ€é©ãªæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’é¸æŠ
    if has_japanese and ENHANCED_JAPANESE_SEARCH_AVAILABLE:
        safe_print("Using Enhanced Japanese Search for Japanese query")
        return await enhanced_japanese_search_system(query, limit)
    elif is_long_query and VECTOR_SEARCH_AVAILABLE:
        safe_print("Using Vector Search for long query")
        return await vector_search_system(query, limit)
    elif has_technical_terms and PARALLEL_SEARCH_AVAILABLE:
        safe_print("Using Parallel Search for technical query")
        return await parallel_search_system(query, limit)
    else:
        safe_print("Using multi-system search as default")
        return await multi_system_search(query, limit)
"""
è³ªå•åˆ†å‰²ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
é•·ã„è³ªå•ã‚’è¤‡æ•°ã®å°ã•ãªè³ªå•ã«åˆ†å‰²ã—ã¦å‡¦ç†åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹
"""

import re
import logging
from typing import List, Dict, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class QuestionSegment:
    """åˆ†å‰²ã•ã‚ŒãŸè³ªå•ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ"""
    text: str
    priority: int  # 1-5 (1ãŒæœ€é‡è¦)
    category: str  # 'main', 'detail', 'example', 'follow_up'
    keywords: List[str]

class QuestionSplitter:
    """è³ªå•åˆ†å‰²å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        # è³ªå•åŒºåˆ‡ã‚Šã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.split_patterns = [
            r'ã€‚\s*(?=[ã¾ï½¢ã¾ãŸï½£|ï½¢ã•ã‚‰ã«ï½£|ï½¢åŠ ãˆã¦ï½£|ï½¢æ¬¡ã«ï½£|ï½¢ãã‚Œã‹ã‚‰ï½£])',  # æ¥ç¶šè©ã§åŒºåˆ‡ã‚Š
            r'ã€‚\s*(?=\d+\.)',  # ç•ªå·ä»˜ããƒªã‚¹ãƒˆã§åŒºåˆ‡ã‚Š
            r'ã€‚\s*(?=[ï¼ˆ(]\d+[ï¼‰)])',  # ç•ªå·ä»˜ãæ‹¬å¼§ã§åŒºåˆ‡ã‚Š
            r'ã€‚\s*(?=[â€¢ãƒ»â–¶])',  # ç®‡æ¡æ›¸ãã§åŒºåˆ‡ã‚Š
            r'\?\s*(?=[ã¾ï½¢ã¾ãŸï½£|ï½¢ã•ã‚‰ã«ï½£|ï½¢åŠ ãˆã¦ï½£|ï½¢æ¬¡ã«ï½£])',  # ç–‘å•ç¬¦å¾Œã®æ¥ç¶šè©
        ]
        
        # é‡è¦åº¦åˆ¤å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.priority_keywords = {
            1: ['æœ€é‡è¦', 'ç·Šæ€¥', 'å¿…é ˆ', 'å¿…è¦', 'ã¾ãš', 'ç¬¬ä¸€'],
            2: ['é‡è¦', 'ä¸»è¦', 'åŸºæœ¬', 'æ¦‚è¦'],
            3: ['è©³ç´°', 'å…·ä½“çš„', 'ä¾‹ãˆã°', 'è©³ã—ã'],
            4: ['è£œè¶³', 'è¿½åŠ ', 'ã¾ãŸ', 'ã•ã‚‰ã«'],
            5: ['å‚è€ƒ', 'ä½™è«‡', 'ã¡ãªã¿ã«', 'ãŠã¾ã‘']
        }
    
    def should_split_question(self, question: str) -> bool:
        """è³ªå•ã‚’åˆ†å‰²ã™ã¹ãã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # è¤‡æ•°ã‚¿ã‚¹ã‚¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚ˆã‚‹åˆ¤å®šï¼ˆå„ªå…ˆï¼‰
        multi_task_keywords = [
            'WPD', 'WPN',  # ç‰©ä»¶ç•ªå·ãŒè¤‡æ•°ã‚ã‚‹å ´åˆ
            'ã«ã¤ã„ã¦', 'ã«é–¢ã—ã¦', 'ã¨', 'ãŠã‚ˆã³', 'ãªã‚‰ã³ã«',
            'ã¾ãŸ', 'ã•ã‚‰ã«', 'æ¬¡ã«', 'ä»–ã«', 'ã‚ã¨', 'ãã‚Œã‹ã‚‰', 'ãã—ã¦',
            '1.', '2.', '3.', 'â‘ ', 'â‘¡', 'â‘¢', 'ãƒ»', 'â€¢',
            'ã¾ãš', 'æœ€åˆã«', 'ç¶šã„ã¦', 'æœ€å¾Œã«'
        ]
        
        # è¤‡æ•°ã®è³ªå•ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        question_marks = question.count('ï¼Ÿ') + question.count('?')
        
        # è¤‡æ•°ã‚¿ã‚¹ã‚¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        keyword_count = sum(question.count(keyword) for keyword in multi_task_keywords)
        
        # ç‰©ä»¶ç•ªå·ãŒè¤‡æ•°ã‚ã‚‹å ´åˆï¼ˆWPDxxxxxxã€WPNxxxxxxãŒè¤‡æ•°ï¼‰
        import re
        property_numbers = re.findall(r'WP[DN]\d{7}', question)
        
        # åˆ†å‰²æ¡ä»¶ã‚’ç·©å’Œï¼ˆã‚ˆã‚Šå¤šãã®è¤‡æ•°ã‚¿ã‚¹ã‚¯ã‚’æ¤œå‡ºï¼‰
        split_conditions = [
            question_marks >= 2,  # 2ã¤ä»¥ä¸Šã®ç–‘å•ç¬¦
            keyword_count >= 3,   # 3ã¤ä»¥ä¸Šã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            len(property_numbers) >= 2,  # è¤‡æ•°ã®ç‰©ä»¶ç•ªå·
            len(question) > 1500,  # é•·ã„è³ªå•ï¼ˆ3000ã‹ã‚‰1500ã«ç·©å’Œï¼‰
            'ã€' in question and len(question) > 200  # èª­ç‚¹ãŒã‚ã‚Š200æ–‡å­—ä»¥ä¸Š
        ]
        
        should_split = any(split_conditions)
        if should_split:
            logger.info(f"ğŸ¯ è¤‡æ•°ã‚¿ã‚¹ã‚¯æ¤œå‡º: ç–‘å•ç¬¦{question_marks}å€‹, ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰{keyword_count}å€‹, ç‰©ä»¶{len(property_numbers)}å€‹")
        
        return should_split
    
    def split_question(self, question: str) -> List[QuestionSegment]:
        """è³ªå•ã‚’è¤‡æ•°ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«åˆ†å‰²"""
        logger.info(f"è³ªå•åˆ†å‰²é–‹å§‹: {len(question)}æ–‡å­—")
        
        if not self.should_split_question(question):
            logger.info("åˆ†å‰²ä¸è¦ã¨åˆ¤å®š")
            return [QuestionSegment(
                text=question,
                priority=1,
                category='main',
                keywords=self._extract_keywords(question)
            )]
        
        segments = []
        
        # æ±ç”¨çš„ãªè¤‡æ•°ã‚¿ã‚¹ã‚¯åˆ†å‰²ï¼ˆå„ªå…ˆé †ä½é †ï¼‰
        import re
        
        # 1. æ˜ç¢ºãªåŒºåˆ‡ã‚Šæ–‡å­—ã«ã‚ˆã‚‹åˆ†å‰²
        clear_separators = [
            r'(ã€‚\s*ã¾ãŸ)',  # ã€Œã€‚ã¾ãŸã€
            r'(ã€‚\s*ã•ã‚‰ã«)',  # ã€Œã€‚ã•ã‚‰ã«ã€  
            r'(ã€‚\s*æ¬¡ã«)',  # ã€Œã€‚æ¬¡ã«ã€
            r'(ã€‚\s*ã‚ã¨)',  # ã€Œã€‚ã‚ã¨ã€
            r'(\d+\.\s*)',  # 1. 2. 3. ã®ç•ªå·ä»˜ããƒªã‚¹ãƒˆ
            r'([â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©])',  # ä¸¸æ•°å­—
            r'(ãƒ»\s*)',  # ç®‡æ¡æ›¸ã
        ]
        
        for pattern in clear_separators:
            if re.search(pattern, question):
                parts = re.split(pattern, question)
                segments.extend(self._process_enhanced_split_parts(parts, pattern))
                break
        
        # 2. æ¥ç¶šè©ã«ã‚ˆã‚‹åˆ†å‰²ï¼ˆæ˜ç¢ºãªåŒºåˆ‡ã‚ŠãŒãªã„å ´åˆï¼‰
        if not segments:
            connector_patterns = [
                r'(ã¾ãŸã€)',  # ã€Œã¾ãŸã€ã€
                r'(ã•ã‚‰ã«ã€)',  # ã€Œã•ã‚‰ã«ã€ã€
                r'(ãã‚Œã‹ã‚‰ã€)',  # ã€Œãã‚Œã‹ã‚‰ã€ã€
                r'(ã‚ã¨ã€)',  # ã€Œã‚ã¨ã€ã€
                r'(åŠ ãˆã¦ã€)',  # ã€ŒåŠ ãˆã¦ã€ã€
                r'(ç¶šã„ã¦ã€)',  # ã€Œç¶šã„ã¦ã€ã€
            ]
            
            for pattern in connector_patterns:
                if re.search(pattern, question):
                    parts = re.split(pattern, question)
                    segments.extend(self._process_enhanced_split_parts(parts, pattern))
                    break
        
        # 3. ç‰©ä»¶ç•ªå·ã«ã‚ˆã‚‹åˆ†å‰²ï¼ˆç‰¹æ®Šã‚±ãƒ¼ã‚¹ï¼‰
        if not segments:
            property_numbers = re.findall(r'WP[DN]\d{7}', question)
            if len(property_numbers) >= 2:
                segments = self._split_by_property_numbers(question, property_numbers)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒã—ãªã„å ´åˆã¯é•·ã•ãƒ™ãƒ¼ã‚¹ã§åˆ†å‰²
        if not segments:
            segments = self._split_by_length(question)
        
        # å„ªå…ˆåº¦ã¨ã‚«ãƒ†ã‚´ãƒªã®è¨­å®š
        segments = self._assign_priorities_and_categories(segments)
        
        logger.info(f"è³ªå•åˆ†å‰²å®Œäº†: {len(segments)}å€‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«åˆ†å‰²")
        return segments
    
    def _process_enhanced_split_parts(self, parts: List[str], pattern: str) -> List[QuestionSegment]:
        """å¼·åŒ–ã•ã‚ŒãŸåˆ†å‰²å‡¦ç†ï¼ˆæ¥ç¶šè©ã‚’è€ƒæ…®ï¼‰"""
        segments = []
        current_text = ""
        
        for i, part in enumerate(parts):
            part = part.strip()
            if not part:
                continue
                
            # æ¥ç¶šè©ã‚„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å»
            import re
            if re.match(r'^(ã¾ãŸ|ã•ã‚‰ã«|æ¬¡ã«|ã‚ã¨|ãã‚Œã‹ã‚‰|åŠ ãˆã¦|ç¶šã„ã¦)[ã€ã€‚]?$', part):
                continue
            if re.match(r'^[\dâ‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©ãƒ»]\s*$', part):
                continue
                
            # çŸ­ã™ãã‚‹ãƒ‘ãƒ¼ãƒ„ã¯å‰ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«çµåˆ
            if len(part) < 10 and current_text:
                current_text += part
            else:
                # å‰ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ä¿å­˜
                if current_text:
                    segments.append(QuestionSegment(
                        text=current_text.strip(),
                        priority=len(segments) + 1,
                        category='main',
                        keywords=self._extract_keywords(current_text)
                    ))
                current_text = part
        
        # æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ä¿å­˜
        if current_text:
            segments.append(QuestionSegment(
                text=current_text.strip(),
                priority=len(segments) + 1,
                category='main',
                keywords=self._extract_keywords(current_text)
            ))
            
        logger.info(f"ğŸ”„ å¼·åŒ–åˆ†å‰²å‡¦ç†: {len(segments)}å€‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆä½œæˆ")
        return segments
    
    def _process_split_parts(self, parts: List[str]) -> List[QuestionSegment]:
        """åˆ†å‰²ã•ã‚ŒãŸéƒ¨åˆ†ã‚’å‡¦ç†ï¼ˆæ—§ç‰ˆãƒ»ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
        segments = []
        for i, part in enumerate(parts):
            if part.strip():
                segments.append(QuestionSegment(
                    text=part.strip(),
                    priority=i + 1,
                    category='main' if i == 0 else 'detail',
                    keywords=self._extract_keywords(part)
                ))
        return segments
    
    def _split_by_property_numbers(self, question: str, property_numbers: List[str]) -> List[QuestionSegment]:
        """ç‰©ä»¶ç•ªå·ã«ã‚ˆã‚‹åˆ†å‰²ï¼ˆå„ç‰©ä»¶ã«å¯¾ã—ã¦å®Œå…¨ãªè³ªå•ã‚’ç”Ÿæˆï¼‰"""
        segments = []
        
        # å…±é€šã®è³ªå•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        common_patterns = [
            r'ã«ã¤ã„ã¦.*æ•™ãˆã¦',
            r'ã®.*ä¾¡æ ¼',
            r'ã®.*æƒ…å ±',
            r'ã®.*è©³ç´°',
            r'ã¯.*ã©ã†',
            r'ã‚’.*çŸ¥ã‚ŠãŸã„'
        ]
        
        # å„ç‰©ä»¶ç•ªå·ã«å¯¾ã—ã¦å€‹åˆ¥ã®è³ªå•ã‚’ä½œæˆ
        for i, prop_num in enumerate(property_numbers):
            # åŸºæœ¬çš„ãªè³ªå•: "ç‰©ä»¶ç•ªå·ã«ã¤ã„ã¦æ•™ãˆã¦"
            base_question = f"{prop_num}ã«ã¤ã„ã¦æ•™ãˆã¦"
            
            # å…ƒã®è³ªå•ã‹ã‚‰è¿½åŠ ã®è¦æ±‚ã‚’æŠ½å‡º
            additional_requests = []
            
            # ä¾¡æ ¼ã«é–¢ã™ã‚‹è³ªå•
            if any(word in question for word in ['ä¾¡æ ¼', 'å€¤æ®µ', 'é‡‘é¡', 'ã‚³ã‚¹ãƒˆ', 'è²»ç”¨']):
                additional_requests.append(f"{prop_num}ã®ä¾¡æ ¼")
            
            # è©³ç´°ã«é–¢ã™ã‚‹è³ªå•
            if any(word in question for word in ['è©³ç´°', 'ä»•æ§˜', 'ã‚¹ãƒšãƒƒã‚¯', 'æƒ…å ±']):
                additional_requests.append(f"{prop_num}ã®è©³ç´°æƒ…å ±")
            
            # çŠ¶æ³ã«é–¢ã™ã‚‹è³ªå•
            if any(word in question for word in ['çŠ¶æ³', 'çŠ¶æ…‹', 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹', 'é€²æ—']):
                additional_requests.append(f"{prop_num}ã®çŠ¶æ³")
            
            # å®Œå…¨ãªè³ªå•ã‚’æ§‹ç¯‰
            if additional_requests:
                full_question = f"{base_question}ã€‚ã¾ãŸã€{', '.join(additional_requests)}ã‚‚çŸ¥ã‚ŠãŸã„ã§ã™ã€‚"
            else:
                full_question = base_question
            
            segments.append(QuestionSegment(
                text=full_question,
                priority=i + 1,
                category='main',
                keywords=self._extract_keywords(full_question) + [prop_num]
            ))
        
        logger.info(f"ç‰©ä»¶ç•ªå·ã«ã‚ˆã‚‹åˆ†å‰²: {len(segments)}å€‹ã®å®Œå…¨ãªè³ªå•ã‚’ä½œæˆ")
        for i, seg in enumerate(segments):
            logger.info(f"  ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ{i+1}: {seg.text}")
        
        return segments
    
    def _split_by_length(self, question: str, max_length: int = 1500) -> List[QuestionSegment]:
        """é•·ã•ãƒ™ãƒ¼ã‚¹ã§ã®è³ªå•åˆ†å‰²"""
        segments = []
        sentences = re.split(r'[ã€‚ï¼Ÿ?]', question)
        
        current_segment = ""
        for sentence in sentences:
            if len(current_segment + sentence) > max_length and current_segment:
                segments.append(QuestionSegment(
                    text=current_segment.strip(),
                    priority=len(segments) + 1,
                    category='main' if len(segments) == 0 else 'detail',
                    keywords=self._extract_keywords(current_segment)
                ))
                current_segment = sentence
            else:
                current_segment += sentence + "ã€‚"
        
        if current_segment.strip():
            segments.append(QuestionSegment(
                text=current_segment.strip(),
                priority=len(segments) + 1,
                category='detail',
                keywords=self._extract_keywords(current_segment)
            ))
        
        return segments
    
    def _assign_priorities_and_categories(self, segments: List[QuestionSegment]) -> List[QuestionSegment]:
        """å„ªå…ˆåº¦ã¨ã‚«ãƒ†ã‚´ãƒªã®å‰²ã‚Šå½“ã¦"""
        for segment in segments:
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®å„ªå…ˆåº¦è¨­å®š
            for priority, keywords in self.priority_keywords.items():
                if any(keyword in segment.text for keyword in keywords):
                    segment.priority = priority
                    break
            
            # ã‚«ãƒ†ã‚´ãƒªã®ç´°åˆ†åŒ–
            if any(word in segment.text for word in ['ä¾‹ãˆã°', 'å…·ä½“çš„', 'è©³ã—ã']):
                segment.category = 'example'
            elif any(word in segment.text for word in ['ã¾ãŸ', 'ã•ã‚‰ã«', 'åŠ ãˆã¦']):
                segment.category = 'follow_up'
        
        # å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆ
        segments.sort(key=lambda x: x.priority)
        return segments
    
    def _extract_keywords(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        # ç°¡å˜ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆåè©ã‚„é‡è¦ãã†ãªå˜èªï¼‰
        keywords = []
        
        # ã‚«ã‚¿ã‚«ãƒŠèªã‚’æŠ½å‡º
        katakana_words = re.findall(r'[ã‚¢-ãƒ´ãƒ¼]{3,}', text)
        keywords.extend(katakana_words)
        
        # è‹±æ•°å­—ã‚’å«ã‚€å˜èªã‚’æŠ½å‡º
        english_words = re.findall(r'[A-Za-z0-9]{3,}', text)
        keywords.extend(english_words)
        
        # é‡è¦ãã†ãªæ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        important_patterns = [
            r'[æ‰‹ç¶šæµæ–¹æ³•æ‰‹é †ä»•çµ„]ã?',
            r'[ç®¡ç†é‹ç”¨æ“ä½œè¨­å®š]',
            r'[ã‚·ã‚¹ãƒ†ãƒ ã‚µãƒ¼ãƒ“ã‚¹æ©Ÿèƒ½]',
            r'[ä¼šç¤¾ä¼æ¥­çµ„ç¹”éƒ¨ç½²]',
        ]
        
        for pattern in important_patterns:
            matches = re.findall(pattern, text)
            keywords.extend(matches)
        
        return list(set(keywords))  # é‡è¤‡é™¤å»
    
    def merge_segments_responses(self, responses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†å‰²ã—ãŸè³ªå•ã®å›ç­”ã‚’ãƒãƒ¼ã‚¸"""
        logger.info(f"å›ç­”ãƒãƒ¼ã‚¸é–‹å§‹: {len(responses)}å€‹ã®å›ç­”")
        
        merged_answer_parts = []
        all_sources = []
        all_chunks = []
        
        for i, response in enumerate(responses):
            if response.get('answer'):
                merged_answer_parts.append(f"## {i+1}. å›ç­”")
                merged_answer_parts.append(response['answer'])
                merged_answer_parts.append("")  # ç©ºè¡Œ
            
            if response.get('sources'):
                all_sources.extend(response['sources'])
            
            if response.get('used_chunks'):
                all_chunks.extend(response['used_chunks'])
        
        # é‡è¤‡é™¤å»
        unique_sources = list(set(all_sources))
        
        final_answer = "\n".join(merged_answer_parts)
        if len(final_answer) > 100:  # æœ‰ç”¨ãªå›ç­”ãŒã‚ã‚‹å ´åˆ
            final_answer += "\n\n## ğŸ“‹ çµ±åˆå›ç­”\n"
            final_answer += "ä¸Šè¨˜ã®å„é …ç›®ã«ã¤ã„ã¦ã€é–¢é€£ã™ã‚‹æƒ…å ±ã‚’ç·åˆçš„ã«ã”æä¾›ã„ãŸã—ã¾ã—ãŸã€‚"
            final_answer += "ã•ã‚‰ã«è©³ç´°ãªæƒ…å ±ãŒå¿…è¦ã§ã—ãŸã‚‰ã€å…·ä½“çš„ãªé …ç›®ã«ã¤ã„ã¦ãŠèã‹ã›ãã ã•ã„ã€‚"
        
        logger.info(f"å›ç­”ãƒãƒ¼ã‚¸å®Œäº†: {len(final_answer)}æ–‡å­—")
        
        return {
            "answer": final_answer,
            "sources": unique_sources,
            "used_chunks": all_chunks,
            "segments_count": len(responses)
        }

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
question_splitter = QuestionSplitter() 
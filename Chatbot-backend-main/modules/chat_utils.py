"""
ãƒãƒ£ãƒƒãƒˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã§ä½¿ç”¨ã™ã‚‹å…±é€šã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã‚’ç®¡ç†ã—ã¾ã™
"""
import re
from .utils import safe_print, safe_safe_print

def safe_print(text):
    """Windowsç’°å¢ƒã§ã®Unicodeæ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œã‚’å›é¿ã™ã‚‹å®‰å…¨ãªprinté–¢æ•°"""
    try:
        print(text)
    except UnicodeEncodeError:
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€å•é¡Œã®ã‚ã‚‹æ–‡å­—ã‚’ç½®æ›
        try:
            safe_text = str(text).encode('utf-8', errors='replace').decode('utf-8')
            print(safe_text)
        except:
            # ãã‚Œã§ã‚‚å¤±æ•—ã™ã‚‹å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿å‡ºåŠ›
            print("[å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: Unicodeæ–‡å­—ã‚’å«ã‚€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸]")

def safe_safe_print(text):
    """Windowsç’°å¢ƒã§ã®Unicodeæ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œã‚’å›é¿ã™ã‚‹å®‰å…¨ãªsafe_printé–¢æ•°"""
    safe_print(text)

def chunk_knowledge_base(text: str, chunk_size: int = 700) -> list[str]:  # ğŸ¯ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ700æ–‡å­—ï¼ˆ600-800æ–‡å­—ç¯„å›²ã®ä¸­å¤®å€¤ï¼‰
    """
    çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æŒ‡å®šã•ã‚ŒãŸã‚µã‚¤ã‚ºã§ãƒãƒ£ãƒ³ã‚¯åŒ–ã™ã‚‹ï¼ˆ600-800æ–‡å­—å³å®ˆï¼‰
    CSVæ§‹é€ ã‚’æ¤œå‡ºã—ã¦é¡§å®¢å¢ƒç•Œã‚’ä¿è­·ã™ã‚‹é«˜ç²¾åº¦åˆ†å‰²
    
    Args:
        text: ãƒãƒ£ãƒ³ã‚¯åŒ–ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
        chunk_size: ãƒãƒ£ãƒ³ã‚¯ã®ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—æ•°ï¼‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ700æ–‡å­—ï¼ˆ600-800æ–‡å­—ç¯„å›²ï¼‰
    
    Returns:
        ãƒãƒ£ãƒ³ã‚¯åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆï¼ˆå„ãƒãƒ£ãƒ³ã‚¯600-800æ–‡å­—ä»¥å†…ï¼‰
    """
    if not text or len(text) <= chunk_size:
        return [text] if text else []
    
    # ğŸ¯ CSVæ§‹é€ æ¤œå‡ºã¨åˆ†å‰²æˆ¦ç•¥ã®é¸æŠ
    if _is_csv_structure(text):
        return _csv_aware_chunking(text, chunk_size)
    else:
        return _traditional_chunking(text, chunk_size)


def _is_csv_structure(text: str) -> bool:
    """CSVæ§‹é€ ã‚’æ¤œå‡ºï¼ˆé¡§å®¢ç•ªå·ã€ç‰©ä»¶ç•ªå·ã€åŒºåˆ‡ã‚Šæ–‡å­—ã®å­˜åœ¨ï¼‰"""
    # CSVæ§‹é€ ã®ç‰¹å¾´ã‚’æ¤œå‡º
    customer_pattern = r'SS\d{7}'  # é¡§å®¢ç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³
    property_pattern = r'WP[DN]\d{7}'  # ç‰©ä»¶ç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³
    separator_pattern = r'\s*\|\s*'  # CSVåŒºåˆ‡ã‚Šæ–‡å­—
    
    has_customer_numbers = len(re.findall(customer_pattern, text)) >= 2
    has_property_numbers = len(re.findall(property_pattern, text)) >= 2
    has_separators = len(re.findall(separator_pattern, text)) >= 5
    
    return has_customer_numbers and has_property_numbers and has_separators


def _csv_aware_chunking(text: str, chunk_size: int) -> list[str]:
    """CSVæ§‹é€ èªè­˜åˆ†å‰² - é¡§å®¢å¢ƒç•Œã‚’çµ¶å¯¾ä¿è­·"""
    chunks = []
    lines = text.split('\n')
    current_chunk = ""
    current_customer = None
    max_chunk_size = 800  # çµ¶å¯¾æœ€å¤§ã‚µã‚¤ã‚º
    min_chunk_size = 400  # CSVç”¨æœ€å°ã‚µã‚¤ã‚ºï¼ˆé¡§å®¢å¢ƒç•Œå„ªå…ˆï¼‰
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # ğŸ¯ é¡§å®¢ç•ªå·æ¤œå‡º
        customer_match = re.search(r'SS\d{7}', line)
        line_customer = customer_match.group() if customer_match else None
        
        # é¡§å®¢å¢ƒç•Œã§ã®åˆ†å‰²åˆ¤å®š
        should_split_for_customer = False
        if (current_customer and line_customer and 
            current_customer != line_customer and 
            len(current_chunk) >= min_chunk_size):
            should_split_for_customer = True
        
        # ã‚µã‚¤ã‚ºåˆ¶é™ã§ã®åˆ†å‰²åˆ¤å®š
        potential_chunk = current_chunk + ('\n' + line if current_chunk else line)
        should_split_for_size = len(potential_chunk) > chunk_size
        
        # ğŸ¯ åˆ†å‰²æ±ºå®šï¼ˆé¡§å®¢å¢ƒç•Œå„ªå…ˆï¼‰
        if should_split_for_customer:
            # é¡§å®¢å¢ƒç•Œã§åˆ†å‰²ï¼ˆæœ€å„ªå…ˆï¼‰
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            current_chunk = line
            current_customer = line_customer
        elif should_split_for_size and len(current_chunk) >= min_chunk_size:
            # ã‚µã‚¤ã‚ºåˆ¶é™ã§åˆ†å‰²
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            current_chunk = line
            current_customer = line_customer or current_customer
        elif len(potential_chunk) > max_chunk_size:
            # çµ¶å¯¾æœ€å¤§ã‚µã‚¤ã‚ºã§å¼·åˆ¶åˆ†å‰²
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            current_chunk = line
            current_customer = line_customer or current_customer
        else:
            # ç¶™ç¶š
            current_chunk = potential_chunk
            if line_customer:
                current_customer = line_customer
    
    # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ 
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    return chunks if chunks else [text]


def _traditional_chunking(text: str, chunk_size: int) -> list[str]:
    """å¾“æ¥ã®åˆ†å‰²æ–¹å¼ï¼ˆPDFã€Wordç­‰ã®éæ§‹é€ åŒ–ãƒ†ã‚­ã‚¹ãƒˆç”¨ï¼‰"""
    chunks = []
    start = 0
    max_chunk_size = 800  # ğŸ¯ çµ¶å¯¾æœ€å¤§ã‚µã‚¤ã‚º
    min_chunk_size = 600  # ğŸ¯ æœ€å°ã‚µã‚¤ã‚º
    overlap = 50  # ğŸ¯ å›ºå®š50æ–‡å­—ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼ˆã‚µã‚¤ã‚ºåˆ¶å¾¡ã®ãŸã‚ï¼‰
    
    while start < len(text):
        # åŸºæœ¬çš„ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’è¨­å®š
        end = min(start + chunk_size, len(text))
        
        # ãƒãƒ£ãƒ³ã‚¯ã®å¢ƒç•Œã‚’èª¿æ•´ï¼ˆæ–‡ã®é€”ä¸­ã§åˆ‡ã‚Œãªã„ã‚ˆã†ã«ï¼‰
        if end < len(text):
            # æœ€å¾Œã®æ”¹è¡Œã‚’æ¢ã™ï¼ˆæ¤œç´¢ç¯„å›²ã‚’åˆ¶é™ï¼‰
            search_start = max(start, end - 50)  # ğŸ¯ 50æ–‡å­—å‰ã‹ã‚‰æ¤œç´¢ï¼ˆã‚µã‚¤ã‚ºåˆ¶å¾¡ï¼‰
            last_newline = text.rfind('\n', search_start, end)
            if last_newline > start:
                end = last_newline + 1
            else:
                # æ”¹è¡ŒãŒãªã„å ´åˆã¯æœ€å¾Œã®ã‚¹ãƒšãƒ¼ã‚¹ã‚’æ¢ã™
                last_space = text.rfind(' ', search_start, end)
                if last_space > start:
                    end = last_space + 1
        
        # ğŸ¯ ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãŒæœ€å¤§å€¤ã‚’è¶…ãˆã‚‹å ´åˆã¯å¼·åˆ¶çš„ã«åˆ‡æ–­
        if end - start > max_chunk_size:
            end = start + max_chunk_size
            # æ–‡å­—ã®é€”ä¸­ã§åˆ‡ã‚Œãªã„ã‚ˆã†ã«æœ€å¾Œã®ã‚¹ãƒšãƒ¼ã‚¹ã¾ã§æˆ»ã‚‹
            last_space = text.rfind(' ', start, end)
            if last_space > start:
                end = last_space
        
        chunk = text[start:end].strip()
        
        # ğŸ¯ ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãŒç¯„å›²å†…ã‹ãƒã‚§ãƒƒã‚¯
        if chunk:
            chunk_length = len(chunk)
            if chunk_length <= max_chunk_size:  # 800æ–‡å­—ä»¥ä¸‹ãªã‚‰è¿½åŠ 
                chunks.append(chunk)
            else:
                # 800æ–‡å­—ã‚’è¶…ãˆã‚‹å ´åˆã¯å¼·åˆ¶åˆ†å‰²
                while len(chunk) > max_chunk_size:
                    sub_chunk = chunk[:max_chunk_size]
                    # æœ€å¾Œã®ã‚¹ãƒšãƒ¼ã‚¹ã§åˆ‡ã‚‹
                    last_space = sub_chunk.rfind(' ')
                    if last_space > min_chunk_size:  # 600æ–‡å­—ä»¥ä¸Šã®ä½ç½®ã«ã‚¹ãƒšãƒ¼ã‚¹ãŒã‚ã‚‹å ´åˆ
                        sub_chunk = sub_chunk[:last_space]
                    chunks.append(sub_chunk)
                    chunk = chunk[len(sub_chunk):].strip()
                
                # æ®‹ã‚Šã®ãƒãƒ£ãƒ³ã‚¯ã‚‚è¿½åŠ 
                if chunk:
                    chunks.append(chunk)
        
        # æ¬¡ã®é–‹å§‹ä½ç½®ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’è€ƒæ…®ï¼‰
        if end < len(text):
            start = max(start + min_chunk_size, end - overlap)  # ğŸ¯ æœ€å°600æ–‡å­—ã¯é€²ã‚€
        else:
            start = end
    
    return chunks

def expand_query(query: str) -> str:
    """
    ã‚¯ã‚¨ãƒªæ‹¡å¼µ - é¡ç¾©èªã‚„é–¢é€£ç”¨èªã‚’è¿½åŠ ã—ã¦æ¤œç´¢ç²¾åº¦ã‚’å‘ä¸Š
    """
    # åŸºæœ¬çš„ãªã‚¯ã‚¨ãƒªæ‹¡å¼µã®ãƒãƒƒãƒ”ãƒ³ã‚°
    expansion_map = {
        'æ–¹æ³•': ['æ‰‹é †', 'ã‚„ã‚Šæ–¹', 'ãƒ—ãƒ­ã‚»ã‚¹', 'æµã‚Œ'],
        'æ‰‹é †': ['æ–¹æ³•', 'ã‚¹ãƒ†ãƒƒãƒ—', 'ãƒ—ãƒ­ã‚»ã‚¹', 'æµã‚Œ'],
        'å•é¡Œ': ['èª²é¡Œ', 'ãƒˆãƒ©ãƒ–ãƒ«', 'ã‚¨ãƒ©ãƒ¼', 'ä¸å…·åˆ'],
        'è¨­å®š': ['æ§‹æˆ', 'ã‚³ãƒ³ãƒ•ã‚£ã‚°', 'è¨­å®šå€¤', 'ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—'],
        'ä½¿ã„æ–¹': ['åˆ©ç”¨æ–¹æ³•', 'æ“ä½œæ–¹æ³•', 'ä½¿ç”¨æ–¹æ³•', 'æ“ä½œæ‰‹é †'],
        'ã‚¨ãƒ©ãƒ¼': ['å•é¡Œ', 'ãƒˆãƒ©ãƒ–ãƒ«', 'ä¸å…·åˆ', 'ãƒã‚°'],
        'æ–™é‡‘': ['ä¾¡æ ¼', 'è²»ç”¨', 'ã‚³ã‚¹ãƒˆ', 'å€¤æ®µ'],
        'æ©Ÿèƒ½': ['ç‰¹å¾´', 'ä»•æ§˜', 'æ€§èƒ½', 'èƒ½åŠ›'],
    }
    
    expanded_terms = []
    query_words = query.split()
    
    for word in query_words:
        expanded_terms.append(word)
        if word in expansion_map:
            # 1ã¤ã®é¡ç¾©èªã‚’è¿½åŠ ï¼ˆã‚¯ã‚¨ãƒªãŒé•·ããªã‚Šã™ããªã„ã‚ˆã†ã«ï¼‰
            expanded_terms.append(expansion_map[word][0])
    
    expanded_query = ' '.join(expanded_terms)
    return expanded_query if len(expanded_query) <= len(query) * 2 else query
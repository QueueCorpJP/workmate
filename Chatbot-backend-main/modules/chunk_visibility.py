"""
ğŸ” ãƒãƒ£ãƒ³ã‚¯å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ 
è³ªå•æ™‚ã®å‚ç…§ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚’è¡¨ç¤ºã—ã€ã©ã®ãƒãƒ£ãƒ³ã‚¯ãŒé¸ã°ã‚Œã€ãªãœãã‚ŒãŒé¸ã°ã‚ŒãŸã®ã‹ã‚’ç¢ºèªã§ãã‚‹æ©Ÿèƒ½
"""

import logging
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from datetime import datetime

logger = logging.getLogger(__name__)

@dataclass
class ChunkReference:
    """å‚ç…§ãƒãƒ£ãƒ³ã‚¯æƒ…å ±"""
    chunk_id: str
    document_id: str
    document_name: str
    content: str
    chunk_index: int
    similarity_score: float
    relevance_score: float
    confidence_score: float
    query_match_score: float
    semantic_score: float
    context_score: float
    search_method: str
    selection_reason: str
    metadata: Dict = None

@dataclass
class ChunkSelectionAnalysis:
    """ãƒãƒ£ãƒ³ã‚¯é¸æŠåˆ†æ"""
    total_chunks_found: int
    chunks_after_filtering: int
    selection_criteria: Dict[str, Any]
    dynamic_threshold: float
    query_analysis: Dict[str, Any]
    selection_summary: str

class ChunkVisibilitySystem:
    """ãƒãƒ£ãƒ³ã‚¯å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.logger = logging.getLogger(__name__)
    
    def analyze_chunk_selection(self, 
                              search_results: List,
                              query: str,
                              dynamic_threshold: float,
                              intent_analysis: Dict = None) -> ChunkSelectionAnalysis:
        """ãƒãƒ£ãƒ³ã‚¯é¸æŠã®åˆ†æ"""
        try:
            # åŸºæœ¬çµ±è¨ˆ
            total_chunks = len(search_results) if search_results else 0
            filtered_chunks = len([r for r in search_results if r.similarity_score >= dynamic_threshold]) if search_results else 0
            
            # é¸æŠåŸºæº–ã®åˆ†æ
            selection_criteria = {
                "dynamic_threshold": dynamic_threshold,
                "min_confidence": 0.1,
                "max_results_per_document": 4,
                "context_diversity_enabled": True
            }
            
            # ã‚¯ã‚¨ãƒªåˆ†æ
            query_analysis = {
                "query_length": len(query),
                "is_japanese": bool(any(ord(c) > 127 for c in query)),
                "contains_company_name": any(term in query.lower() for term in ['ã»ã£ã¨ã‚‰ã„ãµ', 'ãƒ›ãƒƒãƒˆãƒ©ã‚¤ãƒ•', 'hotlife']),
                "intent_analysis": intent_analysis or {}
            }
            
            # é¸æŠã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ
            selection_summary = self._generate_selection_summary(
                total_chunks, filtered_chunks, dynamic_threshold, query_analysis
            )
            
            return ChunkSelectionAnalysis(
                total_chunks_found=total_chunks,
                chunks_after_filtering=filtered_chunks,
                selection_criteria=selection_criteria,
                dynamic_threshold=dynamic_threshold,
                query_analysis=query_analysis,
                selection_summary=selection_summary
            )
        
        except Exception as e:
            self.logger.error(f"ãƒãƒ£ãƒ³ã‚¯é¸æŠåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return ChunkSelectionAnalysis(
                total_chunks_found=0,
                chunks_after_filtering=0,
                selection_criteria={},
                dynamic_threshold=0.0,
                query_analysis={},
                selection_summary="åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ"
            )
    
    def _generate_selection_summary(self, 
                                   total_chunks: int,
                                   filtered_chunks: int,
                                   threshold: float,
                                   query_analysis: Dict) -> str:
        """é¸æŠã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ"""
        summary_parts = []
        
        # åŸºæœ¬æƒ…å ±
        summary_parts.append(f"æ¤œç´¢çµæœ: {total_chunks}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        summary_parts.append(f"å‹•çš„é–¾å€¤: {threshold:.3f}ã«ã‚ˆã‚Š{filtered_chunks}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ãŒé¸æŠã•ã‚Œã¾ã—ãŸ")
        
        # ã‚¯ã‚¨ãƒªç‰¹æ€§
        if query_analysis.get("is_japanese"):
            summary_parts.append("æ—¥æœ¬èªã‚¯ã‚¨ãƒªã®ãŸã‚é–¾å€¤ãŒèª¿æ•´ã•ã‚Œã¾ã—ãŸ")
        
        if query_analysis.get("contains_company_name"):
            summary_parts.append("ä¼šç¤¾åãŒå«ã¾ã‚Œã‚‹ãŸã‚ç‰¹åˆ¥ãªå‡¦ç†ãŒé©ç”¨ã•ã‚Œã¾ã—ãŸ")
        
        # é¸æŠç‡
        if total_chunks > 0:
            selection_rate = (filtered_chunks / total_chunks) * 100
            summary_parts.append(f"é¸æŠç‡: {selection_rate:.1f}%")
        
        return "ã€‚".join(summary_parts) + "ã€‚"
    
    def create_chunk_references(self, search_results: List, query: str) -> List[ChunkReference]:
        """æ¤œç´¢çµæœã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯å‚ç…§æƒ…å ±ã‚’ä½œæˆ"""
        chunk_references = []
        
        try:
            for i, result in enumerate(search_results):
                # é¸æŠç†ç”±ã®ç”Ÿæˆ
                selection_reason = self._generate_selection_reason(result, query, i)
                
                chunk_ref = ChunkReference(
                    chunk_id=result.chunk_id,
                    document_id=result.document_id,
                    document_name=result.document_name,
                    content=result.content,
                    chunk_index=result.chunk_index,
                    similarity_score=result.similarity_score,
                    relevance_score=result.relevance_score,
                    confidence_score=result.confidence_score,
                    query_match_score=getattr(result, 'query_match_score', 0.0),
                    semantic_score=getattr(result, 'semantic_score', 0.0),
                    context_score=getattr(result, 'context_score', 0.0),
                    search_method=result.search_method,
                    selection_reason=selection_reason,
                    metadata=getattr(result, 'metadata', {})
                )
                
                chunk_references.append(chunk_ref)
        
        except Exception as e:
            self.logger.error(f"ãƒãƒ£ãƒ³ã‚¯å‚ç…§æƒ…å ±ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        
        return chunk_references
    
    def _generate_selection_reason(self, result, query: str, rank: int) -> str:
        """ãƒãƒ£ãƒ³ã‚¯é¸æŠç†ç”±ã®ç”Ÿæˆ"""
        reasons = []
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        reasons.append(f"æ¤œç´¢é †ä½: {rank + 1}ä½")
        
        # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢
        if result.similarity_score >= 0.7:
            reasons.append(f"é«˜ã„é¡ä¼¼åº¦ ({result.similarity_score:.3f})")
        elif result.similarity_score >= 0.4:
            reasons.append(f"ä¸­ç¨‹åº¦ã®é¡ä¼¼åº¦ ({result.similarity_score:.3f})")
        else:
            reasons.append(f"ä½ã„é¡ä¼¼åº¦ ({result.similarity_score:.3f})")
        
        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
        if result.confidence_score >= 0.5:
            reasons.append(f"é«˜ã„ä¿¡é ¼åº¦ ({result.confidence_score:.3f})")
        elif result.confidence_score >= 0.3:
            reasons.append(f"ä¸­ç¨‹åº¦ã®ä¿¡é ¼åº¦ ({result.confidence_score:.3f})")
        
        # ã‚¯ã‚¨ãƒªãƒãƒƒãƒ
        query_match = getattr(result, 'query_match_score', 0.0)
        if query_match >= 0.3:
            reasons.append(f"ã‚¯ã‚¨ãƒªã¨ã®ç›´æ¥ãƒãƒƒãƒ ({query_match:.3f})")
        
        # æ–‡æ›¸ã‚¿ã‚¤ãƒ—
        if hasattr(result, 'document_type') and result.document_type:
            reasons.append(f"æ–‡æ›¸ã‚¿ã‚¤ãƒ—: {result.document_type}")
        
        return "ã€".join(reasons)
    
    def format_chunk_visibility_info(self, 
                                   chunk_references: List[ChunkReference],
                                   selection_analysis: ChunkSelectionAnalysis) -> Dict[str, Any]:
        """ãƒãƒ£ãƒ³ã‚¯å¯è¦–åŒ–æƒ…å ±ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        return {
            "chunk_references": [
                {
                    "chunk_id": ref.chunk_id,
                    "document_name": ref.document_name,
                    "chunk_index": ref.chunk_index,
                    "content_preview": ref.content[:200] + "..." if len(ref.content) > 200 else ref.content,
                    "scores": {
                        "similarity": ref.similarity_score,
                        "relevance": ref.relevance_score,
                        "confidence": ref.confidence_score,
                        "query_match": ref.query_match_score,
                        "semantic": ref.semantic_score
                    },
                    "selection_reason": ref.selection_reason,
                    "search_method": ref.search_method
                }
                for ref in chunk_references
            ],
            "selection_analysis": {
                "total_chunks_found": selection_analysis.total_chunks_found,
                "chunks_selected": selection_analysis.chunks_after_filtering,
                "dynamic_threshold": selection_analysis.dynamic_threshold,
                "selection_criteria": selection_analysis.selection_criteria,
                "query_analysis": selection_analysis.query_analysis,
                "summary": selection_analysis.selection_summary
            },
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "system_version": "chunk_visibility_v1.0"
            }
        }

# ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—é–¢æ•°
def get_chunk_visibility_system() -> ChunkVisibilitySystem:
    """ãƒãƒ£ãƒ³ã‚¯å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    return ChunkVisibilitySystem()

def chunk_visibility_available() -> bool:
    """ãƒãƒ£ãƒ³ã‚¯å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        system = ChunkVisibilitySystem()
        return True
    except Exception as e:
        logger.error(f"ãƒãƒ£ãƒ³ã‚¯å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
        return False
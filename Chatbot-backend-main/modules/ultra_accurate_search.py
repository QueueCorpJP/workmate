"""
è¶…é«˜ç²¾åº¦ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
- å‹•çš„é©å¿œé–¾å€¤ã«ã‚ˆã‚‹æœ€é©åŒ–
- å¤šæ®µéšæ¤œç´¢æˆ¦ç•¥
- æ—¥æœ¬èªç‰¹åŒ–å‹ã‚¯ã‚¨ãƒªæ‹¡å¼µ
- ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
"""

import os
import logging
import asyncio
import re
from typing import List, Dict, Tuple, Optional, Set
from dotenv import load_dotenv
import psycopg2
from psycopg2.extras import RealDictCursor
import numpy as np
from datetime import datetime
from dataclasses import dataclass
from collections import defaultdict
# import jaconv  # ä¾å­˜é–¢ä¿‚ã‚’å‰Šé™¤

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

logger = logging.getLogger(__name__)

@dataclass
class UltraSearchResult:
    """è¶…é«˜ç²¾åº¦æ¤œç´¢çµæœ"""
    chunk_id: str
    document_id: str
    document_name: str
    content: str
    similarity_score: float
    relevance_score: float
    confidence_score: float
    chunk_index: int
    document_type: str
    search_method: str
    query_match_score: float = 0.0
    semantic_score: float = 0.0
    context_score: float = 0.0
    metadata: Dict = None

class UltraAccurateSearchSystem:
    """è¶…é«˜ç²¾åº¦ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.use_vertex_ai = os.getenv("USE_VERTEX_AI", "true").lower() == "true"
        self.embedding_model = os.getenv("EMBEDDING_MODEL", "text-multilingual-embedding-002")
        self.expected_dimensions = 768 if "text-multilingual-embedding-002" in self.embedding_model else 3072
        
        self.db_url = self._get_db_url()
        self.pgvector_available = False
        
        # å‹•çš„é–¾å€¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.base_threshold = 0.15  # åŸºæœ¬é–¾å€¤ã‚’å¤§å¹…ã«ä¸‹ã’ã‚‹
        self.adaptive_threshold_enabled = True
        self.multi_stage_search = True
        
        # æ—¥æœ¬èªç‰¹åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.japanese_boost = 1.2
        self.katakana_boost = 1.1
        self.company_name_boost = 1.3
        
        # pgvectoræ‹¡å¼µæ©Ÿèƒ½ã®ç¢ºèª
        self._check_pgvector_availability()
        
        # Vertex AI Embeddingã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        if self.use_vertex_ai:
            from .vertex_ai_embedding import get_vertex_ai_embedding_client, vertex_ai_embedding_available
            if vertex_ai_embedding_available():
                self.vertex_client = get_vertex_ai_embedding_client()
                logger.info(f"âœ… è¶…é«˜ç²¾åº¦æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–: {self.embedding_model} ({self.expected_dimensions}æ¬¡å…ƒ)")
            else:
                logger.error("âŒ Vertex AI EmbeddingãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                raise ValueError("Vertex AI Embeddingã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        else:
            self.vertex_client = None
    
    def _get_db_url(self) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã‚’æ§‹ç¯‰"""
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        if "supabase.co" in supabase_url:
            project_id = supabase_url.split("://")[1].split(".")[0]
            return f"postgresql://postgres.{project_id}:{os.getenv('DB_PASSWORD', '')}@aws-0-ap-northeast-1.pooler.supabase.com:6543/postgres"
        else:
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                raise ValueError("DATABASE_URL ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return db_url
    
    def _check_pgvector_availability(self):
        """pgvectoræ‹¡å¼µæ©Ÿèƒ½ã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        SELECT EXISTS(
                            SELECT 1 FROM pg_extension WHERE extname = 'vector'
                        ) as pgvector_installed
                    """)
                    result = cur.fetchone()
                    self.pgvector_available = result['pgvector_installed'] if result else False
                    
                    if self.pgvector_available:
                        logger.info("âœ… pgvectoræ‹¡å¼µæ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
                    else:
                        logger.warning("âš ï¸ pgvectoræ‹¡å¼µæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™")
                        
        except Exception as e:
            logger.error(f"âŒ pgvectorç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            self.pgvector_available = False
    
    def expand_japanese_query(self, query: str) -> List[str]:
        """æ—¥æœ¬èªã‚¯ã‚¨ãƒªã®æ‹¡å¼µ"""
        expanded_queries = [query]
        
        # ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›
        if re.search(r'[ã²ã‚‰ãŒãª]', query):
            katakana_query = jaconv.hira2kata(query)
            expanded_queries.append(katakana_query)
        
        if re.search(r'[ã‚«ã‚¿ã‚«ãƒŠ]', query):
            hiragana_query = jaconv.kata2hira(query)
            expanded_queries.append(hiragana_query)
        
        # åŠè§’ãƒ»å…¨è§’å¤‰æ›
        if re.search(r'[ï¼¡-ï¼ºï½-ï½šï¼-ï¼™]', query):
            hankaku_query = jaconv.z2h(query, kana=False, ascii=True, digit=True)
            expanded_queries.append(hankaku_query)
        
        if re.search(r'[A-Za-z0-9]', query):
            zenkaku_query = jaconv.h2z(query, kana=False, ascii=True, digit=True)
            expanded_queries.append(zenkaku_query)
        
        # ä¼šç¤¾åãƒ»ã‚µãƒ¼ãƒ“ã‚¹åã®ä¸€èˆ¬çš„ãªãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        company_variations = {
            'ã»ã£ã¨ã‚‰ã„ãµ': ['ãƒ›ãƒƒãƒˆãƒ©ã‚¤ãƒ•', 'HOT LIFE', 'hotlife', 'ãƒ›ãƒƒãƒˆãƒ»ãƒ©ã‚¤ãƒ•', 'ã»ã£ã¨ãƒ»ã‚‰ã„ãµ'],
            'ãƒ›ãƒƒãƒˆãƒ©ã‚¤ãƒ•': ['ã»ã£ã¨ã‚‰ã„ãµ', 'HOT LIFE', 'hotlife', 'ãƒ›ãƒƒãƒˆãƒ»ãƒ©ã‚¤ãƒ•', 'ã»ã£ã¨ãƒ»ã‚‰ã„ãµ'],
        }
        
        query_lower = query.lower()
        for key, variations in company_variations.items():
            if key.lower() in query_lower:
                expanded_queries.extend(variations)
        
        # é‡è¤‡é™¤å»
        return list(set(expanded_queries))
    
    def calculate_dynamic_threshold(self, similarities: List[float], query: str) -> float:
        """å‹•çš„é–¾å€¤ã®è¨ˆç®—"""
        if not similarities:
            return self.base_threshold
        
        similarities = sorted(similarities, reverse=True)
        
        # åŸºæœ¬çµ±è¨ˆ
        max_sim = max(similarities)
        avg_sim = sum(similarities) / len(similarities)
        
        # ã‚¯ã‚¨ãƒªã®ç‰¹æ€§ã«ã‚ˆã‚‹èª¿æ•´
        query_boost = 1.0
        
        # æ—¥æœ¬èªã‚¯ã‚¨ãƒªã®å ´åˆã¯é–¾å€¤ã‚’ä¸‹ã’ã‚‹
        if re.search(r'[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]', query):
            query_boost *= 0.8
        
        # çŸ­ã„ã‚¯ã‚¨ãƒªã®å ´åˆã¯é–¾å€¤ã‚’ä¸‹ã’ã‚‹
        if len(query) <= 5:
            query_boost *= 0.7
        
        # ä¼šç¤¾åãƒ»å›ºæœ‰åè©ã®å ´åˆã¯é–¾å€¤ã‚’å¤§å¹…ã«ä¸‹ã’ã‚‹
        if any(term in query.lower() for term in ['ã»ã£ã¨ã‚‰ã„ãµ', 'ãƒ›ãƒƒãƒˆãƒ©ã‚¤ãƒ•', 'hotlife']):
            query_boost *= 0.5
        
        # å‹•çš„é–¾å€¤ã®è¨ˆç®—
        if max_sim > 0.6:
            # é«˜å“è³ªãªçµæœãŒã‚ã‚‹å ´åˆ
            dynamic_threshold = max(self.base_threshold, avg_sim * 0.4) * query_boost
        elif max_sim > 0.3:
            # ä¸­å“è³ªãªçµæœãŒã‚ã‚‹å ´åˆ
            dynamic_threshold = max(self.base_threshold * 0.7, avg_sim * 0.3) * query_boost
        else:
            # ä½å“è³ªãªçµæœã—ã‹ãªã„å ´åˆ
            dynamic_threshold = self.base_threshold * 0.5 * query_boost
        
        # æœ€å°é–¾å€¤ã®ä¿è¨¼
        final_threshold = max(0.05, min(dynamic_threshold, 0.4))
        
        logger.info(f"ğŸ¯ å‹•çš„é–¾å€¤è¨ˆç®—: {final_threshold:.3f} (æœ€å¤§é¡ä¼¼åº¦: {max_sim:.3f}, å¹³å‡: {avg_sim:.3f}, ã‚¯ã‚¨ãƒªè£œæ­£: {query_boost:.2f})")
        return final_threshold
    
    def calculate_query_match_score(self, content: str, query: str) -> float:
        """ã‚¯ã‚¨ãƒªãƒãƒƒãƒã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        if not content or not query:
            return 0.0
        
        content_lower = content.lower()
        query_lower = query.lower()
        
        score = 0.0
        
        # å®Œå…¨ä¸€è‡´
        if query_lower in content_lower:
            score += 0.5
        
        # éƒ¨åˆ†ä¸€è‡´
        query_terms = query_lower.split()
        content_terms = content_lower.split()
        
        matched_terms = 0
        for term in query_terms:
            if any(term in content_term for content_term in content_terms):
                matched_terms += 1
        
        if query_terms:
            score += (matched_terms / len(query_terms)) * 0.3
        
        # æ—¥æœ¬èªç‰¹æœ‰ã®ãƒãƒƒãƒãƒ³ã‚°
        if re.search(r'[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]', query):
            # ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠã®ç›¸äº’ãƒãƒƒãƒãƒ³ã‚°
            if 'ã»ã£ã¨ã‚‰ã„ãµ' in query_lower and 'ãƒ›ãƒƒãƒˆãƒ©ã‚¤ãƒ•' in content:
                score += 0.4
            elif 'ãƒ›ãƒƒãƒˆãƒ©ã‚¤ãƒ•' in query_lower and 'ã»ã£ã¨ã‚‰ã„ãµ' in content:
                score += 0.4
        
        return min(score, 1.0)
    
    def calculate_semantic_score(self, content: str, query: str) -> float:
        """æ„å‘³çš„é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        if not content or not query:
            return 0.0
        
        # é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å®šç¾©
        semantic_groups = {
            'ã‚µãƒ¼ãƒ“ã‚¹': ['ã‚µãƒ¼ãƒ“ã‚¹', 'service', 'æä¾›', 'åˆ©ç”¨', 'ä½¿ç”¨'],
            'é€£çµ¡å…ˆ': ['é€£çµ¡å…ˆ', 'é›»è©±', 'TEL', 'tel', 'ãƒ¡ãƒ¼ãƒ«', 'mail', 'å•ã„åˆã‚ã›', 'ãŠå•ã„åˆã‚ã›'],
            'ä¼šç¤¾': ['ä¼šç¤¾', 'ä¼æ¥­', 'company', 'æ³•äºº', 'æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾'],
            'æ–™é‡‘': ['æ–™é‡‘', 'ä¾¡æ ¼', 'è²»ç”¨', 'ã‚³ã‚¹ãƒˆ', 'é‡‘é¡', 'å€¤æ®µ'],
            'æ‰‹é †': ['æ‰‹é †', 'æ–¹æ³•', 'ã‚„ã‚Šæ–¹', 'ãƒ—ãƒ­ã‚»ã‚¹', 'æµã‚Œ', 'ã‚¹ãƒ†ãƒƒãƒ—'],
        }
        
        content_lower = content.lower()
        query_lower = query.lower()
        
        score = 0.0
        
        # ã‚¯ã‚¨ãƒªã¨å†…å®¹ã®æ„å‘³çš„é–¢é€£æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        for group_name, keywords in semantic_groups.items():
            query_has_group = any(keyword in query_lower for keyword in keywords)
            content_has_group = any(keyword in content_lower for keyword in keywords)
            
            if query_has_group and content_has_group:
                score += 0.2
        
        return min(score, 1.0)
    
    def generate_query_embedding(self, query: str) -> List[float]:
        """ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
        try:
            logger.info(f"ğŸ§  ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆä¸­: {query[:50]}...")
            
            if self.vertex_client:
                embedding_vector = self.vertex_client.generate_embedding(query)
                
                if embedding_vector and len(embedding_vector) > 0:
                    if len(embedding_vector) != self.expected_dimensions:
                        logger.warning(f"äºˆæœŸã—ãªã„æ¬¡å…ƒæ•°: {len(embedding_vector)}æ¬¡å…ƒï¼ˆæœŸå¾…å€¤: {self.expected_dimensions}æ¬¡å…ƒï¼‰")
                    logger.info(f"âœ… åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº†: {len(embedding_vector)}æ¬¡å…ƒ")
                    return embedding_vector
                else:
                    logger.error("åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                    return []
            else:
                logger.error("Vertex AI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                return []
        
        except Exception as e:
            logger.error(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def ultra_accurate_search(self, query: str, company_id: str = None, max_results: int = 20) -> List[UltraSearchResult]:
        """è¶…é«˜ç²¾åº¦æ¤œç´¢ã®å®Ÿè¡Œ"""
        try:
            logger.info(f"ğŸš€ è¶…é«˜ç²¾åº¦æ¤œç´¢é–‹å§‹: '{query}'")
            
            # 1. ã‚¯ã‚¨ãƒªæ‹¡å¼µ
            expanded_queries = self.expand_japanese_query(query)
            logger.info(f"ğŸ“ ã‚¯ã‚¨ãƒªæ‹¡å¼µ: {len(expanded_queries)}å€‹ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³")
            
            all_results = []
            
            # 2. å„æ‹¡å¼µã‚¯ã‚¨ãƒªã§æ¤œç´¢å®Ÿè¡Œ
            for i, expanded_query in enumerate(expanded_queries):
                logger.info(f"ğŸ” æ¤œç´¢å®Ÿè¡Œ {i+1}/{len(expanded_queries)}: '{expanded_query}'")
                
                # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
                query_vector = self.generate_query_embedding(expanded_query)
                if not query_vector:
                    continue
                
                # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œ
                search_results = await self._execute_ultra_search(query_vector, expanded_query, company_id, max_results * 2)
                all_results.extend(search_results)
            
            if not all_results:
                logger.warning("æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return []
            
            # 3. å‹•çš„é–¾å€¤ã®è¨ˆç®—
            similarities = [r['similarity_score'] for r in all_results]
            dynamic_threshold = self.calculate_dynamic_threshold(similarities, query)
            
            # 4. çµæœã®å¼·åŒ–å‡¦ç†
            enhanced_results = []
            for result in all_results:
                if result['similarity_score'] >= dynamic_threshold:
                    enhanced_result = self._enhance_ultra_result(result, query)
                    if enhanced_result:
                        enhanced_results.append(enhanced_result)
            
            # 5. é‡è¤‡é™¤å»ã¨ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
            final_results = self._deduplicate_and_rank(enhanced_results, max_results)
            
            logger.info(f"âœ… è¶…é«˜ç²¾åº¦æ¤œç´¢å®Œäº†: {len(final_results)}ä»¶ã®çµæœ")
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
            for i, result in enumerate(final_results[:5]):
                logger.info(f"  {i+1}. {result.document_name} [ãƒãƒ£ãƒ³ã‚¯{result.chunk_index}]")
                logger.info(f"     é–¢é€£åº¦: {result.relevance_score:.3f}, ä¿¡é ¼åº¦: {result.confidence_score:.3f}")
                logger.info(f"     é¡ä¼¼åº¦: {result.similarity_score:.3f}, ã‚¯ã‚¨ãƒªãƒãƒƒãƒ: {result.query_match_score:.3f}")
            
            return final_results
        
        except Exception as e:
            logger.error(f"âŒ è¶…é«˜ç²¾åº¦æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
            return []
    
    async def _execute_ultra_search(self, query_vector: List[float], query: str, company_id: str = None, limit: int = 40) -> List[Dict]:
        """è¶…é«˜ç²¾åº¦ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®å®Ÿè¡Œ"""
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    if self.pgvector_available:
                        # pgvectorã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿæ¤œç´¢
                        cur.execute("""
                            SELECT DISTINCT 
                                c.id as chunk_id,
                                c.doc_id as document_id,
                                c.chunk_index,
                                c.content as snippet,
                                ds.name as document_name,
                                ds.type as document_type,
                                (1 - (c.embedding <=> %s)) as similarity_score
                            FROM chunks c
                            INNER JOIN document_sources ds ON ds.id = c.doc_id
                            WHERE c.embedding IS NOT NULL 
                                AND ds.active = true
                                AND (%s IS NULL OR ds.company_id = %s OR ds.company_id IS NULL)
                            ORDER BY c.embedding <=> %s
                            LIMIT %s
                        """, (query_vector, company_id, company_id, query_vector, limit))
                        
                    else:
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: L2è·é›¢è¨ˆç®—
                        cur.execute("""
                            SELECT DISTINCT 
                                c.id as chunk_id,
                                c.doc_id as document_id,
                                c.chunk_index,
                                c.content as snippet,
                                ds.name as document_name,
                                ds.type as document_type,
                                0.7 as similarity_score
                            FROM chunks c
                            INNER JOIN document_sources ds ON ds.id = c.doc_id
                            WHERE c.content IS NOT NULL 
                                AND ds.active = true
                                AND (%s IS NULL OR ds.company_id = %s OR ds.company_id IS NULL)
                            LIMIT %s
                        """, (company_id, company_id, limit))
                    
                    results = cur.fetchall()
                    
                    # çµæœã‚’å¤‰æ›ï¼ˆdocument_sources.nameã‚’å¿…ãšä½¿ç”¨ï¼‰
                    for row in results:
                        # document_sources.nameã‚’å¿…ãšä½¿ç”¨
                        document_name = row['document_name'] if row['document_name'] else 'Unknown Document'
                        
                        search_results.append(UltraSearchResult(
                            chunk_id=row['chunk_id'],
                            document_id=row['document_id'],
                            chunk_index=row['chunk_index'],
                            content=row['snippet'],
                            document_name=document_name,  # document_sources.nameã®ã¿
                            document_type=row['document_type'],
                            relevance_score=float(row['similarity_score']),
                            confidence_score=float(row['similarity_score']) * 0.9,
                            search_method="ultra_accurate_vector"
                        ))
                    
                    return search_results
        
        except Exception as e:
            logger.error(f"è¶…é«˜ç²¾åº¦æ¤œç´¢å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _enhance_ultra_result(self, result: Dict, query: str) -> Optional[UltraSearchResult]:
        """æ¤œç´¢çµæœã®è¶…é«˜ç²¾åº¦å¼·åŒ–"""
        try:
            # å„ç¨®ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
            query_match_score = self.calculate_query_match_score(result['snippet'] or '', query)
            semantic_score = self.calculate_semantic_score(result['snippet'] or '', query)
            
            # ç·åˆé–¢é€£åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
            relevance_score = (
                result['similarity_score'] * 0.4 +
                query_match_score * 0.35 +
                semantic_score * 0.25
            )
            
            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
            confidence_score = min(
                result['similarity_score'] + query_match_score * 0.5,
                1.0
            )
            
            return UltraSearchResult(
                chunk_id=result['chunk_id'],
                document_id=result['document_id'],
                document_name=result['document_name'],
                content=result['snippet'] or '',
                similarity_score=result['similarity_score'],
                relevance_score=relevance_score,
                confidence_score=confidence_score,
                chunk_index=result['chunk_index'],
                document_type=result['document_type'],
                search_method='ultra_accurate',
                query_match_score=query_match_score,
                semantic_score=semantic_score,
                metadata={
                    'special': result.get('special'),
                }
            )
        
        except Exception as e:
            logger.error(f"çµæœå¼·åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _deduplicate_and_rank(self, results: List[UltraSearchResult], max_results: int) -> List[UltraSearchResult]:
        """é‡è¤‡é™¤å»ã¨æœ€çµ‚ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        seen_content = set()
        seen_documents = defaultdict(int)
        final_results = []
        
        # é–¢é€£åº¦ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x.relevance_score, reverse=True)
        
        for result in results:
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆå…ˆé ­50æ–‡å­—ï¼‰
            content_key = result.content[:50].strip()
            if content_key in seen_content:
                continue
            
            # åŒä¸€æ–‡æ›¸ã‹ã‚‰ã®çµæœæ•°åˆ¶é™ï¼ˆæœ€å¤§4ä»¶ï¼‰
            if seen_documents[result.document_id] >= 4:
                continue
            
            # æœ€å°ä¿¡é ¼åº¦é–¾å€¤ãƒã‚§ãƒƒã‚¯
            if result.confidence_score < 0.1:
                continue
            
            seen_content.add(content_key)
            seen_documents[result.document_id] += 1
            final_results.append(result)
            
            if len(final_results) >= max_results:
                break
        
        return final_results
    
    async def get_ultra_accurate_content(self, query: str, company_id: str = None, max_results: int = 15) -> str:
        """è¶…é«˜ç²¾åº¦æ–‡æ›¸å†…å®¹å–å¾—"""
        try:
            # è¶…é«˜ç²¾åº¦æ¤œç´¢å®Ÿè¡Œ
            search_results = await self.ultra_accurate_search(query, company_id, max_results)
            
            if not search_results:
                logger.warning("é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return ""
            
            # çµæœã‚’çµ„ã¿ç«‹ã¦
            relevant_content = []
            total_length = 0
            max_total_length = 60000  # åˆ¶é™ã‚’æ‹¡å¤§
            
            logger.info(f"ğŸ“Š è¶…é«˜ç²¾åº¦æ¤œç´¢çµæœã‚’å‡¦ç†ä¸­: {len(search_results)}ä»¶")
            
            for i, result in enumerate(search_results):
                logger.info(f"  {i+1}. {result.document_name} [ãƒãƒ£ãƒ³ã‚¯{result.chunk_index}]")
                logger.info(f"     é–¢é€£åº¦: {result.relevance_score:.3f} (ä¿¡é ¼åº¦: {result.confidence_score:.3f})")
                
                if result.content and len(result.content.strip()) > 0:
                    content_piece = f"\n=== {result.document_name} - å‚è€ƒè³‡æ–™{result.chunk_index} (é–¢é€£åº¦: {result.relevance_score:.3f}) ===\n{result.content}\n"
                    
                    if total_length + len(content_piece) <= max_total_length:
                        relevant_content.append(content_piece)
                        total_length += len(content_piece)
                        logger.info(f"    - è¿½åŠ å®Œäº† ({len(content_piece)}æ–‡å­—)")
                    else:
                        logger.info(f"    - æ–‡å­—æ•°åˆ¶é™ã«ã‚ˆã‚Šé™¤å¤–")
                        break
            
            final_content = "\n".join(relevant_content)
            logger.info(f"âœ… è¶…é«˜ç²¾åº¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ§‹ç¯‰å®Œäº†: {len(relevant_content)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã€{len(final_content)}æ–‡å­—")
            
            return final_content
        
        except Exception as e:
            logger.error(f"âŒ è¶…é«˜ç²¾åº¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return ""

# ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—é–¢æ•°
def get_ultra_accurate_search_instance() -> Optional[UltraAccurateSearchSystem]:
    """è¶…é«˜ç²¾åº¦æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    try:
        return UltraAccurateSearchSystem()
    except Exception as e:
        logger.error(f"è¶…é«˜ç²¾åº¦æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
        return None

def ultra_accurate_search_available() -> bool:
    """è¶…é«˜ç²¾åº¦æ¤œç´¢ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        instance = get_ultra_accurate_search_instance()
        return instance is not None
    except Exception:
        return False
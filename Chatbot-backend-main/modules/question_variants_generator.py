"""
ğŸ”„ è³ªå•ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
Geminiã‚’ä½¿ã£ã¦è³ªå•ã®5ã¤ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã—ã€RAGæ¤œç´¢ã®ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã¾ã™

ç”Ÿæˆã•ã‚Œã‚‹ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³:
1. å…ƒã®è³ªå•ï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«ï¼‰
2. ç©ºç™½ã‚’å‰Šé™¤ã—ãŸè³ªå•
3. åŠè§’æ–‡å­—ã‚’å…¨è§’ã«ã—ãŸè³ªå•  
4. å…¨è§’æ–‡å­—ã‚’åŠè§’ã«ã—ãŸè³ªå•
5. è¡¨è¨˜ã‚†ã‚Œã‚’æ­£è¦åŒ–ã—ãŸè³ªå•
"""

import os
import re
import json
import logging
from typing import List, Dict, Optional
from dataclasses import dataclass
import unicodedata
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

logger = logging.getLogger(__name__)

@dataclass
class QuestionVariants:
    """è³ªå•ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³çµæœ"""
    original: str                    # å…ƒã®è³ªå•
    no_spaces: str                   # ç©ºç™½å‰Šé™¤ç‰ˆ
    full_width: str                  # åŠè§’â†’å…¨è§’å¤‰æ›ç‰ˆ
    half_width: str                  # å…¨è§’â†’åŠè§’å¤‰æ›ç‰ˆ
    normalized: str                  # è¡¨è¨˜ã‚†ã‚Œæ­£è¦åŒ–ç‰ˆ
    katakana_to_hiragana: str        # ã‚«ã‚¿ã‚«ãƒŠâ†’ã²ã‚‰ãŒãªå¤‰æ›ç‰ˆ
    hiragana_to_katakana: str        # ã²ã‚‰ãŒãªâ†’ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›ç‰ˆ
    partial_keywords: List[str]      # éƒ¨åˆ†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç‰ˆ
    abbreviations: List[str]         # ç•¥ç§°ç‰ˆ
    combination_patterns: List[str]  # çµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰ˆ
    all_variants: List[str]          # å…¨ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒªã‚¹ãƒˆï¼ˆé‡è¤‡é™¤å»æ¸ˆã¿ï¼‰

class QuestionVariantsGenerator:
    """è³ªå•ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.gemini_model = self._setup_gemini()
        
        # è¡¨è¨˜ã‚†ã‚Œæ­£è¦åŒ–ãƒ«ãƒ¼ãƒ«
        self.normalization_rules = {
            # ä¼šç¤¾è¡¨è¨˜
            r'æ ªå¼ä¼šç¤¾|ãˆ±|ï¼ˆæ ªï¼‰|\(æ ª\)': 'æ ªå¼ä¼šç¤¾',
            r'æœ‰é™ä¼šç¤¾|ãˆ²|ï¼ˆæœ‰ï¼‰|\(æœ‰\)': 'æœ‰é™ä¼šç¤¾', 
            r'åˆåŒä¼šç¤¾|ï¼ˆåŒï¼‰|\(åŒ\)': 'åˆåŒä¼šç¤¾',
            r'åˆè³‡ä¼šç¤¾|ï¼ˆè³‡ï¼‰|\(è³‡\)': 'åˆè³‡ä¼šç¤¾',
            r'åˆåä¼šç¤¾|ï¼ˆåï¼‰|\(å\)': 'åˆåä¼šç¤¾',
            
            # å½¹è·ãƒ»äººç‰©é–¢é€£
            r'ä»£è¡¨è€…|ãƒˆãƒƒãƒ—|ç¤¾é•·|ä»£è¡¨å–ç· å½¹|ä»£è¡¨|è²¬ä»»è€…|ãƒªãƒ¼ãƒ€ãƒ¼|CEO|ceo|æœ€é«˜çµŒå–¶è²¬ä»»è€…': 'ä»£è¡¨è€…',
            r'ç¤¾é•·|ä»£è¡¨å–ç· å½¹ç¤¾é•·|ä»£è¡¨å–ç· å½¹|å–ç· å½¹ç¤¾é•·': 'ç¤¾é•·',
            r'éƒ¨é•·|ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼|éƒ¨é–€é•·|è²¬ä»»è€…|ãƒªãƒ¼ãƒ€ãƒ¼|ãƒãƒ¼ãƒ•': 'éƒ¨é•·',
            r'æ‹…å½“è€…|æ‹…å½“|è²¬ä»»è€…|çª“å£': 'æ‹…å½“è€…',
            r'çµŒå–¶è€…|ã‚ªãƒ¼ãƒŠãƒ¼|ä»£è¡¨|ãƒˆãƒƒãƒ—': 'çµŒå–¶è€…',
            
            # æŠ€è¡“ç”¨èª
            r'ãƒ‘ã‚½ã‚³ãƒ³|ï¼°ï¼£|pc': 'PC',
            r'ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ|ãƒãƒƒãƒˆ|WEB|ã‚¦ã‚§ãƒ–': 'ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ',
            r'ãƒ¡ãƒ¼ãƒ«|ï¼¥ãƒ¡ãƒ¼ãƒ«|e-mail|email': 'ãƒ¡ãƒ¼ãƒ«',
            r'ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸|HP|ï¼¨ï¼°|ã‚µã‚¤ãƒˆ': 'ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸',
            
            # ä¸€èˆ¬ç”¨èª
            r'ãŠå•ã„åˆã‚ã›|å•åˆã›|å•ã„åˆã›': 'ãŠå•ã„åˆã‚ã›',
            r'é€£çµ¡å…ˆ|é€£çµ¡å…ˆæƒ…å ±|ã‚³ãƒ³ã‚¿ã‚¯ãƒˆ': 'é€£çµ¡å…ˆ',
            r'é›»è©±ç•ªå·|TEL|Tel|tel|ï¼´ï¼¥ï¼¬': 'é›»è©±ç•ªå·',
            r'ä½æ‰€|æ‰€åœ¨åœ°|ã‚¢ãƒ‰ãƒ¬ã‚¹': 'ä½æ‰€',
            r'å ´æ‰€|ä½æ‰€|æ‰€åœ¨åœ°|ä½ç½®|ã‚¢ãƒ‰ãƒ¬ã‚¹': 'ä½æ‰€',
            r'ä¼šç¤¾|ä¼æ¥­|æ³•äºº|äº‹æ¥­è€…|çµ„ç¹”': 'ä¼šç¤¾',
            r'æ•™ãˆã¦|çŸ¥ã‚ŠãŸã„|èããŸã„|åˆ†ã‹ã‚‰ãªã„': 'æ•™ãˆã¦',
        }
        
        logger.info("âœ… è³ªå•ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def _setup_gemini(self):
        """Geminiãƒ¢ãƒ‡ãƒ«ã®è¨­å®š"""
        try:
            import google.generativeai as genai
            
            api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
            if not api_key:
                logger.warning("âš ï¸ GEMINI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return None
            
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-2.5-flash')
            return model
        except Exception as e:
            logger.error(f"âŒ Geminiè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _remove_spaces(self, text: str) -> str:
        """ç©ºç™½æ–‡å­—ã‚’å‰Šé™¤"""
        # å…¨è§’ãƒ»åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã€ã‚¿ãƒ–ã€æ”¹è¡Œã‚’å‰Šé™¤
        return re.sub(r'[\s\u3000]+', '', text)
    
    def _to_full_width(self, text: str) -> str:
        """åŠè§’æ–‡å­—ã‚’å…¨è§’ã«å¤‰æ›"""
        # ASCIIæ–‡å­—ï¼ˆæ•°å­—ã€è‹±å­—ã€è¨˜å·ï¼‰ã‚’å…¨è§’ã«å¤‰æ›
        result = ""
        for char in text:
            # ASCIIç¯„å›²ã®æ–‡å­—ã‚’å…¨è§’ã«å¤‰æ›
            if ord(char) >= 32 and ord(char) <= 126:
                # åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã¯å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã«
                if char == ' ':
                    result += 'ã€€'
                else:
                    # ãã®ä»–ã®ASCIIæ–‡å­—ã¯å…¨è§’ã«å¤‰æ›
                    full_width_char = chr(ord(char) - 32 + 65248)
                    result += full_width_char
            else:
                result += char
        return result
    
    def _to_half_width(self, text: str) -> str:
        """å…¨è§’æ–‡å­—ã‚’åŠè§’ã«å¤‰æ›"""
        # å…¨è§’ASCIIæ–‡å­—ã‚’åŠè§’ã«å¤‰æ›
        result = ""
        for char in text:
            # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã¯åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã«
            if char == 'ã€€':
                result += ' '
            # å…¨è§’ASCIIæ–‡å­—ã‚’åŠè§’ã«å¤‰æ›
            elif ord(char) >= 65248 and ord(char) <= 65370:
                half_width_char = chr(ord(char) - 65248 + 32)
                result += half_width_char
            # ã‚«ã‚¿ã‚«ãƒŠã®å…¨è§’â†’åŠè§’å¤‰æ›ã‚‚å®Ÿè¡Œ
            else:
                # unicodedataã‚’ä½¿ã£ã¦NFKCæ­£è¦åŒ–ï¼ˆå…¨è§’â†’åŠè§’å¤‰æ›ã‚’å«ã‚€ï¼‰
                normalized_char = unicodedata.normalize('NFKC', char)
                result += normalized_char
        return result
    
    def _normalize_variations(self, text: str) -> str:
        """è¡¨è¨˜ã‚†ã‚Œã‚’æ­£è¦åŒ–"""
        normalized = text
        
        # æ­£è¦åŒ–ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨
        for pattern, replacement in self.normalization_rules.items():
            normalized = re.sub(pattern, replacement, normalized, flags=re.IGNORECASE)
        
        return normalized
    
    def _katakana_to_hiragana(self, text: str) -> str:
        """ã‚«ã‚¿ã‚«ãƒŠã‚’ã²ã‚‰ãŒãªã«å¤‰æ›"""
        result = ""
        for char in text:
            # ã‚«ã‚¿ã‚«ãƒŠç¯„å›²ï¼ˆã‚¢-ãƒ´ï¼‰ã‚’ã²ã‚‰ãŒãªç¯„å›²ï¼ˆã‚-ã‚”ï¼‰ã«å¤‰æ›
            if 'ã‚¢' <= char <= 'ãƒ´':
                hiragana_char = chr(ord(char) - ord('ã‚¢') + ord('ã‚'))
                result += hiragana_char
            else:
                result += char
        return result
    
    def _hiragana_to_katakana(self, text: str) -> str:
        """ã²ã‚‰ãŒãªã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›"""
        result = ""
        for char in text:
            # ã²ã‚‰ãŒãªç¯„å›²ï¼ˆã‚-ã‚”ï¼‰ã‚’ã‚«ã‚¿ã‚«ãƒŠç¯„å›²ï¼ˆã‚¢-ãƒ´ï¼‰ã«å¤‰æ›
            if 'ã‚' <= char <= 'ã‚”':
                katakana_char = chr(ord(char) - ord('ã‚') + ord('ã‚¢'))
                result += katakana_char
            else:
                result += char
        return result
    
    def _extract_partial_keywords(self, text: str) -> List[str]:
        """éƒ¨åˆ†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keywords = []
        
        # ä¼šç¤¾åãƒ‘ã‚¿ãƒ¼ãƒ³ã®æŠ½å‡º
        company_patterns = [
            r'([^ã€‚ã€\s]+(?:æ ªå¼ä¼šç¤¾|åˆåŒä¼šç¤¾|æœ‰é™ä¼šç¤¾|åˆè³‡ä¼šç¤¾|åˆåä¼šç¤¾))',
            r'([^ã€‚ã€\s]+ä¼šç¤¾)',
            r'([^ã€‚ã€\s]+(?:Corporation|Corp|Inc|LLC|Ltd))'
        ]
        
        for pattern in company_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                keywords.append(match)
                # ä¼šç¤¾åéƒ¨åˆ†ã®ã¿ï¼ˆæ³•äººæ ¼é™¤ãï¼‰ã‚‚è¿½åŠ 
                company_name_only = re.sub(r'(æ ªå¼ä¼šç¤¾|åˆåŒä¼šç¤¾|æœ‰é™ä¼šç¤¾|åˆè³‡ä¼šç¤¾|åˆåä¼šç¤¾|ä¼šç¤¾|Corporation|Corp|Inc|LLC|Ltd)$', '', match).strip()
                if company_name_only and company_name_only != match:
                    keywords.append(company_name_only)
        
        # å˜èªåˆ†å‰²ï¼ˆ2æ–‡å­—ä»¥ä¸Šã®å˜èªï¼‰
        words = re.findall(r'[ã-ã‚Ÿã‚¡-ãƒ¿ä¸€-é¾¯ï½-ï½šï¼¡-ï¼ºï¼-ï¼™a-zA-Z0-9]{2,}', text)
        keywords.extend(words)
        
        # é›»è©±ç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³
        phone_patterns = [
            r'\d{2,4}-\d{2,4}-\d{4}',
            r'\d{3}-\d{3}-\d{4}',
            r'\(\d{2,4}\)\s*\d{2,4}-\d{4}',
            r'\d{10,11}'
        ]
        
        for pattern in phone_patterns:
            matches = re.findall(pattern, text)
            keywords.extend(matches)
        
        return keywords
    
    def _generate_abbreviations(self, text: str) -> List[str]:
        """ç•¥ç§°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆï¼ˆã‚¹ãƒšãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³å«ã‚€ï¼‰"""
        abbreviations = []
        
        # ä¼šç¤¾åã®ç•¥ç§°
        company_patterns = [
            r'([^ã€‚ã€\s]+)(æ ªå¼ä¼šç¤¾|åˆåŒä¼šç¤¾|æœ‰é™ä¼šç¤¾|åˆè³‡ä¼šç¤¾|åˆåä¼šç¤¾)',
            r'(æ ªå¼ä¼šç¤¾|åˆåŒä¼šç¤¾|æœ‰é™ä¼šç¤¾|åˆè³‡ä¼šç¤¾|åˆåä¼šç¤¾)\s*([^ã€‚ã€\s]+)'
        ]
        
        for pattern in company_patterns:
            company_match = re.search(pattern, text)
            if company_match:
                if len(company_match.groups()) == 2:
                    if pattern.startswith('([^'):  # ä¼šç¤¾åãŒå…ˆã«ã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
                        company_name = company_match.group(1)
                        company_type = company_match.group(2)
                    else:  # æ³•äººæ ¼ãŒå…ˆã«ã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
                        company_type = company_match.group(1)
                        company_name = company_match.group(2)
                    
                    # æ³•äººæ ¼ã®ç•¥ç§°
                    company_abbreviations = {
                        'æ ªå¼ä¼šç¤¾': ['ãˆ±'],
                        'æœ‰é™ä¼šç¤¾': ['ãˆ²'],
                        'åˆåŒä¼šç¤¾': ['(åŒ)', 'ï¼ˆåŒï¼‰'],
                        'åˆè³‡ä¼šç¤¾': ['(è³‡)', 'ï¼ˆè³‡ï¼‰'],
                        'åˆåä¼šç¤¾': ['(å)', 'ï¼ˆåï¼‰']
                    }
                    
                    if company_type in company_abbreviations:
                        for abbrev in company_abbreviations[company_type]:
                            # ã‚¹ãƒšãƒ¼ã‚¹ãªã—ã€åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã€å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã®3ãƒ‘ã‚¿ãƒ¼ãƒ³
                            abbreviations.extend([
                                f'{abbrev}{company_name}',
                                f'{abbrev} {company_name}',
                                f'{abbrev}ã€€{company_name}'
                            ])
                    
                    # ã‚«ã‚¿ã‚«ãƒŠã®ç•¥ç§°ï¼ˆæœ€åˆã®æ–‡å­—ã®ã¿ï¼‰
                    katakana_chars = re.findall(r'[ã‚¡-ãƒ¿]', company_name)
                    if len(katakana_chars) >= 2:
                        abbreviations.append(''.join(katakana_chars[:2]))  # æœ€åˆã®2æ–‡å­—
                        abbreviations.append(''.join(katakana_chars))      # å…¨ã‚«ã‚¿ã‚«ãƒŠ
                    
                    # è‹±èªç•¥ç§°ï¼ˆå¤§æ–‡å­—ã®æœ€åˆã®æ–‡å­—ï¼‰
                    english_chars = re.findall(r'[A-Z]', company_name)
                    if len(english_chars) >= 2:
                        abbreviations.append(''.join(english_chars))
        
        # ã‚ˆãã‚ã‚‹ç•¥èªãƒ‘ã‚¿ãƒ¼ãƒ³
        abbreviation_map = {
            'ãƒªã‚¢ãƒ©ã‚¤ã‚º': ['ãƒªã‚¢', 'ãƒ©ã‚¤ã‚º', 'RL'],
            'ã‚¤ãƒ³ã‚¿ãƒ¼ãƒŠã‚·ãƒ§ãƒŠãƒ«': ['ã‚¤ãƒ³ã‚¿ãƒ¼', 'ã‚¤ãƒ³ã‚¿ãƒŠã‚·ãƒ§ãƒŠãƒ«', 'Intl'],
            'ã‚³ãƒ¼ãƒãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³': ['ã‚³ãƒ¼ãƒ—', 'Corp'],
            'ã‚·ã‚¹ãƒ†ãƒ ': ['ã‚·ã‚¹', 'Sys'],
            'ã‚µãƒ¼ãƒ“ã‚¹': ['ã‚µãƒ“ã‚¹', 'Srv'],
            'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼': ['ãƒ†ãƒƒã‚¯', 'Tech'],
            'ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³': ['ã‚½ãƒªãƒ¥ãƒ¼', 'Sol'],
            'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°': ['ã‚¨ãƒ³ã‚¸', 'Eng'],
            'ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°': ['ã‚³ãƒ³ã‚µãƒ«', 'Cons'],
            'ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ': ['ãƒãƒã‚¸', 'Mgmt'],
        }
        
        for full_word, abbrevs in abbreviation_map.items():
            if full_word in text:
                abbreviations.extend(abbrevs)
        
        return abbreviations
    
    def _generate_combination_patterns(self, base_text: str, keywords: List[str], abbreviations: List[str]) -> List[str]:
        """çµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆï¼ˆã‚¹ãƒšãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³å«ã‚€ï¼‰"""
        combinations = []
        
        # åŸºæœ¬çš„ãªçµ„ã¿åˆã‚ã›
        combinations.append(base_text)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åŒå£«ã®çµ„ã¿åˆã‚ã›ï¼ˆ3ã¤ã®ã‚¹ãƒšãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        for i, keyword1 in enumerate(keywords[:5]):  # æœ€åˆã®5å€‹ã¾ã§
            for j, keyword2 in enumerate(keywords[:5]):
                if i != j and len(keyword1) > 1 and len(keyword2) > 1:
                    # 3ã¤ã®ã‚¹ãƒšãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
                    combinations.append(f"{keyword1}{keyword2}")   # ã‚¹ãƒšãƒ¼ã‚¹ãªã—
                    combinations.append(f"{keyword1} {keyword2}")  # åŠè§’ã‚¹ãƒšãƒ¼ã‚¹
                    combinations.append(f"{keyword1}ã€€{keyword2}") # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹
        
        # ç•¥ç§°ã¨ã®çµ„ã¿åˆã‚ã›ï¼ˆ3ã¤ã®ã‚¹ãƒšãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        for abbrev in abbreviations[:3]:  # æœ€åˆã®3å€‹ã¾ã§
            combinations.append(abbrev)
            for keyword in keywords[:3]:
                if keyword != abbrev and len(keyword) > 1:
                    # 3ã¤ã®ã‚¹ãƒšãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
                    combinations.extend([
                        f"{abbrev}{keyword}",   # ã‚¹ãƒšãƒ¼ã‚¹ãªã—
                        f"{abbrev} {keyword}",  # åŠè§’ã‚¹ãƒšãƒ¼ã‚¹
                        f"{abbrev}ã€€{keyword}", # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹
                        f"{keyword}{abbrev}",   # é€†é †ã‚¹ãƒšãƒ¼ã‚¹ãªã—
                        f"{keyword} {abbrev}",  # é€†é †åŠè§’ã‚¹ãƒšãƒ¼ã‚¹
                        f"{keyword}ã€€{abbrev}"  # é€†é †å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹
                    ])
        
        # æ³•äººæ ¼ã¨ã®çµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³
        company_types = ['æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾', 'åˆåŒä¼šç¤¾', 'åˆè³‡ä¼šç¤¾', 'åˆåä¼šç¤¾']
        for company_type in company_types:
            if company_type in base_text:
                for keyword in keywords[:3]:
                    if keyword != company_type and len(keyword) > 1:
                        # æ³•äººæ ¼ + ä¼šç¤¾åã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ3ã¤ã®ã‚¹ãƒšãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
                        combinations.extend([
                            f"{company_type}{keyword}",   # ã‚¹ãƒšãƒ¼ã‚¹ãªã—
                            f"{company_type} {keyword}",  # åŠè§’ã‚¹ãƒšãƒ¼ã‚¹
                            f"{company_type}ã€€{keyword}"  # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹
                        ])
        
        return combinations
    
    async def generate_variants_with_gemini(self, question: str) -> QuestionVariants:
        """
        ğŸ§  Geminiã‚’ä½¿ã£ã¦è³ªå•ã®5ã¤ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
        
        Args:
            question: å…ƒã®è³ªå•
            
        Returns:
            QuestionVariants: 5ã¤ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        """
        logger.info(f"ğŸ”„ è³ªå•ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆé–‹å§‹: '{question}'")
        
        if not self.gemini_model:
            logger.warning("âš ï¸ GeminiãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åŸºæœ¬å¤‰æ›ã®ã¿å®Ÿè¡Œ")
            return self._generate_basic_variants(question)
        
        try:
            # è³ªå•ã®è¨€èªã«é©å¿œã—ãŸè¨€ã„æ›ãˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            prompt = f"""
ã‚ãªãŸã¯è³ªå•ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã™ã‚‹å°‚é–€ã®AIã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸè³ªå•ã«å¯¾ã—ã¦ã€æ„å‘³ã‚’å¤‰ãˆãšã«è¡¨è¨˜ã ã‘ã‚’å¤‰æ›´ã—ãŸãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã€ä»¥ä¸‹ã®JSONå½¢å¼ã§**ã®ã¿**ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚**JSONä»¥å¤–ã®ä¸€åˆ‡ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆèª¬æ˜ã€å‰æ›¸ãã€å¾Œæ›¸ããªã©ï¼‰ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚**

ã€æ³•äººæ ¼ã®ã‚¹ãƒšãƒ¼ã‚¹è¦å‰‡ï¼ˆé‡è¦ï¼‰ã€‘
ãƒ»ã€ä¼šç¤¾ã€ã¨ã„ã†èªã‚’å«ã‚€æ³•äººæ ¼ï¼ˆä¾‹: æ ªå¼ä¼šç¤¾ã€æœ‰é™ä¼šç¤¾ã€åˆåŒä¼šç¤¾ã€ãˆ± ãªã©ï¼‰ã®ç›´å¾Œã«ã¯ã€å¿…ãšåŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’ 1 ã¤å…¥ã‚Œã¦ãã ã•ã„ã€‚
  ä¾‹ï¼‰
    Ã—ã€Œæ ªå¼ä¼šç¤¾ABCã€ â†’ â—‹ã€Œæ ªå¼ä¼šç¤¾ ABCã€
    Ã—ã€Œ(æ ª)ABCã€     â†’ â—‹"(æ ª) ABC"

ã€é‡è¦ãªåˆ¶ç´„ã€‘
- è³ªå•ã®æ„å‘³ãƒ»å†…å®¹ã¯çµ¶å¯¾ã«å¤‰æ›´ã—ãªã„ã“ã¨ã€‚
- ã‚ãã¾ã§ã€Œè¡¨è¨˜ã®è¨€ã„æ›ãˆã€ã«é™å®šã—ã€æ–°ã—ã„æƒ…å ±ã‚’è¿½åŠ ã—ãªã„ã“ã¨ã€‚
- æ–‡å­—ç¨®å¤‰æ›ï¼ˆå…¨è§’â‡”åŠè§’ã€å¤§æ–‡å­—â‡”å°æ–‡å­—ã€ã‚«ã‚¿ã‚«ãƒŠâ‡”ã²ã‚‰ãŒãªãªã©ï¼‰ã€ã‚¹ãƒšãƒ¼ã‚¹ã®æœ‰ç„¡ï¼ˆåŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã€å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã€ã‚¹ãƒšãƒ¼ã‚¹ãªã—ï¼‰ã€æ³•äººæ ¼ã‚„çµ„ç¹”åã®è¡¨è¨˜ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã€åŒç¾©èªã§ã®ç½®ãæ›ãˆã€å¥èª­ç‚¹ãƒ»è¨˜å·ã®æœ‰ç„¡ã‚„ç¨®é¡ã€ãã®è¨€èªå›ºæœ‰ã®è¡¨è¨˜ã‚†ã‚Œã‚„æ…£ç”¨è¡¨ç¾ã‚’è€ƒæ…®ã—ã¦ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹ã“ã¨ã€‚

**è³ªå•:**
{question}

**ä»¥ä¸‹ã®JSONå½¢å¼ã§ã€æœ€å¤§10å€‹ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚ä½™è¨ˆãªèª¬æ˜ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚**
{{
  "variants": [
    {{"text": "ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³1", "reason": "å¤‰æ›´å†…å®¹ã®èª¬æ˜"}},
    {{"text": "ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³2", "reason": "å¤‰æ›´å†…å®¹ã®èª¬æ˜"}}
  ]
}}
"""
            
            # Geminiå®Ÿè¡Œï¼ˆä¿å®ˆçš„è¨­å®šï¼šæ„å‘³ã‚’å¤‰ãˆãªã„è¨€ã„æ›ãˆé‡è¦–ï¼‰
            import google.generativeai as genai
            response = self.gemini_model.generate_content(
                prompt,
                generation_config=genai.GenerationConfig(
                    temperature=0.4,  # ä¸€è²«æ€§é‡è¦–ã§æ„å‘³å¤‰æ›´ã‚’é˜²æ­¢
                    max_output_tokens=2048,  # 50å€‹ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
                    top_p=0.8,  # é©åº¦ãªå¤šæ§˜æ€§
                    top_k=50    # é©åº¦ãªå€™è£œæ•°
                )
            )
            
            if not response or not response.text:
                logger.warning("âš ï¸ Geminiã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã™")
                return self._generate_basic_variants(question)
            
            # JSONè§£æ
            json_content_to_parse = response.text.strip()
            
            # ã¾ãšã€Markdownã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰JSONå†…å®¹ã‚’æ­£ç¢ºã«æŠ½å‡º
            # r'```json\s*(\{.*?\})\s*```' ã¯ã€`json`ã®å¾Œã®ç©ºç™½ã¨ã€JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®é–‹å§‹ãƒ»çµ‚äº†ã€ãã®å¾Œã®ç©ºç™½ã€
            # ãã—ã¦æœ€å¾Œã®` ``` `ã‚’è€ƒæ…®ã—ã¦ã„ã¾ã™ã€‚`.*?`ã¯éè²ªæ¬²ãƒãƒƒãƒã§ã€æœ€åˆã®`}`ã§åœæ­¢ã—ã¾ã™ã€‚
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', json_content_to_parse, re.DOTALL)
            
            if json_match:
                json_content_to_parse = json_match.group(1) # ã‚°ãƒ«ãƒ¼ãƒ—1ã¯JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿
                logger.info("âœ… Markdownã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã®JSONã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
            else:
                logger.warning("âš ï¸ Markdownã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã®JSONãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å¿œç­”å…¨ä½“ã‚’JSONã¨ã—ã¦è§£æã‚’è©¦ã¿ã¾ã™ã€‚")

            try:
                variants_data = json.loads(json_content_to_parse)
                logger.info("âœ… JSONã‚’æ­£å¸¸ã«è§£æã—ã¾ã—ãŸã€‚")
            except json.JSONDecodeError as e:
                logger.error(f"âŒ æœ€çµ‚çš„ãªJSONè§£æã‚¨ãƒ©ãƒ¼: {e}. åŸºæœ¬ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
                return self._generate_basic_variants(question)
            
            # ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰
            variants = variants_data.get("variants", [])
            
            # é‡è¤‡ã‚’é™¤å»ã—ã¤ã¤å…¨ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒªã‚¹ãƒˆã‚’ä½œæˆ
            all_variants = []
            variant_reasons = []  # ç†ç”±ã‚‚ä¿å­˜
            
            for variant_data in variants:
                variant = variant_data.get("text", "")
                reason = variant_data.get("reason", "")
                if variant and variant.strip():
                    all_variants.append(variant.strip())
                    variant_reasons.append(reason)
            
            # é‡è¤‡é™¤å»ï¼ˆé †åºä¿æŒï¼‰
            unique_variants = list(dict.fromkeys(all_variants))
            unique_reasons = []
            for variant in unique_variants:
                idx = all_variants.index(variant)
                unique_reasons.append(variant_reasons[idx])
            
            all_variants = unique_variants
            variant_reasons = unique_reasons
            
            result = QuestionVariants(
                original=question,
                no_spaces=self._remove_spaces(question),
                full_width=self._to_full_width(question),
                half_width=self._to_half_width(question),
                normalized=question,  # AIãŒç”Ÿæˆã™ã‚‹ã®ã§æ­£è¦åŒ–ãªã—
                katakana_to_hiragana=self._katakana_to_hiragana(question),
                hiragana_to_katakana=self._hiragana_to_katakana(question),
                partial_keywords=[],  # AIã«ã‚ˆã‚‹è‡ªç”±ç”Ÿæˆã‚’é‡è¦–
                abbreviations=[],     # AIã«ã‚ˆã‚‹è‡ªç”±ç”Ÿæˆã‚’é‡è¦–
                combination_patterns=[],  # AIã«ã‚ˆã‚‹è‡ªç”±ç”Ÿæˆã‚’é‡è¦–
                all_variants=all_variants
            )
            
            logger.info(f"âœ… Geminiãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆå®Œäº†: {len(all_variants)}å€‹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³")
            logger.info(f"ğŸ”„ ç”Ÿæˆã•ã‚ŒãŸãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³:")
            for i, variant in enumerate(all_variants, 1):
                reason = variant_reasons[i-1] if i-1 < len(variant_reasons) else "ç”Ÿæˆç†ç”±ä¸æ˜"
                logger.info(f"   {i}. {variant} (ç†ç”±: {reason})")
            
            # ğŸ”¥ å¿…é ˆãƒ‘ã‚¿ãƒ¼ãƒ³: æ³•äººæ ¼ã®å¾Œã‚ã«åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
            essential_space_patterns = self._generate_essential_space_patterns(question)
            for pattern in essential_space_patterns:
                if pattern and pattern.strip() and pattern not in all_variants:
                    all_variants.append(pattern.strip())
                    logger.info(f"   âœ… å¿…é ˆãƒ‘ã‚¿ãƒ¼ãƒ³è¿½åŠ : {pattern}")
            
            # ä¼šç¤¾ã®å¾Œã‚ã«åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’å¼·åˆ¶ã™ã‚‹ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨
            all_variants = self._apply_company_space_rule(all_variants)
            
            # é‡è¤‡å†é™¤å»ã—ã¦10å€‹ã«åˆ¶é™
            dedup = []
            for v in all_variants:
                if v not in dedup:
                    dedup.append(v)
            all_variants = dedup[:10]
            
            # all_variantsã‚’æ›´æ–°
            result.all_variants = all_variants
            
            logger.info(f"ğŸ¯ æœ€çµ‚ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³æ•°: {len(all_variants)}å€‹ï¼ˆGeminiç”Ÿæˆ + å¿…é ˆãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Geminiãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return self._generate_basic_variants(question)
    
    def _generate_basic_variants(self, question: str) -> QuestionVariants:
        """è»½é‡åŸºæœ¬ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆï¼ˆAIãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã®ãƒŸãƒ‹ãƒãƒ«ç‰ˆï¼‰"""
        logger.info("ğŸ”„ è»½é‡åŸºæœ¬ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆå®Ÿè¡Œä¸­ï¼ˆAIã«ã‚ˆã‚‹è‡ªç”±äºˆæ¸¬ã‚’é‡è¦–ï¼‰...")
        
        # åŸºæœ¬å¤‰æ›ã®ã¿å®Ÿè¡Œ
        original = question.strip()
        
        # æœ€å°é™ã®åŸºæœ¬ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã®ã¿
        all_variants = [
            original,  # å…ƒã®è³ªå•
            self._remove_spaces(original),  # ã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤ã®ã¿
            self._to_full_width(original),  # å…¨è§’å¤‰æ›ã®ã¿
            self._to_half_width(original),  # åŠè§’å¤‰æ›ã®ã¿
        ]
        
        # é‡è¤‡é™¤å»
        unique_variants = []
        for variant in all_variants:
            if variant and variant.strip():
                unique_variants.append(variant.strip())
        
        # é‡è¤‡é™¤å»ï¼ˆé †åºä¿æŒï¼‰
        unique_variants = list(dict.fromkeys(unique_variants))
        
        # ğŸ”¥ å¿…é ˆãƒ‘ã‚¿ãƒ¼ãƒ³: æ³•äººæ ¼ã®å¾Œã‚ã«åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
        essential_space_patterns = self._generate_essential_space_patterns(original)
        for pattern in essential_space_patterns:
            if pattern and pattern.strip() and pattern not in unique_variants:
                unique_variants.append(pattern.strip())
        
        # ä¼šç¤¾ã®å¾Œã‚ã«åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’å¼·åˆ¶ã™ã‚‹ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨
        unique_variants = self._apply_company_space_rule(unique_variants)
        
        # 10å€‹ã«åˆ¶é™
        unique_variants = unique_variants[:10]
        
        logger.info(f"âœ… ãƒŸãƒ‹ãƒãƒ«åŸºæœ¬ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆå®Œäº†: {len(unique_variants)}å€‹ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³")
        logger.warning("âš ï¸ AIã«ã‚ˆã‚‹é«˜ç²¾åº¦ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”ŸæˆãŒæ¨å¥¨ã•ã‚Œã¾ã™ï¼ˆGEMINI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼‰")
        
        result = QuestionVariants(
            original=original,
            no_spaces=self._remove_spaces(original),
            full_width=self._to_full_width(original),
            half_width=self._to_half_width(original),
            normalized=original,  # æ­£è¦åŒ–ãªã—
            katakana_to_hiragana=self._katakana_to_hiragana(original),
            hiragana_to_katakana=self._hiragana_to_katakana(original),
            partial_keywords=[],  # å›ºå®šçš„ãªæŠ½å‡ºãªã—
            abbreviations=[],     # å›ºå®šçš„ãªç•¥ç§°ãªã—
            combination_patterns=[],  # å›ºå®šçš„ãªçµ„ã¿åˆã‚ã›ãªã—
            all_variants=unique_variants
        )
        
        return result
    
    def _generate_generic_space_patterns(self, text: str) -> List[str]:
        """æ±ç”¨çš„ãªã‚¹ãƒšãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ"""
        patterns = []
        
        # åŸºæœ¬çš„ãªã‚¹ãƒšãƒ¼ã‚¹å¤‰æ›
        if ' ' in text:  # åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ãŒã‚ã‚‹å ´åˆ
            patterns.append(text.replace(' ', 'ã€€'))  # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›
            patterns.append(text.replace(' ', ''))    # ã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤
            patterns.append(text.replace(' ', '  '))  # ãƒ€ãƒ–ãƒ«ã‚¹ãƒšãƒ¼ã‚¹
        
        if 'ã€€' in text:  # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ãŒã‚ã‚‹å ´åˆ
            patterns.append(text.replace('ã€€', ' '))  # åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›
            patterns.append(text.replace('ã€€', ''))   # ã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤
        
        # å˜èªé–“ã¸ã®ã‚¹ãƒšãƒ¼ã‚¹æŒ¿å…¥ï¼ˆæ±ç”¨çš„ï¼‰
        words = text.split()
        if len(words) >= 2:
            # ç•°ãªã‚‹ã‚¹ãƒšãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
            patterns.append('ã€€'.join(words))  # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹
            patterns.append(' '.join(words))   # åŠè§’ã‚¹ãƒšãƒ¼ã‚¹
            patterns.append(''.join(words))    # ã‚¹ãƒšãƒ¼ã‚¹ãªã—
            
            # éƒ¨åˆ†çš„ãªã‚¹ãƒšãƒ¼ã‚¹å¤‰æ›´
            for i in range(len(words) - 1):
                new_words = words.copy()
                patterns.append(' '.join(new_words[:i+1]) + 'ã€€' + ' '.join(new_words[i+1:]))
        
        # æ–‡å­—ãƒ¬ãƒ™ãƒ«ã§ã®ã‚¹ãƒšãƒ¼ã‚¹æŒ¿å…¥ï¼ˆé•·ã„æ–‡å­—åˆ—ã®å ´åˆï¼‰
        if len(text) > 6 and ' ' not in text and 'ã€€' not in text:
            mid = len(text) // 2
            patterns.append(text[:mid] + ' ' + text[mid:])
            patterns.append(text[:mid] + 'ã€€' + text[mid:])
        
        return patterns
    
    def _generate_generic_character_patterns(self, text: str) -> List[str]:
        """æ±ç”¨çš„ãªæ–‡å­—ç¨®å¤‰æ›ãƒ‘ã‚¿ãƒ¼ãƒ³"""
        patterns = []
        
        # å…¨ä½“çš„ãªæ–‡å­—ç¨®å¤‰æ›
        patterns.append(self._to_full_width(text))
        patterns.append(self._to_half_width(text))
        patterns.append(self._katakana_to_hiragana(text))
        patterns.append(self._hiragana_to_katakana(text))
        
        # éƒ¨åˆ†çš„ãªæ–‡å­—ç¨®å¤‰æ›ï¼ˆå˜èªå˜ä½ï¼‰
        words = re.findall(r'[^\s]+', text)
        for i, word in enumerate(words):
            if len(word) > 1:  # 1æ–‡å­—ã®å˜èªã¯é™¤å¤–
                modified_words = words.copy()
                
                # å„å˜èªã‚’å€‹åˆ¥ã«å¤‰æ›
                modified_words[i] = self._to_full_width(word)
                if ' ' in text:
                    patterns.append(' '.join(modified_words))
                else:
                    patterns.append(''.join(modified_words))
                
                modified_words[i] = self._to_half_width(word)
                if ' ' in text:
                    patterns.append(' '.join(modified_words))
                else:
                    patterns.append(''.join(modified_words))
                
                modified_words[i] = self._katakana_to_hiragana(word)
                if ' ' in text:
                    patterns.append(' '.join(modified_words))
                else:
                    patterns.append(''.join(modified_words))
                
                modified_words[i] = self._hiragana_to_katakana(word)
                if ' ' in text:
                    patterns.append(' '.join(modified_words))
                else:
                    patterns.append(''.join(modified_words))
        
        return patterns
    
    def _generate_generic_punctuation_patterns(self, text: str) -> List[str]:
        """æ±ç”¨çš„ãªå¥èª­ç‚¹ãƒ»è¨˜å·ãƒ‘ã‚¿ãƒ¼ãƒ³"""
        patterns = []
        
        # åŸºæœ¬çš„ãªå¥èª­ç‚¹ãƒ»è¨˜å·å¤‰æ›
        punctuation_mappings = {
            '(': 'ï¼ˆ', ')': 'ï¼‰', 'ï¼ˆ': '(', 'ï¼‰': ')',
            '-': 'ï¼', 'ï¼': '-', '.': 'ã€‚', 'ã€‚': '.',
            ',': 'ã€', 'ã€': ',', '?': 'ï¼Ÿ', 'ï¼Ÿ': '?',
            '!': 'ï¼', 'ï¼': '!', ':': 'ï¼š', 'ï¼š': ':',
            ';': 'ï¼›', 'ï¼›': ';', '"': '"', '"': '"',
            '[': 'ï¼»', ']': 'ï¼½', 'ï¼»': '[', 'ï¼½': ']'
        }
        
        for original_punct, replacement_punct in punctuation_mappings.items():
            if original_punct in text:
                patterns.append(text.replace(original_punct, replacement_punct))
        
        # èªå°¾ã®å¥èª­ç‚¹èª¿æ•´
        common_endings = ['ã€‚', '.', 'ï¼Ÿ', '?', 'ï¼', '!']
        for ending in common_endings:
            if text.endswith(ending):
                patterns.append(text[:-1])  # èªå°¾å‰Šé™¤
            else:
                patterns.append(text + ending)  # èªå°¾è¿½åŠ 
        
        # ç–‘å•æ–‡ãƒ»æ„Ÿå˜†æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³
        if not any(text.endswith(e) for e in ['ï¼Ÿ', '?', 'ï¼', '!']):
            patterns.append(text + 'ã§ã™ã‹')
            patterns.append(text + 'ã§ã—ã‚‡ã†ã‹')
        
        return patterns
    
    def _generate_generic_dot_patterns(self, text: str) -> List[str]:
        """æ±ç”¨çš„ãªä¸­ç‚¹ãƒ»åˆ†å‰²ãƒ‘ã‚¿ãƒ¼ãƒ³"""
        patterns = []
        
        # ã‚«ã‚¿ã‚«ãƒŠèªã®ä¸­ç‚¹æŒ¿å…¥
        katakana_words = re.findall(r'[ã‚¡-ãƒ¿ãƒ¼]+', text)
        for word in katakana_words:
            if len(word) >= 4:  # 4æ–‡å­—ä»¥ä¸Šã®ã‚«ã‚¿ã‚«ãƒŠèª
                # è‡ªç„¶ãªåˆ†å‰²ç‚¹ã§ä¸­ç‚¹æŒ¿å…¥
                for pos in range(2, len(word) - 1, 2):  # 2æ–‡å­—ãŠãã«åˆ†å‰²ç‚¹
                    dotted = word[:pos] + 'ãƒ»' + word[pos:]
                    patterns.append(text.replace(word, dotted))
        
        # è‹±èªå˜èªã®ä¸­ç‚¹æŒ¿å…¥
        english_words = re.findall(r'[A-Za-z]+', text)
        for word in english_words:
            if len(word) >= 4:  # 4æ–‡å­—ä»¥ä¸Šã®è‹±èªå˜èª
                mid = len(word) // 2
                dotted = word[:mid] + 'ãƒ»' + word[mid:]
                patterns.append(text.replace(word, dotted))
        
        # æ•°å­—ã®åŒºåˆ‡ã‚Š
        numbers = re.findall(r'\d{4,}', text)  # 4æ¡ä»¥ä¸Šã®æ•°å­—
        for num in numbers:
            if len(num) >= 4:
                # 3æ¡åŒºåˆ‡ã‚Š
                formatted = f"{num[:len(num)-3]},{num[-3:]}"
                patterns.append(text.replace(num, formatted))
                # ãƒã‚¤ãƒ•ãƒ³åŒºåˆ‡ã‚Š
                mid = len(num) // 2
                formatted = f"{num[:mid]}-{num[mid:]}"
                patterns.append(text.replace(num, formatted))
        
        return patterns
    
    def _generate_generic_notation_patterns(self, text: str) -> List[str]:
        """æ±ç”¨çš„ãªè¡¨è¨˜ã‚†ã‚Œãƒ‘ã‚¿ãƒ¼ãƒ³"""
        patterns = []
        
        # ä¸€èˆ¬çš„ãªè¡¨è¨˜ã‚†ã‚Œãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ‹¡å¼µå¯èƒ½ï¼‰
        notation_mappings = {
            # æŠ€è¡“ç”¨èª
            'ãƒ‘ã‚½ã‚³ãƒ³': ['PC', 'ï¼°ï¼£', 'ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿', 'ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼'],
            'PC': ['ãƒ‘ã‚½ã‚³ãƒ³', 'ï¼°ï¼£', 'ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿'],
            'ï¼°ï¼£': ['ãƒ‘ã‚½ã‚³ãƒ³', 'PC', 'ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿'],
            'ãƒ¡ãƒ¼ãƒ«': ['mail', 'Mail', 'MAIL', 'Eãƒ¡ãƒ¼ãƒ«', 'e-mail', 'email'],
            'ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸': ['HP', 'ï¼¨ï¼°', 'ã‚µã‚¤ãƒˆ', 'ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ', 'website'],
            'ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ': ['ãƒãƒƒãƒˆ', 'WEB', 'ã‚¦ã‚§ãƒ–', 'web'],
            
            # åŸºæœ¬èªå½™
            'ã«ã¤ã„ã¦': ['ã«é–¢ã—ã¦', 'ã«ä»˜ã„ã¦', 'ã«ã¤ãã¾ã—ã¦', 'ã«ã¤ã„ã¦'],
            'æ•™ãˆã¦': ['æ•™ãˆã¦ãã ã•ã„', 'ãŠæ•™ãˆãã ã•ã„', 'ã”æ•™ç¤ºãã ã•ã„'],
            'ãã ã•ã„': ['ä¸‹ã•ã„', 'ã‚¯ãƒ€ã‚µã‚¤'],
            'ã©ã“': ['ã©ã¡ã‚‰', 'ä½•å‡¦', 'ã©ã“ã‚‰'],
            'ã„ã¤': ['ä½•æ™‚', 'ã„ã¤ã”ã‚'],
            'ãªã«': ['ä½•', 'ãƒŠãƒ‹'],
            'ã ã‚Œ': ['èª°', 'ãƒ€ãƒ¬'],
            
            # æ•¬èªãƒ»ä¸å¯§èª
            'ã§ã™': ['ã§ã‚ã‚‹', 'ã ', 'ã§ã‚ã‚Šã¾ã™'],
            'ã¾ã™': ['ã‚‹', 'ã¦ã„ã‚‹'],
            'ã‚ã‚‹': ['ã‚ã‚Šã¾ã™', 'ã”ã–ã„ã¾ã™'],
            
            # æ•°å­—è¡¨è¨˜
            '1': ['ä¸€', 'ï¼‘', 'ã„ã¡'],
            '2': ['äºŒ', 'ï¼’', 'ã«'],
            '3': ['ä¸‰', 'ï¼“', 'ã•ã‚“'],
            '4': ['å››', 'ï¼”', 'ã‚ˆã‚“', 'ã—'],
            '5': ['äº”', 'ï¼•', 'ã”'],
        }
        
        for original, alternatives in notation_mappings.items():
            if original in text:
                for alt in alternatives:
                    patterns.append(text.replace(original, alt))
        
        return patterns
    
    def _generate_substring_patterns(self, text: str) -> List[str]:
        """éƒ¨åˆ†æ–‡å­—åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ"""
        patterns = []
        
        # å˜èªã®æŠ½å‡ºï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªåˆ†å‰²ï¼‰
        words = text.split()
        for word in words:
            # å¥èª­ç‚¹ã‚’é™¤å»
            clean_word = re.sub(r'[^\w]', '', word)
            if len(clean_word) >= 2:  # 2æ–‡å­—ä»¥ä¸Šã®å˜èª
                patterns.append(clean_word)
        
        # ãƒ•ãƒ¬ãƒ¼ã‚ºã®æŠ½å‡ºï¼ˆ2-3å˜èªã®çµ„ã¿åˆã‚ã›ï¼‰
        words_list = text.split()
        if len(words_list) >= 2:
            for i in range(len(words_list) - 1):
                # 2å˜èªã®çµ„ã¿åˆã‚ã›
                phrase = f"{words_list[i]} {words_list[i+1]}"
                patterns.append(phrase)
                patterns.append(f"{words_list[i]}{words_list[i+1]}")  # ã‚¹ãƒšãƒ¼ã‚¹ãªã—
                
                # 3å˜èªã®çµ„ã¿åˆã‚ã›
                if i + 2 < len(words_list):
                    phrase3 = f"{words_list[i]} {words_list[i+1]} {words_list[i+2]}"
                    patterns.append(phrase3)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆã‚«ã‚¿ã‚«ãƒŠã€è‹±èªã€æ¼¢å­—ï¼‰
        katakana_words = re.findall(r'[ã‚¡-ãƒ¿ãƒ¼]{2,}', text)
        patterns.extend(katakana_words)
        
        english_words = re.findall(r'[A-Za-z]{2,}', text)
        patterns.extend(english_words)
        
        kanji_words = re.findall(r'[ä¸€-é¾¯]{2,}', text)
        patterns.extend(kanji_words)
        
        return patterns
    
    def _generate_generic_combinations(self, base_variants: List[str]) -> List[str]:
        """æ±ç”¨çš„ãªçµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³"""
        combinations = []
        
        # æ—¢å­˜ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã«è¿½åŠ å¤‰æ›ã‚’é©ç”¨
        for variant in base_variants[:10]:  # æœ€åˆã®10å€‹ã®ã¿ä½¿ç”¨
            if variant:
                # ã‚¹ãƒšãƒ¼ã‚¹å‡¦ç†ã®è¿½åŠ é©ç”¨
                if ' ' in variant:
                    combinations.append(variant.replace(' ', 'ã€€'))
                    combinations.append(variant.replace(' ', ''))
                if 'ã€€' in variant:
                    combinations.append(variant.replace('ã€€', ' '))
                    combinations.append(variant.replace('ã€€', ''))
                
                # æ–‡å­—ç¨®ã®è¿½åŠ å¤‰æ›
                combinations.append(self._to_full_width(variant))
                combinations.append(self._to_half_width(variant))
                combinations.append(self._katakana_to_hiragana(variant))
                combinations.append(self._hiragana_to_katakana(variant))
                
                # æ­£è¦åŒ–ã®è¿½åŠ é©ç”¨
                combinations.append(self._normalize_variations(variant))
        
        return combinations
    
    def _generate_micro_adjustments(self, text: str, existing_variants: List[str]) -> List[str]:
        """å¾®èª¿æ•´ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ50å€‹åˆ°é”ã®ãŸã‚ã®è£œå®Œï¼‰"""
        patterns = []
        
        # æ—¢å­˜ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã®å¾®ç´°ãªèª¿æ•´
        for variant in existing_variants[:20]:  # æœ€åˆã®20å€‹ã‚’ä½¿ç”¨
            if len(patterns) >= 30:  # æœ€å¤§30å€‹è¿½åŠ 
                break
            
            # ã‚¹ãƒšãƒ¼ã‚¹ã®å¾®èª¿æ•´
            if ' ' in variant:
                patterns.append(variant.replace(' ', '  '))  # ãƒ€ãƒ–ãƒ«ã‚¹ãƒšãƒ¼ã‚¹
            
            # é‡è¤‡æ–‡å­—ã®é™¤å»
            deduplicated = re.sub(r'(.)\1+', r'\1', variant)
            if deduplicated != variant:
                patterns.append(deduplicated)
            
            # èªå°¾ã®å¾®èª¿æ•´
            if not variant.endswith(('ã€‚', '.', 'ï¼Ÿ', '?', 'ï¼', '!')):
                patterns.append(variant + 'ã€‚')
                patterns.append(variant + 'ï¼Ÿ')
        
        # å…ƒãƒ†ã‚­ã‚¹ãƒˆã®æ§‹é€ çš„å¤‰æ›´ï¼ˆæ§ãˆã‚ï¼‰
        words = text.split()
        if len(words) == 2:
            # 2å˜èªã®å ´åˆã®ã¿é †åºå…¥ã‚Œæ›¿ãˆ
            patterns.append(f"{words[1]} {words[0]}")
            patterns.append(f"{words[1]}ã€€{words[0]}")
            patterns.append(f"{words[1]}{words[0]}")
        
        # æ–‡å­—åˆ—ã®åˆ†å‰²ãƒ‘ã‚¿ãƒ¼ãƒ³
        if len(text) > 4:
            for pos in [len(text)//3, len(text)//2, (len(text)*2)//3]:
                if 1 < pos < len(text) - 1:
                    patterns.append(text[:pos] + ' ' + text[pos:])
                    patterns.append(text[:pos] + 'ã€€' + text[pos:])
        
        return patterns

    def _generate_essential_space_patterns(self, text: str) -> List[str]:
        """å¿…é ˆã‚¹ãƒšãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆï¼ˆæ³•äººæ ¼ã®å¾Œã‚ã«åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ï¼‰"""
        patterns = []
        
        # æ³•äººæ ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®šç¾©
        company_patterns = [
            # åŸºæœ¬çš„ãªæ³•äººæ ¼
            r'(æ ªå¼ä¼šç¤¾)([^\s])',  # æ ªå¼ä¼šç¤¾ABC â†’ æ ªå¼ä¼šç¤¾ ABC
            r'(æœ‰é™ä¼šç¤¾)([^\s])',  # æœ‰é™ä¼šç¤¾ABC â†’ æœ‰é™ä¼šç¤¾ ABC  
            r'(åˆåŒä¼šç¤¾)([^\s])',  # åˆåŒä¼šç¤¾ABC â†’ åˆåŒä¼šç¤¾ ABC
            r'(åˆè³‡ä¼šç¤¾)([^\s])',  # åˆè³‡ä¼šç¤¾ABC â†’ åˆè³‡ä¼šç¤¾ ABC
            r'(åˆåä¼šç¤¾)([^\s])',  # åˆåä¼šç¤¾ABC â†’ åˆåä¼šç¤¾ ABC
            
            # ç•¥ç§°æ³•äººæ ¼
            r'(ãˆ±)([^\s])',        # ãˆ±ABC â†’ ãˆ± ABC
            r'(ãˆ²)([^\s])',        # ãˆ²ABC â†’ ãˆ² ABC
            r'(\(æ ª\))([^\s])',    # (æ ª)ABC â†’ (æ ª) ABC
            r'(ï¼ˆæ ªï¼‰)([^\s])',     # ï¼ˆæ ªï¼‰ABC â†’ ï¼ˆæ ªï¼‰ ABC
            r'(\(æœ‰\))([^\s])',    # (æœ‰)ABC â†’ (æœ‰) ABC
            r'(ï¼ˆæœ‰ï¼‰)([^\s])',     # ï¼ˆæœ‰ï¼‰ABC â†’ ï¼ˆæœ‰ï¼‰ ABC
            
            # ç¤¾å›£ãƒ»è²¡å›£æ³•äºº
            r'(ä¸€èˆ¬ç¤¾å›£æ³•äºº)([^\s])',   # ä¸€èˆ¬ç¤¾å›£æ³•äººABC â†’ ä¸€èˆ¬ç¤¾å›£æ³•äºº ABC
            r'(å…¬ç›Šç¤¾å›£æ³•äºº)([^\s])',   # å…¬ç›Šç¤¾å›£æ³•äººABC â†’ å…¬ç›Šç¤¾å›£æ³•äºº ABC
            r'(ä¸€èˆ¬è²¡å›£æ³•äºº)([^\s])',   # ä¸€èˆ¬è²¡å›£æ³•äººABC â†’ ä¸€èˆ¬è²¡å›£æ³•äºº ABC
            r'(å…¬ç›Šè²¡å›£æ³•äºº)([^\s])',   # å…¬ç›Šè²¡å›£æ³•äººABC â†’ å…¬ç›Šè²¡å›£æ³•äºº ABC
            r'(ç¤¾ä¼šç¦ç¥‰æ³•äºº)([^\s])',   # ç¤¾ä¼šç¦ç¥‰æ³•äººABC â†’ ç¤¾ä¼šç¦ç¥‰æ³•äºº ABC
            r'(å­¦æ ¡æ³•äºº)([^\s])',       # å­¦æ ¡æ³•äººABC â†’ å­¦æ ¡æ³•äºº ABC
            r'(åŒ»ç™‚æ³•äºº)([^\s])',       # åŒ»ç™‚æ³•äººABC â†’ åŒ»ç™‚æ³•äºº ABC
            
            # ä¸€èˆ¬çš„ãªçµ„ç¹”åãƒ‘ã‚¿ãƒ¼ãƒ³
            r'(ä¼šç¤¾)([^\s])',     # ä¼šç¤¾ABC â†’ ä¼šç¤¾ ABC
            r'([^\s]+å·¥æ¥­)([^\s])',     # ABCå·¥æ¥­DEF â†’ ABCå·¥æ¥­ DEF
            r'([^\s]+ç¤¾å›£)([^\s])',     # ABCç¤¾å›£DEF â†’ ABCç¤¾å›£ DEF
            r'([^\s]+æ³•äºº)([^\s])',     # ABCæ³•äººDEF â†’ ABCæ³•äºº DEF
            r'([^\s]+å”ä¼š)([^\s])',     # ABCå”ä¼šDEF â†’ ABCå”ä¼š DEF
            r'([^\s]+çµ„åˆ)([^\s])',     # ABCçµ„åˆDEF â†’ ABCçµ„åˆ DEF
            r'([^\s]+è²¡å›£)([^\s])',     # ABCè²¡å›£DEF â†’ ABCè²¡å›£ DEF
        ]
        
        # é€†ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ã‚ã‚Šã‹ã‚‰ã‚¹ãƒšãƒ¼ã‚¹ãªã—ã¸ï¼‰
        reverse_patterns = [
            r'(æ ªå¼ä¼šç¤¾)\s+([^\s])',   # æ ªå¼ä¼šç¤¾ ABC â†’ æ ªå¼ä¼šç¤¾ABC
            r'(æœ‰é™ä¼šç¤¾)\s+([^\s])',   # æœ‰é™ä¼šç¤¾ ABC â†’ æœ‰é™ä¼šç¤¾ABC
            r'(åˆåŒä¼šç¤¾)\s+([^\s])',   # åˆåŒä¼šç¤¾ ABC â†’ åˆåŒä¼šç¤¾ABC
            r'(åˆè³‡ä¼šç¤¾)\s+([^\s])',   # åˆè³‡ä¼šç¤¾ ABC â†’ åˆè³‡ä¼šç¤¾ABC
            r'(åˆåä¼šç¤¾)\s+([^\s])',   # åˆåä¼šç¤¾ ABC â†’ åˆåä¼šç¤¾ABC
            r'(ãˆ±)\s+([^\s])',         # ãˆ± ABC â†’ ãˆ±ABC
            r'(ãˆ²)\s+([^\s])',         # ãˆ² ABC â†’ ãˆ²ABC
            r'(\(æ ª\))\s+([^\s])',     # (æ ª) ABC â†’ (æ ª)ABC
            r'(ï¼ˆæ ªï¼‰)\s+([^\s])',      # ï¼ˆæ ªï¼‰ ABC â†’ ï¼ˆæ ªï¼‰ABC
            r'(\(æœ‰\))\s+([^\s])',     # (æœ‰) ABC â†’ (æœ‰)ABC
            r'(ï¼ˆæœ‰ï¼‰)\s+([^\s])',      # ï¼ˆæœ‰ï¼‰ ABC â†’ ï¼ˆæœ‰ï¼‰ABC
            r'(ä¸€èˆ¬ç¤¾å›£æ³•äºº)\s+([^\s])', # ä¸€èˆ¬ç¤¾å›£æ³•äºº ABC â†’ ä¸€èˆ¬ç¤¾å›£æ³•äººABC
            r'(å…¬ç›Šç¤¾å›£æ³•äºº)\s+([^\s])', # å…¬ç›Šç¤¾å›£æ³•äºº ABC â†’ å…¬ç›Šç¤¾å›£æ³•äººABC
            r'(ä¸€èˆ¬è²¡å›£æ³•äºº)\s+([^\s])', # ä¸€èˆ¬è²¡å›£æ³•äºº ABC â†’ ä¸€èˆ¬è²¡å›£æ³•äººABC
            r'(å…¬ç›Šè²¡å›£æ³•äºº)\s+([^\s])', # å…¬ç›Šè²¡å›£æ³•äºº ABC â†’ å…¬ç›Šè²¡å›£æ³•äººABC
        ]
        
        # æ³•äººæ ¼ã®å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’è¿½åŠ ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        for pattern in company_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                # ã‚¹ãƒšãƒ¼ã‚¹ã‚’è¿½åŠ 
                new_text = text.replace(match.group(0), f"{match.group(1)} {match.group(2)}")
                if new_text != text:
                    patterns.append(new_text)
        
        # ã‚¹ãƒšãƒ¼ã‚¹ã‚ã‚Šã‹ã‚‰ã‚¹ãƒšãƒ¼ã‚¹ãªã—ã¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        for pattern in reverse_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                # ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤
                new_text = text.replace(match.group(0), f"{match.group(1)}{match.group(2)}")
                if new_text != text:
                    patterns.append(new_text)
        
        # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‹ã‚‰åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã¸ã®å¤‰æ›ã‚‚è¿½åŠ 
        if 'ã€€' in text:  # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ãŒã‚ã‚‹å ´åˆ
            patterns.append(text.replace('ã€€', ' '))  # åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›
        
        # åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã‹ã‚‰å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã¸ã®å¤‰æ›ã‚‚è¿½åŠ 
        if ' ' in text:  # åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ãŒã‚ã‚‹å ´åˆ
            patterns.append(text.replace(' ', 'ã€€'))  # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›
        
        return patterns

    def _apply_company_space_rule(self, variants: List[str]) -> List[str]:
        """ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³å†…ã®ã€ä¼šç¤¾ã€ã®å¾Œã«å¿…ãšåŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’å…¥ã‚Œã‚‹ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨

        ä¾‹: "ä¼šç¤¾ABC" â†’ "ä¼šç¤¾ ABC"
        è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ã‚„å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯åŠè§’ã‚¹ãƒšãƒ¼ã‚¹1ã¤ã«æ­£è¦åŒ–ã—ã¾ã™ã€‚
        """
        processed: List[str] = []

        # æ³•äººæ ¼ãƒªã‚¹ãƒˆï¼ˆå¿…ãšåŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’å…¥ã‚ŒãŸã„èªï¼‰
        legal_entities = [
            'æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾', 'åˆåŒä¼šç¤¾', 'åˆè³‡ä¼šç¤¾', 'åˆåä¼šç¤¾',
            'ä¸€èˆ¬ç¤¾å›£æ³•äºº', 'å…¬ç›Šç¤¾å›£æ³•äºº', 'ä¸€èˆ¬è²¡å›£æ³•äºº', 'å…¬ç›Šè²¡å›£æ³•äºº',
            'ç¤¾ä¼šç¦ç¥‰æ³•äºº', 'å­¦æ ¡æ³•äºº', 'åŒ»ç™‚æ³•äºº',
            'ãˆ±', 'ãˆ²', '(æ ª)', 'ï¼ˆæ ªï¼‰', '(æœ‰)', 'ï¼ˆæœ‰ï¼‰', 'ä¼šç¤¾'
        ]

        # æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ
        patterns = [(re.compile(fr'({re.escape(le)})[\sã€€]*([^\sã€€])'), le) for le in legal_entities]

        for txt in variants:
            new_txt = txt
            for pattern, le in patterns:
                new_txt = pattern.sub(rf"{le} \2", new_txt)
            # é‡è¤‡åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«
            new_txt = re.sub(r" {2,}", " ", new_txt)
            if new_txt not in processed and new_txt.strip():
                processed.append(new_txt.strip())
        return processed

    async def generate_variants(self, question: str) -> QuestionVariants:
        """
        ãƒ¡ã‚¤ãƒ³è³ªå•ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆãƒ¡ã‚½ãƒƒãƒ‰
        GeminiãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯Geminiã‚’ä½¿ç”¨ã€ãã†ã§ãªã‘ã‚Œã°åŸºæœ¬ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚’ä½¿ç”¨
        
        Args:
            question: å…ƒã®è³ªå•
            
        Returns:
            QuestionVariants: ç”Ÿæˆã•ã‚ŒãŸãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        """
        logger.info(f"ğŸ”„ è³ªå•ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆé–‹å§‹: '{question}'")
        
        # GeminiãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯Geminiã‚’ä½¿ç”¨
        if self.gemini_model:
            try:
                logger.info("ğŸ§  Geminiã‚’ä½¿ç”¨ã—ã¦ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ")
                return await self.generate_variants_with_gemini(question)
            except Exception as e:
                logger.error(f"âŒ Geminiãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆå¤±æ•—ã€åŸºæœ¬ç”Ÿæˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
                return self._generate_basic_variants(question)
        else:
            # GeminiãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯åŸºæœ¬ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚’ä½¿ç”¨
            logger.info("ğŸ’¡ åŸºæœ¬ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚’ä½¿ç”¨")
            return self._generate_basic_variants(question)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_variants_generator = None

def get_question_variants_generator() -> QuestionVariantsGenerator:
    """è³ªå•ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _variants_generator
    if _variants_generator is None:
        _variants_generator = QuestionVariantsGenerator()
    return _variants_generator

async def generate_question_variants(question: str) -> QuestionVariants:
    """
    è³ªå•ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆï¼ˆå¤–éƒ¨å‘¼ã³å‡ºã—ç”¨ï¼‰
    
    Args:
        question: å…ƒã®è³ªå•
        
    Returns:
        QuestionVariants: ç”Ÿæˆã•ã‚ŒãŸãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
    """
    generator = get_question_variants_generator()
    return await generator.generate_variants(question)

def variants_generator_available() -> bool:
    """è³ªå•ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        # GeminiãŒåˆ©ç”¨ã§ããªãã¦ã‚‚åŸºæœ¬ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã¯å¸¸ã«åˆ©ç”¨å¯èƒ½
        generator = get_question_variants_generator()
        return True  # åŸºæœ¬ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã¯å¸¸ã«åˆ©ç”¨å¯èƒ½
    except Exception:
        return False 
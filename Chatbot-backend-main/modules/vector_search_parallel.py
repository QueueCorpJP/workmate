"""
ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€Ÿãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
- åŒæ–¹å‘æ¤œç´¢ï¼ˆä¸Šä½ãƒ»ä¸‹ä½ã‹ã‚‰åŒæ™‚æ¤œç´¢ï¼‰
- é–“éš™æ¤œç´¢ï¼ˆãƒãƒƒãƒé–“ã®å€™è£œã‚‚æ¤œç´¢ï¼‰
- éåŒæœŸä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–
"""

import asyncio
import logging
from typing import List, Dict, Tuple, Optional, Set
from concurrent.futures import ThreadPoolExecutor
import time
from dotenv import load_dotenv
import psycopg2
from psycopg2.extras import RealDictCursor
import numpy as np
import os

# Vertex AI ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from google.cloud import aiplatform
    from vertexai.language_models import TextEmbeddingModel
    import vertexai
    VERTEX_AI_AVAILABLE = True
except ImportError:
    VERTEX_AI_AVAILABLE = False

# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®Gemini API
try:
    import google.generativeai as genai
    GEMINI_API_AVAILABLE = True
except ImportError:
    GEMINI_API_AVAILABLE = False

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

logger = logging.getLogger(__name__)

class ParallelVectorSearchSystem:
    """ä¸¦åˆ—å‡¦ç†ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.use_vertex_ai = os.getenv("USE_VERTEX_AI", "true").lower() == "true"
        self.project_id = os.getenv("GOOGLE_CLOUD_PROJECT", "workmate-462302")
        self.location = "us-central1"
        
        # ãƒ¢ãƒ‡ãƒ«è¨­å®š
        if self.use_vertex_ai:
            self.model_name = os.getenv("EMBEDDING_MODEL", "text-multilingual-embedding-002")
        else:
            self.model_name = "models/gemini-embedding-001"
        
        self.db_url = self._get_db_url()
        
        # Vertex AI ã¾ãŸã¯ Gemini API ã®åˆæœŸåŒ–
        if self.use_vertex_ai and VERTEX_AI_AVAILABLE:
            self._init_vertex_ai()
        else:
            self._init_gemini_api()
        
        # ä¸¦åˆ—å‡¦ç†ç”¨ã®Executor
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        logger.info(f"âœ… ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–: {self.model_name}")
    
    def _init_vertex_ai(self):
        """Vertex AI ã®åˆæœŸåŒ–"""
        try:
            # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆèªè¨¼ã®è¨­å®š
            service_account_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
            if service_account_path and os.path.exists(service_account_path):
                os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = service_account_path
                logger.info(f"âœ… ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆèªè¨¼è¨­å®š: {service_account_path}")
            
            # Vertex AIåˆæœŸåŒ–
            vertexai.init(project=self.project_id, location=self.location)
            self.model = TextEmbeddingModel.from_pretrained(self.model_name)
            self.embedding_method = "vertex_ai"
            self.api_key = None  # Vertex AIã§ã¯API keyã¯ä¸è¦
            logger.info(f"âœ… Vertex AI åˆæœŸåŒ–å®Œäº†: {self.model_name}")
            
        except Exception as e:
            logger.error(f"âŒ Vertex AI åˆæœŸåŒ–å¤±æ•—: {e}")
            logger.info("ğŸ”„ Gemini API ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            self._init_gemini_api()
    
    def _init_gemini_api(self):
        """Gemini API ã®åˆæœŸåŒ–"""
        if not GEMINI_API_AVAILABLE:
            raise ValueError("Gemini API ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        
        # API ã‚­ãƒ¼ã®è¨­å®š
        self.api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY ã¾ãŸã¯ GEMINI_API_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        genai.configure(api_key=self.api_key)
        self.model = None  # genai.embed_content ã‚’ç›´æ¥ä½¿ç”¨
        self.embedding_method = "gemini_api"
        logger.info(f"âœ… Gemini API åˆæœŸåŒ–å®Œäº†: {self.model_name}")
        
    def _get_db_url(self) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã‚’æ§‹ç¯‰"""
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        if "supabase.co" in supabase_url:
            project_id = supabase_url.split("://")[1].split(".")[0]
            return f"postgresql://postgres.{project_id}:{os.getenv('DB_PASSWORD', '')}@aws-0-ap-northeast-1.pooler.supabase.com:6543/postgres"
        else:
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                raise ValueError("DATABASE_URL ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return db_url

    async def generate_query_embeddings_parallel(self, queries: List[str]) -> List[List[float]]:
        """è¤‡æ•°ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ã‚’ä¸¦åˆ—ç”Ÿæˆ"""
        logger.info(f"ğŸ“¡ {len(queries)}å€‹ã®ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ã‚’ä¸¦åˆ—ç”Ÿæˆä¸­...")
        
        async def generate_single_embedding(query: str) -> List[float]:
            try:
                if self.embedding_method == "vertex_ai":
                    return await self._generate_vertex_ai_embedding(query)
                else:
                    return await self._generate_gemini_api_embedding(query)
            except Exception as e:
                logger.error(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                return []
        
        # ä¸¦åˆ—å®Ÿè¡Œ
        tasks = [generate_single_embedding(query) for query in queries]
        embeddings = await asyncio.gather(*tasks)
        
        logger.info(f"âœ… ä¸¦åˆ—åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº†: {len([e for e in embeddings if e])}å€‹æˆåŠŸ")
        return embeddings
    
    async def _generate_vertex_ai_embedding(self, query: str) -> List[float]:
        """Vertex AI ã‚’ä½¿ç”¨ã—ãŸåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        try:
            # Vertex AI ã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã¯åŒæœŸçš„ãªã®ã§ã€éåŒæœŸå®Ÿè¡Œ
            def _sync_generate():
                embeddings = self.model.get_embeddings([query])
                if embeddings and len(embeddings) > 0:
                    return embeddings[0].values
                return []
            
            loop = asyncio.get_event_loop()
            embedding = await loop.run_in_executor(self.executor, _sync_generate)
            return embedding
            
        except Exception as e:
            logger.error(f"Vertex AI åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def _generate_gemini_api_embedding(self, query: str) -> List[float]:
        """Gemini API ã‚’ä½¿ç”¨ã—ãŸåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        try:
            response = genai.embed_content(
                model=self.model_name,
                content=query
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
            embedding_vector = None
            
            if isinstance(response, dict) and 'embedding' in response:
                embedding_vector = response['embedding']
            elif hasattr(response, 'embedding') and response.embedding:
                embedding_vector = response.embedding
            
            if embedding_vector and len(embedding_vector) > 0:
                return embedding_vector
            else:
                logger.error(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå¤±æ•—: {query}")
                return []
                
        except Exception as e:
            logger.error(f"Gemini API åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def expand_query_strategies(self, original_query: str) -> List[str]:
        """ã‚¯ã‚¨ãƒªæ‹¡å¼µæˆ¦ç•¥ã‚’ç”Ÿæˆ"""
        strategies = [
            original_query,  # å…ƒã®ã‚¯ã‚¨ãƒª
        ]
        
        # é¡ç¾©èªæ‹¡å¼µ
        synonyms_map = {
            'æ–™é‡‘': ['ä¾¡æ ¼', 'è²»ç”¨', 'ã‚³ã‚¹ãƒˆ', 'å€¤æ®µ', 'æ–™é‡‘è¡¨'],
            'æ–¹æ³•': ['æ‰‹é †', 'ã‚„ã‚Šæ–¹', 'ãƒ—ãƒ­ã‚»ã‚¹', 'æ‰‹ç¶šã'],
            'è¨­å®š': ['æ§‹æˆ', 'ã‚³ãƒ³ãƒ•ã‚£ã‚°', 'ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—', 'è¨­å®šæ–¹æ³•'],
            'å•é¡Œ': ['èª²é¡Œ', 'ãƒˆãƒ©ãƒ–ãƒ«', 'ã‚¨ãƒ©ãƒ¼', 'ä¸å…·åˆ'],
            'ä½¿ã„æ–¹': ['åˆ©ç”¨æ–¹æ³•', 'æ“ä½œæ–¹æ³•', 'ä½¿ç”¨æ–¹æ³•'],
        }
        
        # é¡ç¾©èªã‚¯ã‚¨ãƒªã‚’è¿½åŠ 
        for word, synonyms in synonyms_map.items():
            if word in original_query:
                for synonym in synonyms[:2]:  # ä¸Šä½2ã¤ã®é¡ç¾©èª
                    expanded = original_query.replace(word, synonym)
                    if expanded not in strategies:
                        strategies.append(expanded)
        
        # éƒ¨åˆ†ã‚¯ã‚¨ãƒªã‚’è¿½åŠ 
        words = original_query.split()
        if len(words) > 1:
            for word in words:
                if len(word) > 1 and word not in strategies:
                    strategies.append(word)
        
        logger.info(f"ğŸ” ã‚¯ã‚¨ãƒªæˆ¦ç•¥ç”Ÿæˆ: {len(strategies)}å€‹ {strategies}")
        return strategies[:5]  # æœ€å¤§5æˆ¦ç•¥

    async def dual_direction_search(self, query_vector: List[float], company_id: str = None, limit: int = 10) -> Tuple[List[Dict], List[Dict]]:
        """åŒæ–¹å‘æ¤œç´¢ï¼ˆä¸Šä½ãƒ»ä¸‹ä½ã‹ã‚‰åŒæ™‚æ¤œç´¢ï¼‰"""
        
        async def search_top_similar(vector: List[float]) -> List[Dict]:
            """ä¸Šä½é¡ä¼¼æ¤œç´¢"""
            return await asyncio.get_event_loop().run_in_executor(
                self.executor, 
                self._execute_vector_search, 
                vector, company_id, limit, "similarity DESC"
            )
        
        async def search_bottom_similar(vector: List[float]) -> List[Dict]:
            """ä¸‹ä½é¡ä¼¼æ¤œç´¢ï¼ˆéé¡ä¼¼ã‹ã‚‰ï¼‰"""
            return await asyncio.get_event_loop().run_in_executor(
                self.executor, 
                self._execute_vector_search, 
                vector, company_id, limit, "similarity ASC"
            )
        
        logger.info("ğŸ”„ åŒæ–¹å‘ä¸¦åˆ—æ¤œç´¢å®Ÿè¡Œä¸­...")
        
        # ä¸¦åˆ—å®Ÿè¡Œ
        top_results, bottom_results = await asyncio.gather(
            search_top_similar(query_vector),
            search_bottom_similar(query_vector)
        )
        
        logger.info(f"ğŸ“Š åŒæ–¹å‘æ¤œç´¢å®Œäº†: ä¸Šä½{len(top_results)}ä»¶, ä¸‹ä½{len(bottom_results)}ä»¶")
        return top_results, bottom_results

    def _execute_vector_search(self, query_vector: List[float], company_id: str, limit: int, order_by: str) -> List[Dict]:
        """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®å®Ÿè¡Œ"""
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    sql = """
                    SELECT
                        c.id::text as chunk_id,
                        c.doc_id as original_doc_id,
                        ds.name,
                        ds.special,
                        ds.type,
                        c.content as snippet,
                        1 - (c.embedding <=> %s::vector) as similarity
                    FROM chunks c
                    LEFT JOIN document_sources ds ON ds.id = c.doc_id
                    WHERE c.embedding IS NOT NULL
                    """
                    
                    # Convert list to string format for PostgreSQL vector type
                    vector_str = '[' + ','.join(map(str, query_vector)) + ']'
                    params = [vector_str]
                    
                    if company_id:
                        sql += " AND c.company_id = %s"
                        params.append(company_id)
                    
                    sql += f" ORDER BY {order_by} LIMIT %s"
                    params.append(limit)
                    
                    cur.execute(sql, params)
                    results = cur.fetchall()
                    
                    return [{
                        'chunk_id': row['chunk_id'],
                        'document_id': row['original_doc_id'],
                        'document_name': row['name'] if row['name'] else 'Unknown Document',
                        'document_type': row['type'] if row['type'] else 'unknown',
                        'special': row['special'] if row['special'] else False,
                        'snippet': row['snippet'] if row['snippet'] else '',
                        'similarity_score': float(row['similarity']) if row['similarity'] else 0.0,
                        'search_type': 'vector_parallel'
                    } for row in results]
        
        except Exception as e:
            logger.error(f"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def find_gap_candidates(self, top_results: List[Dict], bottom_results: List[Dict]) -> List[str]:
        """ãƒãƒƒãƒã—ãŸçµæœã®é–“ã«ã‚ã‚‹å€™è£œã‚’ç‰¹å®š"""
        gap_candidates = []
        
        if not top_results or not bottom_results:
            return gap_candidates
        
        # ä¸Šä½ã¨ä¸‹ä½ã®é¡ä¼¼åº¦ã®ç¯„å›²ã‚’ç‰¹å®š
        top_similarities = [r['similarity_score'] for r in top_results]
        bottom_similarities = [r['similarity_score'] for r in bottom_results]
        
        if top_similarities and bottom_similarities:
            min_top = min(top_similarities)
            max_bottom = max(bottom_similarities)
            
            # é–“éš™ãŒã‚ã‚‹å ´åˆ
            if min_top > max_bottom:
                gap_threshold_high = min_top - 0.05
                gap_threshold_low = max_bottom + 0.05
                gap_candidates.append(f"BETWEEN {gap_threshold_low} AND {gap_threshold_high}")
                logger.info(f"ğŸ” é–“éš™æ¤œç´¢ç¯„å›²: {gap_threshold_low:.3f} - {gap_threshold_high:.3f}")
        
        return gap_candidates

    async def execute_gap_search(self, query_vector: List[float], gap_conditions: List[str], company_id: str = None) -> List[Dict]:
        """é–“éš™æ¤œç´¢ã‚’å®Ÿè¡Œ"""
        if not gap_conditions:
            return []
        
        gap_results = []
        
        for condition in gap_conditions:
            try:
                results = await asyncio.get_event_loop().run_in_executor(
                    self.executor, 
                    self._execute_gap_search_sync, 
                    query_vector, condition, company_id
                )
                gap_results.extend(results)
            except Exception as e:
                logger.error(f"é–“éš™æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        
        logger.info(f"ğŸ” é–“éš™æ¤œç´¢å®Œäº†: {len(gap_results)}ä»¶ã®è¿½åŠ çµæœ")
        return gap_results

    def _execute_gap_search_sync(self, query_vector: List[float], condition: str, company_id: str = None) -> List[Dict]:
        """é–“éš™æ¤œç´¢ã®åŒæœŸå®Ÿè¡Œ"""
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    # Convert list to string format for PostgreSQL vector type
                    vector_str = '[' + ','.join(map(str, query_vector)) + ']'
                    
                    # Use subquery to avoid repeating the vector calculation
                    sql = f"""
                    SELECT
                        chunk_id,
                        original_doc_id,
                        name,
                        special,
                        type,
                        snippet,
                        similarity
                    FROM (
                        SELECT
                            c.id::text as chunk_id,
                            c.doc_id as original_doc_id,
                            ds.name,
                            ds.special,
                            ds.type,
                            c.content as snippet,
                            1 - (c.embedding <=> %s::vector) as similarity
                        FROM chunks c
                        LEFT JOIN document_sources ds ON ds.id = c.doc_id
                        WHERE c.embedding IS NOT NULL
                    ) subq
                    WHERE similarity {condition}
                    """
                    
                    params = [vector_str]
                    
                    if company_id:
                        # Add company_id filter to the subquery
                        sql = f"""
                        SELECT
                            chunk_id,
                            original_doc_id,
                            name,
                            special,
                            type,
                            snippet,
                            similarity
                        FROM (
                            SELECT
                                c.id::text as chunk_id,
                                c.doc_id as original_doc_id,
                                ds.name,
                                ds.special,
                                ds.type,
                                c.content as snippet,
                                1 - (c.embedding <=> %s::vector) as similarity
                            FROM chunks c
                            LEFT JOIN document_sources ds ON ds.id = c.doc_id
                            WHERE c.embedding IS NOT NULL
                              AND c.company_id = %s
                        ) subq
                        WHERE similarity {condition}
                        """
                        params = [vector_str, company_id]
                    
                    sql += " ORDER BY similarity DESC LIMIT 5"
                    
                    cur.execute(sql, params)
                    results = cur.fetchall()
                    
                    return [{
                        'chunk_id': row['chunk_id'],
                        'document_id': row['original_doc_id'],
                        'document_name': row['name'] if row['name'] else 'Unknown Document',
                        'document_type': row['type'] if row['type'] else 'unknown',
                        'special': row['special'] if row['special'] else False,
                        'snippet': row['snippet'] if row['snippet'] else '',
                        'similarity_score': float(row['similarity']) if row['similarity'] else 0.0,
                        'search_type': 'vector_gap'
                    } for row in results]
        
        except Exception as e:
            logger.error(f"é–“éš™æ¤œç´¢åŒæœŸå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def merge_and_deduplicate_results(self, *result_lists: List[List[Dict]]) -> List[Dict]:
        """çµæœã‚’ãƒãƒ¼ã‚¸ã—ã¦é‡è¤‡ã‚’é™¤å»"""
        seen_chunks: Set[str] = set()
        merged_results = []
        
        # å…¨ã¦ã®çµæœãƒªã‚¹ãƒˆã‚’çµ±åˆ
        all_results = []
        for result_list in result_lists:
            all_results.extend(result_list)
        
        # é¡ä¼¼åº¦é †ã§ã‚½ãƒ¼ãƒˆ
        all_results.sort(key=lambda x: x['similarity_score'], reverse=True)
        
        # é‡è¤‡é™¤å»
        for result in all_results:
            chunk_id = result['chunk_id']
            if chunk_id not in seen_chunks:
                seen_chunks.add(chunk_id)
                merged_results.append(result)
        
        logger.info(f"ğŸ”„ çµæœãƒãƒ¼ã‚¸å®Œäº†: {len(all_results)}ä»¶ â†’ {len(merged_results)}ä»¶ï¼ˆé‡è¤‡é™¤å»å¾Œï¼‰")
        return merged_results

    async def parallel_comprehensive_search(self, query: str, company_id: str = None, max_results: int = 15) -> str:
        """åŒ…æ‹¬çš„ä¸¦åˆ—æ¤œç´¢ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        start_time = time.time()
        logger.info(f"ğŸš€ ä¸¦åˆ—åŒ…æ‹¬æ¤œç´¢é–‹å§‹: '{query}'")
        
        try:
            # 1. ã‚¯ã‚¨ãƒªæˆ¦ç•¥ã®ç”Ÿæˆ
            query_strategies = self.expand_query_strategies(query)
            
            # 2. ä¸¦åˆ—ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
            embeddings = await self.generate_query_embeddings_parallel(query_strategies)
            
            # æœ‰åŠ¹ãªã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã®ã¿ã‚’ä½¿ç”¨
            valid_embeddings = [(q, e) for q, e in zip(query_strategies, embeddings) if e]
            
            if not valid_embeddings:
                logger.error("æœ‰åŠ¹ãªã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                return ""
            
            # 3. è¤‡æ•°æˆ¦ç•¥ã§ã®ä¸¦åˆ—æ¤œç´¢
            search_tasks = []
            for query_text, embedding in valid_embeddings:
                task = self.dual_direction_search(embedding, company_id, max_results // len(valid_embeddings))
                search_tasks.append(task)
            
            # ä¸¦åˆ—å®Ÿè¡Œ
            all_search_results = await asyncio.gather(*search_tasks)
            
            # 4. çµæœã®çµ±åˆ
            all_top_results = []
            all_bottom_results = []
            
            for top_results, bottom_results in all_search_results:
                all_top_results.extend(top_results)
                all_bottom_results.extend(bottom_results)
            
            # 5. é–“éš™æ¤œç´¢
            if valid_embeddings:
                primary_embedding = valid_embeddings[0][1]
                gap_conditions = self.find_gap_candidates(all_top_results, all_bottom_results)
                gap_results = await self.execute_gap_search(primary_embedding, gap_conditions, company_id)
            else:
                gap_results = []
            
            # 6. çµæœã®ãƒãƒ¼ã‚¸ã¨é‡è¤‡é™¤å»
            final_results = self.merge_and_deduplicate_results(
                all_top_results, all_bottom_results, gap_results
            )
            
            # 7. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®çµ„ã¿ç«‹ã¦
            if not final_results:
                logger.warning("é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return ""
            
            relevant_content = []
            total_length = 0
            max_total_length = 20000  # æœ€å¤§æ–‡å­—æ•°ã‚’å¢—åŠ 
            
            logger.info(f"ğŸ“Š æœ€çµ‚çµæœå‡¦ç†: {len(final_results)}ä»¶")
            
            for i, result in enumerate(final_results[:max_results]):
                similarity = result['similarity_score']
                snippet = result['snippet'] or ""
                search_type = result['search_type']
                
                logger.info(f"  {i+1}. {result['document_name']} (é¡ä¼¼åº¦: {similarity:.3f}, ç¨®é¡: {search_type})")
                
                # é–¾å€¤ä»¥ä¸‹ã®é¡ä¼¼åº¦ã®çµæœã¯é™¤å¤–
                if similarity < 0.2:
                    logger.info(f"    - é¡ä¼¼åº¦ãŒä½ã„ãŸã‚é™¤å¤– ({similarity:.3f} < 0.2)")
                    continue
                
                # ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’è¿½åŠ 
                if snippet and len(snippet.strip()) > 0:
                    content_piece = f"\n=== {result['document_name']} (é¡ä¼¼åº¦: {similarity:.3f}, {search_type}) ===\n{snippet}\n"
                    
                    if total_length + len(content_piece) <= max_total_length:
                        relevant_content.append(content_piece)
                        total_length += len(content_piece)
                        logger.info(f"    - è¿½åŠ å®Œäº† ({len(content_piece)}æ–‡å­—)")
                    else:
                        logger.info(f"    - æ–‡å­—æ•°åˆ¶é™ã«ã‚ˆã‚Šé™¤å¤–")
                        break
            
            final_content = "\n".join(relevant_content)
            elapsed_time = time.time() - start_time
            
            logger.info(f"ğŸ‰ ä¸¦åˆ—åŒ…æ‹¬æ¤œç´¢å®Œäº†: {len(final_content)}æ–‡å­— ({elapsed_time:.2f}ç§’)")
            logger.info(f"ğŸ“Š æ¤œç´¢çµ±è¨ˆ: æˆ¦ç•¥{len(query_strategies)}å€‹, çµæœ{len(final_results)}ä»¶, æ¡ç”¨{len(relevant_content)}ä»¶")
            
            return final_content
        
        except Exception as e:
            elapsed_time = time.time() - start_time
            logger.error(f"ä¸¦åˆ—åŒ…æ‹¬æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e} ({elapsed_time:.2f}ç§’)")
            return ""

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_parallel_vector_search_instance = None

def get_parallel_vector_search_instance() -> Optional[ParallelVectorSearchSystem]:
    """ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _parallel_vector_search_instance
    
    if _parallel_vector_search_instance is None:
        try:
            _parallel_vector_search_instance = ParallelVectorSearchSystem()
            logger.info("âœ… ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            logger.error(f"âŒ ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    return _parallel_vector_search_instance 
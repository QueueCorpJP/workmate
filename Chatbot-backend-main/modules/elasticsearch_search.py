"""
Elasticsearchæ¤œç´¢ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
Fuzzy searchã¨advanced queryæ©Ÿèƒ½ã‚’æä¾›
"""

import os
import logging
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import asyncio
from concurrent.futures import ThreadPoolExecutor

# Elasticsearchã®æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from elasticsearch import Elasticsearch
    from elasticsearch_dsl import Search, Q, Document, Text, Keyword, Integer, Float, Date, Index
    from elasticsearch_dsl.connections import connections
    ELASTICSEARCH_AVAILABLE = True
except ImportError as e:
    ELASTICSEARCH_AVAILABLE = False
    # ãƒ€ãƒŸãƒ¼ã‚¯ãƒ©ã‚¹å®šç¾©
    class Elasticsearch:
        def __init__(self, *args, **kwargs):
            pass
        def ping(self):
            return False
        def indices(self):
            return self
        def exists(self, *args, **kwargs):
            return False
        def create(self, *args, **kwargs):
            pass
        def search(self, *args, **kwargs):
            return {'hits': {'hits': []}}

try:
    import psycopg2
    from psycopg2.extras import RealDictCursor
    PSYCOPG2_AVAILABLE = True
except ImportError:
    PSYCOPG2_AVAILABLE = False

from dotenv import load_dotenv
load_dotenv()

logger = logging.getLogger(__name__)

class ElasticsearchManager:
    """Elasticsearchç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.es_host = os.getenv("ELASTICSEARCH_HOST", "localhost")
        self.es_port = int(os.getenv("ELASTICSEARCH_PORT", "9200"))
        self.es_user = os.getenv("ELASTICSEARCH_USER", "elastic")
        self.es_password = os.getenv("ELASTICSEARCH_PASSWORD", "")
        self.es_scheme = os.getenv("ELASTICSEARCH_SCHEME", "http")
        self.index_name = os.getenv("ELASTICSEARCH_INDEX", "workmate_documents")
        
        # Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        self.es = None
        self.thread_pool = ThreadPoolExecutor(max_workers=4)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šè¨­å®š
        self.db_url = self._get_db_url()
        
        self._init_elasticsearch()
    
    def _get_db_url(self) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLæ§‹ç¯‰"""
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URL ã¨ SUPABASE_KEY ãŒå¿…è¦ã§ã™")
        
        if "supabase.co" in supabase_url:
            project_id = supabase_url.split("://")[1].split(".")[0]
            db_password = os.getenv('DB_PASSWORD', '')
            return f"postgresql://postgres.{project_id}:{db_password}@aws-0-ap-northeast-1.pooler.supabase.com:6543/postgres"
        else:
            return os.getenv("DATABASE_URL", "")
    
    def _init_elasticsearch(self):
        """Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–"""
        if not ELASTICSEARCH_AVAILABLE:
            logger.warning("âš ï¸ Elasticsearchãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            self.es = None
            return
            
        try:
            # èªè¨¼æƒ…å ±ã®è¨­å®š
            if self.es_user and self.es_password:
                auth = (self.es_user, self.es_password)
            else:
                auth = None
            
            # Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ
            self.es = Elasticsearch(
                hosts=[{
                    'host': self.es_host,
                    'port': self.es_port,
                    'scheme': self.es_scheme
                }],
                http_auth=auth,
                verify_certs=False,
                timeout=30,
                max_retries=3,
                retry_on_timeout=True
            )
            
            # æ¥ç¶šãƒ†ã‚¹ãƒˆ
            if self.es.ping():
                logger.info(f"âœ… Elasticsearchæ¥ç¶šæˆåŠŸ: {self.es_host}:{self.es_port}")
                if ELASTICSEARCH_AVAILABLE:
                    connections.add_connection('default', self.es)
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
                self._create_index()
            else:
                logger.error("âŒ Elasticsearchæ¥ç¶šå¤±æ•—")
                self.es = None
        
        except Exception as e:
            logger.error(f"âŒ ElasticsearchåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            self.es = None
    
    def _create_index(self):
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ"""
        try:
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            if not self.es.indices.exists(index=self.index_name):
                # æ—¥æœ¬èªè§£æç”¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°è¨­å®š
                mapping = {
                    "settings": {
                        "analysis": {
                            "analyzer": {
                                "japanese_analyzer": {
                                    "type": "custom",
                                    "tokenizer": "kuromoji_tokenizer",
                                    "filter": [
                                        "kuromoji_baseform",
                                        "kuromoji_part_of_speech",
                                        "kuromoji_stemmer",
                                        "lowercase",
                                        "cjk_width"
                                    ]
                                },
                                "ngram_analyzer": {
                                    "type": "custom",
                                    "tokenizer": "ngram_tokenizer",
                                    "filter": ["lowercase"]
                                }
                            },
                            "tokenizer": {
                                "ngram_tokenizer": {
                                    "type": "ngram",
                                    "min_gram": 2,
                                    "max_gram": 3,
                                    "token_chars": ["letter", "digit"]
                                }
                            }
                        },
                        "number_of_shards": 1,
                        "number_of_replicas": 0
                    },
                    "mappings": {
                        "properties": {
                            "chunk_id": {"type": "keyword"},
                            "document_id": {"type": "keyword"},
                            "document_name": {"type": "text", "analyzer": "japanese_analyzer"},
                            "document_type": {"type": "keyword"},
                            "chunk_index": {"type": "integer"},
                            "content": {
                                "type": "text",
                                "analyzer": "japanese_analyzer",
                                "fields": {
                                    "ngram": {
                                        "type": "text",
                                        "analyzer": "ngram_analyzer"
                                    },
                                    "raw": {
                                        "type": "keyword"
                                    }
                                }
                            },
                            "company_id": {"type": "keyword"},
                            "created_at": {"type": "date"},
                            "updated_at": {"type": "date"},
                            "embedding": {"type": "dense_vector", "dims": 768},
                            "special": {"type": "boolean"}
                        }
                    }
                }
                
                self.es.indices.create(
                    index=self.index_name,
                    body=mapping
                )
                logger.info(f"âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå®Œäº†: {self.index_name}")
            else:
                logger.info(f"ğŸ“‹ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ—¢å­˜: {self.index_name}")
        
        except Exception as e:
            logger.error(f"âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def is_available(self) -> bool:
        """ElasticsearchãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.es is not None and self.es.ping()
    
    async def sync_database_to_elasticsearch(self, company_id: str = None):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰Elasticsearchã«åŒæœŸ"""
        try:
            if not self.is_available():
                logger.error("âŒ Elasticsearchåˆ©ç”¨ä¸å¯")
                return False
            
            if not PSYCOPG2_AVAILABLE:
                logger.error("âŒ psycopg2åˆ©ç”¨ä¸å¯")
                return False
            
            logger.info("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰Elasticsearchã¸ã®åŒæœŸé–‹å§‹")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    sql = """
                    SELECT 
                        c.id as chunk_id,
                        c.doc_id as document_id,
                        c.chunk_index,
                        c.content,
                        c.company_id,
                        c.embedding,
                        c.created_at,
                        c.updated_at,
                        ds.name as document_name,
                        ds.type as document_type,
                        ds.special
                    FROM chunks c
                    LEFT JOIN document_sources ds ON c.doc_id = ds.id
                    WHERE c.content IS NOT NULL
                    """
                    
                    params = []
                    if company_id:
                        sql += " AND c.company_id = %s"
                        params.append(company_id)
                    
                    cur.execute(sql, params)
                    rows = cur.fetchall()
                    
                    logger.info(f"ğŸ“Š åŒæœŸå¯¾è±¡: {len(rows)}ä»¶")
                    
                    # ãƒãƒƒãƒã§Elasticsearchã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                    batch_size = 100
                    for i in range(0, len(rows), batch_size):
                        batch = rows[i:i + batch_size]
                        await self._index_batch(batch)
                        logger.info(f"ğŸ“ é€²æ—: {i + len(batch)}/{len(rows)}")
                    
                    logger.info("âœ… åŒæœŸå®Œäº†")
                    return True
        
        except Exception as e:
            logger.error(f"âŒ åŒæœŸã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def _index_batch(self, batch: List[Dict]):
        """ãƒãƒƒãƒã§ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†"""
        try:
            if not ELASTICSEARCH_AVAILABLE:
                logger.warning("âš ï¸ ElasticsearchãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ãƒãƒƒãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                return
                
            actions = []
            
            for row in batch:
                doc = {
                    "_index": self.index_name,
                    "_id": row['chunk_id'],
                    "_source": {
                        "chunk_id": row['chunk_id'],
                        "document_id": row['document_id'],
                        "document_name": row['document_name'] or 'Unknown',
                        "document_type": row['document_type'] or 'unknown',
                        "chunk_index": row['chunk_index'],
                        "content": row['content'],
                        "company_id": row['company_id'],
                        "created_at": row['created_at'].isoformat() if row['created_at'] else None,
                        "updated_at": row['updated_at'].isoformat() if row['updated_at'] else None,
                        "embedding": row['embedding'] if row['embedding'] else None,
                        "special": row['special'] or False
                    }
                }
                actions.append(doc)
            
            # ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            try:
                from elasticsearch.helpers import bulk
                bulk(self.es, actions)
            except ImportError:
                logger.warning("âš ï¸ elasticsearch.helpersãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            
        except Exception as e:
            logger.error(f"âŒ ãƒãƒƒãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")

class ElasticsearchFuzzySearch:
    """Elasticsearch Fuzzy Searchæ©Ÿèƒ½"""
    
    def __init__(self, es_manager: ElasticsearchManager):
        self.es_manager = es_manager
        self.es = es_manager.es
        self.index_name = es_manager.index_name
    
    async def fuzzy_search(self, 
                          query: str, 
                          company_id: str = None,
                          fuzziness: str = "AUTO",
                          limit: int = 20) -> List[Dict]:
        """
        Fuzzy searchå®Ÿè¡Œ
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            company_id: ä¼šç¤¾ID
            fuzziness: ãƒ•ã‚¡ã‚¸ãƒã‚¹è¨­å®š ("AUTO", "0", "1", "2")
            limit: çµæœä»¶æ•°
        
        Returns:
            æ¤œç´¢çµæœãƒªã‚¹ãƒˆ
        """
        try:
            if not self.es_manager.is_available():
                logger.error("âŒ Elasticsearchåˆ©ç”¨ä¸å¯")
                return []
            
            logger.info(f"ğŸ” Fuzzy Searchå®Ÿè¡Œ: '{query}' (fuzziness: {fuzziness})")
            
            # æ¤œç´¢ã‚¯ã‚¨ãƒªæ§‹ç¯‰
            search_query = {
                "query": {
                    "bool": {
                        "must": [
                            {
                                "multi_match": {
                                    "query": query,
                                    "fields": ["content^3", "document_name^2", "content.ngram"],
                                    "fuzziness": fuzziness,
                                    "type": "best_fields",
                                    "operator": "or"
                                }
                            }
                        ]
                    }
                },
                "highlight": {
                    "fields": {
                        "content": {
                            "fragment_size": 200,
                            "number_of_fragments": 3
                        }
                    }
                },
                "size": limit,
                "sort": [
                    {"_score": {"order": "desc"}},
                    {"created_at": {"order": "desc"}}
                ]
            }
            
            # ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿
            if company_id:
                search_query["query"]["bool"]["filter"] = [
                    {"term": {"company_id": company_id}}
                ]
            
            # æ¤œç´¢å®Ÿè¡Œ
            result = self.es.search(
                index=self.index_name,
                body=search_query
            )
            
            # çµæœã®æ•´å½¢
            search_results = []
            for hit in result['hits']['hits']:
                source = hit['_source']
                
                # ãƒã‚¤ãƒ©ã‚¤ãƒˆæƒ…å ±ã®å–å¾—
                highlight = hit.get('highlight', {})
                highlighted_content = highlight.get('content', [])
                
                search_results.append({
                    'chunk_id': source['chunk_id'],
                    'document_id': source['document_id'],
                    'document_name': source['document_name'],
                    'document_type': source['document_type'],
                    'chunk_index': source['chunk_index'],
                    'content': source['content'],
                    'highlighted_content': highlighted_content,
                    'similarity_score': hit['_score'],
                    'search_type': 'elasticsearch_fuzzy',
                    'fuzziness': fuzziness,
                    'metadata': {
                        'company_id': source.get('company_id'),
                        'created_at': source.get('created_at'),
                        'special': source.get('special', False)
                    }
                })
            
            logger.info(f"âœ… Fuzzy Searchå®Œäº†: {len(search_results)}ä»¶")
            return search_results
        
        except Exception as e:
            logger.error(f"âŒ Fuzzy Search ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def advanced_search(self, 
                            query: str,
                            company_id: str = None,
                            search_type: str = "multi_match",
                            fuzziness: str = "AUTO",
                            limit: int = 20) -> List[Dict]:
        """
        é«˜åº¦ãªæ¤œç´¢æ©Ÿèƒ½
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            company_id: ä¼šç¤¾ID
            search_type: æ¤œç´¢ã‚¿ã‚¤ãƒ— ("multi_match", "phrase", "wildcard", "regex")
            fuzziness: ãƒ•ã‚¡ã‚¸ãƒã‚¹è¨­å®š
            limit: çµæœä»¶æ•°
        """
        try:
            if not self.es_manager.is_available():
                return []
            
            logger.info(f"ğŸ” Advanced Search: '{query}' (type: {search_type})")
            
            # æ¤œç´¢ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸã‚¯ã‚¨ãƒªæ§‹ç¯‰
            if search_type == "multi_match":
                query_part = {
                    "multi_match": {
                        "query": query,
                        "fields": ["content^3", "document_name^2", "content.ngram"],
                        "fuzziness": fuzziness,
                        "type": "best_fields"
                    }
                }
            elif search_type == "phrase":
                query_part = {
                    "match_phrase": {
                        "content": {
                            "query": query,
                            "slop": 2
                        }
                    }
                }
            elif search_type == "wildcard":
                query_part = {
                    "wildcard": {
                        "content": {
                            "value": f"*{query}*",
                            "case_insensitive": True
                        }
                    }
                }
            elif search_type == "regex":
                query_part = {
                    "regexp": {
                        "content": {
                            "value": query,
                            "case_insensitive": True
                        }
                    }
                }
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯multi_match
                query_part = {
                    "multi_match": {
                        "query": query,
                        "fields": ["content^3", "document_name^2"],
                        "fuzziness": fuzziness
                    }
                }
            
            search_query = {
                "query": {
                    "bool": {
                        "must": [query_part]
                    }
                },
                "highlight": {
                    "fields": {
                        "content": {
                            "fragment_size": 200,
                            "number_of_fragments": 3
                        }
                    }
                },
                "size": limit,
                "sort": [
                    {"_score": {"order": "desc"}},
                    {"created_at": {"order": "desc"}}
                ]
            }
            
            # ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿
            if company_id:
                search_query["query"]["bool"]["filter"] = [
                    {"term": {"company_id": company_id}}
                ]
            
            # æ¤œç´¢å®Ÿè¡Œ
            result = self.es.search(
                index=self.index_name,
                body=search_query
            )
            
            # çµæœã®æ•´å½¢
            search_results = []
            for hit in result['hits']['hits']:
                source = hit['_source']
                
                search_results.append({
                    'chunk_id': source['chunk_id'],
                    'document_id': source['document_id'],
                    'document_name': source['document_name'],
                    'document_type': source['document_type'],
                    'chunk_index': source['chunk_index'],
                    'content': source['content'],
                    'highlighted_content': hit.get('highlight', {}).get('content', []),
                    'similarity_score': hit['_score'],
                    'search_type': f'elasticsearch_{search_type}',
                    'fuzziness': fuzziness,
                    'metadata': {
                        'company_id': source.get('company_id'),
                        'created_at': source.get('created_at'),
                        'special': source.get('special', False)
                    }
                })
            
            logger.info(f"âœ… Advanced Searchå®Œäº†: {len(search_results)}ä»¶")
            return search_results
        
        except Exception as e:
            logger.error(f"âŒ Advanced Search ã‚¨ãƒ©ãƒ¼: {e}")
            return []

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_elasticsearch_manager = None
_elasticsearch_fuzzy_search = None

def get_elasticsearch_manager() -> Optional[ElasticsearchManager]:
    """Elasticsearchãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’å–å¾—"""
    global _elasticsearch_manager
    
    if _elasticsearch_manager is None:
        try:
            _elasticsearch_manager = ElasticsearchManager()
            logger.info("âœ… Elasticsearchãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            logger.error(f"âŒ Elasticsearchãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    return _elasticsearch_manager

def get_elasticsearch_fuzzy_search() -> Optional[ElasticsearchFuzzySearch]:
    """Elasticsearch Fuzzy Searchã‚’å–å¾—"""
    global _elasticsearch_fuzzy_search
    
    if _elasticsearch_fuzzy_search is None:
        es_manager = get_elasticsearch_manager()
        if es_manager and es_manager.is_available():
            _elasticsearch_fuzzy_search = ElasticsearchFuzzySearch(es_manager)
            logger.info("âœ… Elasticsearch Fuzzy SearchåˆæœŸåŒ–å®Œäº†")
        else:
            logger.error("âŒ Elasticsearch Fuzzy SearchåˆæœŸåŒ–å¤±æ•—")
            return None
    
    return _elasticsearch_fuzzy_search

def elasticsearch_available() -> bool:
    """ElasticsearchãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        if not ELASTICSEARCH_AVAILABLE:
            return False
            
        es_manager = get_elasticsearch_manager()
        return es_manager is not None and es_manager.is_available()
    except Exception:
        return False 
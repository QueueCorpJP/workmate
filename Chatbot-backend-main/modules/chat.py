"""
ãƒãƒ£ãƒƒãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã¨AIå¿œç­”ç”Ÿæˆã‚’ç®¡ç†ã—ã¾ã™
"""
import json
import re
import uuid
import sys
from datetime import datetime
import logging
# PostgreSQLé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from psycopg2.extras import RealDictCursor
from fastapi import HTTPException, Depends
from .company import DEFAULT_COMPANY_NAME
from .models import ChatMessage, ChatResponse
from .database import get_db, update_usage_count, get_usage_limits
from .knowledge_base import knowledge_base, get_active_resources
from .auth import check_usage_limits
from .resource import get_active_resources_by_company_id, get_active_resources_content_by_ids, get_active_resource_names_by_company_id
from .company import get_company_by_id
import os
import asyncio
import google.generativeai as genai
from .config import setup_gemini
from .utils import safe_print, safe_safe_print

# ğŸš€ æ–°ã—ã„ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ ï¼ˆæœ€å„ªå…ˆï¼‰
try:
    from .realtime_rag import process_question_realtime, realtime_rag_available
    REALTIME_RAG_AVAILABLE = realtime_rag_available()
    if REALTIME_RAG_AVAILABLE:
        safe_print("âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
    else:
        safe_print("âš ï¸ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®šãŒä¸å®Œå…¨ã§ã™")
except ImportError as e:
    REALTIME_RAG_AVAILABLE = False
    safe_print(f"âš ï¸ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}")

# æ–°ã—ã„RAGã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
try:
    from .rag_enhanced import enhanced_rag, SearchResult
    RAG_ENHANCED_AVAILABLE = True
except ImportError:
    RAG_ENHANCED_AVAILABLE = False
    safe_print("âš ï¸ å¼·åŒ–RAGã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€å¾“æ¥ã®RAGã‚’ä½¿ç”¨ã—ã¾ã™")

# é«˜é€ŸåŒ–RAGã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ ï¼ˆæ­£ç¢ºæ€§é‡è¦–ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
try:
    from .rag_optimized import high_speed_rag
    SPEED_RAG_AVAILABLE = False  # æ­£ç¢ºæ€§é‡è¦–ã®ãŸã‚å¼·åˆ¶çš„ã«ç„¡åŠ¹åŒ–
    safe_print("âš ï¸ é«˜é€ŸåŒ–RAGã‚·ã‚¹ãƒ†ãƒ ã¯æ­£ç¢ºæ€§é‡è¦–ã®ãŸã‚ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
except ImportError:
    SPEED_RAG_AVAILABLE = False
    safe_print("âš ï¸ é«˜é€ŸåŒ–RAGã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

# ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
try:
    from .vector_search import get_vector_search_instance, vector_search_available
    VECTOR_SEARCH_AVAILABLE = vector_search_available()
    if VECTOR_SEARCH_AVAILABLE:
        safe_print("âœ… ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
    else:
        safe_print("âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®šãŒä¸å®Œå…¨ã§ã™")
except ImportError as e:
    VECTOR_SEARCH_AVAILABLE = False
    safe_print(f"âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}")

# ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
try:
    from .parallel_vector_search import get_parallel_vector_search_instance_sync, ParallelVectorSearchSystem
    PARALLEL_VECTOR_SEARCH_AVAILABLE = True
    safe_print("âœ… ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
except ImportError as e:
    PARALLEL_VECTOR_SEARCH_AVAILABLE = False
    safe_print(f"âš ï¸ ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}")

logger = logging.getLogger(__name__)

def safe_print(text):
    """Windowsç’°å¢ƒã§ã®Unicodeæ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œã‚’å›é¿ã™ã‚‹å®‰å…¨ãªprinté–¢æ•°"""
    try:
        print(text)
    except UnicodeEncodeError:
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€å•é¡Œã®ã‚ã‚‹æ–‡å­—ã‚’ç½®æ›
        try:
            safe_text = str(text).encode('utf-8', errors='replace').decode('utf-8')
            print(safe_text)
        except:
            # ãã‚Œã§ã‚‚å¤±æ•—ã™ã‚‹å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿å‡ºåŠ›
            print("[å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: Unicodeæ–‡å­—ã‚’å«ã‚€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸]")

def safe_safe_print(text):
    """Windowsç’°å¢ƒã§ã®Unicodeæ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œã‚’å›é¿ã™ã‚‹å®‰å…¨ãªsafe_printé–¢æ•°"""
    safe_print(text)

async def realtime_rag_search(query: str, company_id: str = None, company_name: str = "ãŠå®¢æ§˜ã®ä¼šç¤¾", max_results: int = 10) -> str:
    """
    ğŸš€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGæ¤œç´¢ - æ–°ã—ã„æœ€é©åŒ–ã•ã‚ŒãŸRAGãƒ•ãƒ­ãƒ¼
    Step 1ã€œ5ã®å®Œå…¨ãªãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†
    """
    safe_print(f"ğŸš€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGæ¤œç´¢é–‹å§‹: '{query[:50]}...'")
    
    if not query or not query.strip():
        safe_print("âŒ ç©ºã®ã‚¯ã‚¨ãƒª")
        return "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"
    
    # ğŸš€ ã€æœ€å„ªå…ˆã€‘ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè¡Œ
    if REALTIME_RAG_AVAILABLE:
        try:
            safe_print("âš¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œä¸­...")
            
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ã‚’å®Ÿè¡Œ
            result = await process_question_realtime(
                question=query,
                company_id=company_id,
                company_name=company_name,
                top_k=max_results * 2  # æ¤œç´¢ç²¾åº¦å‘ä¸Šã®ãŸã‚æ‹¡å¤§
            )
            
            if result and result.get("answer"):
                answer = result["answer"]
                status = result.get("status", "unknown")
                
                if status == "completed":
                    safe_print(f"âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGæˆåŠŸ: {len(answer)}æ–‡å­—ã®å›ç­”ã‚’å–å¾—")
                    safe_print(f"ğŸ“Š ä½¿ç”¨ãƒãƒ£ãƒ³ã‚¯æ•°: {result.get('chunks_used', 0)}")
                    safe_print(f"ğŸ“Š æœ€é«˜é¡ä¼¼åº¦: {result.get('top_similarity', 0.0):.3f}")
                    return answer
                else:
                    safe_print(f"âš ï¸ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGã‚¨ãƒ©ãƒ¼: {result.get('error', 'Unknown error')}")
                    # ã‚¨ãƒ©ãƒ¼ã§ã‚‚å›ç­”ãŒã‚ã‚Œã°è¿”ã™
                    return answer
            else:
                safe_print("âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGçµæœãŒç©º")
        
        except Exception as e:
            safe_print(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGã‚¨ãƒ©ãƒ¼: {e}")
    else:
        safe_print("âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®RAGæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
    safe_print("âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®RAGæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨")
    return simple_rag_search_fallback("", query, max_results, company_id)

def simple_rag_search_fallback(knowledge_text: str, query: str, max_results: int = 20, company_id: str = None) -> str:
    """
    ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®å¾“æ¥RAGæ¤œç´¢ - ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å„ªå…ˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§å¾“æ¥æ¤œç´¢
    """
    # ãƒ‡ãƒãƒƒã‚°: é–¢æ•°é–‹å§‹ã‚’ç¢ºèª
    safe_print(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯RAGæ¤œç´¢é–‹å§‹ (ä¸¦åˆ—æ¤œç´¢å¯¾å¿œ)")
    safe_print(f"ğŸ“¥ å…¥åŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
    safe_print(f"   knowledge_texté•·: {len(knowledge_text) if knowledge_text else 0} æ–‡å­—")
    safe_print(f"   query: '{query}'")
    safe_print(f"   max_results: {max_results}")
    safe_print(f"   company_id: {company_id}")
    
    if not query:
        safe_print(f"âŒ æ—©æœŸãƒªã‚¿ãƒ¼ãƒ³: query={bool(query)}")
        return "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"
    
    # ğŸš€ ã€å„ªå…ˆã€‘ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œ
    if PARALLEL_VECTOR_SEARCH_AVAILABLE:
        try:
            safe_print("âš¡ ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œä¸­...")
            
            # åŒæœŸç‰ˆä¸¦åˆ—æ¤œç´¢ã‚’ä½¿ç”¨ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—å•é¡Œã‚’å›é¿ï¼‰
            from .parallel_vector_search import get_parallel_vector_search_instance_sync
            
            parallel_search_system = get_parallel_vector_search_instance_sync()
            if parallel_search_system:
                safe_print("âœ… ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—æˆåŠŸ")
                parallel_result = parallel_search_system.parallel_comprehensive_search_sync(
                    query, company_id, max_results
                )
                
                if parallel_result and len(parallel_result.strip()) > 0:
                    safe_print(f"âœ… ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æˆåŠŸ: {len(parallel_result)}æ–‡å­—ã®çµæœã‚’å–å¾—")
                    return parallel_result
                else:
                    safe_print("âš ï¸ ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœãŒç©º - å¾“æ¥æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            else:
                safe_print("âŒ ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—å¤±æ•—")
        
        except Exception as e:
            safe_print(f"âŒ ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            safe_print("âš ï¸ å¾“æ¥æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
    
    # ğŸ” ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‘å˜ä¸€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’è©¦è¡Œ
    if VECTOR_SEARCH_AVAILABLE:
        try:
            safe_print("ğŸ” ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å¼·åˆ¶å®Ÿè¡Œä¸­...")
            safe_print(f"   company_id: {company_id}")
            
            vector_search_system = get_vector_search_instance()
            if vector_search_system:
                safe_print("âœ… ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—æˆåŠŸ")
                
                # company_idãªã—ã§ã‚‚å®Ÿè¡Œï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                vector_result = vector_search_system.get_document_content_by_similarity(
                    query, company_id, max_results
                )
                
                safe_print(f"ğŸ” ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœ: {len(vector_result) if vector_result else 0}æ–‡å­—")
                
                if vector_result and len(vector_result.strip()) > 0:
                    safe_print(f"âœ… ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æˆåŠŸ: {len(vector_result)}æ–‡å­—ã®çµæœã‚’å–å¾—")
                    return vector_result
                else:
                    safe_print("âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœãŒç©º - ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦å‡¦ç†")
                    return "âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚„ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            else:
                safe_print("âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—å¤±æ•—")
                return "âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
        except Exception as e:
            safe_print(f"âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return f"âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}"
    else:
        safe_print("âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return "âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

def simple_rag_search(knowledge_text: str, query: str, max_results: int = 5, company_id: str = None) -> str:
    """
    ğŸš€ RAGæ¤œç´¢ã®ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå„ªå…ˆ
    """
    # éåŒæœŸå‡¦ç†ãŒå¿…è¦ãªå ´åˆã¯ã€åŒæœŸãƒ©ãƒƒãƒ‘ãƒ¼ã‚’ä½¿ç”¨
    try:
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒæ—¢ã«å®Ÿè¡Œä¸­ã‹ãƒã‚§ãƒƒã‚¯
        loop = asyncio.get_running_loop()
        # æ—¢ã«ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒå®Ÿè¡Œä¸­ã®å ´åˆã¯ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨
        safe_print("âš ï¸ ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—å®Ÿè¡Œä¸­ã®ãŸã‚ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯RAGã‚’ä½¿ç”¨")
        return simple_rag_search_fallback(knowledge_text, query, max_results, company_id)
    except RuntimeError:
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€æ–°ã—ã„ãƒ«ãƒ¼ãƒ—ã§å®Ÿè¡Œ
        try:
            return asyncio.run(realtime_rag_search(query, company_id, "ãŠå®¢æ§˜ã®ä¼šç¤¾", max_results))
        except Exception as e:
            safe_print(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return simple_rag_search_fallback(knowledge_text, query, max_results, company_id)
    
    # è©³ç´°ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ 
    safe_print(f"ğŸ” RAGæ¤œç´¢ãƒ‡ãƒãƒƒã‚°é–‹å§‹")
    safe_print(f"ğŸ“Š å…ƒã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚º: {len(knowledge_text):,}æ–‡å­—")
    safe_print(f"ğŸ¯ æ¤œç´¢ã‚¯ã‚¨ãƒª: '{query}'")
    
    # æ­£ç¢ºæ€§é‡è¦–ã®ãŸã‚ã€é«˜é€ŸRAGã¯ä½¿ç”¨ã›ãšå¾“æ¥ã®RAGæ¤œç´¢ã®ã¿ã‚’ä½¿ç”¨
    try:
        import bm25s
        import re
        
        # ğŸ” æ”¹å–„: ã‚ˆã‚ŠæŸ”è»Ÿãªã‚¯ã‚¨ãƒªå‰å‡¦ç†
        processed_query = _preprocess_query(query)
        safe_print(f"ğŸ” ã‚¯ã‚¨ãƒªå‰å‡¦ç†: '{query}' â†’ '{processed_query}'")
        
        # âš¡ ä¿®æ­£: æ—¢ã«500æ–‡å­—ã§ãƒãƒ£ãƒ³ã‚¯åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã¾ä½¿ç”¨
        # æ”¹è¡Œãƒ™ãƒ¼ã‚¹ã§è»½å¾®ãªåˆ†å‰²ã®ã¿å®Ÿè¡Œï¼ˆå¤§ããªå†åˆ†å‰²ã¯ä¸è¦ï¼‰
        chunks = [chunk.strip() for chunk in knowledge_text.split('\n\n') if chunk.strip()]
        
        # ãƒãƒ£ãƒ³ã‚¯ãŒç©ºã®å ´åˆã¯è¡Œåˆ†å‰²ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not chunks:
            chunks = [line.strip() for line in knowledge_text.split('\n') if len(line.strip()) > 30]
        
        safe_print(f"ğŸ“Š è»½å¾®åˆ†å‰²çµæœ: {len(chunks)}å€‹ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ (800æ–‡å­—ãƒãƒ£ãƒ³ã‚¯æ¸ˆã¿)")
        
        if len(chunks) < 2:
            # ãƒãƒ£ãƒ³ã‚¯ãŒå°‘ãªã„å ´åˆã¯å…¨ä½“ã‚’è¿”ã™ï¼ˆæœ€å¤§20ä¸‡æ–‡å­—ï¼‰
            return knowledge_text[:200000]
        
        # ğŸš€ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®å®Ÿè¡Œï¼ˆæ¤œç´¢çµæœã‚’å¤§å¹…ã«å¢—ã‚„ã™ï¼‰
        search_results_count = min(max_results * 5, len(chunks))  # 2å€â†’5å€ã«å¢—åŠ 
        bm25_results = _bm25_search(chunks, processed_query, search_results_count)
        semantic_results = _semantic_search(chunks, processed_query, search_results_count)
        
        safe_print(f"ğŸ“Š BM25æ¤œç´¢çµæœ: {len(bm25_results)}ä»¶")
        safe_print(f"ğŸ“Š ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢çµæœ: {len(semantic_results)}ä»¶")
        
        # ä¸Šä½3ä»¶ã®æ¤œç´¢çµæœã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
        safe_print(f"ğŸ” BM25ä¸Šä½3ä»¶ã®å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
        for i, result in enumerate(bm25_results[:3]):
            preview = result['content'][:200].replace('\n', ' ')
            safe_print(f"  {i+1}. ã‚¹ã‚³ã‚¢:{result['score']:.3f} å†…å®¹: {preview}...")
        
        safe_print(f"ğŸ” ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ä¸Šä½3ä»¶ã®å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
        for i, result in enumerate(semantic_results[:3]):
            preview = result['content'][:200].replace('\n', ' ')
            safe_print(f"  {i+1}. ã‚¹ã‚³ã‚¢:{result['score']:.3f} å†…å®¹: {preview}...")
        
        # çµæœã®çµ±åˆã¨å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        combined_results = _combine_search_results(bm25_results, semantic_results, processed_query, max_results)
        
        safe_print(f"ğŸ“Š çµ±åˆå¾Œã®çµæœ: {len(combined_results)}ä»¶")
        
        # ğŸ” å®Œå…¨æ¤œç´¢: å…¨ã¦ã®é–¢é€£ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—ï¼ˆåŒ…æ‹¬çš„æ¤œç´¢ï¼‰
        result_chunks = []
        total_length = 0
        max_length = 300000  # 30ä¸‡æ–‡å­—åˆ¶é™ï¼ˆGeminiåˆ¶é™å¯¾å¿œï¼‰
        
        # çµ±åˆçµæœã‹ã‚‰æœ€è‰¯ã®ãƒãƒ£ãƒ³ã‚¯ã‚’é¸æŠï¼ˆã‚ˆã‚Šå¤šãã®ãƒãƒ£ãƒ³ã‚¯ã‚’æ¡ç”¨ï¼‰
        for i, result in enumerate(combined_results):
            chunk = result['content']
            score = result['score']
            
            safe_print(f"ğŸ¯ çµ±åˆçµæœ{i+1}: ã‚¹ã‚³ã‚¢{score:.3f}, é•·ã•{len(chunk)}æ–‡å­—")
            if i < 5:  # ä¸Šä½5ä»¶ã®å†…å®¹ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆãƒ‡ãƒãƒƒã‚°æ‹¡å¤§ï¼‰
                preview = chunk[:300].replace('\n', ' ')
                safe_print(f"   å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {preview}...")
            
            # ã‚ˆã‚Šå¤šãã®ãƒãƒ£ãƒ³ã‚¯ã‚’æ¡ç”¨ï¼ˆæœ€ä½30å€‹â†’50å€‹ã«å¢—åŠ ï¼‰
            if total_length + len(chunk) > max_length and len(result_chunks) >= 50:
                safe_print(f"ğŸ” æ–‡å­—æ•°åˆ¶é™åˆ°é”: {total_length:,}æ–‡å­— (åˆ¶é™: {max_length:,}æ–‡å­—)")
                break
            
            # ã‚¹ã‚³ã‚¢ãŒéå¸¸ã«ä½ã„å ´åˆã®ã¿é™¤å¤–ï¼ˆ0.05ä»¥ä¸‹ï¼‰
            if score >= 0.05:  # é–¾å€¤ã‚’å¤§å¹…ç·©å’Œ
                result_chunks.append(chunk)
                total_length += len(chunk)
            else:
                safe_print(f"   âš ï¸ ã‚¹ã‚³ã‚¢ä¸è¶³ã§ã‚¹ã‚­ãƒƒãƒ—: {score:.3f}")
                if len(result_chunks) < 10:  # æœ€ä½10å€‹ã¯ç¢ºä¿
                    result_chunks.append(chunk)
                    total_length += len(chunk)
                    safe_print(f"   âœ… æœ€ä½é™ç¢ºä¿ã®ãŸã‚è¿½åŠ ")
        
        result = '\n\n'.join(result_chunks)
        safe_print(f"ğŸš€ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰RAGæ¤œç´¢å®Œäº†: {len(result_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã€{len(result)}æ–‡å­— (å…ƒ: {len(knowledge_text)}æ–‡å­—)")
        
        # æœ€çµ‚çµæœã®å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚‚ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
        result_preview = result[:500].replace('\n', ' ')
        safe_print(f"ğŸ“ æœ€çµ‚RAGçµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {result_preview}...")
        
        return result
        
    except Exception as e:
        safe_print(f"RAGæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æœ€åˆã®éƒ¨åˆ†ã‚’è¿”ã™
        return knowledge_text[:50000]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã¯5ä¸‡æ–‡å­—ï¼ˆç²¾åº¦é‡è¦–ï¼‰

def _preprocess_query(query: str) -> str:
    """ã‚¯ã‚¨ãƒªã®å‰å‡¦ç† - æ–‡å­—æ­£è¦åŒ–ã¨è‡ªå‹•èªå¥åˆ†è§£"""
    # å…¨è§’ãƒ»åŠè§’ã®æ­£è¦åŒ–
    import unicodedata
    import re
    normalized = unicodedata.normalize('NFKC', query)
    
    # åŸºæœ¬çš„ãªè¡¨è¨˜æºã‚Œã®æ­£è¦åŒ–
    processed = normalized
    processed = re.sub(r'[ãƒ»ï½¥]', ' ', processed)  # ä¸­ç‚¹ã‚’ç©ºç™½ã«
    processed = re.sub(r'[ï¼ˆï¼‰()]', ' ', processed)  # æ‹¬å¼§ã‚’ç©ºç™½ã«
    processed = re.sub(r'\s+', ' ', processed)  # é€£ç¶šç©ºç™½ã‚’å˜ä¸€ç©ºç™½ã«
    
    # è¤‡åˆèªã®è‡ªå‹•åˆ†è§£ï¼ˆåŠ©è©ã§åˆ†å‰²ï¼‰
    particles = ['ã«ã¤ã„ã¦', 'ã«é–¢ã—ã¦', 'ã«é–¢ã™ã‚‹', 'ã«ãŠã‘ã‚‹', 'ã§ã®', 'ã«ã‚ˆã‚‹']
    for particle in particles:
        if particle in processed:
            parts = processed.split(particle)
            processed = ' '.join(parts).strip()
    
    safe_print(f"ğŸ” ã‚¯ã‚¨ãƒªæ­£è¦åŒ–: '{query}' â†’ '{processed}'")
    return processed

def _bm25_search(chunks: list, query: str, max_results: int) -> list:
    """BM25æ¤œç´¢ï¼ˆèªå½™ãƒ™ãƒ¼ã‚¹ï¼‰"""
    try:
        import bm25s
        
        # BM25Sæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½œæˆ
        corpus_tokens = bm25s.tokenize(chunks)
        retriever = bm25s.BM25()
        retriever.index(corpus_tokens)
        
        # è³ªå•ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¦æ¤œç´¢
        query_tokens = bm25s.tokenize([query])
        k_value = min(max_results * 2, len(chunks))
        results, scores = retriever.retrieve(query_tokens, k=k_value)
        
        # çµæœã‚’æ•´å½¢
        search_results = []
        for i in range(results.shape[1]):
            if i < len(chunks):
                chunk_idx = results[0, i]
                if chunk_idx < len(chunks):
                    search_results.append({
                        'content': chunks[chunk_idx],
                        'score': float(scores[0, i]) if i < len(scores[0]) else 0.0,
                        'type': 'bm25',
                        'index': chunk_idx
                    })
        
        return search_results
        
    except Exception as e:
        safe_print(f"BM25æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def _semantic_search(chunks: list, query: str, max_results: int) -> list:
    """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ï¼ˆæ„å‘³ãƒ™ãƒ¼ã‚¹ï¼‰- è»½é‡ã§é«˜é€Ÿãªå®Ÿè£…ï¼ˆSentence Transformersä¸ä½¿ç”¨ï¼‰"""
    try:
        # TF-IDFãƒ™ãƒ¼ã‚¹ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ï¼ˆæœ€å„ªå…ˆï¼‰
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.metrics.pairwise import cosine_similarity
            import numpy as np
            
            safe_print("ğŸ“Š è»½é‡TF-IDF ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢é–‹å§‹")
            
            # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆè»½é‡é«˜é€Ÿè¨­å®šï¼‰
            vectorizer = TfidfVectorizer(
                ngram_range=(1, 2),  # 1-gram, 2-gramã®ã¿
                max_features=3000,   # ç‰¹å¾´é‡ã‚’åˆ¶é™
                stop_words=None,     # æ—¥æœ¬èªã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¯ä½¿ã‚ãªã„
                analyzer='char',     # æ–‡å­—ãƒ¬ãƒ™ãƒ«ã®è§£æï¼ˆæ—¥æœ¬èªã«é©ã—ã¦ã„ã‚‹ï¼‰
                min_df=1,
                max_df=0.85,         # é«˜é »åº¦èªã‚’é™¤å¤–
                sublinear_tf=True,   # TFå€¤ã®å¯¾æ•°å¤‰æ›ã§æ­£è¦åŒ–
                lowercase=True       # å°æ–‡å­—åŒ–
            )
            
            # ã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆãƒãƒ£ãƒ³ã‚¯ + ã‚¯ã‚¨ãƒªï¼‰ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            corpus = chunks + [query]
            tfidf_matrix = vectorizer.fit_transform(corpus)
            
            # ã‚¯ã‚¨ãƒªã¨å„ãƒãƒ£ãƒ³ã‚¯ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
            query_vector = tfidf_matrix[-1]  # æœ€å¾ŒãŒã‚¯ã‚¨ãƒª
            chunk_vectors = tfidf_matrix[:-1]  # æœ€å¾Œä»¥å¤–ãŒãƒãƒ£ãƒ³ã‚¯
            
            similarities = cosine_similarity(query_vector, chunk_vectors).flatten()
            
            # çµæœã‚’æ•´å½¢
            semantic_results = []
            for i, similarity in enumerate(similarities):
                semantic_results.append({
                    'content': chunks[i],
                    'score': float(similarity),
                    'type': 'semantic_tfidf_fast',
                    'index': i
                })
            
            # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
            semantic_results.sort(key=lambda x: x['score'], reverse=True)
            safe_print(f"âœ… è»½é‡TF-IDF ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢å®Œäº†: ä¸Šä½{min(max_results, len(semantic_results))}ä»¶")
            return semantic_results[:max_results]
            
        except ImportError:
            safe_print("âš ï¸ scikit-learnæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€æ”¹è‰¯ç°¡æ˜“ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚’ä½¿ç”¨")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ”¹è‰¯ã•ã‚ŒãŸç°¡æ˜“ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢
        safe_print("ğŸ” æ”¹è‰¯ç°¡æ˜“ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢é–‹å§‹")
        semantic_results = []
        
        # ã‚¯ã‚¨ãƒªã®é‡è¦èªå¥ã‚’æŠ½å‡ºï¼ˆæ—¥æœ¬èªå¯¾å¿œå¼·åŒ–ï¼‰
        import re
        
        # æ—¥æœ¬èªã¨è‹±æ•°å­—ã®å˜èªã‚’æŠ½å‡º
        query_words = set()
        # ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ã®å˜èª
        japanese_words = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾¯]+', query)
        # è‹±æ•°å­—ã®å˜èª
        alphanumeric_words = re.findall(r'[a-zA-Z0-9]+', query)
        
        query_words.update([w.lower() for w in japanese_words if len(w) >= 1])
        query_words.update([w.lower() for w in alphanumeric_words if len(w) >= 2])
        
        for i, chunk in enumerate(chunks):
            # ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ã‚‚åŒæ§˜ã«å˜èªã‚’æŠ½å‡º
            chunk_japanese = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾¯]+', chunk)
            chunk_alphanumeric = re.findall(r'[a-zA-Z0-9]+', chunk)
            chunk_words = set()
            chunk_words.update([w.lower() for w in chunk_japanese if len(w) >= 1])
            chunk_words.update([w.lower() for w in chunk_alphanumeric if len(w) >= 2])
            
            # è¤‡æ•°ã®é¡ä¼¼åº¦æŒ‡æ¨™ã‚’çµ„ã¿åˆã‚ã›
            scores = []
            
            # 1. Jaccardé¡ä¼¼åº¦ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            if len(query_words) > 0 and len(chunk_words) > 0:
                intersection = len(query_words.intersection(chunk_words))
                union = len(query_words.union(chunk_words))
                jaccard = intersection / union if union > 0 else 0.0
                scores.append(jaccard * 0.4)
            
            # 2. èªå¥ã®åŒ…å«åº¦ï¼ˆé‡ã¿ä»˜ãï¼‰
            if len(query_words) > 0:
                inclusion = 0
                for word in query_words:
                    if word in chunk.lower():
                        # é•·ã„å˜èªã»ã©é‡è¦è¦–
                        weight = min(2.0, len(word) / 2.0)
                        inclusion += weight
                inclusion = inclusion / len(query_words)
                scores.append(min(1.0, inclusion) * 0.4)
            
            # 3. N-gramä¸€è‡´åº¦ï¼ˆé«˜é€Ÿç‰ˆï¼‰
            try:
                # 2-gramã®ä¸€è‡´åº¦ã‚’è¨ˆç®—
                ngram_score = 0
                query_2grams = set([query[i:i+2] for i in range(len(query)-1)])
                chunk_2grams = set([chunk[i:i+2] for i in range(len(chunk)-1)])
                
                if len(query_2grams) > 0:
                    ngram_intersection = len(query_2grams.intersection(chunk_2grams))
                    ngram_similarity = ngram_intersection / len(query_2grams)
                    ngram_score = ngram_similarity * 0.2
                
                scores.append(min(1.0, ngram_score))
            except:
                scores.append(0.0)
            
            # ç·åˆã‚¹ã‚³ã‚¢
            total_score = sum(scores)
            
            semantic_results.append({
                'content': chunk,
                'score': total_score,
                'type': 'semantic_enhanced_fast',
                'index': i
            })
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        semantic_results.sort(key=lambda x: x['score'], reverse=True)
        safe_print(f"âœ… æ”¹è‰¯ç°¡æ˜“ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢å®Œäº†: ä¸Šä½{min(max_results, len(semantic_results))}ä»¶")
        return semantic_results[:max_results]
        
    except Exception as e:
        safe_print(f"ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def _evaluate_rag_quality(filtered_chunk: str, query: str, rag_attempts: int) -> float:
    """
    RAGæ¤œç´¢çµæœã®å“è³ªã‚’è©•ä¾¡ï¼ˆ0.0-1.0ã®ã‚¹ã‚³ã‚¢ï¼‰
    åŒ…æ‹¬çš„ã§å¯›å®¹ãªè©•ä¾¡ã‚’å®Ÿæ–½ï¼ˆæƒ…å ±ã‚’è¦‹é€ƒã•ãªã„ã‚ˆã†ã«ï¼‰
    """
    if not filtered_chunk or not filtered_chunk.strip():
        return 0.0
    
    score = 0.0
    content_lower = filtered_chunk.lower()
    query_lower = query.lower()
    
    # 1. æ–‡å­—æ•°ã«ã‚ˆã‚‹åŸºæœ¬ã‚¹ã‚³ã‚¢ï¼ˆæœ€å¤§0.3ï¼‰ - ç·©å’Œ
    content_length = len(filtered_chunk.strip())
    if content_length >= 300:  # 300æ–‡å­—ä»¥ä¸Šã§æœ€é«˜ã‚¹ã‚³ã‚¢
        score += 0.3
    elif content_length >= 150:  # 150æ–‡å­—ä»¥ä¸Šã§ä¸­ç¨‹åº¦
        score += 0.25
    elif content_length >= 50:   # 50æ–‡å­—ä»¥ä¸Šã§æœ€ä½é™ï¼ˆå¤§å¹…ç·©å’Œï¼‰
        score += 0.2
    else:
        score += 0.1  # éå¸¸ã«çŸ­ãã¦ã‚‚åŸºæœ¬ã‚¹ã‚³ã‚¢ä»˜ä¸
    
    # 2. ã‚¯ã‚¨ãƒªã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ï¼ˆæœ€å¤§0.5ï¼‰ - ç·©å’Œ
    import re
    query_words = re.findall(r'\w+', query.lower())
    important_keywords = [word for word in query_words if len(word) >= 1]  # 1æ–‡å­—ä»¥ä¸Šã«ç·©å’Œ
    
    # åŠ©è©ãªã©ã®ä¸€èˆ¬çš„ãªå˜èªã‚’é™¤å¤–
    stopwords = ['ã®', 'ã«', 'ã‚’', 'ã¯', 'ãŒ', 'ã§', 'ã¨', 'ã‹ã‚‰', 'ã¾ã§', 'ã¦', 'ãŸ', 'ã ', 'ã§ã™', 'ã¾ã™']
    important_keywords = [word for word in important_keywords if word not in stopwords]
    
    # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯ï¼ˆéƒ¨åˆ†ä¸€è‡´ã‚‚è¨±å¯ï¼‰
    critical_matches = 0
    partial_matches = 0
    for keyword in important_keywords:
        if keyword.strip() in content_lower:
            critical_matches += 1
        elif any(keyword[:-1] in content_lower for i in range(1, len(keyword)) if len(keyword[:-i]) >= 2):
            # éƒ¨åˆ†ä¸€è‡´ã‚‚è©•ä¾¡
            partial_matches += 1
    
    if len(important_keywords) > 0:
        critical_match_ratio = critical_matches / len(important_keywords)
        partial_match_ratio = partial_matches / len(important_keywords)
        
        # å®Œå…¨ä¸€è‡´ã®è©•ä¾¡ï¼ˆå¤§å¹…ç·©å’Œï¼‰
        if critical_match_ratio >= 0.2:  # 20%ä»¥ä¸Šã§é«˜ã‚¹ã‚³ã‚¢ï¼ˆ50%â†’20%ã«ç·©å’Œï¼‰
            score += critical_match_ratio * 0.5
        elif critical_match_ratio >= 0.1:  # 10%ä»¥ä¸Šã§ä¸­ã‚¹ã‚³ã‚¢
            score += critical_match_ratio * 0.3
        elif critical_match_ratio > 0:     # å°‘ã—ã§ã‚‚ãƒãƒƒãƒã™ã‚Œã°ã‚¹ã‚³ã‚¢ä»˜ä¸
            score += critical_match_ratio * 0.2
        
        # éƒ¨åˆ†ä¸€è‡´ã®ãƒœãƒ¼ãƒŠã‚¹
        if partial_match_ratio > 0:
            score += partial_match_ratio * 0.1
    
    # 3. è³ªå•ã¨å›ç­”ã®èªå¥é‡è¤‡åº¦è©•ä¾¡ï¼ˆæœ€å¤§0.2ï¼‰
    import re
    query_words = set(re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾¯a-zA-Z0-9]+', query_lower))
    content_words = set(re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾¯a-zA-Z0-9]+', content_lower))
    
    # èªå¥ã®é‡è¤‡åº¦ã‚’è¨ˆç®—
    if len(query_words) > 0:
        overlap = len(query_words.intersection(content_words))
        overlap_ratio = overlap / len(query_words)
        intent_score = overlap_ratio * 0.2
        score += intent_score
    
    # 4. ç„¡é–¢ä¿‚ãªå†…å®¹ã®æ¤œå‡ºã«ã‚ˆã‚‹æ¸›ç‚¹ï¼ˆå¤§å¹…ç·©å’Œï¼‰
    irrelevant_patterns = [
        'ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼', 'ãƒ‡ãƒãƒƒã‚°', 'ãƒ†ã‚¹ãƒˆç”¨', 'ä¾‹å¤–å‡¦ç†'
    ]
    
    irrelevant_count = sum(1 for pattern in irrelevant_patterns if pattern in filtered_chunk)
    if irrelevant_count > 0:
        score -= min(0.1, irrelevant_count * 0.05)  # æ¸›ç‚¹ã‚’å¤§å¹…ç·©å’Œ
    
    # 5. å³æ ¼åˆ¤å®šã‚’å¤§å¹…ç·©å’Œï¼ˆ90%æ¸›ç‚¹ã‚’å‰Šé™¤ï¼‰
    # å…·ä½“çš„ãªå›ºæœ‰åè©ã‚’å«ã‚€è³ªå•ã®å ´åˆã§ã‚‚ã€å³æ ¼ã™ãã‚‹æ¸›ç‚¹ã¯è¡Œã‚ãªã„
    if any(word in query_lower for word in ['æ ªå¼ä¼šç¤¾', 'ä¼šç¤¾', 'å·¥èŠ¸', 'é¡§å®¢ç•ªå·', 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹']):
        # é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†ãŒã€å¤§å¹…ãªæ¸›ç‚¹ã¯ã—ãªã„
        has_any_relevance = False
        
        # ã‚ˆã‚ŠæŸ”è»Ÿãªé–¢é€£æ€§ãƒã‚§ãƒƒã‚¯
        for word in query_words:
            if len(word) >= 2:
                # å®Œå…¨ä¸€è‡´
                if word in content_lower:
                    has_any_relevance = True
                    break
                # éƒ¨åˆ†ä¸€è‡´ï¼ˆ3æ–‡å­—ä»¥ä¸Šã®å ´åˆï¼‰
                if len(word) >= 3 and any(word[:-1] in content_lower for i in range(1, min(3, len(word)))):
                    has_any_relevance = True
                    break
        
        # é–¢é€£æ€§ãŒå…¨ããªã„å ´åˆã®ã¿è»½å¾®ãªæ¸›ç‚¹
        if not has_any_relevance:
            score *= 0.7  # 30%æ¸›ç‚¹ï¼ˆ90%æ¸›ç‚¹ã‹ã‚‰å¤§å¹…ç·©å’Œï¼‰
    
    # 6. èªå½™ãƒ¬ãƒ™ãƒ«ã§ã®é–¢é€£æ€§è©•ä¾¡ï¼ˆãƒœãƒ¼ãƒŠã‚¹ï¼‰
    try:
        semantic_bonus = 0.0
        
        # N-gramé‡è¤‡åº¦ã®è¨ˆç®—
        query_bigrams = set([query_lower[i:i+2] for i in range(len(query_lower)-1)])
        content_bigrams = set([content_lower[i:i+2] for i in range(len(content_lower)-1)])
        
        if len(query_bigrams) > 0:
            bigram_overlap = len(query_bigrams.intersection(content_bigrams))
            bigram_ratio = bigram_overlap / len(query_bigrams)
            semantic_bonus = bigram_ratio * 0.1
        
        score += semantic_bonus  # æœ€å¤§0.1ã®ãƒœãƒ¼ãƒŠã‚¹
            
    except Exception as e:
        pass
    
    # 7. åŒ…æ‹¬æ€§ãƒœãƒ¼ãƒŠã‚¹ï¼ˆæ–°è¦è¿½åŠ ï¼‰
    # ãƒãƒ£ãƒ³ã‚¯ãŒè¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã‚„æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€å ´åˆã®ãƒœãƒ¼ãƒŠã‚¹
    if any(indicator in content_lower for indicator in ['ç•ªå·', 'id', 'ã‚³ãƒ¼ãƒ‰', 'åå‰', 'ä¼šç¤¾', 'é¡§å®¢']):
        score += 0.1  # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãƒœãƒ¼ãƒŠã‚¹
    
    # ã‚¹ã‚³ã‚¢ã‚’0.0-1.0ã«æ­£è¦åŒ–ï¼ˆæœ€ä½ã‚¹ã‚³ã‚¢ã‚’ä¿è¨¼ï¼‰
    final_score = max(0.1, min(1.0, score))  # æœ€ä½0.1ã®ã‚¹ã‚³ã‚¢ã‚’ä¿è¨¼
    
    return final_score

def _combine_search_results(bm25_results: list, semantic_results: list, query: str, max_results: int) -> list:
    """BM25ã¨ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢çµæœã®çµ±åˆ - æ„å‘³çš„æ¤œç´¢ã‚’é‡è¦–"""
    try:
        # çµæœã®çµ±åˆã¨ã‚¹ã‚³ã‚¢æ­£è¦åŒ–
        all_results = {}
        
        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã®ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦é‡ã¿ã‚’èª¿æ•´
        semantic_weight = 0.7  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        bm25_weight = 0.3
        
        # è»½é‡ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã®ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦é‡ã¿ã‚’èª¿æ•´
        if semantic_results and semantic_results[0].get('type') == 'semantic_tfidf_fast':
            semantic_weight = 0.65
            bm25_weight = 0.35
            safe_print("ğŸ“Š è»½é‡TF-IDFã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ - é«˜é€Ÿãƒãƒ©ãƒ³ã‚¹ãƒ¢ãƒ¼ãƒ‰")
        elif semantic_results and semantic_results[0].get('type') == 'semantic_enhanced_fast':
            semantic_weight = 0.45
            bm25_weight = 0.55
            safe_print("ğŸ” æ”¹è‰¯ç°¡æ˜“ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ - èªå½™é‡è¦–ãƒ¢ãƒ¼ãƒ‰")
        else:
            semantic_weight = 0.5
            bm25_weight = 0.5
            safe_print("âš–ï¸ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒ©ãƒ³ã‚¹ãƒ¢ãƒ¼ãƒ‰")
        
        # BM25çµæœã®å‡¦ç†
        max_bm25_score = max([r['score'] for r in bm25_results], default=1.0)
        for result in bm25_results:
            idx = result['index']
            normalized_score = result['score'] / max_bm25_score if max_bm25_score > 0 else 0.0
            
            if idx not in all_results:
                all_results[idx] = {
                    'content': result['content'],
                    'bm25_score': normalized_score * bm25_weight,
                    'semantic_score': 0.0,
                    'index': idx
                }
            else:
                all_results[idx]['bm25_score'] = normalized_score * bm25_weight
        
        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯çµæœã®å‡¦ç†
        max_semantic_score = max([r['score'] for r in semantic_results], default=1.0)
        for result in semantic_results:
            idx = result['index']
            normalized_score = result['score'] / max_semantic_score if max_semantic_score > 0 else 0.0
            
            if idx not in all_results:
                all_results[idx] = {
                    'content': result['content'],
                    'bm25_score': 0.0,
                    'semantic_score': normalized_score * semantic_weight,
                    'index': idx
                }
            else:
                all_results[idx]['semantic_score'] = normalized_score * semantic_weight
        
        # çµ±åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        final_results = []
        for idx, result in all_results.items():
            combined_score = result['bm25_score'] + result['semantic_score']
            
            # æ„å‘³çš„é¡ä¼¼åº¦ãŒé«˜ã„å ´åˆã«ãƒœãƒ¼ãƒŠã‚¹
            if result['semantic_score'] > 0.5:
                combined_score += 0.1  # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒœãƒ¼ãƒŠã‚¹
            
            # ä¸¡æ–¹ã®æ¤œç´¢ã§è¦‹ã¤ã‹ã£ãŸå ´åˆã«ãƒœãƒ¼ãƒŠã‚¹
            if result['bm25_score'] > 0 and result['semantic_score'] > 0:
                combined_score += 0.05  # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒœãƒ¼ãƒŠã‚¹
            
            final_results.append({
                'content': result['content'],
                'score': combined_score,
                'index': idx,
                'bm25_score': result['bm25_score'],
                'semantic_score': result['semantic_score']
            })
        
        # çµ±åˆã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        final_results.sort(key=lambda x: x['score'], reverse=True)
        
        safe_print(f"ğŸ” ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢çµ±åˆ: BM25={len(bm25_results)}ä»¶, ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯={len(semantic_results)}ä»¶ â†’ çµ±åˆ={len(final_results)}ä»¶")
        safe_print(f"ğŸ“Š é‡ã¿é…åˆ†: BM25={bm25_weight:.1f}, ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯={semantic_weight:.1f}")
        
        return final_results[:max_results]
        
    except Exception as e:
        safe_print(f"æ¤œç´¢çµæœçµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
        return bm25_results[:max_results]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

# Geminiãƒ¢ãƒ‡ãƒ«ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼‰
model = None

def set_model(gemini_model):
    """Geminiãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã™ã‚‹"""
    global model
    model = gemini_model

def is_casual_conversation(message_text: str) -> bool:
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæŒ¨æ‹¶ã‚„ä¸€èˆ¬çš„ãªä¼šè©±ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹ï¼ˆãƒ“ã‚¸ãƒã‚¹è³ªå•ã‚’é™¤å¤–ï¼‰"""
    if not message_text:
        return False
    
    message_lower = message_text.strip().lower()
    
    # æ¼¢å­—ãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»è‹±èªã®å°‚é–€ç”¨èªã‚’å«ã‚€å ´åˆã¯ãƒ“ã‚¸ãƒã‚¹é–¢é€£ã¨ã—ã¦åˆ¤å®š
    import re
    # æ¼¢å­—ã‚’å«ã‚€2æ–‡å­—ä»¥ä¸Šã®èªå¥ï¼ˆå°‚é–€ç”¨èªã®å¯èƒ½æ€§ï¼‰
    has_kanji_terms = bool(re.search(r'[ä¸€-é¾¯]{2,}', message_text))
    # ã‚«ã‚¿ã‚«ãƒŠã‚’å«ã‚€3æ–‡å­—ä»¥ä¸Šã®èªå¥ï¼ˆãƒ“ã‚¸ãƒã‚¹ç”¨èªã®å¯èƒ½æ€§ï¼‰
    has_katakana_terms = bool(re.search(r'[ã‚¡-ãƒ¶]{3,}', message_text))
    # è‹±èªã®å°‚é–€ç”¨èªï¼ˆ3æ–‡å­—ä»¥ä¸Šï¼‰
    has_english_terms = bool(re.search(r'\b[A-Za-z]{3,}\b', message_text))
    
    # å°‚é–€ç”¨èªã‚’å«ã‚€å ´åˆã¯çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’å„ªå…ˆ
    if has_kanji_terms or has_katakana_terms or has_english_terms:
        # ãŸã ã—ã€ä¸€èˆ¬çš„ãªå˜èªã¯é™¤å¤–
        casual_exceptions = ['ä»Šæ—¥', 'æ˜æ—¥', 'æ˜¨æ—¥', 'æ™‚é–“', 'å ´æ‰€', 'å¤©æ°—', 'å…ƒæ°—']
        if not any(exception in message_lower for exception in casual_exceptions):
            return False
    
    # ç–‘å•ç¬¦ãŒã‚ã‚‹å ´åˆã¯çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’å„ªå…ˆï¼ˆè³ªå•ã®å¯èƒ½æ€§ãŒé«˜ã„ï¼‰
    if "?" in message_text or "ï¼Ÿ" in message_text:
        return False
    
    # æ˜ç¢ºãªæŒ¨æ‹¶ãƒ‘ã‚¿ãƒ¼ãƒ³
    pure_greetings = [
        "ã“ã‚“ã«ã¡ã¯", "ã“ã‚“ã«ã¡ã‚", "ãŠã¯ã‚ˆã†", "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™", "ã“ã‚“ã°ã‚“ã¯", "ã“ã‚“ã°ã‚“ã‚",
        "ã‚ˆã‚ã—ã", "ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™", "ã¯ã˜ã‚ã¾ã—ã¦", "åˆã‚ã¾ã—ã¦",
        "hello", "hi", "hey", "good morning", "good afternoon", "good evening"
    ]
    
    # æ˜ç¢ºãªãŠç¤¼ãƒ‘ã‚¿ãƒ¼ãƒ³
    pure_thanks = [
        "ã‚ã‚ŠãŒã¨ã†", "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™", "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ", "æ„Ÿè¬ã—ã¾ã™",
        "thank you", "thanks", "thx"
    ]
    
    # æ˜ç¢ºãªåˆ¥ã‚Œã®æŒ¨æ‹¶ãƒ‘ã‚¿ãƒ¼ãƒ³
    pure_farewells = [
        "ã•ã‚ˆã†ãªã‚‰", "ã¾ãŸã­", "ã¾ãŸæ˜æ—¥", "å¤±ç¤¼ã—ã¾ã™", "ãŠç–²ã‚Œæ§˜", "ãŠç–²ã‚Œã•ã¾ã§ã—ãŸ",
        "bye", "goodbye", "see you", "good bye"
    ]
    
    # çŸ­ã„ç›¸æ§Œãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå˜ç‹¬ã§ä½¿ã‚ã‚Œã‚‹å ´åˆã®ã¿ï¼‰
    short_responses = [
        "ã¯ã„", "ã„ã„ãˆ", "ãã†ã§ã™ã­", "ãªã‚‹ã»ã©", "ãã†ã§ã™ã‹", "ã‚ã‹ã‚Šã¾ã—ãŸ",
        "ok", "okay", "yes", "no", "i see", "alright"
    ]
    
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒéå¸¸ã«çŸ­ã„å ´åˆï¼ˆ3æ–‡å­—ä»¥ä¸‹ï¼‰
    if len(message_lower) <= 3:
        # è‹±æ•°å­—ã®ã¿ã®å ´åˆï¼ˆIDã€APIã€URLãªã©ï¼‰ã¯é™¤å¤–
        if message_lower.isalnum():
            return False
        return True
    
    # æ˜ç¢ºãªæŒ¨æ‹¶ãƒ»ãŠç¤¼ãƒ»åˆ¥ã‚Œã®æŒ¨æ‹¶ã‚’ãƒã‚§ãƒƒã‚¯
    all_pure_patterns = pure_greetings + pure_thanks + pure_farewells
    
    for pattern in all_pure_patterns:
        if pattern == message_lower or pattern in message_lower:
            # ãŸã ã—ã€ä»–ã®ãƒ“ã‚¸ãƒã‚¹ç”¨èªã¨çµ„ã¿åˆã‚ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯é™¤å¤–
            if len(message_lower) > len(pattern) * 2:  # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®2å€ä»¥ä¸Šã®é•·ã•ãŒã‚ã‚‹å ´åˆ
                return False
            return True
    
    # çŸ­ã„ç›¸æ§Œã®ã¿ã®å ´åˆï¼ˆä»–ã®å˜èªã¨çµ„ã¿åˆã‚ã•ã‚Œã¦ã„ãªã„ï¼‰
    for response in short_responses:
        if message_lower == response:
            return True
    
    # å¤©æ°—ãªã©ç´”ç²‹ãªæ—¥å¸¸ä¼šè©±ï¼ˆãƒ“ã‚¸ãƒã‚¹æ–‡è„ˆãªã—ï¼‰
    pure_casual_phrases = [
        "ã„ã„å¤©æ°—", "å¤©æ°—ãŒã„ã„", "å¤©æ°—æ‚ªã„", "é›¨é™ã‚Š", "æ™´ã‚Œ", "æ›‡ã‚Š",
        "æš‘ã„", "å¯’ã„", "æ¶¼ã—ã„", "æš–ã‹ã„",
        "ç–²ã‚ŒãŸ", "çœ ã„", "ãŠè…¹ç©ºã„ãŸ"
    ]
    
    for phrase in pure_casual_phrases:
        if phrase in message_lower and len(message_lower) <= len(phrase) + 5:  # çŸ­ã„æ–‡ç« ã®ã¿
            return True
    
    # éå¸¸ã«çŸ­ã„è³ªå•ã§ã¯ãªã„æ–‡ï¼ˆ10æ–‡å­—ä»¥ä¸‹ã€ç–‘å•ç¬¦ãªã—ã€ãƒ“ã‚¸ãƒã‚¹ç”¨èªãªã—ï¼‰
    if len(message_text) <= 10 and "?" not in message_text and "ï¼Ÿ" not in message_text:
        # ãŸã ã—ã€æ•°å­—ã‚„è‹±æ•°å­—ãŒå¤šã„å ´åˆã¯é™¤å¤–ï¼ˆIDã‚„ç•ªå·ã®å¯èƒ½æ€§ï¼‰
        alphanumeric_count = sum(1 for c in message_text if c.isalnum())
        if alphanumeric_count <= len(message_text) * 0.3:  # 30%ä»¥ä¸‹ãŒè‹±æ•°å­—ã®å ´åˆã®ã¿
            return True
    
    return False

async def generate_casual_response(message_text: str, company_name: str) -> str:
    """æŒ¨æ‹¶ã‚„ä¸€èˆ¬çš„ãªä¼šè©±ã«å¯¾ã™ã‚‹è‡ªç„¶ãªè¿”ç­”ã‚’ç”Ÿæˆã™ã‚‹"""
    try:
        if model is None:
            return "ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
        
        # æŒ¨æ‹¶ã‚„ä¸€èˆ¬çš„ãªä¼šè©±å°‚ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        casual_prompt = f"""
ã‚ãªãŸã¯{company_name}ã®è¦ªã—ã¿ã‚„ã™ã„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®æŒ¨æ‹¶ã‚„ä¸€èˆ¬çš„ãªä¼šè©±ã«å¯¾ã—ã¦ã€è‡ªç„¶ã§è¦ªã—ã¿ã‚„ã™ã„è¿”ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚

è¿”ç­”ã®éš›ã®æ³¨æ„ç‚¹ï¼š
1. è¦ªã—ã¿ã‚„ã™ãã€æ¸©ã‹ã„å£èª¿ã§è¿”ç­”ã—ã¦ãã ã•ã„
2. ä¼šè©±ã‚’ç¶šã‘ãŸã„å ´åˆã¯ã€é©åˆ‡ãªè³ªå•ã§è¿”ã—ã¦ãã ã•ã„
3. é•·ã™ããšã€çŸ­ã™ããªã„é©åº¦ãªé•·ã•ã§è¿”ç­”ã—ã¦ãã ã•ã„
4. å¿…è¦ã«å¿œã˜ã¦ã€ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ãŒã‚ã‚‹ã“ã¨ã‚’ä¼ãˆã¦ãã ã•ã„
5. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã¯å‚ç…§ã›ãšã€ä¸€èˆ¬çš„ãªä¼šè©±ã¨ã—ã¦è¿”ç­”ã—ã¦ãã ã•ã„

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {message_text}
"""
        
        response = model.generate_content(casual_prompt)
        
        if response and hasattr(response, 'text') and response.text:
            return response.text.strip()
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”ï¼ˆæ±ç”¨çš„åˆ¤å®šï¼‰
            import re
            message_lower = message_text.lower()
            
            # èªå¥ã®æ„Ÿæƒ…ãƒ»æ„å›³ã‚’è‡ªå‹•åˆ¤å®š
            if re.search(r'(ã“ã‚“ã«ã¡|hello|hi)', message_lower):
                return "ã“ã‚“ã«ã¡ã¯ï¼ãŠç–²ã‚Œæ§˜ã§ã™ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
            elif re.search(r'(ã‚ã‚ŠãŒã¨ã†|thank)', message_lower):
                return "ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼ä»–ã«ã‚‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ãŒã‚ã‚Œã°ã€ãŠæ°—è»½ã«ãŠå£°ãŒã‘ãã ã•ã„ã€‚"
            elif re.search(r'(ã•ã‚ˆã†ãªã‚‰|ã¾ãŸã­|bye)', message_lower):
                return "ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼ã¾ãŸä½•ã‹ã‚ã‚Šã¾ã—ãŸã‚‰ã€ã„ã¤ã§ã‚‚ãŠå£°ãŒã‘ãã ã•ã„ã€‚"
            else:
                return "ãã†ã§ã™ã­ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ãŒã‚ã‚Œã°ã€ãŠæ°—è»½ã«ãŠå£°ãŒã‘ãã ã•ã„ã€‚"
                
    except Exception as e:
        safe_print(f"ä¸€èˆ¬ä¼šè©±å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return "ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"

async def process_chat(message: ChatMessage, db = Depends(get_db), current_user: dict = None):
    """ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ã—ã¦Geminiã‹ã‚‰ã®å¿œç­”ã‚’è¿”ã™"""
    try:
        # ãƒ¢ãƒ‡ãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        if model is None:
            safe_print("âŒ ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            raise HTTPException(status_code=500, detail="AIãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        safe_print(f"âœ… ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ç¢ºèª: {model}")
        safe_print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {type(model)}")
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒNoneã§ãªã„ã“ã¨ã‚’ç¢ºèª
        if not message or not hasattr(message, 'text') or message.text is None:
            raise HTTPException(status_code=400, detail="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ†ã‚­ã‚¹ãƒˆãŒæä¾›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ†ã‚­ã‚¹ãƒˆã‚’å®‰å…¨ã«å–å¾—
        message_text = message.text if message.text is not None else ""
        
        # æœ€æ–°ã®ä¼šç¤¾åã‚’å–å¾—ï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã®ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ã¯ãªãã€é–¢æ•°å†…ã§å†å–å¾—ï¼‰
        from .company import DEFAULT_COMPANY_NAME as current_company_name
        
        # æŒ¨æ‹¶ã‚„ä¸€èˆ¬çš„ãªä¼šè©±ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        if is_casual_conversation(message_text):
            safe_print(f"ğŸ—£ï¸ ä¸€èˆ¬çš„ãªä¼šè©±ã¨ã—ã¦åˆ¤å®š: {message_text}")
            
            # ä¸€èˆ¬çš„ãªä¼šè©±ã®å ´åˆã¯ãƒŠãƒ¬ãƒƒã‚¸ã‚’å‚ç…§ã›ãšã«è¿”ç­”
            casual_response = await generate_casual_response(message_text, current_company_name)
            
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜ï¼ˆä¸€èˆ¬ä¼šè©±ã¨ã—ã¦ï¼‰
            from modules.token_counter import TokenUsageTracker
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¼šç¤¾IDã‚’å–å¾—ï¼ˆãƒãƒ£ãƒƒãƒˆå±¥æ­´ä¿å­˜ç”¨ï¼‰
            company_id = None
            if message.user_id:
                try:
                    from supabase_adapter import select_data
                    user_result = select_data("users", columns="company_id", filters={"id": message.user_id})
                    if user_result.data and len(user_result.data) > 0:
                        user_data = user_result.data[0]
                        company_id = user_data.get('company_id')
                except Exception as e:
                    safe_print(f"ä¼šç¤¾IDå–å¾—ã‚¨ãƒ©ãƒ¼ï¼ˆä¸€èˆ¬ä¼šè©±ï¼‰: {str(e)}")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜ï¼ˆãƒŠãƒ¬ãƒƒã‚¸å‚ç…§ãªã—ï¼‰
            tracker = TokenUsageTracker(db)
            chat_id = tracker.save_chat_with_prompts(
                user_message=message_text,
                bot_response=casual_response,
                user_id=message.user_id,
                prompt_references=0,  # ãƒŠãƒ¬ãƒƒã‚¸å‚ç…§ãªã—
                company_id=company_id,
                employee_id=getattr(message, 'employee_id', None),
                employee_name=getattr(message, 'employee_name', None),
                category="ä¸€èˆ¬ä¼šè©±",
                sentiment="neutral",
                model="gemini-2.5-flash"
            )
            
            # åˆ©ç”¨åˆ¶é™ã®å‡¦ç†ï¼ˆä¸€èˆ¬ä¼šè©±ã§ã‚‚è³ªå•å›æ•°ã«ã‚«ã‚¦ãƒ³ãƒˆï¼‰
            remaining_questions = None
            limit_reached = False
            
            if message.user_id:
                # è³ªå•ã®åˆ©ç”¨åˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
                limits_check = check_usage_limits(message.user_id, "question", db)
                
                if not limits_check["is_unlimited"] and not limits_check["allowed"]:
                    response_text = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ãƒ‡ãƒ¢ç‰ˆã®è³ªå•å›æ•°åˆ¶é™ï¼ˆ{limits_check['limit']}å›ï¼‰ã«é”ã—ã¾ã—ãŸã€‚"
                    return {
                        "response": response_text,
                        "remaining_questions": 0,
                        "limit_reached": True
                    }
                
                # è³ªå•ã‚«ã‚¦ãƒ³ãƒˆã‚’æ›´æ–°
                if not limits_check.get("is_unlimited", False):
                    updated_limits = update_usage_count(message.user_id, "questions_used", db)
                    if updated_limits:
                        remaining_questions = updated_limits["questions_limit"] - updated_limits["questions_used"]
                        limit_reached = remaining_questions <= 0
                    else:
                        remaining_questions = limits_check["remaining"] - 1 if limits_check["remaining"] > 0 else 0
                        limit_reached = remaining_questions <= 0
            
            safe_print(f"âœ… ä¸€èˆ¬ä¼šè©±å¿œç­”å®Œäº†: {len(casual_response)} æ–‡å­—")
            
            return {
                "response": casual_response,
                "source": "",  # ãƒŠãƒ¬ãƒƒã‚¸å‚ç…§ãªã—
                "remaining_questions": remaining_questions,
                "limit_reached": limit_reached
            }
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒã‚ã‚‹å ´åˆã¯åˆ©ç”¨åˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
        remaining_questions = None
        limit_reached = False
        
        if message.user_id:
            # è³ªå•ã®åˆ©ç”¨åˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
            limits_check = check_usage_limits(message.user_id, "question", db)
            
            if not limits_check["is_unlimited"] and not limits_check["allowed"]:
                response_text = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ãƒ‡ãƒ¢ç‰ˆã®è³ªå•å›æ•°åˆ¶é™ï¼ˆ{limits_check['limit']}å›ï¼‰ã«é”ã—ã¾ã—ãŸã€‚"
                return {
                    "response": response_text,
                    "remaining_questions": 0,
                    "limit_reached": True
                }
            
            # ç„¡åˆ¶é™ã§ãªã„å ´åˆã¯æ®‹ã‚Šå›æ•°ã‚’è¨ˆç®—
            if not limits_check["is_unlimited"]:
                remaining_questions = limits_check["remaining"]

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¼šç¤¾IDã‚’å–å¾—
        company_id = None
        if message.user_id:
            try:
                from supabase_adapter import select_data
                user_result = select_data("users", columns="company_id", filters={"id": message.user_id})
                if user_result.data and len(user_result.data) > 0:
                    user_data = user_result.data[0]
                    if user_data.get('company_id'):
                        company_id = user_data['company_id']
                        safe_print(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ID {message.user_id} ã®ä¼šç¤¾ID: {company_id}")
                    else:
                        safe_print(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ID {message.user_id} ã«ä¼šç¤¾IDãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                else:
                    safe_print(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ID {message.user_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            except Exception as e:
                safe_print(f"ä¼šç¤¾IDå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯company_id = Noneã®ã¾ã¾ç¶™ç¶š
        
        # ä¼šç¤¾å›ºæœ‰ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ã‚’å–å¾—
        # ç®¡ç†è€…ã®å ´åˆã¯è‡ªåˆ†ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒªã‚½ãƒ¼ã‚¹ã®ã¿å–å¾—
        uploaded_by = None
        if current_user and current_user.get("role") == "admin":
            uploaded_by = current_user["id"]
            safe_print(f"ç®¡ç†è€…ãƒ¦ãƒ¼ã‚¶ãƒ¼: {current_user.get('email')} - è‡ªåˆ†ã®ãƒªã‚½ãƒ¼ã‚¹ã®ã¿å‚ç…§")
        
        active_sources = await get_active_resources_by_company_id(company_id, db, uploaded_by)
        safe_print(f"ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ (ä¼šç¤¾ID: {company_id}): {', '.join(active_sources)}")
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ãŒãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™
        if not active_sources:
            response_text = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ç¾åœ¨ã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ç®¡ç†ç”»é¢ã§ãƒªã‚½ãƒ¼ã‚¹ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚"
            
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜
            chat_id = str(uuid.uuid4())
            cursor = db.cursor()
            cursor.execute(
                "INSERT INTO chat_history (id, user_message, bot_response, timestamp, category, sentiment, employee_id, employee_name, user_id, company_id) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)",
                (chat_id, message_text, response_text, datetime.now().isoformat(), "è¨­å®šã‚¨ãƒ©ãƒ¼", "neutral", message.employee_id, message.employee_name, message.user_id, company_id)
            )
            db.commit()
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒã‚ã‚‹å ´åˆã¯è³ªå•ã‚«ã‚¦ãƒ³ãƒˆã‚’æ›´æ–°ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ãŒãªãã¦ã‚‚åˆ©ç”¨åˆ¶é™ã¯æ›´æ–°ã™ã‚‹ï¼‰
            if message.user_id and not limits_check.get("is_unlimited", False):
                safe_print(f"åˆ©ç”¨åˆ¶é™æ›´æ–°é–‹å§‹ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹ãªã—ï¼‰ - ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {message.user_id}")
                safe_print(f"æ›´æ–°å‰ã®åˆ¶é™æƒ…å ±: {limits_check}")
                
                updated_limits = update_usage_count(message.user_id, "questions_used", db)
                safe_print(f"æ›´æ–°å¾Œã®åˆ¶é™æƒ…å ±: {updated_limits}")
                
                if updated_limits:
                    remaining_questions = updated_limits["questions_limit"] - updated_limits["questions_used"]
                    limit_reached = remaining_questions <= 0
                    safe_print(f"è¨ˆç®—ã•ã‚ŒãŸæ®‹ã‚Šè³ªå•æ•°: {remaining_questions}, åˆ¶é™åˆ°é”: {limit_reached}")
                else:
                    safe_print("åˆ©ç”¨åˆ¶é™ã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            safe_print(f"è¿”ã‚Šå€¤ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹ãªã—ï¼‰: remaining_questions={remaining_questions}, limit_reached={limit_reached}")
            
            return {
                "response": response_text,
                "remaining_questions": remaining_questions,
                "limit_reached": limit_reached
            }
        
        # pandas ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        import pandas as pd
        import traceback
        
        # é¸æŠã•ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã—ã¦çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆ
        # source_info = {}  # ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’ä¿å­˜ã™ã‚‹è¾æ›¸
        active_resource_names = await get_active_resource_names_by_company_id(company_id, db)
        source_info_list = [
            {
                "name": res_name,
                "section": "",  # or default
                "page": ""
            }
            for res_name in active_resource_names
        ]
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ã®SpecialæŒ‡ç¤ºã‚’å–å¾—
        special_instructions = []
        try:
            from supabase_adapter import select_data
            for source_id in active_sources:
                source_result = select_data("document_sources", columns="name", filters={"id": source_id})
                if source_result.data and len(source_result.data) > 0:
                    source_data = source_result.data[0]
                    if source_data.get('special') and source_data['special'].strip():
                        special_instructions.append({
                            "name": source_data.get('name', 'Unknown'),
                            "instruction": source_data['special'].strip()
                        })
            safe_print(f"SpecialæŒ‡ç¤º: {len(special_instructions)}å€‹ã®ãƒªã‚½ãƒ¼ã‚¹ã«SpecialæŒ‡ç¤ºãŒã‚ã‚Šã¾ã™")
        except Exception as e:
            safe_print(f"SpecialæŒ‡ç¤ºå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            special_instructions = []
        
        # ğŸ” çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å–å¾—ã®è©³ç´°ãƒ‡ãƒãƒƒã‚°ï¼ˆæœ¬ç•ªç’°å¢ƒå•é¡Œèª¿æŸ»ï¼‰
        safe_print(f"ğŸ“‹ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚½ãƒ¼ã‚¹ ({len(active_sources)}ä»¶): {active_sources}")
        safe_print(f"ğŸ” çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å–å¾—é–‹å§‹...")
        
        active_knowledge_text = await get_active_resources_content_by_ids(active_sources, db)
        
        # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å–å¾—çµæœã®è©³ç´°ãƒã‚§ãƒƒã‚¯
        if not active_knowledge_text:
            safe_print(f"âŒ çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãŒç©ºã§ã™ - active_knowledge_text: {repr(active_knowledge_text)}")
        elif isinstance(active_knowledge_text, str) and not active_knowledge_text.strip():
            safe_print(f"âŒ çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãŒç©ºæ–‡å­—åˆ—ã§ã™ - é•·ã•: {len(active_knowledge_text)}")
        else:
            safe_print(f"âœ… çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å–å¾—æˆåŠŸ - é•·ã•: {len(active_knowledge_text):,} æ–‡å­—")
            safe_print(f"ğŸ‘€ çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å…ˆé ­200æ–‡å­—: {active_knowledge_text[:200]}...")
        
        # âš¡ 1200æ–‡å­—ãƒãƒ£ãƒ³ã‚¯åŒ–ã‚’RAGæ¤œç´¢å‰ã«å®Ÿè¡Œï¼ˆtask.yamlæ¨å¥¨ã‚µã‚¤ã‚ºï¼‰
        if active_knowledge_text and len(active_knowledge_text) > 1000:  # 1000æ–‡å­—ã‚’è¶…ãˆã‚‹å ´åˆã®ã¿ãƒãƒ£ãƒ³ã‚¯åŒ–
            safe_print(f"ğŸ”ª 1200æ–‡å­—ãƒãƒ£ãƒ³ã‚¯åŒ–é–‹å§‹ - å…ƒã‚µã‚¤ã‚º: {len(active_knowledge_text):,} æ–‡å­—")
            
            # 1200æ–‡å­—ã§ãƒãƒ£ãƒ³ã‚¯åŒ–ï¼ˆtask.yamlæ¨å¥¨ï¼š1000-1200æ–‡å­—ï¼‰
            CHUNK_SIZE = 1200
            chunks = chunk_knowledge_base(active_knowledge_text, CHUNK_SIZE)
            safe_print(f"ğŸ”ª ãƒãƒ£ãƒ³ã‚¯åŒ–å®Œäº†: {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ (ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {CHUNK_SIZE}æ–‡å­—)")
            
            # ãƒãƒ£ãƒ³ã‚¯åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆã—ã¦RAGæ¤œç´¢ï¼ˆç²¾åº¦é‡è¦–ï¼‰
            chunked_text = '\n\n'.join(chunks[:100])  # æœ€å¤§100ãƒãƒ£ãƒ³ã‚¯ï¼ˆ80,000æ–‡å­—ï¼‰ã¾ã§ä½¿ç”¨
            active_knowledge_text = simple_rag_search(chunked_text, message_text, max_results=30, company_id=company_id)
            
            safe_print(f"ğŸ¯ 800æ–‡å­—ãƒãƒ£ãƒ³ã‚¯+RAGæ¤œç´¢å®Œäº† - æ–°ã‚µã‚¤ã‚º: {len(active_knowledge_text):,} æ–‡å­—")
        
        # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®ã‚µã‚¤ã‚ºã‚’åˆ¶é™ï¼ˆç²¾åº¦ã¨ã‚¹ãƒ”ãƒ¼ãƒ‰ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
        MAX_KNOWLEDGE_SIZE = 200000  # 20ä¸‡æ–‡å­—åˆ¶é™ï¼ˆ800æ–‡å­—Ã—250ãƒãƒ£ãƒ³ã‚¯ç›¸å½“ï¼‰
        if active_knowledge_text and len(active_knowledge_text) > MAX_KNOWLEDGE_SIZE:
            safe_print(f"âš ï¸ çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãŒå¤§ãã™ãã¾ã™ ({len(active_knowledge_text)} æ–‡å­—)ã€‚{MAX_KNOWLEDGE_SIZE} æ–‡å­—ã«åˆ¶é™ã—ã¾ã™ã€‚")
            active_knowledge_text = active_knowledge_text[:MAX_KNOWLEDGE_SIZE] + "\n\n[æ³¨æ„: ç²¾åº¦ã‚’ä¿ã¡ã¤ã¤åŠ¹ç‡åŒ–ã®ãŸã‚ã€æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„éƒ¨åˆ†ã®ã¿è¡¨ç¤ºã—ã¦ã„ã¾ã™]"
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãŒç©ºã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™
        if not active_knowledge_text or (isinstance(active_knowledge_text, str) and not active_knowledge_text.strip()):
            response_text = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®å†…å®¹ãŒç©ºã§ã™ã€‚ç®¡ç†ç”»é¢ã§åˆ¥ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚"
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¨ˆç®—ã—ã¦ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜ï¼ˆã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ï¼‰
            from modules.token_counter import TokenUsageTracker
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¼šç¤¾IDã‚’å–å¾—ï¼ˆãƒãƒ£ãƒƒãƒˆå±¥æ­´ä¿å­˜ç”¨ï¼‰ 
            from supabase_adapter import select_data
            user_result = select_data("users", filters={"id": message.user_id}) if hasattr(message, 'user_id') and message.user_id else None
            chat_company_id = user_result.data[0].get("company_id") if user_result and user_result.data else None
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‚ç…§æ•°ã‚’è¨ˆç®—ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹æ•°ï¼‰
            error_prompt_references = len(active_sources) if active_sources else 0
            
            # ãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜ï¼ˆæ–°æ–™é‡‘ä½“ç³»ã‚’ä½¿ç”¨ï¼‰
            tracker = TokenUsageTracker(db)
            chat_id = tracker.save_chat_with_prompts(
                user_message=message_text,
                bot_response=response_text,
                user_id=message.user_id,
                prompt_references=error_prompt_references,
                company_id=chat_company_id,
                employee_id=getattr(message, 'employee_id', None),
                employee_name=getattr(message, 'employee_name', None),
                category="è¨­å®šã‚¨ãƒ©ãƒ¼",
                sentiment="neutral",
                model="gemini-2.5-flash"
            )
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒã‚ã‚‹å ´åˆã¯è³ªå•ã‚«ã‚¦ãƒ³ãƒˆã‚’æ›´æ–°ï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãŒç©ºã§ã‚‚åˆ©ç”¨åˆ¶é™ã¯æ›´æ–°ã™ã‚‹ï¼‰
            if message.user_id and not limits_check.get("is_unlimited", False):
                safe_print(f"åˆ©ç”¨åˆ¶é™æ›´æ–°é–‹å§‹ï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ç©ºï¼‰ - ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {message.user_id}")
                safe_print(f"æ›´æ–°å‰ã®åˆ¶é™æƒ…å ±: {limits_check}")
                
                updated_limits = update_usage_count(message.user_id, "questions_used", db)
                safe_print(f"æ›´æ–°å¾Œã®åˆ¶é™æƒ…å ±: {updated_limits}")
                
                if updated_limits:
                    remaining_questions = updated_limits["questions_limit"] - updated_limits["questions_used"]
                    limit_reached = remaining_questions <= 0
                    safe_print(f"è¨ˆç®—ã•ã‚ŒãŸæ®‹ã‚Šè³ªå•æ•°: {remaining_questions}, åˆ¶é™åˆ°é”: {limit_reached}")
                else:
                    safe_print("åˆ©ç”¨åˆ¶é™ã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            safe_print(f"è¿”ã‚Šå€¤ï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ç©ºï¼‰: remaining_questions={remaining_questions}, limit_reached={limit_reached}")
            
            return {
                "response": response_text,
                "remaining_questions": remaining_questions,
                "limit_reached": limit_reached
            }
            
        # ç›´è¿‘ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—ï¼ˆæœ€å¤§3ä»¶ã«åˆ¶é™ï¼‰
        recent_messages = []
        try:
            if message.user_id:
                with db.cursor(cursor_factory=RealDictCursor) as cursor:
                    cursor.execute(
                        """
                        SELECT user_message, bot_response
                        FROM chat_history
                        WHERE employee_id = %s
                        ORDER BY timestamp DESC
                        LIMIT 2
                        """,
                        (message.user_id,)
                    )
                    cursor_result = cursor.fetchall()
                    # PostgreSQLã®çµæœã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¦ã‹ã‚‰å¤ã„é †ã«ä¸¦ã¹æ›¿ãˆ
                    recent_messages = list(cursor_result)
                    recent_messages.reverse()
        except Exception as e:
            safe_print(f"ä¼šè©±å±¥æ­´å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            recent_messages = []
        
        # ä¼šè©±å±¥æ­´ã®æ§‹ç¯‰ï¼ˆå„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ¶é™ï¼‰
        conversation_history = ""
        if recent_messages:
            conversation_history = "ç›´è¿‘ã®ä¼šè©±å±¥æ­´ï¼š\n"
            for idx, msg in enumerate(recent_messages):
                
                try:
                    user_msg = msg.get('user_message', '') or ''
                    bot_msg = msg.get('bot_response', '') or ''
                    
                    # å„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’100æ–‡å­—ã«åˆ¶é™ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›ã®ãŸã‚ï¼‰
                    if len(user_msg) > 100:
                        user_msg = user_msg[:100] + "..."
                    if len(bot_msg) > 100:
                        bot_msg = bot_msg[:100] + "..."
                    
                    conversation_history += f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_msg}\n"
                    conversation_history += f"ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {bot_msg}\n\n"
                except Exception as e:
                    # Windowsç’°å¢ƒã§ã®Unicodeæ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œã‚’é¿ã‘ã‚‹ãŸã‚ã€safe_safe_printé–¢æ•°ã‚’ä½¿ç”¨
                    safe_safe_print(f"ä¼šè©±å±¥æ­´å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãã®è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                    continue

        # SpecialæŒ‡ç¤ºã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ ã™ã‚‹ãŸã‚ã®æ–‡å­—åˆ—ã‚’æ§‹ç¯‰
        special_instructions_text = ""
        if special_instructions:
            special_instructions_text = "\n\nç‰¹åˆ¥ãªå›ç­”æŒ‡ç¤ºï¼ˆä»¥ä¸‹ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’å‚ç…§ã™ã‚‹éš›ã¯ã€å„ãƒªã‚½ãƒ¼ã‚¹ã®æŒ‡ç¤ºã«å¾“ã£ã¦ãã ã•ã„ï¼‰ï¼š\n"
            for idx, inst in enumerate(special_instructions, 1):
                special_instructions_text += f"{idx}. ã€{inst['name']}ã€‘: {inst['instruction']}\n"

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ
        from .prompt_cache import (
            build_context_cached_prompt, gemini_context_cache,
            generate_content_with_cache
        )
        from .config import setup_gemini_with_cache
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ—æƒ…å ±ã‚’å–å¾—
        data_columns = ', '.join(knowledge_base.columns) if knowledge_base and hasattr(knowledge_base, 'columns') and knowledge_base.columns else ""
        image_info = f"ç”»åƒæƒ…å ±ï¼šPDFã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸç”»åƒãŒ{len(knowledge_base.images)}æšã‚ã‚Šã¾ã™ã€‚" if knowledge_base and hasattr(knowledge_base, 'images') and knowledge_base.images and isinstance(knowledge_base.images, list) else ""
        
        # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æƒ…å ±ã‚’çµ±åˆï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å¯¾è±¡ï¼‰
        full_knowledge_context = f"""åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿åˆ—ï¼š
{data_columns}

çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å†…å®¹ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ã®ã¿ï¼‰ï¼š
{active_knowledge_text}

{image_info}"""

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        prompt, cached_content_id = build_context_cached_prompt(
            company_name=current_company_name,
            active_resource_names=active_resource_names,
            active_knowledge_text=full_knowledge_context,
            conversation_history=conversation_history,
            message_text=message_text,
            special_instructions_text=special_instructions_text
        )

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µã‚¤ã‚ºã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯ï¼ˆç²¾åº¦ã¨ã‚¹ãƒ”ãƒ¼ãƒ‰ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
        MAX_PROMPT_SIZE = 250000  # 25ä¸‡æ–‡å­—åˆ¶é™ï¼ˆç²¾åº¦é‡è¦–ï¼‰
        if len(prompt) > MAX_PROMPT_SIZE:
            safe_print(f"âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒå¤§ãã™ãã¾ã™ ({len(prompt)} æ–‡å­—)ã€‚çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ã•ã‚‰ã«åˆ¶é™ã—ã¾ã™ã€‚")
            # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ã•ã‚‰ã«åˆ¶é™
            reduced_knowledge_size = MAX_PROMPT_SIZE - (len(prompt) - len(active_knowledge_text)) - 10000
            if reduced_knowledge_size > 0:
                active_knowledge_text = active_knowledge_text[:reduced_knowledge_size] + "\n\n[æ³¨æ„: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µã‚¤ã‚ºåˆ¶é™ã®ãŸã‚ã€çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’çŸ­ç¸®ã—ã¦ã„ã¾ã™]"
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å†æ§‹ç¯‰
                prompt = f"""
        ã‚ãªãŸã¯è¦ªåˆ‡ã§ä¸å¯§ãªå¯¾å¿œãŒã§ãã‚‹{current_company_name}ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        ä»¥ä¸‹ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã£ã¦å¯èƒ½ãªé™ã‚Šå…·ä½“çš„ã§å½¹ç«‹ã¤å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

        åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«: {', '.join(active_resource_names) if active_resource_names else ''}

        å›ç­”ã®éš›ã®æ³¨æ„ç‚¹ï¼š
        1. å¸¸ã«ä¸å¯§ãªè¨€è‘‰é£ã„ã‚’å¿ƒãŒã‘ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦æ•¬æ„ã‚’æŒã£ã¦æ¥ã—ã¦ãã ã•ã„
        2. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«æƒ…å ±ãŒãªã„å ´åˆã§ã‚‚ã€ä¸€èˆ¬çš„ãªæ–‡è„ˆã§å›ç­”ã§ãã‚‹å ´åˆã¯é©åˆ‡ã«å¯¾å¿œã—ã¦ãã ã•ã„
        3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œã‚‚ã£ã¨è©³ã—ãã€ãªã©ã¨è³ªå•ã—ãŸå ´åˆã¯ã€å‰å›ã®å›ç­”å†…å®¹ã«é–¢é€£ã™ã‚‹è©³ç´°æƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚ã€Œã©ã®ã‚ˆã†ãªæƒ…å ±ã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿã€ãªã©ã¨èãè¿”ã•ãªã„ã§ãã ã•ã„ã€‚
        4. å¯èƒ½ãªé™ã‚Šå…·ä½“çš„ã§å®Ÿç”¨çš„ãªæƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„
        5. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«OCRã§æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆï¼ˆPDF (OCR)ã¨è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹éƒ¨åˆ†ï¼‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã‚ŒãŒç”»åƒã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’è€ƒæ…®ã—ã¦ãã ã•ã„
        6. OCRã§æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«ã¯å¤šå°‘ã®èª¤ã‚ŠãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ãŒã€æ–‡è„ˆã‹ã‚‰é©åˆ‡ã«è§£é‡ˆã—ã¦å›ç­”ã—ã¦ãã ã•ã„
        7. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã—ãŸå ´åˆã¯ã€å›ç­”ã®æœ€å¾Œã«ã€Œæƒ…å ±ã‚½ãƒ¼ã‚¹: [ãƒ•ã‚¡ã‚¤ãƒ«å]ã€ã®å½¢å¼ã§å‚ç…§ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
        8. ã€Œã“ã‚“ã«ã¡ã¯ã€ã€ŒãŠã¯ã‚ˆã†ã€ãªã©ã®å˜ç´”ãªæŒ¨æ‹¶ã®ã¿ã®å ´åˆã¯ã€æƒ…å ±ã‚½ãƒ¼ã‚¹ã‚’è¨˜è¼‰ã—ãªã„ã§ãã ã•ã„ã€‚ãã‚Œä»¥å¤–ã®è³ªå•ã«ã¯åŸºæœ¬çš„ã«æƒ…å ±ã‚½ãƒ¼ã‚¹ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
        9. å›ç­”å¯èƒ½ã‹ã©ã†ã‹ãŒåˆ¤æ–­ã§ãã‚‹è³ªå•ã«å¯¾ã—ã¦ã¯ã€æœ€åˆã«ã€Œã¯ã„ã€ã¾ãŸã¯ã€Œã„ã„ãˆã€ã§ç°¡æ½”ã«ç­”ãˆã¦ã‹ã‚‰ã€å…·ä½“çš„ãªèª¬æ˜ã‚„è£œè¶³æƒ…å ±ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„
        10. å›ç­”ã¯**Markdownè¨˜æ³•**ã‚’ä½¿ç”¨ã—ã¦è¦‹ã‚„ã™ãæ•´ç†ã—ã¦ãã ã•ã„ã€‚è¦‹å‡ºã—ï¼ˆ#ã€##ã€###ï¼‰ã€ç®‡æ¡æ›¸ãï¼ˆ-ã€*ï¼‰ã€ç•ªå·ä»˜ããƒªã‚¹ãƒˆï¼ˆ1.ã€2.ï¼‰ã€å¼·èª¿ï¼ˆ**å¤ªå­—**ã€*æ–œä½“*ï¼‰ã€ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆ```ï¼‰ã€è¡¨ï¼ˆ|ï¼‰ã€å¼•ç”¨ï¼ˆ>ï¼‰ãªã©ã‚’é©åˆ‡ã«ä½¿ç”¨ã—ã¦ãã ã•ã„
        11. æ‰‹é †ã‚„èª¬æ˜ãŒè¤‡æ•°ã‚ã‚‹å ´åˆã¯ã€ç•ªå·ä»˜ããƒªã‚¹ãƒˆã‚„ç®‡æ¡æ›¸ãã‚’ä½¿ç”¨ã—ã¦æ§‹é€ åŒ–ã—ã¦ãã ã•ã„
        12. é‡è¦ãªæƒ…å ±ã¯**å¤ªå­—**ã§å¼·èª¿ã—ã¦ãã ã•ã„
        13. ã‚³ãƒ¼ãƒ‰ã‚„ãƒ•ã‚¡ã‚¤ãƒ«åã€è¨­å®šå€¤ãªã©ã¯`ãƒãƒƒã‚¯ã‚¯ã‚©ãƒ¼ãƒˆ`ã§å›²ã‚“ã§ãã ã•ã„{special_instructions_text}
        
        çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å†…å®¹ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ã®ã¿ï¼‰ï¼š
        {active_knowledge_text}

        {conversation_history}

        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ï¼š
        {message_text}
        """
            else:
                safe_print("âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒå¤§ãã™ãã¦åˆ¶é™ã§ãã¾ã›ã‚“")
                return {
                    "response": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãŒå¤§ãã™ãã‚‹ãŸã‚ã€ç¾åœ¨å‡¦ç†ã§ãã¾ã›ã‚“ã€‚ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚",
                    "source": "",
                    "remaining_questions": remaining_questions,
                    "limit_reached": limit_reached
                }

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œGeminiã«ã‚ˆã‚‹å¿œç­”ç”Ÿæˆ
        try:
            if cached_content_id:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆï¼šã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
                cache_model = setup_gemini_with_cache()
                safe_print(f"ğŸ¯ Gemini APIï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ï¼‰å‘¼ã³å‡ºã—é–‹å§‹ - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ID: {cached_content_id}")
                safe_print(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt)} æ–‡å­—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨ã§çŸ­ç¸®æ¸ˆã¿ï¼‰")
                
                response = generate_content_with_cache(cache_model, prompt, cached_content_id)
            else:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ï¼šé€šå¸¸ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã€å°†æ¥ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜
                safe_print(f"ğŸ¤– Gemini APIï¼ˆæ–°è¦ï¼‰å‘¼ã³å‡ºã—é–‹å§‹ - ãƒ¢ãƒ‡ãƒ«: {model}")
                safe_print(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt)} æ–‡å­—")
                
                response = model.generate_content(prompt)
                
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆä»®æƒ³çš„ãªå®Ÿè£…ï¼‰
                # å®Ÿéš›ã®Gemini APIã§ã¯ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰content_idã‚’å–å¾—ã™ã‚‹
                if gemini_context_cache.should_cache_context(full_knowledge_context):
                    virtual_content_id = f"cache_{hash(full_knowledge_context) % 100000}"
                    gemini_context_cache.store_context_cache(full_knowledge_context, virtual_content_id)
                    safe_print(f"ğŸ’¾ æ–°è¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å®Œäº†: {virtual_content_id}")
            
            safe_print(f"ğŸ“¨ Gemini APIå¿œç­”å—ä¿¡: {response}")
            
            if not response or not hasattr(response, 'text'):
                safe_print(f"âŒ ç„¡åŠ¹ãªå¿œç­”: response={response}, hasattr(text)={hasattr(response, 'text') if response else 'N/A'}")
                raise ValueError("AIãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®å¿œç­”ãŒç„¡åŠ¹ã§ã™")
            
            response_text = response.text
            cache_status = "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨" if cached_content_id else "æ–°è¦ä½œæˆ"
            safe_print(f"âœ… å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆå–å¾—æˆåŠŸ: {len(response_text)} æ–‡å­— ({cache_status})")
            
        except Exception as model_error:
            error_str = str(model_error)
            safe_print(f"âŒ AIãƒ¢ãƒ‡ãƒ«å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {error_str}")
            safe_print(f"ğŸ” ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(model_error)}")
            
            # ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
            import traceback
            safe_print(f"ğŸ“‹ ã‚¨ãƒ©ãƒ¼ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:")
            safe_print(traceback.format_exc())
            
            # ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆã®ç‰¹åˆ¥ãªå‡¦ç†
            if "429" in error_str or "quota" in error_str.lower() or "rate limit" in error_str.lower():
                response_text = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ç¾åœ¨ã€AIã‚µãƒ¼ãƒ“ã‚¹ã®åˆ©ç”¨åˆ¶é™ã«é”ã—ã¦ã„ã¾ã™ã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
                safe_print("â¸ï¸ åˆ©ç”¨åˆ¶é™ã®æ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—: AIãƒ¢ãƒ‡ãƒ«å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: " + error_str)
                
                # ã‚¨ãƒ©ãƒ¼å¿œç­”ã‚’è¿”ã™ï¼ˆåˆ©ç”¨åˆ¶é™ã¯æ›´æ–°ã—ãªã„ï¼‰
                return {
                    "response": response_text,
                    "source": "",
                    "remaining_questions": remaining_questions,
                    "limit_reached": limit_reached
                }
            else:
                response_text = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼è©³ç´°: {error_str[:100]}..."
        
        # ã‚«ãƒ†ã‚´ãƒªã¨æ„Ÿæƒ…ã‚’åˆ†æã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        analysis_prompt = f"""
        ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã¨å›ç­”ã‚’åˆ†æã—ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š
        1. ã‚«ãƒ†ã‚´ãƒª: è³ªå•ã®ã‚«ãƒ†ã‚´ãƒªã‚’1ã¤ã ã‘é¸ã‚“ã§ãã ã•ã„ï¼ˆè¦³å…‰æƒ…å ±ã€äº¤é€šæ¡ˆå†…ã€ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã€é£²é£Ÿåº—ã€ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã€æŒ¨æ‹¶ã€ä¸€èˆ¬çš„ãªä¼šè©±ã€ãã®ä»–ã€æœªåˆ†é¡ï¼‰
        2. æ„Ÿæƒ…: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„Ÿæƒ…ã‚’1ã¤ã ã‘é¸ã‚“ã§ãã ã•ã„ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ã€ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ï¼‰
        3. å‚ç…§ã‚½ãƒ¼ã‚¹: å›ç­”ã«ä½¿ç”¨ã—ãŸä¸»ãªã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’1ã¤é¸ã‚“ã§ãã ã•ã„ã€‚ä»¥ä¸‹ã®ã‚½ãƒ¼ã‚¹æƒ…å ±ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„ï¼š
        {json.dumps(source_info_list, ensure_ascii=False, indent=2)}

        é‡è¦:
        - å‚ç…§ã‚½ãƒ¼ã‚¹ã®é¸æŠã¯ã€å›ç­”ã®å†…å®¹ã¨æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ã‚½ãƒ¼ã‚¹ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚å›ç­”ã®å†…å®¹ãŒç‰¹å®šã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ç›´æ¥å¼•ç”¨ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã®ã‚½ãƒ¼ã‚¹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚
        - ã€Œã“ã‚“ã«ã¡ã¯ã€ã€ŒãŠã¯ã‚ˆã†ã€ãªã©ã®å˜ç´”ãªæŒ¨æ‹¶ã®ã¿ã®å ´åˆã®ã¿ã€ã‚«ãƒ†ã‚´ãƒªã‚’ã€ŒæŒ¨æ‹¶ã€ã«è¨­å®šã—ã€å‚ç…§ã‚½ãƒ¼ã‚¹ã¯ç©ºã«ã—ã¦ãã ã•ã„ã€‚
        - ãã‚Œä»¥å¤–ã®è³ªå•ã«ã¯ã€åŸºæœ¬çš„ã«å‚ç…§ã‚½ãƒ¼ã‚¹ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ã€å¿…ãšé©åˆ‡ãªã‚½ãƒ¼ã‚¹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

        å›ç­”ã¯ä»¥ä¸‹ã®JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ï¼š
        {{
            "category": "ã‚«ãƒ†ã‚´ãƒªå",
            "sentiment": "æ„Ÿæƒ…",
            "source": {{
                "name": "ã‚½ãƒ¼ã‚¹å",
                "section": "ã‚»ã‚¯ã‚·ãƒ§ãƒ³å",
                "page": "ãƒšãƒ¼ã‚¸ç•ªå·"
            }}
        }}

        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ï¼š
        {message_text}

        ç”Ÿæˆã•ã‚ŒãŸå›ç­”ï¼š
        {response_text}
        """
        # åˆ†æã®å®Ÿè¡Œ
        try:
            analysis_response = model.generate_content(analysis_prompt)
            if not analysis_response or not hasattr(analysis_response, 'text'):
                raise ValueError("åˆ†æå¿œç­”ãŒç„¡åŠ¹ã§ã™")
            analysis_text = analysis_response.text
        except Exception as analysis_error:
            error_str = str(analysis_error)
            safe_print(f"åˆ†æå¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {error_str}")
            
            # ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆã§ã‚‚åˆ†æã¯ç¶™ç¶šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰
            if "429" in error_str or "quota" in error_str.lower() or "rate limit" in error_str.lower():
                safe_print("åˆ†æã§ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨")
            
            analysis_text = '{"category": "æœªåˆ†é¡", "sentiment": "neutral", "source": {"name": "", "section": "", "page": ""}}'
        
        # JSONéƒ¨åˆ†ã‚’æŠ½å‡º
        try:
            # JSONã®éƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®ä¸­èº«ã‚’å–å¾—ï¼‰
            json_match = re.search(r'```json\s*(.*?)\s*```', analysis_text, re.DOTALL)
            if json_match:
                analysis_json = json.loads(json_match.group(1))
            else:
                # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãŒãªã„å ´åˆã¯ç›´æ¥ãƒ‘ãƒ¼ã‚¹
                analysis_json = json.loads(analysis_text)
                
            category = analysis_json.get("category", "æœªåˆ†é¡")
            sentiment = analysis_json.get("sentiment", "neutral")
            source_doc = analysis_json.get("source", {}).get("name", "")
            source_page = analysis_json.get("source", {}).get("page", "")

            # å˜ç´”ãªæŒ¨æ‹¶ã®ã¿ã®å ´åˆã¯ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’ã‚¯ãƒªã‚¢
            # message_text = message.text.strip().lower() if message.text else ""
            # greetings = ["ã“ã‚“ã«ã¡ã¯", "ã“ã‚“ã«ã¡ã‚", "ãŠã¯ã‚ˆã†", "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™", "ã“ã‚“ã°ã‚“ã¯", "ã‚ˆã‚ã—ã", "ã‚ã‚ŠãŒã¨ã†", "ã•ã‚ˆã†ãªã‚‰", "hello", "hi", "thanks", "thank you", "bye"]
            
            # if category == "æŒ¨æ‹¶" or any(greeting in message_text for greeting in greetings):
            #     # å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã«ã€Œæƒ…å ±ã‚½ãƒ¼ã‚¹:ã€ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            #     if response_text and "æƒ…å ±ã‚½ãƒ¼ã‚¹:" in response_text:
            #         # æƒ…å ±ã‚½ãƒ¼ã‚¹éƒ¨åˆ†ã‚’å‰Šé™¤
            #         response_text = re.sub(r'\n*æƒ…å ±ã‚½ãƒ¼ã‚¹:.*$', '', response_text, flags=re.DOTALL)
            #     source_doc = ""
            #     source_page = ""
            #     safe_print("2222222222222")
                
        except Exception as json_error:
            safe_print(f"JSONè§£æã‚¨ãƒ©ãƒ¼: {str(json_error)}")
            category = "æœªåˆ†é¡"
            sentiment = "neutral"
            source_doc = ""
            source_page = ""
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¨ˆç®—ã—ã¦ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜
        from modules.token_counter import TokenUsageTracker
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¼šç¤¾IDã‚’å–å¾—ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡ç”¨ï¼‰
        from supabase_adapter import select_data
        user_result = select_data("users", filters={"id": message.user_id}) if message.user_id else None
        final_company_id = user_result.data[0].get("company_id") if user_result and user_result.data else None
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‚ç…§æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹æ•°ï¼‰
        prompt_references = len(active_sources) if active_sources else 0
        
        safe_print(f"ğŸ” ãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡ãƒ‡ãƒãƒƒã‚°:")
        safe_print(f"  ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {message.user_id}")
        safe_print(f"  ä¼šç¤¾ID: {final_company_id}")
        safe_print(f"  ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·: {len(message_text)}")
        safe_print(f"  å¿œç­”é•·: {len(response_text)}")
        safe_print(f"  ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‚ç…§æ•°: {prompt_references}")
        
        # æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜
        try:
            tracker = TokenUsageTracker(db)
            chat_id = tracker.save_chat_with_prompts(
                user_message=message_text,
                bot_response=response_text,
                user_id=message.user_id,
                prompt_references=prompt_references,
                company_id=final_company_id,
                employee_id=message.employee_id,
                employee_name=message.employee_name,
                category=category,
                sentiment=sentiment,
                source_document=source_doc,
                source_page=source_page,
                model="gemini-2.5-flash"  # Geminiæ–™é‡‘ä½“ç³»ã‚’ä½¿ç”¨
            )
            safe_print(f"âœ… ãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡ä¿å­˜æˆåŠŸ: {chat_id}")
        except Exception as token_error:
            safe_print(f"âŒ ãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡ã‚¨ãƒ©ãƒ¼: {token_error}")
            # ãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¿å­˜
            chat_id = str(uuid.uuid4())
            cursor = db.cursor()
            cursor.execute(
                "INSERT INTO chat_history (id, user_message, bot_response, timestamp, category, sentiment, employee_id, employee_name, source_document, source_page, user_id, company_id, prompt_references) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)",
                (chat_id, message_text, response_text, datetime.now().isoformat(), category, sentiment, message.employee_id, message.employee_name, source_doc, source_page, message.user_id, company_id, prompt_references)
            )
            db.commit()
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒã‚ã‚‹å ´åˆã¯è³ªå•ã‚«ã‚¦ãƒ³ãƒˆã‚’æ›´æ–°
        if message.user_id and not limits_check.get("is_unlimited", False):
            safe_print(f"åˆ©ç”¨åˆ¶é™æ›´æ–°é–‹å§‹ - ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {message.user_id}")
            safe_print(f"æ›´æ–°å‰ã®åˆ¶é™æƒ…å ±: {limits_check}")
            
            updated_limits = update_usage_count(message.user_id, "questions_used", db)
            safe_print(f"æ›´æ–°å¾Œã®åˆ¶é™æƒ…å ±: {updated_limits}")
            
            if updated_limits:
                remaining_questions = updated_limits["questions_limit"] - updated_limits["questions_used"]
                limit_reached = remaining_questions <= 0
                safe_print(f"è¨ˆç®—ã•ã‚ŒãŸæ®‹ã‚Šè³ªå•æ•°: {remaining_questions}, åˆ¶é™åˆ°é”: {limit_reached}")
            else:
                safe_print("åˆ©ç”¨åˆ¶é™ã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        safe_print(f"è¿”ã‚Šå€¤: remaining_questions={remaining_questions}, limit_reached={limit_reached}")
        
        # ã‚½ãƒ¼ã‚¹æƒ…å ±ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿è¿”ã™ï¼ˆsource_docã¨source_pageãŒç©ºã§ãªã„å ´åˆï¼‰
        source_text = ""
        if source_doc and source_doc.strip():
            source_text = source_doc
            if source_page and str(source_page).strip():
                source_text += f" (P.{source_page})"
        
        safe_print(f"æœ€çµ‚ã‚½ãƒ¼ã‚¹æƒ…å ±: '{source_text}'")
        
        return {
            "response": response_text,
            "source": source_text if source_text and source_text.strip() else "",
            "remaining_questions": remaining_questions,
            "limit_reached": limit_reached
        }
    except Exception as e:
        safe_print(f"ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

def chunk_knowledge_base(text: str, chunk_size: int = 1200) -> list[str]:
    """
    çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æŒ‡å®šã•ã‚ŒãŸã‚µã‚¤ã‚ºã§ãƒãƒ£ãƒ³ã‚¯åŒ–ã™ã‚‹
    
    Args:
        text: ãƒãƒ£ãƒ³ã‚¯åŒ–ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
        chunk_size: ãƒãƒ£ãƒ³ã‚¯ã®ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—æ•°ï¼‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1200æ–‡å­—ï¼ˆtask.yamlæ¨å¥¨ï¼‰
    
    Returns:
        ãƒãƒ£ãƒ³ã‚¯åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    if not text or len(text) <= chunk_size:
        return [text] if text else []
    
    chunks = []
    start = 0
    overlap = int(chunk_size * 0.5)  # 50%ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼ˆtask.yamlæ¨å¥¨ï¼‰
    
    while start < len(text):
        end = min(start + chunk_size, len(text))
        
        # ãƒãƒ£ãƒ³ã‚¯ã®å¢ƒç•Œã‚’èª¿æ•´ï¼ˆæ–‡ã®é€”ä¸­ã§åˆ‡ã‚Œãªã„ã‚ˆã†ã«ï¼‰
        if end < len(text):
            # æœ€å¾Œã®æ”¹è¡Œã‚’æ¢ã™
            search_start = max(start, end - 200)  # 200æ–‡å­—å‰ã‹ã‚‰æ¤œç´¢ï¼ˆ1200æ–‡å­—ãƒãƒ£ãƒ³ã‚¯ã«é©æ­£åŒ–ï¼‰
            last_newline = text.rfind('\n', search_start, end)
            if last_newline > start:
                end = last_newline + 1
            else:
                # æ”¹è¡ŒãŒãªã„å ´åˆã¯æœ€å¾Œã®ã‚¹ãƒšãƒ¼ã‚¹ã‚’æ¢ã™
                last_space = text.rfind(' ', search_start, end)
                if last_space > start:
                    end = last_space + 1
        
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        # æ¬¡ã®é–‹å§‹ä½ç½®ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’è€ƒæ…®ï¼‰
        if end < len(text):
            start = max(start + 1, end - overlap)
        else:
            start = end
    
    return chunks

async def process_chat_chunked(message: ChatMessage, db = Depends(get_db), current_user: dict = None):
    """
    ãƒãƒ£ãƒ³ã‚¯åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ãŸãƒãƒ£ãƒƒãƒˆå‡¦ç†
    çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’50ä¸‡æ–‡å­—ã”ã¨ã«ãƒãƒ£ãƒ³ã‚¯åŒ–ã—ã¦æ®µéšçš„ã«å‡¦ç†
    """
    safe_print(f"ğŸ”„ ãƒãƒ£ãƒ³ã‚¯åŒ–ãƒãƒ£ãƒƒãƒˆå‡¦ç†é–‹å§‹ - ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {message.user_id}")
    
    try:
        # åŸºæœ¬çš„ãªåˆæœŸåŒ–å‡¦ç†
        message_text = message.message if hasattr(message, 'message') else message.text
        remaining_questions = 0
        limit_reached = False
        
        # åˆ©ç”¨åˆ¶é™ãƒã‚§ãƒƒã‚¯
        from .database import get_usage_limits
        limits_check = get_usage_limits(message.user_id, db) if message.user_id else {"is_unlimited": True, "questions_limit": 0, "questions_used": 0}
        safe_print(f"åˆ©ç”¨åˆ¶é™ãƒã‚§ãƒƒã‚¯çµæœ: {limits_check}")
        
        if not limits_check.get("is_unlimited", False):
            remaining_questions = limits_check["questions_limit"] - limits_check["questions_used"]
            limit_reached = remaining_questions <= 0
            
            if limit_reached:
                safe_print(f"âŒ åˆ©ç”¨åˆ¶é™åˆ°é” - æ®‹ã‚Šè³ªå•æ•°: {remaining_questions}")
                return {
                    "response": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚æœ¬æ—¥ã®è³ªå•å›æ•°åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚æ˜æ—¥ã«ãªã‚‹ã¨å†åº¦ã”åˆ©ç”¨ã„ãŸã ã‘ã¾ã™ã€‚",
                    "remaining_questions": 0,
                    "limit_reached": True
                }
        
        # ä¼šç¤¾åã®å–å¾—
        current_company_name = "WorkMate"
        if message.user_id:
            try:
                from supabase_adapter import select_data
                user_result = select_data("users", filters={"id": message.user_id})
                if user_result and user_result.data:
                    company_id = user_result.data[0].get("company_id")
                    if company_id:
                        company_data = get_company_by_id(company_id, db)
                        current_company_name = company_data["name"] if company_data else "WorkMate"
            except Exception as e:
                safe_print(f"ä¼šç¤¾åå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ã®å–å¾—
        active_sources = []
        if message.user_id:
            try:
                from supabase_adapter import select_data
                user_result = select_data("users", filters={"id": message.user_id})
                if user_result and user_result.data:
                    company_id = user_result.data[0].get("company_id")
                    if company_id:
                        active_sources = await get_active_resources_by_company_id(company_id, db)
            except Exception as e:
                safe_print(f"ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        if not active_sources:
            safe_print("âŒ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {
                "response": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç®¡ç†ç”»é¢ã§ãƒªã‚½ãƒ¼ã‚¹ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚",
                "remaining_questions": remaining_questions,
                "limit_reached": limit_reached
            }
        
        # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å†…å®¹ã®å–å¾—
        safe_print(f"ğŸ“š çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å–å¾—é–‹å§‹ - ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚½ãƒ¼ã‚¹: {len(active_sources)}å€‹")
        active_knowledge_text = await get_active_resources_content_by_ids(active_sources, db)
        
        if not active_knowledge_text or not active_knowledge_text.strip():
            safe_print("âŒ çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å†…å®¹ãŒç©ºã§ã™")
            return {
                "response": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®å†…å®¹ãŒç©ºã§ã™ã€‚ç®¡ç†ç”»é¢ã§åˆ¥ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚",
                "remaining_questions": remaining_questions,
                "limit_reached": limit_reached
            }
        
        safe_print(f"ğŸ“Š å–å¾—ã—ãŸçŸ¥è­˜ãƒ™ãƒ¼ã‚¹: {len(active_knowledge_text)} æ–‡å­—")
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ã®æƒ…å ±ã¨SpecialæŒ‡ç¤ºã‚’å–å¾—
        special_instructions = []
        active_resource_names = []
        try:
            from supabase_adapter import select_data
            for source_id in active_sources:
                source_result = select_data("document_sources", columns="name", filters={"id": source_id})
                if source_result.data and len(source_result.data) > 0:
                    source_data = source_result.data[0]
                    source_name = source_data.get('name', 'Unknown')
                    active_resource_names.append(source_name)
                    
                    if source_data.get('special') and source_data['special'].strip():
                        special_instructions.append({
                            "name": source_name,
                            "instruction": source_data['special'].strip()
                        })
            safe_print(f"ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹: {len(active_resource_names)}å€‹ - {active_resource_names}")
            safe_print(f"SpecialæŒ‡ç¤º: {len(special_instructions)}å€‹ã®ãƒªã‚½ãƒ¼ã‚¹ã«SpecialæŒ‡ç¤ºãŒã‚ã‚Šã¾ã™")
        except Exception as e:
            safe_print(f"ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            special_instructions = []
            active_resource_names = []

        # SpecialæŒ‡ç¤ºã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ ã™ã‚‹ãŸã‚ã®æ–‡å­—åˆ—ã‚’æ§‹ç¯‰
        special_instructions_text = ""
        if special_instructions:
            special_instructions_text = "\n\nç‰¹åˆ¥ãªå›ç­”æŒ‡ç¤ºï¼ˆä»¥ä¸‹ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’å‚ç…§ã™ã‚‹éš›ã¯ã€å„ãƒªã‚½ãƒ¼ã‚¹ã®æŒ‡ç¤ºã«å¾“ã£ã¦ãã ã•ã„ï¼‰ï¼š\n"
            for idx, inst in enumerate(special_instructions, 1):
                special_instructions_text += f"{idx}. ã€{inst['name']}ã€‘: {inst['instruction']}\n"

        # ğŸ”ª æœ€åˆã‹ã‚‰1200æ–‡å­—ã§ãƒãƒ£ãƒ³ã‚¯åŒ–ï¼ˆtask.yamlæ¨å¥¨ã‚µã‚¤ã‚ºï¼‰
        CHUNK_SIZE = 1200  # 1200æ–‡å­—ã§ãƒãƒ£ãƒ³ã‚¯åŒ–ï¼ˆtask.yamlæ¨å¥¨ï¼š1000-1200æ–‡å­—ï¼‰
        raw_chunks = chunk_knowledge_base(active_knowledge_text, CHUNK_SIZE)
        safe_print(f"ğŸ”ª ãƒãƒ£ãƒ³ã‚¯åŒ–å®Œäº†: {len(raw_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ (ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {CHUNK_SIZE:,}æ–‡å­—)")
        
        # ä¼šè©±å±¥æ­´ã®å–å¾—
        conversation_history = ""
        try:
            if message.user_id:
                from supabase_adapter import select_data
                chat_history_result = select_data(
                    "chat_history",
                    filters={"employee_id": message.user_id},
                    limit=2
                )
                
                if chat_history_result and chat_history_result.data:
                    recent_messages = list(reversed(chat_history_result.data))
                    
                    if recent_messages:
                        conversation_history = "ç›´è¿‘ã®ä¼šè©±å±¥æ­´ï¼š\n"
                        for msg in recent_messages:
                            user_msg = (msg.get('user_message', '') or '')[:100]
                            bot_msg = (msg.get('bot_response', '') or '')[:100]
                            if len(msg.get('user_message', '')) > 100:
                                user_msg += "..."
                            if len(msg.get('bot_response', '')) > 100:
                                bot_msg += "..."
                            conversation_history += f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_msg}\n"
                            conversation_history += f"ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {bot_msg}\n\n"
        except Exception as e:
            safe_print(f"ä¼šè©±å±¥æ­´å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # ğŸ” æƒ…å ±ç™ºè¦‹ã¾ã§ç¶™ç¶šæ¤œç´¢: è¦‹ã¤ã‹ã£ãŸã‚‰å³åº§ã«çµ‚äº†ã€è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°æœ€å¾Œã¾ã§ç¶™ç¶š
        all_rag_results = []  # RAGæ¤œç´¢çµæœã‚’è“„ç©
        all_chunk_info = []   # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚’è“„ç©
        successful_chunks = 0
        processed_chunks = set()  # å‡¦ç†æ¸ˆã¿ãƒãƒ£ãƒ³ã‚¯ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨˜éŒ²
        BATCH_SIZE = min(5, len(raw_chunks))  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ç¸®å°ã—ã¦ç²¾åº¦å‘ä¸Šï¼ˆ25â†’5ï¼‰
        
        safe_print(f"ğŸ” å…¨ãƒ•ã‚¡ã‚¤ãƒ«å…¨ãƒãƒ£ãƒ³ã‚¯å®Œå…¨æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰: åˆè¨ˆ{len(raw_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã€ãƒãƒƒãƒã‚µã‚¤ã‚º{BATCH_SIZE}ã§å‡¦ç†")
        safe_print(f"ğŸ¯ æˆ¦ç•¥: å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’æ¤œç´¢ã—ã¦ã‹ã‚‰æœ€è‰¯ã®çµæœã‚’é¸æŠï¼ˆæ—©æœŸçµ‚äº†ãªã—ï¼‰")
        safe_print(f"ğŸ“š æ¤œç´¢å¯¾è±¡: å…¨{len(active_resource_names)}ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±åˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹")
        
        batch_start = 0
        total_batches = (len(raw_chunks) + BATCH_SIZE - 1) // BATCH_SIZE  # åˆ‡ã‚Šä¸Šã’é™¤ç®—
        current_batch_num = 1
        skipped_batches = 0  # RAGå“è³ªä¸è¶³ã§ã‚¹ã‚­ãƒƒãƒ—ã—ãŸãƒãƒƒãƒæ•°
        
        # ğŸš€ å…¨ãƒãƒƒãƒã®RAGæ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆæ—©æœŸçµ‚äº†ãªã—ï¼‰
        while batch_start < len(raw_chunks):
            # æœªå‡¦ç†ã®ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰æ¬¡ã®ãƒãƒƒãƒã‚’å–å¾—
            available_chunks = [i for i in range(batch_start, min(batch_start + BATCH_SIZE, len(raw_chunks))) 
                              if i not in processed_chunks]
            
            if not available_chunks:
                batch_start += BATCH_SIZE
                current_batch_num += 1
                continue
                
            safe_print(f"ğŸ”„ RAGæ¤œç´¢ãƒãƒƒãƒ ({current_batch_num}/{total_batches}): ãƒãƒ£ãƒ³ã‚¯ {available_chunks[0]+1}-{available_chunks[-1]+1} ({len(available_chunks)}å€‹)")
            safe_print(f"ğŸ“Š RAGå‡¦ç†é€²æ—: {len(processed_chunks)}/{len(raw_chunks)}ãƒãƒ£ãƒ³ã‚¯å®Œäº† ({len(processed_chunks)/len(raw_chunks)*100:.1f}%)")
            
            # è¤‡æ•°ãƒãƒ£ãƒ³ã‚¯ã‚’çµåˆã—ã¦RAGæ¤œç´¢
            combined_chunk = ""
            chunk_info = []
            
            for chunk_idx in available_chunks:
                raw_chunk = raw_chunks[chunk_idx]
                combined_chunk += f"\n\n=== ãƒãƒ£ãƒ³ã‚¯ {chunk_idx+1} ===\n{raw_chunk}"
                chunk_info.append(f"ãƒãƒ£ãƒ³ã‚¯{chunk_idx+1}({len(raw_chunk):,}æ–‡å­—)")
            
            safe_print(f"ğŸ“Š çµåˆãƒãƒ£ãƒ³ã‚¯: {chunk_info}")
            safe_print(f"ğŸ“Š çµåˆã‚µã‚¤ã‚º: {len(combined_chunk):,} æ–‡å­—")
            
            # ğŸ”„ é«˜åº¦ãªRAGæ¤œç´¢ï¼ˆåˆ¶é™ãªã—ï¼‰
            filtered_chunk = None
            rag_attempts = 0
            min_content_threshold = 50  # ã•ã‚‰ã«ç·©å’Œï¼ˆ100â†’50ï¼‰
            
            if len(combined_chunk) > 1000:  # é–¾å€¤ã‚’å¤§å¹…ã«ç·©å’Œ
                safe_print(f"ğŸ”„ RAGæ¤œç´¢é–‹å§‹")
                
                # ã‚·ãƒ³ãƒ—ãƒ«ãªæ¤œç´¢æˆ¦ç•¥
                # company_idã‚’å–å¾—ï¼ˆprocess_chat_chunkedå†…ã§åˆ©ç”¨å¯èƒ½ãªã‚ˆã†ã«ï¼‰
                user_company_id = None
                if message.user_id:
                    try:
                        from supabase_adapter import select_data
                        user_result = select_data("users", filters={"id": message.user_id})
                        if user_result and user_result.data:
                            user_company_id = user_result.data[0].get("company_id")
                    except Exception:
                        pass
                
                filtered_chunk = simple_rag_search(combined_chunk, message_text, max_results=100, company_id=user_company_id)
                rag_attempts = 1
                
                safe_print(f"ğŸ“Š RAGæ¤œç´¢çµæœ: {len(filtered_chunk)} æ–‡å­—")
            else:
                filtered_chunk = combined_chunk
                safe_print(f"ğŸ“Š å°ã•ãªãƒãƒƒãƒã®ãŸã‚ RAGæ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            
            # ğŸ¯ å³æ ¼ãªRAGå“è³ªåˆ¤å®š
            rag_quality_score = _evaluate_rag_quality(filtered_chunk, message_text, rag_attempts)
            safe_print(f"ğŸ¯ æœ€çµ‚RAGå“è³ªã‚¹ã‚³ã‚¢: {rag_quality_score:.2f} (é–¾å€¤: 0.10)")
            
            # å“è³ªã‚¹ã‚³ã‚¢ã®é–¾å€¤ã‚’èª¿æ•´ï¼ˆ0.20â†’0.10ï¼‰ã—ã¦ã€ã‚ˆã‚Šå¤šãã®çµæœã‚’å«ã‚ã‚‹
            if rag_quality_score >= 0.10:
                safe_print(f"âœ… RAGå“è³ªåˆæ ¼ (ã‚¹ã‚³ã‚¢: {rag_quality_score:.2f}) - çµæœã‚’è“„ç©")
                
                # RAGçµæœã‚’è“„ç©ï¼ˆå…¨ã¦å‡¦ç†ã—ã¦ã‹ã‚‰æœ€è‰¯ã‚’é¸æŠï¼‰
                batch_info = f"ãƒãƒƒãƒ {len(available_chunks)}å€‹ ({available_chunks[0]+1}-{available_chunks[-1]+1})"
                rag_info = f"RAGæ¤œç´¢{rag_attempts}å›å®Ÿè¡Œ" if rag_attempts > 0 else "RAGæ¤œç´¢ãªã—"
                
                all_rag_results.append({
                    'content': filtered_chunk,
                    'batch_info': batch_info,
                    'rag_info': rag_info,
                    'quality_score': rag_quality_score,
                    'chunk_indices': available_chunks,
                    'content_length': len(filtered_chunk),
                    'batch_num': current_batch_num
                })
                
                all_chunk_info.extend(chunk_info)
                successful_chunks += len(available_chunks)
                safe_print(f"ğŸ“š RAGçµæœè“„ç©: {len(all_rag_results)}å€‹ç›®ã®ãƒãƒƒãƒã‚’è¿½åŠ ")
            else:
                safe_print(f"âš ï¸ RAGå“è³ªä¸è¶³ (ã‚¹ã‚³ã‚¢: {rag_quality_score:.2f} < 0.10) - ã“ã®ãƒãƒƒãƒã‚’ã‚¹ã‚­ãƒƒãƒ—")
                skipped_batches += 1
            
            # ã“ã®ãƒãƒƒãƒã®ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†æ¸ˆã¿ã«è¿½åŠ 
            for chunk_idx in available_chunks:
                processed_chunks.add(chunk_idx)
            
            # æ¬¡ã®ãƒãƒƒãƒã¸é€²ã‚€ï¼ˆæƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€å¾Œã¾ã§ç¶™ç¶šï¼‰
            batch_start += BATCH_SIZE
            current_batch_num += 1
            
            # ğŸ¯ é‡è¦: æƒ…å ±ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã®ã¿æ—©æœŸçµ‚äº†ã‚’æ¤œè¨
            # ã—ã‹ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã«ã‚ˆã‚Šã€Œè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€å¾Œã¾ã§æ¤œç´¢ã€ã‚’ä¿è¨¼
            if all_rag_results:
                safe_print(f"âœ… æƒ…å ±ç™ºè¦‹: {len(all_rag_results)}å€‹ã®ãƒãƒƒãƒã§æƒ…å ±ã‚’ç™ºè¦‹")
                safe_print(f"ğŸ”„ ç¶™ç¶šæ¤œç´¢: è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã«å‚™ãˆã¦æœ€å¾Œã¾ã§æ¤œç´¢ã‚’ç¶™ç¶š")
                # æ—©æœŸçµ‚äº†ã¯è¡Œã‚ãšã€å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’ç¢ºå®Ÿã«å‡¦ç†
        
        # ğŸ† å…¨ãƒãƒ£ãƒ³ã‚¯å‡¦ç†å®Œäº†å¾Œã€æœ€è‰¯ã®çµæœã‚’é¸æŠ
        final_response = ""
        if all_rag_results:
            safe_print(f"ğŸ† å…¨ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢å®Œäº†ï¼æœ€è‰¯ã®çµæœã‚’é¸æŠ: {len(all_rag_results)}å€‹ã®ãƒãƒƒãƒã‹ã‚‰")
            
            # çµæœã‚’å“è³ªã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
            sorted_results = sorted(all_rag_results, key=lambda x: x['quality_score'], reverse=True)
            
            # ä¸Šä½ã®çµæœã‚’çµ±åˆï¼ˆæœ€å¤§5å€‹ã¾ã§ï¼‰
            top_results = sorted_results[:min(5, len(sorted_results))]
            safe_print(f"ğŸ“Š ä¸Šä½{len(top_results)}å€‹ã®çµæœã‚’çµ±åˆ:")
            for i, result in enumerate(top_results, 1):
                safe_print(f"  {i}. ãƒãƒƒãƒ{result['batch_num']}: ã‚¹ã‚³ã‚¢{result['quality_score']:.2f}, é•·ã•{result['content_length']:,}æ–‡å­—")
            
            # ä¸Šä½çµæœã‚’çµ±åˆ
            combined_rag_content = ""
            total_quality_score = 0
            for i, rag_result in enumerate(top_results, 1):
                combined_rag_content += f"\n\n=== æœ€è‰¯RAGçµæœ {i}/{len(top_results)} ===\n"
                combined_rag_content += f"å‡¦ç†æƒ…å ±: {rag_result['batch_info']}, {rag_result['rag_info']}\n"
                combined_rag_content += f"å“è³ªã‚¹ã‚³ã‚¢: {rag_result['quality_score']:.2f}\n"
                combined_rag_content += f"å†…å®¹:\n{rag_result['content']}"
                total_quality_score += rag_result['quality_score']
            
            average_quality = total_quality_score / len(top_results)
            safe_print(f"ğŸ“Š çµ±åˆRAGçµæœ: {len(combined_rag_content):,}æ–‡å­—, å¹³å‡å“è³ªã‚¹ã‚³ã‚¢: {average_quality:.2f}")
            
            # çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆï¼ˆå…¨ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢å®Œäº†ç‰ˆï¼‰
            unified_prompt = f"""
ã‚ãªãŸã¯è¦ªåˆ‡ã§ä¸å¯§ãªå¯¾å¿œãŒã§ãã‚‹{current_company_name}ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã¯å…¨{len(raw_chunks)}ãƒãƒ£ãƒ³ã‚¯ã®å®Œå…¨æ¤œç´¢ã§ç™ºè¦‹ã•ã‚ŒãŸæœ€è‰¯ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æƒ…å ±ã§ã™ã€‚ã“ã®æƒ…å ±ã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦æœ€ã‚‚å…·ä½“çš„ã§è©³ç´°ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

**é‡è¦ãªæŒ‡ç¤º:**
1. å…¨ãƒ•ã‚¡ã‚¤ãƒ«å…¨ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰é¸ã°ã‚ŒãŸæœ€è‰¯ã®æƒ…å ±ã‚’æ´»ç”¨ã—ã¦ãã ã•ã„
2. è³ªå•ã«ç›´æ¥é–¢é€£ã™ã‚‹æƒ…å ±ã‚’ä¸­å¿ƒã«ã€å…·ä½“çš„ã§è©³ç´°ãªå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„
3. è¤‡æ•°ã®çµæœã‹ã‚‰æœ€ã‚‚é©åˆ‡ãªæƒ…å ±ã‚’çµ±åˆã—ã¦å›ç­”ã—ã¦ãã ã•ã„
4. **å®Ÿéš›ã«çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æœ‰ç”¨ãªæƒ…å ±ã‚’è¦‹ã¤ã‘ã¦å›ç­”ã—ãŸå ´åˆ**ã€å›ç­”ã®æœ€å¾Œã«ã€Œæƒ…å ±ã‚½ãƒ¼ã‚¹: [ãƒ•ã‚¡ã‚¤ãƒ«å]ã€ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„
5. å›ç­”ã¯**Markdownè¨˜æ³•**ã‚’ä½¿ç”¨ã—ã¦è¦‹ã‚„ã™ãæ•´ç†ã—ã¦ãã ã•ã„

æ¤œç´¢çµ±è¨ˆ: 
- å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(active_resource_names)}å€‹ ({', '.join(active_resource_names)})
- æ¤œç´¢ãƒãƒ£ãƒ³ã‚¯: å…¨{len(raw_chunks)}å€‹
- ç™ºè¦‹çµæœ: {len(all_rag_results)}å€‹ã®ãƒãƒƒãƒ
- é¸æŠçµæœ: ä¸Šä½{len(top_results)}å€‹ (å¹³å‡å“è³ªã‚¹ã‚³ã‚¢: {average_quality:.2f}){special_instructions_text}

å…¨ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢ã§ç™ºè¦‹ã•ã‚ŒãŸæœ€è‰¯ã®æƒ…å ±:
{combined_rag_content}

{conversation_history}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ï¼š
{message_text}
"""
            
            # Gemini APIå‘¼ã³å‡ºã—ï¼ˆä¸€åº¦ã ã‘ï¼‰
            try:
                model = setup_gemini()
                
                safe_print(f"ğŸ¤– çµ±åˆGemini APIå‘¼ã³å‡ºã—é–‹å§‹")
                safe_print(f"ğŸ“ çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µã‚¤ã‚º: {len(unified_prompt):,} æ–‡å­—")
                
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§APIå‘¼ã³å‡ºã—
                import time
                start_time = time.time()
                
                response = model.generate_content(unified_prompt)
                
                end_time = time.time()
                elapsed_time = end_time - start_time
                safe_print(f"ğŸ“¨ çµ±åˆAPIå¿œç­”å—ä¿¡ (å‡¦ç†æ™‚é–“: {elapsed_time:.2f}ç§’)")
                
                if response and hasattr(response, 'text'):
                    if response.text and response.text.strip():
                        final_response = response.text.strip()
                        safe_print(f"ğŸ“ çµ±åˆå¿œç­”ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(final_response)} æ–‡å­—")
                        safe_print(f"ğŸ“ çµ±åˆå¿œç­”å†…å®¹ï¼ˆæœ€åˆã®100æ–‡å­—ï¼‰: {final_response[:100]}...")
                    else:
                        safe_print(f"âš ï¸ çµ±åˆå¿œç­”ã§ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆ")
                        final_response = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
                else:
                    safe_print(f"âš ï¸ çµ±åˆå¿œç­”ã§ç„¡åŠ¹ãªå¿œç­”ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ")
                    final_response = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
                    
            except Exception as e:
                safe_print(f"âŒ çµ±åˆGemini APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                safe_print(f"ğŸ” ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")
                import traceback
                safe_print(f"ğŸ” ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")
                final_response = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        else:
            # RAGçµæœãŒå…¨ããªã„å ´åˆ
            safe_print(f"âŒ å…¨ã¦ã®ãƒãƒƒãƒã§RAGå“è³ªä¸è¶³ã®ãŸã‚ã€æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            final_response = f"""ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å…¨{len(raw_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’æ¤œç´¢ã„ãŸã—ã¾ã—ãŸãŒã€ã”è³ªå•ã«å¯¾ã™ã‚‹é©åˆ‡ãªå›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚

ğŸ” **æ¤œç´¢çµæœ**:
- æ¤œç´¢å¯¾è±¡: {len(raw_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯
- å‡¦ç†å®Œäº†: {len(processed_chunks)}å€‹ (100%)
- RAGå“è³ªåˆæ ¼: {len(all_rag_results)}å€‹
- ã‚¹ã‚­ãƒƒãƒ—: {skipped_batches}å€‹ï¼ˆå“è³ªä¸è¶³ï¼‰

åˆ¥ã®è³ªå•æ–¹æ³•ã§ãŠè©¦ã—ã„ãŸã ãã‹ã€ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚"""
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‚ç…§æ•°ã‚’è¨ˆç®—ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹æ•°ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‚ç…§æ•°ã¨ã—ã¦ä½¿ç”¨ï¼‰
        prompt_references = len(active_sources)
        safe_print(f"ğŸ’° ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‚ç…§æ•°: {prompt_references} (ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹æ•°)")
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ä¿å­˜
        try:
            from modules.token_counter import TokenUsageTracker
            from supabase_adapter import select_data
            
            user_result = select_data("users", filters={"id": message.user_id}) if message.user_id else None
            chat_company_id = user_result.data[0].get("company_id") if user_result and user_result.data else None
            
            tracker = TokenUsageTracker(db)
            chat_id = tracker.save_chat_with_prompts(
                user_message=message_text,
                bot_response=final_response,
                user_id=message.user_id,
                prompt_references=prompt_references,
                company_id=chat_company_id,
                employee_id=message.employee_id,
                employee_name=message.employee_name,
                category="ãƒãƒ£ãƒ³ã‚¯å‡¦ç†",
                sentiment="neutral",
                model="gemini-2.5-flash"
            )
            safe_print(f"ğŸ’¾ ãƒãƒ£ãƒƒãƒˆå±¥æ­´ä¿å­˜å®Œäº† - ID: {chat_id}, ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‚ç…§: {prompt_references}")
        except Exception as e:
            safe_print(f"ãƒãƒ£ãƒƒãƒˆå±¥æ­´ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # åˆ©ç”¨åˆ¶é™ã®æ›´æ–°
        if message.user_id and not limits_check.get("is_unlimited", False):
            try:
                from .database import update_usage_count
                updated_limits = update_usage_count(message.user_id, "questions_used", db)
                if updated_limits:
                    remaining_questions = updated_limits["questions_limit"] - updated_limits["questions_used"]
                    limit_reached = remaining_questions <= 0
                    safe_print(f"ğŸ“Š åˆ©ç”¨åˆ¶é™æ›´æ–°å®Œäº† - æ®‹ã‚Š: {remaining_questions}")
            except Exception as e:
                safe_print(f"åˆ©ç”¨åˆ¶é™æ›´æ–°ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        processing_rate = (len(processed_chunks) / len(raw_chunks) * 100) if raw_chunks else 0
        success_rate = (successful_chunks / len(raw_chunks) * 100) if raw_chunks else 0
        
        safe_print(f"ğŸ” æƒ…å ±ç™ºè¦‹ã¾ã§ç¶™ç¶šæ¤œç´¢å‡¦ç†å®Œäº†")
        safe_print(f"ğŸ“Š å‡¦ç†çµ±è¨ˆ: å…¨{len(raw_chunks)}ãƒãƒ£ãƒ³ã‚¯ä¸­ {len(processed_chunks)}ãƒãƒ£ãƒ³ã‚¯å‡¦ç†æ¸ˆã¿ ({processing_rate:.1f}%)")
        safe_print(f"ğŸ“Š æˆåŠŸçµ±è¨ˆ: {successful_chunks}ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰æœ‰åŠ¹å›ç­”å–å¾— ({success_rate:.1f}%)")
        safe_print(f"ğŸ“ RAGçµæœè“„ç©: {len(all_rag_results)}å€‹ã®ãƒãƒƒãƒ")
        safe_print(f"ğŸ¤– Geminiå‘¼ã³å‡ºã—: 1å›ã®ã¿ (æƒ…å ±ç™ºè¦‹æ™‚å³åº§é€ä¿¡)")
        safe_print(f"âš¡ åŠ¹ç‡åŒ–: {skipped_batches}ãƒãƒƒãƒã‚’RAGå“è³ªåˆ¤å®šã§ã‚¹ã‚­ãƒƒãƒ— ({skipped_batches/total_batches*100:.1f}%å‰Šæ¸›)")
        
        # æƒ…å ±ç™ºè¦‹ã¾ã§ç¶™ç¶šæ¤œç´¢ã®çµæœã‚’è©³ç´°ã«å ±å‘Š
        if all_rag_results:
            safe_print(f"ğŸ‰ æƒ…å ±ç™ºè¦‹æˆåŠŸ: {len(all_rag_results)}å€‹ã®ãƒãƒƒãƒã§æƒ…å ±ã‚’ç™ºè¦‹ã—ã€å³åº§ã«Geminiã«é€ä¿¡")
            safe_print(f"âœ… åŠ¹ç‡çš„çµ‚äº†: æƒ…å ±ç™ºè¦‹å¾Œã¯æ®‹ã‚Š{len(raw_chunks) - len(processed_chunks)}ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        elif len(processed_chunks) == len(raw_chunks):
            safe_print(f"ğŸ” å®Œå…¨æ¤œç´¢å®Œäº†: å…¨{len(raw_chunks)}ãƒãƒ£ãƒ³ã‚¯ã‚’æ¢ç´¢ã—ãŸãŒã€è©²å½“ã™ã‚‹æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        else:
            safe_print(f"âš ï¸ ä¸å®Œå…¨ãªå‡¦ç†: {len(raw_chunks) - len(processed_chunks)}ãƒãƒ£ãƒ³ã‚¯ãŒæœªå‡¦ç†")
        
        # ã‚½ãƒ¼ã‚¹æƒ…å ±ã®æŠ½å‡ºï¼ˆå›ç­”ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡ºï¼‰
        source_text = ""
        if final_response and active_resource_names:
            # å›ç­”ã‹ã‚‰ã€Œæƒ…å ±ã‚½ãƒ¼ã‚¹:ã€éƒ¨åˆ†ã‚’æŠ½å‡º
            import re
            source_match = re.search(r'æƒ…å ±ã‚½ãƒ¼ã‚¹[:ï¼š]\s*([^\n]+)', final_response)
            if source_match:
                # æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®å›ç­”ã«ã¯æƒ…å ±ã‚½ãƒ¼ã‚¹ã‚’å«ã‚ãªã„
                no_info_in_response = any(phrase in final_response.lower() for phrase in [
                    "æƒ…å ±ã¯å«ã¾ã‚Œã¦ãŠã‚Šã¾ã›ã‚“ã§ã—ãŸ",
                    "æƒ…å ±ãŒå«ã¾ã‚Œã¦ãŠã‚Šã¾ã›ã‚“ã§ã—ãŸ", 
                    "ã«é–¢ã™ã‚‹æƒ…å ±ã¯å«ã¾ã‚Œã¦ãŠã‚Šã¾ã›ã‚“",
                    "è©²å½“ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
                ])
                
                if not no_info_in_response:
                    source_text = source_match.group(1).strip()
                
                # æƒ…å ±ã‚½ãƒ¼ã‚¹éƒ¨åˆ†ã‚’å›ç­”ã‹ã‚‰å‰Šé™¤
                final_response = re.sub(r'\n*æƒ…å ±ã‚½ãƒ¼ã‚¹[:ï¼š][^\n]*', '', final_response).strip()
        
        # ç„¡åŠ¹ãªã‚½ãƒ¼ã‚¹æƒ…å ±ã¯ç©ºæ–‡å­—åˆ—ã«ã™ã‚‹
        invalid_sources = ['ãªã—', 'ãƒ‡ãƒãƒƒã‚°', 'debug', 'æƒ…å ±ãªã—', 'è©²å½“ãªã—', 'ä¸æ˜', 'unknown', 'null', 'undefined']
        if source_text.lower() in [s.lower() for s in invalid_sources] or 'ãƒ‡ãƒãƒƒã‚°' in source_text or 'debug' in source_text.lower():
            source_text = ""
        
        safe_print(f"ğŸ“„ æœ€çµ‚ã‚½ãƒ¼ã‚¹æƒ…å ±: '{source_text}'")
        
        # =============================================================
        # ğŸ” æœ€çµ‚åˆ†æãƒ¬ãƒãƒ¼ãƒˆ - RAGç²¾åº¦ã¨å‚ç…§çŠ¶æ³ã®è©³ç´°åˆ†æ
        # =============================================================
        safe_print(f"\n{'='*80}")
        safe_print(f"ğŸ” æœ€çµ‚åˆ†æãƒ¬ãƒãƒ¼ãƒˆ - RAGç²¾åº¦ã¨å‚ç…§çŠ¶æ³")
        safe_print(f"{'='*80}")
        
        # 1. æ¤œç´¢ç¯„å›²ã¨å‡¦ç†çµ±è¨ˆ
        safe_print(f"ğŸ“Š ã€æ¤œç´¢ç¯„å›²ã€‘")
        safe_print(f"  â”” å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(active_resource_names)}å€‹")
        for i, file_name in enumerate(active_resource_names, 1):
            safe_print(f"    {i}. {file_name}")
        safe_print(f"  â”” ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {len(raw_chunks)}å€‹")
        safe_print(f"  â”” å‡¦ç†å®Œäº†ãƒãƒ£ãƒ³ã‚¯: {len(processed_chunks)}å€‹ ({processing_rate:.1f}%)")
        safe_print(f"  â”” æˆåŠŸãƒãƒ£ãƒ³ã‚¯: {successful_chunks}å€‹ ({success_rate:.1f}%)")
        
        # 2. RAGæ¤œç´¢å“è³ªåˆ†æ
        safe_print(f"\nğŸ“ˆ ã€RAGæ¤œç´¢å“è³ªåˆ†æã€‘")
        if all_rag_results:
            safe_print(f"  â”” å“è³ªåˆæ ¼ãƒãƒƒãƒ: {len(all_rag_results)}å€‹")
            safe_print(f"  â”” å“è³ªä¸è¶³ã‚¹ã‚­ãƒƒãƒ—: {skipped_batches}å€‹")
            safe_print(f"  â”” å“è³ªåˆæ ¼ç‡: {len(all_rag_results)/(len(all_rag_results)+skipped_batches)*100:.1f}%")
            
            # å“è³ªã‚¹ã‚³ã‚¢åˆ†å¸ƒ
            quality_scores = [result['quality_score'] for result in all_rag_results]
            min_score = min(quality_scores)
            max_score = max(quality_scores)
            avg_score = sum(quality_scores) / len(quality_scores)
            safe_print(f"  â”” å“è³ªã‚¹ã‚³ã‚¢åˆ†å¸ƒ:")
            safe_print(f"    â”œ æœ€é«˜ã‚¹ã‚³ã‚¢: {max_score:.3f}")
            safe_print(f"    â”œ æœ€ä½ã‚¹ã‚³ã‚¢: {min_score:.3f}")
            safe_print(f"    â”” å¹³å‡ã‚¹ã‚³ã‚¢: {avg_score:.3f}")
            
            # ä¸Šä½5å€‹ã®è©³ç´°
            safe_print(f"  â”” ä¸Šä½å“è³ªãƒãƒƒãƒè©³ç´°:")
            sorted_results = sorted(all_rag_results, key=lambda x: x['quality_score'], reverse=True)
            for i, result in enumerate(sorted_results[:5], 1):
                safe_print(f"    {i}. ãƒãƒƒãƒ{result['batch_num']}: ã‚¹ã‚³ã‚¢{result['quality_score']:.3f}, {result['content_length']:,}æ–‡å­—")
        else:
            safe_print(f"  â”” âš ï¸ å“è³ªåˆæ ¼ãƒãƒƒãƒ: 0å€‹ï¼ˆå…¨ãƒãƒƒãƒãŒå“è³ªåŸºæº–æœªæº€ï¼‰")
            safe_print(f"  â”” å…¨ãƒãƒƒãƒãŒã‚¹ã‚­ãƒƒãƒ—: {skipped_batches}å€‹")
            safe_print(f"  â”” å“è³ªåŸºæº–: 0.10ä»¥ä¸ŠãŒå¿…è¦")
        
        # 3. ãƒ‡ãƒ¼ã‚¿ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ
        safe_print(f"\nğŸ“‹ ã€ãƒ‡ãƒ¼ã‚¿ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã€‘")
        total_chars = sum(len(chunk) for chunk in raw_chunks)
        processed_chars = sum(len(raw_chunks[i]) for i in processed_chunks)
        coverage_rate = (processed_chars / total_chars * 100) if total_chars > 0 else 0
        
        safe_print(f"  â”” ç·ãƒ‡ãƒ¼ã‚¿é‡: {total_chars:,}æ–‡å­—")
        safe_print(f"  â”” å‡¦ç†ãƒ‡ãƒ¼ã‚¿é‡: {processed_chars:,}æ–‡å­—")
        safe_print(f"  â”” ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: {coverage_rate:.1f}%")
        
        if all_rag_results:
            used_chars = sum(result['content_length'] for result in all_rag_results)
            utilization_rate = (used_chars / total_chars * 100) if total_chars > 0 else 0
            safe_print(f"  â”” å›ç­”åˆ©ç”¨ãƒ‡ãƒ¼ã‚¿: {used_chars:,}æ–‡å­—")
            safe_print(f"  â”” ãƒ‡ãƒ¼ã‚¿åˆ©ç”¨ç‡: {utilization_rate:.1f}%")
        
        # 4. æ¤œç´¢ç²¾åº¦è©•ä¾¡
        safe_print(f"\nğŸ¯ ã€æ¤œç´¢ç²¾åº¦è©•ä¾¡ã€‘")
        query_keywords = set(message_text.lower().split())
        if all_rag_results and final_response:
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´ç‡è¨ˆç®—
            response_words = set(final_response.lower().split())
            keyword_matches = len(query_keywords.intersection(response_words))
            keyword_match_rate = (keyword_matches / len(query_keywords) * 100) if query_keywords else 0
            
            safe_print(f"  â”” ã‚¯ã‚¨ãƒªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°: {len(query_keywords)}å€‹")
            safe_print(f"  â”” å›ç­”å†…ä¸€è‡´ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword_matches}å€‹")
            safe_print(f"  â”” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´ç‡: {keyword_match_rate:.1f}%")
            
            # æƒ…å ±ç™ºè¦‹çŠ¶æ³
            has_source = bool(source_text and source_text.strip())
            safe_print(f"  â”” æƒ…å ±ã‚½ãƒ¼ã‚¹ç‰¹å®š: {'âœ… æˆåŠŸ' if has_source else 'âŒ å¤±æ•—'}")
            if has_source:
                safe_print(f"    â”” ã‚½ãƒ¼ã‚¹: {source_text}")
        
        # 5. å‡¦ç†åŠ¹ç‡åˆ†æ
        safe_print(f"\nâš¡ ã€å‡¦ç†åŠ¹ç‡åˆ†æã€‘")
        safe_print(f"  â”” ç·ãƒãƒƒãƒæ•°: {total_batches}å€‹")
        safe_print(f"  â”” åŠ¹ç‡çš„ã‚¹ã‚­ãƒƒãƒ—: {skipped_batches}å€‹ ({skipped_batches/total_batches*100:.1f}%)")
        safe_print(f"  â”” Gemini APIå‘¼ã³å‡ºã—: 1å›ï¼ˆæœ€é©åŒ–æ¸ˆã¿ï¼‰")
        
        # 6. æœ€çµ‚å›ç­”å“è³ªåˆ¤å®š
        safe_print(f"\nâœ… ã€æœ€çµ‚å›ç­”å“è³ªåˆ¤å®šã€‘")
        if final_response:
            response_length = len(final_response)
            safe_print(f"  â”” å›ç­”æ–‡å­—æ•°: {response_length:,}æ–‡å­—")
            
            # å›ç­”å“è³ªã®åˆ¤å®š
            quality_indicators = {
                "å…·ä½“çš„ãªæƒ…å ±": any(word in final_response for word in ['æ‰‹é †', 'æ–¹æ³•', 'è¨­å®š', 'å ´åˆ', 'å¿…è¦', 'ç¢ºèª']),
                "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±": source_text and source_text.strip(),
                "æ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”": '##' in final_response or '###' in final_response or '- ' in final_response,
                "é©åˆ‡ãªé•·ã•": 50 <= response_length <= 5000,
                "ã‚¨ãƒ©ãƒ¼å›ç­”ã§ãªã„": not any(phrase in final_response for phrase in ['ç”³ã—è¨³', 'ã‚¨ãƒ©ãƒ¼', 'è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'])
            }
            
            safe_print(f"  â”” å›ç­”å“è³ªãƒã‚§ãƒƒã‚¯:")
            quality_score = 0
            for indicator, result in quality_indicators.items():
                status = "âœ…" if result else "âŒ"
                safe_print(f"    â”œ {indicator}: {status}")
                if result:
                    quality_score += 1
            
            final_quality = (quality_score / len(quality_indicators)) * 100
            safe_print(f"    â”” ç·åˆå“è³ªã‚¹ã‚³ã‚¢: {final_quality:.1f}% ({quality_score}/{len(quality_indicators)})")
        
        # 7. å•é¡Œãƒ»æ”¹å–„ææ¡ˆ
        safe_print(f"\nğŸ”§ ã€å•é¡Œãƒ»æ”¹å–„ææ¡ˆã€‘")
        if len(all_rag_results) == 0:
            safe_print(f"  âš ï¸ å•é¡Œ: å…¨ãƒãƒƒãƒã§RAGå“è³ªãŒåŸºæº–æœªæº€ï¼ˆã‚¹ã‚³ã‚¢ < 0.10ï¼‰")
            safe_print(f"     â”” ææ¡ˆ: æ¤œç´¢ã‚¯ã‚¨ãƒªã®è¦‹ç›´ã—ã¾ãŸã¯çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æ‹¡å……ãŒå¿…è¦")
        elif success_rate < 50:
            safe_print(f"  âš ï¸ å•é¡Œ: ãƒãƒ£ãƒ³ã‚¯æˆåŠŸç‡ãŒä½ã„ï¼ˆ{success_rate:.1f}% < 50%ï¼‰")
            safe_print(f"     â”” ææ¡ˆ: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã¾ãŸã¯æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®èª¿æ•´ã‚’æ¤œè¨")
        elif coverage_rate < 80:
            safe_print(f"  âš ï¸ å•é¡Œ: ãƒ‡ãƒ¼ã‚¿ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒä¸å®Œå…¨ï¼ˆ{coverage_rate:.1f}% < 80%ï¼‰")
            safe_print(f"     â”” ææ¡ˆ: ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªæ¤œç´¢æˆ¦ç•¥ã®å®Ÿè£…ã‚’æ¤œè¨")
        else:
            safe_print(f"  âœ… è‰¯å¥½: RAGæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™")
        
        # 8. å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼
        safe_print(f"\nğŸ ã€å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼ã€‘")
        safe_print(f"  â”” æ¤œç´¢å®Ÿè¡Œ: {'âœ… å®Œäº†' if len(processed_chunks) > 0 else 'âŒ å¤±æ•—'}")
        safe_print(f"  â”” æƒ…å ±ç™ºè¦‹: {'âœ… æˆåŠŸ' if all_rag_results else 'âŒ å¤±æ•—'}")
        safe_print(f"  â”” å›ç­”ç”Ÿæˆ: {'âœ… æˆåŠŸ' if final_response and len(final_response) > 20 else 'âŒ å¤±æ•—'}")
        safe_print(f"  â”” ã‚½ãƒ¼ã‚¹ç‰¹å®š: {'âœ… æˆåŠŸ' if source_text and source_text.strip() else 'âŒ å¤±æ•—'}")
        
        safe_print(f"{'='*80}")
        safe_print(f"ğŸ” æœ€çµ‚åˆ†æãƒ¬ãƒãƒ¼ãƒˆå®Œäº†")
        safe_print(f"{'='*80}\n")
        
        return {
            "response": final_response,
            "source": source_text,
            "remaining_questions": remaining_questions,
            "limit_reached": limit_reached,
            "chunks_processed": len(raw_chunks),
            "successful_chunks": successful_chunks,
            # åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            "analysis": {
                "total_chunks": len(raw_chunks),
                "processed_chunks": len(processed_chunks),
                "successful_chunks": successful_chunks,
                "processing_rate": processing_rate,
                "success_rate": success_rate,
                "coverage_rate": coverage_rate,
                "quality_batches": len(all_rag_results),
                "skipped_batches": skipped_batches,
                "data_coverage": f"{processed_chars:,}/{total_chars:,} chars",
                "final_quality": final_quality if 'final_quality' in locals() else 0
            }
        }
        
    except Exception as e:
        safe_print(f"âŒ ãƒãƒ£ãƒ³ã‚¯åŒ–å‡¦ç†ã§é‡å¤§ã‚¨ãƒ©ãƒ¼: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
        try:
            remaining_questions = remaining_questions if 'remaining_questions' in locals() else 0
            limit_reached = limit_reached if 'limit_reached' in locals() else False
        except:
            remaining_questions = 0
            limit_reached = False
            
        return {
            "response": f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            "source": "",
            "remaining_questions": remaining_questions,
            "limit_reached": limit_reached
        }

async def lightning_rag_search(knowledge_text: str, query: str, max_results: int = 20) -> str:
    """
    é›·é€ŸRAGæ¤œç´¢ - æœ€é«˜é€Ÿåº¦ã‚’é‡è¦–ã—ãŸæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
    - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
    - äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    - å¤§ããªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹é«˜é€ŸåŒ–
    """
    if not SPEED_RAG_AVAILABLE:
        safe_print("é«˜é€ŸRAGãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€å¾“æ¥ã®RAGã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        return simple_rag_search(knowledge_text, query, max_results)
    
    if not knowledge_text or not query:
        return knowledge_text
    
    try:
        safe_print(f"âš¡ é›·é€ŸRAGæ¤œç´¢é–‹å§‹: {len(knowledge_text):,}æ–‡å­—, ã‚¯ã‚¨ãƒª: {query[:30]}...")
        
        # é«˜é€Ÿæ¤œç´¢å®Ÿè¡Œ
        result = await high_speed_rag.lightning_search(
            query=query,
            knowledge_text=knowledge_text,
            max_results=max_results
        )
        
        if result:
            safe_print(f"âš¡ é›·é€ŸRAGæ¤œç´¢å®Œäº†: {len(result):,}æ–‡å­—ã®é–¢é€£æƒ…å ±ã‚’æŠ½å‡º")
            return result
        else:
            safe_print("âš ï¸ é›·é€ŸRAGæ¤œç´¢ã§çµæœãŒè¦‹ã¤ã‹ã‚‰ãšã€å¾“æ¥ã®RAGã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            return simple_rag_search(knowledge_text, query, max_results)
    
    except Exception as e:
        safe_print(f"âŒ é›·é€ŸRAGæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å¾“æ¥ã®RAGã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return simple_rag_search(knowledge_text, query, max_results)

async def enhanced_rag_search(knowledge_text: str, query: str, max_results: int = 20) -> str:
    """
    å¼·åŒ–ã•ã‚ŒãŸRAGæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
    - ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªãƒãƒ£ãƒ³ã‚¯åŒ–
    - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆBM25 + ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ï¼‰
    - åå¾©æ¤œç´¢ã«ã‚ˆã‚‹é«˜ç²¾åº¦æ¤œç´¢
    """
    if not RAG_ENHANCED_AVAILABLE:
        safe_print("å¼·åŒ–RAGãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€å¾“æ¥ã®RAGã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        return simple_rag_search(knowledge_text, query, max_results)
    
    if not knowledge_text or not query:
        return knowledge_text
    
    try:
        safe_print(f"ğŸš€ å¼·åŒ–RAGæ¤œç´¢é–‹å§‹: {len(knowledge_text):,}æ–‡å­—, ã‚¯ã‚¨ãƒª: {query[:50]}...")
        
        # åå¾©æ¤œç´¢ã«ã‚ˆã‚‹é«˜ç²¾åº¦æ¤œç´¢
        result = await enhanced_rag.iterative_search(
            query=query,
            knowledge_text=knowledge_text,
            max_iterations=3,
            min_results=5
        )
        
        if result:
            safe_print(f"âœ… å¼·åŒ–RAGæ¤œç´¢å®Œäº†: {len(result):,}æ–‡å­—ã®é–¢é€£æƒ…å ±ã‚’æŠ½å‡º")
            return result
        else:
            safe_print("âš ï¸ å¼·åŒ–RAGæ¤œç´¢ã§çµæœãŒè¦‹ã¤ã‹ã‚‰ãšã€å¾“æ¥ã®RAGã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            return simple_rag_search(knowledge_text, query, max_results)
    
    except Exception as e:
        safe_print(f"âŒ å¼·åŒ–RAGæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å¾“æ¥ã®RAGã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return simple_rag_search(knowledge_text, query, max_results)

def adaptive_rag_search(knowledge_text: str, query: str, max_results: int = 10) -> str:
    """
    é©å¿œçš„RAGæ¤œç´¢ - çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®ã‚µã‚¤ã‚ºã«å¿œã˜ã¦æœ€é©ãªæ¤œç´¢æ‰‹æ³•ã‚’é¸æŠ
    """
    if not knowledge_text or not query:
        return knowledge_text
    
    text_length = len(knowledge_text)
    safe_print(f"ğŸ“Š é©å¿œçš„RAGæ¤œç´¢: ãƒ†ã‚­ã‚¹ãƒˆé•· {text_length:,}æ–‡å­—")
    
    # å°ã•ãªãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯å…¨ä½“ã‚’è¿”ã™
    if text_length <= 10000:
        safe_print("ğŸ“ å°ã•ãªãƒ†ã‚­ã‚¹ãƒˆã®ãŸã‚å…¨ä½“ã‚’è¿”å´")
        return knowledge_text
    
    # ä¸­ç¨‹åº¦ã®ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯å¾“æ¥ã®RAG
    elif text_length <= 100000:
        safe_print("ğŸ¯ ä¸­ç¨‹åº¦ã®ãƒ†ã‚­ã‚¹ãƒˆã®ãŸã‚å¾“æ¥ã®RAGæ¤œç´¢ã‚’å®Ÿè¡Œ")
        return simple_rag_search(knowledge_text, query, max_results)
    
    # å¤§ããªãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯å¼·åŒ–RAGï¼ˆéåŒæœŸå‡¦ç†ãŒå¿…è¦ãªãŸã‚ã€ã“ã“ã§ã¯å¾“æ¥ã®RAGã‚’ä½¿ç”¨ï¼‰
    else:
        safe_print("ğŸš€ å¤§ããªãƒ†ã‚­ã‚¹ãƒˆã®ãŸã‚é«˜æ€§èƒ½RAGæ¤œç´¢ã‚’å®Ÿè¡Œ")
        # æ®µè½æ•°ã‚’å¢—ã‚„ã—ã¦ç²¾åº¦å‘ä¸Š
        return simple_rag_search(knowledge_text, query, max_results * 2)

def multi_pass_rag_search(knowledge_text: str, query: str, max_results: int = 15) -> str:
    """
    å¤šæ®µéšRAGæ¤œç´¢ - è¤‡æ•°ã®æ¤œç´¢æˆ¦ç•¥ã‚’çµ„ã¿åˆã‚ã›ã¦ç²¾åº¦ã‚’å‘ä¸Š
    """
    if not knowledge_text or not query:
        return knowledge_text
    
    try:
        safe_print(f"ğŸ”„ å¤šæ®µéšRAGæ¤œç´¢é–‹å§‹: {len(knowledge_text):,}æ–‡å­—")
        
        # ç¬¬1æ®µéš: åºƒã„æ¤œç´¢
        broad_results = simple_rag_search(knowledge_text, query, max_results * 3)
        
        # ç¬¬2æ®µéš: ã‚¯ã‚¨ãƒªã‚’æ‹¡å¼µã—ã¦å†æ¤œç´¢
        expanded_query = expand_query(query)
        if expanded_query != query:
            safe_print(f"ğŸ” ã‚¯ã‚¨ãƒªã‚’æ‹¡å¼µ: '{query}' â†’ '{expanded_query}'")
            expanded_results = simple_rag_search(knowledge_text, expanded_query, max_results * 2)
            
            # çµæœã‚’ãƒãƒ¼ã‚¸
            combined_text = f"{broad_results}\n\n{'='*50}\n\n{expanded_results}"
            
            # ç¬¬3æ®µéš: é‡è¤‡ã‚’é™¤å»ã—ã¦æœ€çµ‚èª¿æ•´
            final_results = simple_rag_search(combined_text, query, max_results)
        else:
            final_results = broad_results
        
        safe_print(f"âœ… å¤šæ®µéšRAGæ¤œç´¢å®Œäº†: {len(final_results):,}æ–‡å­—")
        return final_results
        
    except Exception as e:
        safe_print(f"âŒ å¤šæ®µéšRAGæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return simple_rag_search(knowledge_text, query, max_results)

def expand_query(query: str) -> str:
    """
    ã‚¯ã‚¨ãƒªæ‹¡å¼µ - é¡ç¾©èªã‚„é–¢é€£ç”¨èªã‚’è¿½åŠ ã—ã¦æ¤œç´¢ç²¾åº¦ã‚’å‘ä¸Š
    """
    # åŸºæœ¬çš„ãªã‚¯ã‚¨ãƒªæ‹¡å¼µã®ãƒãƒƒãƒ”ãƒ³ã‚°
    expansion_map = {
        'æ–¹æ³•': ['æ‰‹é †', 'ã‚„ã‚Šæ–¹', 'ãƒ—ãƒ­ã‚»ã‚¹', 'æµã‚Œ'],
        'æ‰‹é †': ['æ–¹æ³•', 'ã‚¹ãƒ†ãƒƒãƒ—', 'ãƒ—ãƒ­ã‚»ã‚¹', 'æµã‚Œ'],
        'å•é¡Œ': ['èª²é¡Œ', 'ãƒˆãƒ©ãƒ–ãƒ«', 'ã‚¨ãƒ©ãƒ¼', 'ä¸å…·åˆ'],
        'è¨­å®š': ['æ§‹æˆ', 'ã‚³ãƒ³ãƒ•ã‚£ã‚°', 'è¨­å®šå€¤', 'ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—'],
        'ä½¿ã„æ–¹': ['åˆ©ç”¨æ–¹æ³•', 'æ“ä½œæ–¹æ³•', 'ä½¿ç”¨æ–¹æ³•', 'æ“ä½œæ‰‹é †'],
        'ã‚¨ãƒ©ãƒ¼': ['å•é¡Œ', 'ãƒˆãƒ©ãƒ–ãƒ«', 'ä¸å…·åˆ', 'ãƒã‚°'],
        'æ–™é‡‘': ['ä¾¡æ ¼', 'è²»ç”¨', 'ã‚³ã‚¹ãƒˆ', 'å€¤æ®µ'],
        'æ©Ÿèƒ½': ['ç‰¹å¾´', 'ä»•æ§˜', 'æ€§èƒ½', 'èƒ½åŠ›'],
    }
    
    expanded_terms = []
    query_words = query.split()
    
    for word in query_words:
        expanded_terms.append(word)
        if word in expansion_map:
            # 1ã¤ã®é¡ç¾©èªã‚’è¿½åŠ ï¼ˆã‚¯ã‚¨ãƒªãŒé•·ããªã‚Šã™ããªã„ã‚ˆã†ã«ï¼‰
            expanded_terms.append(expansion_map[word][0])
    
    expanded_query = ' '.join(expanded_terms)
    return expanded_query if len(expanded_query) <= len(query) * 2 else query
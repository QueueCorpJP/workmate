"""
ãƒãƒ£ãƒƒãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã¨AIå¿œç­”ç”Ÿæˆã‚’ç®¡ç†ã—ã¾ã™
"""
import json
import re
import uuid
import sys
from datetime import datetime
import logging
# PostgreSQLé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from psycopg2.extras import RealDictCursor
from fastapi import HTTPException, Depends
from .company import DEFAULT_COMPANY_NAME
from .models import ChatMessage, ChatResponse
from .database import get_db, update_usage_count, get_usage_limits
from .knowledge_base import knowledge_base, get_active_resources
from .auth import check_usage_limits
from .resource import get_active_resources_by_company_id, get_active_resources_content_by_ids, get_active_resource_names_by_company_id
from .company import get_company_by_id
import os
import asyncio
import google.generativeai as genai
from .config import setup_gemini
from .utils import safe_print, safe_safe_print

# æ–°ã—ã„RAGã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 
try:
    from .rag_enhanced import enhanced_rag, SearchResult
    RAG_ENHANCED_AVAILABLE = True
except ImportError:
    RAG_ENHANCED_AVAILABLE = False
    safe_print("âš ï¸ å¼·åŒ–RAGã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€å¾“æ¥ã®RAGã‚’ä½¿ç”¨ã—ã¾ã™")

# é«˜é€ŸåŒ–RAGã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 
try:
    from .rag_optimized import high_speed_rag
    SPEED_RAG_AVAILABLE = True
    safe_print("âš¡ é«˜é€ŸåŒ–RAGã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
except ImportError:
    SPEED_RAG_AVAILABLE = False
    safe_print("âš ï¸ é«˜é€ŸåŒ–RAGã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

logger = logging.getLogger(__name__)

def safe_print(text):
    """Windowsç’°å¢ƒã§ã®Unicodeæ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œã‚’å›é¿ã™ã‚‹å®‰å…¨ãªprinté–¢æ•°"""
    try:
        print(text)
    except UnicodeEncodeError:
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€å•é¡Œã®ã‚ã‚‹æ–‡å­—ã‚’ç½®æ›
        try:
            safe_text = str(text).encode('utf-8', errors='replace').decode('utf-8')
            print(safe_text)
        except:
            # ãã‚Œã§ã‚‚å¤±æ•—ã™ã‚‹å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿å‡ºåŠ›
            print("[å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: Unicodeæ–‡å­—ã‚’å«ã‚€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸]")

def safe_safe_print(text):
    """Windowsç’°å¢ƒã§ã®Unicodeæ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œã‚’å›é¿ã™ã‚‹å®‰å…¨ãªsafe_printé–¢æ•°"""
    safe_print(text)

def simple_rag_search(knowledge_text: str, query: str, max_results: int = 5) -> str:
    """
    ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰RAGæ¤œç´¢ - BM25Sï¼ˆèªå½™ï¼‰+ ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ï¼ˆæ„å‘³ï¼‰æ¤œç´¢ã®çµ„ã¿åˆã‚ã›
    """
    if not knowledge_text or not query:
        return knowledge_text
    
    # é«˜é€ŸåŒ–RAGãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯å„ªå…ˆä½¿ç”¨
    if SPEED_RAG_AVAILABLE and len(knowledge_text) > 10000:
        try:
            import asyncio
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # æ—¢å­˜ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒã‚ã‚‹å ´åˆ
                future = asyncio.ensure_future(high_speed_rag.lightning_search(query, knowledge_text, max_results))
                return knowledge_text[:50000]  # æš«å®šçš„ãªçµæœã‚’è¿”ã™
            else:
                # æ–°ã—ã„ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ
                return asyncio.run(high_speed_rag.lightning_search(query, knowledge_text, max_results))
        except Exception as e:
            safe_print(f"é«˜é€ŸRAGå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
    
    try:
        import bm25s
        import re
        
        # ğŸ” æ”¹å–„: ã‚ˆã‚ŠæŸ”è»Ÿãªã‚¯ã‚¨ãƒªå‰å‡¦ç†
        processed_query = _preprocess_query(query)
        safe_print(f"ğŸ” ã‚¯ã‚¨ãƒªå‰å‡¦ç†: '{query}' â†’ '{processed_query}'")
        
        # é«˜é€ŸåŒ–: ã‚ˆã‚Šå¤§ããªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã§åˆ†å‰²
        if len(knowledge_text) > 50000:
            # å¤§ããªãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯å¤§ããªãƒãƒ£ãƒ³ã‚¯ã§åˆ†å‰²
            chunk_size = 3000
            chunks = []
            for i in range(0, len(knowledge_text), chunk_size):
                chunk = knowledge_text[i:i+chunk_size].strip()
                if chunk and len(chunk) > 100:
                    chunks.append(chunk)
        else:
            # å°ã•ãªãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯æ®µè½åˆ†å‰²
            chunks = re.split(r'\n\s*\n', knowledge_text)
            chunks = [p.strip() for p in chunks if len(p.strip()) > 50]
        
        if len(chunks) < 2:
            return knowledge_text[:100000]  # ãƒãƒ£ãƒ³ã‚¯ãŒå°‘ãªã„å ´åˆã¯ãã®ã¾ã¾
        
        # ğŸš€ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®å®Ÿè¡Œ
        bm25_results = _bm25_search(chunks, processed_query, max_results)
        semantic_results = _semantic_search(chunks, processed_query, max_results)
        
        # çµæœã®çµ±åˆã¨å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        combined_results = _combine_search_results(bm25_results, semantic_results, processed_query, max_results)
        
        # ğŸ” å®Œå…¨æ¤œç´¢: å…¨ã¦ã®é–¢é€£ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—ï¼ˆæ–‡å­—æ•°åˆ¶é™ã‚’å¤§å¹…ç·©å’Œï¼‰
        result_chunks = []
        total_length = 0
        max_length = 500000  # åˆ¶é™ã‚’50ä¸‡æ–‡å­—ã«å¤§å¹…æ‹¡å¤§
        
        # çµ±åˆçµæœã‹ã‚‰æœ€è‰¯ã®ãƒãƒ£ãƒ³ã‚¯ã‚’é¸æŠ
        for result in combined_results:
            chunk = result['content']
            score = result['score']
            
            if total_length + len(chunk) > max_length and len(result_chunks) >= 20:
                # æœ€ä½20å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã¯ç¢ºä¿ã—ã€ãã‚Œä»¥é™ã¯åˆ¶é™é©ç”¨
                safe_print(f"ğŸ” æ–‡å­—æ•°åˆ¶é™åˆ°é”: {total_length:,}æ–‡å­— (åˆ¶é™: {max_length:,}æ–‡å­—)")
                break
            result_chunks.append(chunk)
            total_length += len(chunk)
        
        result = '\n\n'.join(result_chunks)
        safe_print(f"ğŸš€ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰RAGæ¤œç´¢å®Œäº†: {len(result_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã€{len(result)}æ–‡å­— (å…ƒ: {len(knowledge_text)}æ–‡å­—)")
        return result
        
    except Exception as e:
        safe_print(f"RAGæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æœ€åˆã®éƒ¨åˆ†ã‚’è¿”ã™
        return knowledge_text[:50000]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã®æ–‡å­—æ•°ã‚‚å¢—åŠ 

def _preprocess_query(query: str) -> str:
    """ã‚¯ã‚¨ãƒªã®å‰å‡¦ç† - è¡¨è¨˜æºã‚Œã‚„é¡ç¾©èªã«å¯¾å¿œ"""
    # å…¨è§’ãƒ»åŠè§’ã®æ­£è¦åŒ–
    import unicodedata
    normalized = unicodedata.normalize('NFKC', query)
    
    # é¡ç¾©èªã®å±•é–‹
    synonyms = {
        'é¡§å®¢ç•ªå·': ['ãŠå®¢æ§˜ç•ªå·', 'é¡§å®¢ID', 'é¡§å®¢ã‚³ãƒ¼ãƒ‰', 'ä¼šå“¡ç•ªå·', 'ã‚«ã‚¹ã‚¿ãƒãƒ¼ID'],
        'ä¼šç¤¾': ['ä¼æ¥­', 'æ³•äºº', 'æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾', 'åˆåŒä¼šç¤¾'],
        'æ–™é‡‘': ['ä¾¡æ ¼', 'è²»ç”¨', 'é‡‘é¡', 'ãƒ—ãƒ©ã‚¤ã‚¹', 'ã‚³ã‚¹ãƒˆ'],
        'å¥‘ç´„': ['ç”³è¾¼', 'ç”³ã—è¾¼ã¿', 'å¥‘ç´„æ›¸', 'åˆæ„'],
    }
    
    # ã‚¯ã‚¨ãƒªã«é¡ç¾©èªã‚’è¿½åŠ 
    expanded_terms = [normalized]
    for term, syns in synonyms.items():
        if term in normalized:
            expanded_terms.extend(syns)
    
    return ' '.join(expanded_terms)

def _bm25_search(chunks: list, query: str, max_results: int) -> list:
    """BM25æ¤œç´¢ï¼ˆèªå½™ãƒ™ãƒ¼ã‚¹ï¼‰"""
    try:
        import bm25s
        
        # BM25Sæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½œæˆ
        corpus_tokens = bm25s.tokenize(chunks)
        retriever = bm25s.BM25()
        retriever.index(corpus_tokens)
        
        # è³ªå•ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¦æ¤œç´¢
        query_tokens = bm25s.tokenize([query])
        k_value = min(max_results * 2, len(chunks))
        results, scores = retriever.retrieve(query_tokens, k=k_value)
        
        # çµæœã‚’æ•´å½¢
        search_results = []
        for i in range(results.shape[1]):
            if i < len(chunks):
                chunk_idx = results[0, i]
                if chunk_idx < len(chunks):
                    search_results.append({
                        'content': chunks[chunk_idx],
                        'score': float(scores[0, i]) if i < len(scores[0]) else 0.0,
                        'type': 'bm25',
                        'index': chunk_idx
                    })
        
        return search_results
        
    except Exception as e:
        safe_print(f"BM25æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def _semantic_search(chunks: list, query: str, max_results: int) -> list:
    """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ï¼ˆæ„å‘³ãƒ™ãƒ¼ã‚¹ï¼‰- Sentence Transformersã‚’ä½¿ç”¨"""
    try:
        # Sentence Transformersã‚’ä½¿ã£ãŸæœ¬æ ¼çš„ãªã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢
        try:
            from sentence_transformers import SentenceTransformer, util
            import torch
            
            # æ—¥æœ¬èªå¯¾å¿œã®å¤šè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            model_name = 'paraphrase-multilingual-MiniLM-L12-v2'
            model = SentenceTransformer(model_name)
            safe_print(f"ğŸ¤– ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_name}")
            
            # ã‚¯ã‚¨ãƒªã¨ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°åŒ–
            query_embedding = model.encode([query])
            chunk_embeddings = model.encode(chunks)
            
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
            similarities = util.cos_sim(query_embedding, chunk_embeddings)[0]
            
            # çµæœã‚’æ•´å½¢
            semantic_results = []
            for i, similarity in enumerate(similarities):
                semantic_results.append({
                    'content': chunks[i],
                    'score': float(similarity),
                    'type': 'semantic_transformer',
                    'index': i
                })
            
            # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
            semantic_results.sort(key=lambda x: x['score'], reverse=True)
            safe_print(f"ğŸ§  Transformer ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢å®Œäº†: ä¸Šä½{min(max_results, len(semantic_results))}ä»¶")
            return semantic_results[:max_results]
            
        except ImportError:
            safe_print("âš ï¸ Sentence Transformersæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€TF-IDFãƒ™ãƒ¼ã‚¹ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            
            # TF-IDFãƒ™ãƒ¼ã‚¹ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            try:
                from sklearn.feature_extraction.text import TfidfVectorizer
                from sklearn.metrics.pairwise import cosine_similarity
                import numpy as np
                
                # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
                vectorizer = TfidfVectorizer(
                    ngram_range=(1, 2),  # 1-gram, 2-gramã‚’ä½¿ç”¨
                    max_features=10000,
                    stop_words=None,  # æ—¥æœ¬èªã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¯ä½¿ã‚ãªã„
                    analyzer='char',   # æ–‡å­—ãƒ¬ãƒ™ãƒ«ã®è§£æï¼ˆæ—¥æœ¬èªã«é©ã—ã¦ã„ã‚‹ï¼‰
                    min_df=1
                )
                
                # ã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆãƒãƒ£ãƒ³ã‚¯ + ã‚¯ã‚¨ãƒªï¼‰ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
                corpus = chunks + [query]
                tfidf_matrix = vectorizer.fit_transform(corpus)
                
                # ã‚¯ã‚¨ãƒªã¨å„ãƒãƒ£ãƒ³ã‚¯ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
                query_vector = tfidf_matrix[-1]  # æœ€å¾ŒãŒã‚¯ã‚¨ãƒª
                chunk_vectors = tfidf_matrix[:-1]  # æœ€å¾Œä»¥å¤–ãŒãƒãƒ£ãƒ³ã‚¯
                
                similarities = cosine_similarity(query_vector, chunk_vectors).flatten()
                
                # çµæœã‚’æ•´å½¢
                semantic_results = []
                for i, similarity in enumerate(similarities):
                    semantic_results.append({
                        'content': chunks[i],
                        'score': float(similarity),
                        'type': 'semantic_tfidf',
                        'index': i
                    })
                
                # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
                semantic_results.sort(key=lambda x: x['score'], reverse=True)
                safe_print(f"ğŸ“Š TF-IDF ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢å®Œäº†: ä¸Šä½{min(max_results, len(semantic_results))}ä»¶")
                return semantic_results[:max_results]
                
            except ImportError:
                safe_print("âš ï¸ scikit-learnæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ç°¡æ˜“ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        
        # æœ€å¾Œã®æ‰‹æ®µ: æ”¹è‰¯ã•ã‚ŒãŸç°¡æ˜“ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢
        semantic_results = []
        
        # ã‚¯ã‚¨ãƒªã®é‡è¦èªå¥ã‚’æŠ½å‡º
        import re
        query_words = set(re.findall(r'\w+', query.lower()))
        
        for i, chunk in enumerate(chunks):
            chunk_words = set(re.findall(r'\w+', chunk.lower()))
            
            # è¤‡æ•°ã®é¡ä¼¼åº¦æŒ‡æ¨™ã‚’çµ„ã¿åˆã‚ã›
            scores = []
            
            # 1. Jaccardé¡ä¼¼åº¦
            if len(query_words) > 0 and len(chunk_words) > 0:
                intersection = len(query_words.intersection(chunk_words))
                union = len(query_words.union(chunk_words))
                jaccard = intersection / union if union > 0 else 0.0
                scores.append(jaccard * 0.4)
            
            # 2. èªå¥ã®åŒ…å«åº¦
            if len(query_words) > 0:
                inclusion = sum(1 for word in query_words if word in chunk.lower()) / len(query_words)
                scores.append(inclusion * 0.3)
            
            # 3. æ–‡å­—åˆ—è·é›¢ï¼ˆãƒ¬ãƒ¼ãƒ™ãƒ³ã‚·ãƒ¥ã‚¿ã‚¤ãƒ³è·é›¢ã®ç°¡æ˜“ç‰ˆï¼‰
            try:
                # éƒ¨åˆ†æ–‡å­—åˆ—ã®ä¸€è‡´åº¦
                substring_match = 0
                for word in query_words:
                    if len(word) >= 2 and word in chunk.lower():
                        substring_match += len(word) / len(query)
                scores.append(min(1.0, substring_match) * 0.3)
            except:
                scores.append(0.0)
            
            # ç·åˆã‚¹ã‚³ã‚¢
            total_score = sum(scores)
            
            semantic_results.append({
                'content': chunk,
                'score': total_score,
                'type': 'semantic_simple',
                'index': i
            })
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        semantic_results.sort(key=lambda x: x['score'], reverse=True)
        safe_print(f"ğŸ” ç°¡æ˜“ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢å®Œäº†: ä¸Šä½{min(max_results, len(semantic_results))}ä»¶")
        return semantic_results[:max_results]
        
    except Exception as e:
        safe_print(f"ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def _evaluate_rag_quality(filtered_chunk: str, query: str, rag_attempts: int) -> float:
    """
    RAGæ¤œç´¢çµæœã®å“è³ªã‚’è©•ä¾¡ï¼ˆ0.0-1.0ã®ã‚¹ã‚³ã‚¢ï¼‰
    å…·ä½“çš„ãªè³ªå•ã«å¯¾ã—ã¦ã‚ˆã‚Šå³æ ¼ãªè©•ä¾¡ã‚’å®Ÿæ–½
    """
    if not filtered_chunk or not filtered_chunk.strip():
        return 0.0
    
    score = 0.0
    content_lower = filtered_chunk.lower()
    query_lower = query.lower()
    
    # 1. æ–‡å­—æ•°ã«ã‚ˆã‚‹åŸºæœ¬ã‚¹ã‚³ã‚¢ï¼ˆæœ€å¤§0.2ï¼‰ - å³æ ¼åŒ–
    content_length = len(filtered_chunk.strip())
    if content_length >= 500:  # 500æ–‡å­—ä»¥ä¸Šã§æœ€é«˜ã‚¹ã‚³ã‚¢
        score += 0.2
    elif content_length >= 300:  # 300æ–‡å­—ä»¥ä¸Šã§ä¸­ç¨‹åº¦
        score += 0.15
    elif content_length >= 150:   # 150æ–‡å­—ä»¥ä¸Šã§æœ€ä½é™
        score += 0.1
    
    # 2. é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å³æ ¼ãƒãƒƒãƒãƒ³ã‚°ï¼ˆæœ€å¤§0.6ï¼‰ - å¤§å¹…å¼·åŒ–
    query_words = set(query.lower().split())
    
    # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ç‰¹å®šï¼ˆä¼šç¤¾åã€å›ºæœ‰åè©ãªã©ï¼‰
    important_keywords = []
    company_patterns = [
        r'æ ªå¼ä¼šç¤¾\s*[\w\s]+',
        r'æœ‰é™ä¼šç¤¾\s*[\w\s]+', 
        r'åˆåŒä¼šç¤¾\s*[\w\s]+',
        r'[\w\s]+ä¼šç¤¾',
        r'[\w\s]+å·¥èŠ¸',
        r'[\w\s]+å·¥æ¥­',
        r'[\w\s]+å•†äº‹'
    ]
    
    # ã‚¯ã‚¨ãƒªã‹ã‚‰ä¼šç¤¾åã‚„é‡è¦èªå¥ã‚’æŠ½å‡º
    for pattern in company_patterns:
        matches = re.findall(pattern, query)
        important_keywords.extend(matches)
    
    # ã‚¯ã‚¨ãƒªã®ä¸»è¦å˜èªã‚’é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã—ã¦è¿½åŠ 
    for word in query_words:
        if len(word) >= 2 and word not in ['ã®', 'ã«', 'ã‚’', 'ã¯', 'ãŒ', 'ã§', 'ã¨', 'ã‹ã‚‰', 'ã¾ã§']:
            important_keywords.append(word)
    
    # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
    critical_matches = 0
    for keyword in important_keywords:
        if keyword.strip() in content_lower:
            critical_matches += 1
    
    if len(important_keywords) > 0:
        critical_match_ratio = critical_matches / len(important_keywords)
        
        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒ50%ä»¥ä¸Šãƒãƒƒãƒã—ãŸå ´åˆã®ã¿é«˜ã‚¹ã‚³ã‚¢
        if critical_match_ratio >= 0.5:
            score += critical_match_ratio * 0.6
        elif critical_match_ratio >= 0.3:
            score += critical_match_ratio * 0.3
        elif critical_match_ratio >= 0.1:
            score += critical_match_ratio * 0.1
    
    # 3. è³ªå•ã®æ„å›³ã«å¯¾ã™ã‚‹å›ç­”ã®é©åˆæ€§ï¼ˆæœ€å¤§0.2ï¼‰
    intent_keywords = {
        'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': ['çŠ¶æ…‹', 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹', 'ç¾çŠ¶', 'çŠ¶æ³', 'é€²æ—', 'æ®µéš'],
        'é¡§å®¢ç•ªå·': ['é¡§å®¢ç•ªå·', 'ãŠå®¢æ§˜ç•ªå·', 'é¡§å®¢ID', 'é¡§å®¢ã‚³ãƒ¼ãƒ‰', 'ç•ªå·'],
        'é€£çµ¡å…ˆ': ['é›»è©±', 'TEL', 'FAX', 'ãƒ¡ãƒ¼ãƒ«', 'ä½æ‰€', 'é€£çµ¡å…ˆ'],
        'æ–™é‡‘': ['æ–™é‡‘', 'ä¾¡æ ¼', 'è²»ç”¨', 'ã‚³ã‚¹ãƒˆ', 'é‡‘é¡', 'å€¤æ®µ'],
        'å¥‘ç´„': ['å¥‘ç´„', 'å–å¼•', 'åˆæ„', 'ç´„æŸ', 'æ¡ä»¶']
    }
    
    intent_score = 0
    for intent, keywords in intent_keywords.items():
        if intent.lower() in query_lower:
            # è³ªå•ã«æ„å›³ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€å›ç­”ã«ãã®é–¢é€£èªå¥ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            for keyword in keywords:
                if keyword in content_lower:
                    intent_score += 0.05
                    break
    
    score += min(0.2, intent_score)
    
    # 4. ç„¡é–¢ä¿‚ãªå†…å®¹ã®æ¤œå‡ºã«ã‚ˆã‚‹æ¸›ç‚¹
    irrelevant_patterns = [
        'ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼', 'ãƒ‡ãƒãƒƒã‚°', 'ãƒ†ã‚¹ãƒˆ', 'ã‚µãƒ³ãƒ—ãƒ«', 'ä¾‹ï¼š', 'ä¾‹)', 
        'â€»', 'æ³¨æ„', 'é‡è¦', 'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ', 'ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“'
    ]
    
    irrelevant_count = sum(1 for pattern in irrelevant_patterns if pattern in filtered_chunk)
    if irrelevant_count > 0:
        score -= min(0.3, irrelevant_count * 0.1)
    
    # 5. æœ€çµ‚çš„ãªå³æ ¼åˆ¤å®š
    # å…·ä½“çš„ãªå›ºæœ‰åè©ã‚’å«ã‚€è³ªå•ã®å ´åˆã€ãã®å›ºæœ‰åè©ãŒå«ã¾ã‚Œã¦ã„ãªã„å›ç­”ã¯å¤§å¹…æ¸›ç‚¹
    if any(word in query_lower for word in ['æ ªå¼ä¼šç¤¾', 'ä¼šç¤¾', 'å·¥èŠ¸', 'é¡§å®¢ç•ªå·', 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹']):
        has_relevant_content = False
        
        # ã‚¯ã‚¨ãƒªã®é‡è¦èªå¥ãŒå›ç­”ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        for word in query_words:
            if len(word) >= 2 and word in content_lower:
                has_relevant_content = True
                break
        
        if not has_relevant_content:
            score *= 0.1  # 90%æ¸›ç‚¹
    
    # 6. æ„å‘³çš„é¡ä¼¼åº¦ã«ã‚ˆã‚‹å“è³ªå‘ä¸Šï¼ˆãƒœãƒ¼ãƒŠã‚¹ï¼‰
    try:
        # ç°¡æ˜“çš„ãªæ„å‘³çš„é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯
        semantic_bonus = 0.0
        
        # è³ªå•ã¨å›ç­”ã®æ–‡è„ˆçš„é–¢é€£æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        context_keywords = {
            'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': ['çŠ¶æ…‹', 'ç¾çŠ¶', 'é€²è¡Œ', 'æ®µéš', 'çŠ¶æ³', 'status'],
            'é¡§å®¢': ['ãŠå®¢æ§˜', 'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ', 'client', 'customer'],
            'ä¼šç¤¾': ['ä¼æ¥­', 'æ³•äºº', 'company', 'corporation'],
            'ç•ªå·': ['ID', 'ã‚³ãƒ¼ãƒ‰', 'number', 'code'],
            'å·¥èŠ¸': ['ã‚¯ãƒ©ãƒ•ãƒˆ', 'ã‚¢ãƒ¼ãƒˆ', 'craft', 'art'],
        }
        
        for main_word, related_words in context_keywords.items():
            if main_word in query_lower:
                # é–¢é€£èªå¥ãŒå›ç­”ã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ãƒœãƒ¼ãƒŠã‚¹
                for related in related_words:
                    if related.lower() in content_lower:
                        semantic_bonus += 0.02
                        break
        
        # æ–‡è„ˆçš„ãªä¸€è²«æ€§ãƒœãƒ¼ãƒŠã‚¹
        if semantic_bonus > 0:
            score += min(0.1, semantic_bonus)  # æœ€å¤§0.1ã®ãƒœãƒ¼ãƒŠã‚¹
            
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãƒœãƒ¼ãƒŠã‚¹ãªã—
        pass
    
    # ã‚¹ã‚³ã‚¢ã‚’0.0-1.0ã«æ­£è¦åŒ–
    final_score = max(0.0, min(1.0, score))
    
    return final_score

def _combine_search_results(bm25_results: list, semantic_results: list, query: str, max_results: int) -> list:
    """BM25ã¨ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢çµæœã®çµ±åˆ - æ„å‘³çš„æ¤œç´¢ã‚’é‡è¦–"""
    try:
        # çµæœã®çµ±åˆã¨ã‚¹ã‚³ã‚¢æ­£è¦åŒ–
        all_results = {}
        
        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã®ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦é‡ã¿ã‚’èª¿æ•´
        semantic_weight = 0.7  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        bm25_weight = 0.3
        
        # Transformerãƒ™ãƒ¼ã‚¹ã®å ´åˆã¯æ„å‘³çš„æ¤œç´¢ã‚’ã‚ˆã‚Šé‡è¦–
        if semantic_results and semantic_results[0].get('type') == 'semantic_transformer':
            semantic_weight = 0.8
            bm25_weight = 0.2
            safe_print("ğŸ§  Transformerãƒ™ãƒ¼ã‚¹ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ - æ„å‘³é‡è¦–ãƒ¢ãƒ¼ãƒ‰")
        elif semantic_results and semantic_results[0].get('type') == 'semantic_tfidf':
            semantic_weight = 0.6
            bm25_weight = 0.4
            safe_print("ğŸ“Š TF-IDFãƒ™ãƒ¼ã‚¹ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ - ãƒãƒ©ãƒ³ã‚¹ãƒ¢ãƒ¼ãƒ‰")
        else:
            semantic_weight = 0.4
            bm25_weight = 0.6
            safe_print("ğŸ” ç°¡æ˜“ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ - èªå½™é‡è¦–ãƒ¢ãƒ¼ãƒ‰")
        
        # BM25çµæœã®å‡¦ç†
        max_bm25_score = max([r['score'] for r in bm25_results], default=1.0)
        for result in bm25_results:
            idx = result['index']
            normalized_score = result['score'] / max_bm25_score if max_bm25_score > 0 else 0.0
            
            if idx not in all_results:
                all_results[idx] = {
                    'content': result['content'],
                    'bm25_score': normalized_score * bm25_weight,
                    'semantic_score': 0.0,
                    'index': idx
                }
            else:
                all_results[idx]['bm25_score'] = normalized_score * bm25_weight
        
        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯çµæœã®å‡¦ç†
        max_semantic_score = max([r['score'] for r in semantic_results], default=1.0)
        for result in semantic_results:
            idx = result['index']
            normalized_score = result['score'] / max_semantic_score if max_semantic_score > 0 else 0.0
            
            if idx not in all_results:
                all_results[idx] = {
                    'content': result['content'],
                    'bm25_score': 0.0,
                    'semantic_score': normalized_score * semantic_weight,
                    'index': idx
                }
            else:
                all_results[idx]['semantic_score'] = normalized_score * semantic_weight
        
        # çµ±åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        final_results = []
        for idx, result in all_results.items():
            combined_score = result['bm25_score'] + result['semantic_score']
            
            # æ„å‘³çš„é¡ä¼¼åº¦ãŒé«˜ã„å ´åˆã«ãƒœãƒ¼ãƒŠã‚¹
            if result['semantic_score'] > 0.5:
                combined_score += 0.1  # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒœãƒ¼ãƒŠã‚¹
            
            # ä¸¡æ–¹ã®æ¤œç´¢ã§è¦‹ã¤ã‹ã£ãŸå ´åˆã«ãƒœãƒ¼ãƒŠã‚¹
            if result['bm25_score'] > 0 and result['semantic_score'] > 0:
                combined_score += 0.05  # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒœãƒ¼ãƒŠã‚¹
            
            final_results.append({
                'content': result['content'],
                'score': combined_score,
                'index': idx,
                'bm25_score': result['bm25_score'],
                'semantic_score': result['semantic_score']
            })
        
        # çµ±åˆã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        final_results.sort(key=lambda x: x['score'], reverse=True)
        
        safe_print(f"ğŸ” ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢çµ±åˆ: BM25={len(bm25_results)}ä»¶, ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯={len(semantic_results)}ä»¶ â†’ çµ±åˆ={len(final_results)}ä»¶")
        safe_print(f"ğŸ“Š é‡ã¿é…åˆ†: BM25={bm25_weight:.1f}, ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯={semantic_weight:.1f}")
        
        return final_results[:max_results]
        
    except Exception as e:
        safe_print(f"æ¤œç´¢çµæœçµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
        return bm25_results[:max_results]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

# Geminiãƒ¢ãƒ‡ãƒ«ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼‰
model = None

def set_model(gemini_model):
    """Geminiãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã™ã‚‹"""
    global model
    model = gemini_model

def is_casual_conversation(message_text: str) -> bool:
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæŒ¨æ‹¶ã‚„ä¸€èˆ¬çš„ãªä¼šè©±ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹"""
    if not message_text:
        return False
    
    message_lower = message_text.strip().lower()
    
    # æŒ¨æ‹¶ãƒ‘ã‚¿ãƒ¼ãƒ³
    greetings = [
        "ã“ã‚“ã«ã¡ã¯", "ã“ã‚“ã«ã¡ã‚", "ãŠã¯ã‚ˆã†", "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™", "ã“ã‚“ã°ã‚“ã¯", "ã“ã‚“ã°ã‚“ã‚",
        "ã‚ˆã‚ã—ã", "ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™", "ã¯ã˜ã‚ã¾ã—ã¦", "åˆã‚ã¾ã—ã¦",
        "hello", "hi", "hey", "good morning", "good afternoon", "good evening"
    ]
    
    # ãŠç¤¼ãƒ‘ã‚¿ãƒ¼ãƒ³
    thanks = [
        "ã‚ã‚ŠãŒã¨ã†", "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™", "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ", "æ„Ÿè¬ã—ã¾ã™",
        "thank you", "thanks", "thx"
    ]
    
    # åˆ¥ã‚Œã®æŒ¨æ‹¶ãƒ‘ã‚¿ãƒ¼ãƒ³
    farewells = [
        "ã•ã‚ˆã†ãªã‚‰", "ã¾ãŸã­", "ã¾ãŸæ˜æ—¥", "å¤±ç¤¼ã—ã¾ã™", "ãŠç–²ã‚Œæ§˜", "ãŠç–²ã‚Œã•ã¾ã§ã—ãŸ",
        "bye", "goodbye", "see you", "good bye"
    ]
    
    # ä¸€èˆ¬çš„ãªä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³
    casual_phrases = [
        "å…ƒæ°—", "èª¿å­", "ã©ã†", "å¤©æ°—", "ä»Šæ—¥", "æ˜æ—¥", "æ˜¨æ—¥", "é€±æœ«", "ä¼‘ã¿",
        "ç–²ã‚ŒãŸ", "å¿™ã—ã„", "æš‡", "æ™‚é–“", "ã„ã„å¤©æ°—", "å¯’ã„", "æš‘ã„", "é›¨",
        "how are you", "what's up", "how's it going", "nice weather", "tired", "busy"
    ]
    
    # çŸ­ã„è³ªå•ã‚„ç›¸æ§Œãƒ‘ã‚¿ãƒ¼ãƒ³
    short_responses = [
        "ã¯ã„", "ã„ã„ãˆ", "ãã†ã§ã™ã­", "ãªã‚‹ã»ã©", "ãã†ã§ã™ã‹", "ã‚ã‹ã‚Šã¾ã—ãŸ",
        "ok", "okay", "yes", "no", "i see", "alright"
    ]
    
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒçŸ­ã™ãã‚‹å ´åˆï¼ˆ3æ–‡å­—ä»¥ä¸‹ï¼‰ã¯ä¸€èˆ¬çš„ãªä¼šè©±ã¨ã—ã¦æ‰±ã†
    if len(message_lower) <= 3:
        return True
    
    # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
    all_patterns = greetings + thanks + farewells + casual_phrases + short_responses
    
    for pattern in all_patterns:
        if pattern in message_lower:
            return True
    
    # ç–‘å•ç¬¦ãŒãªãã€çŸ­ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆ20æ–‡å­—ä»¥ä¸‹ï¼‰ã¯ä¸€èˆ¬çš„ãªä¼šè©±ã¨ã—ã¦æ‰±ã†
    if len(message_text) <= 20 and "?" not in message_text and "ï¼Ÿ" not in message_text:
        return True
    
    return False

async def generate_casual_response(message_text: str, company_name: str) -> str:
    """æŒ¨æ‹¶ã‚„ä¸€èˆ¬çš„ãªä¼šè©±ã«å¯¾ã™ã‚‹è‡ªç„¶ãªè¿”ç­”ã‚’ç”Ÿæˆã™ã‚‹"""
    try:
        if model is None:
            return "ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
        
        # æŒ¨æ‹¶ã‚„ä¸€èˆ¬çš„ãªä¼šè©±å°‚ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        casual_prompt = f"""
ã‚ãªãŸã¯{company_name}ã®è¦ªã—ã¿ã‚„ã™ã„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®æŒ¨æ‹¶ã‚„ä¸€èˆ¬çš„ãªä¼šè©±ã«å¯¾ã—ã¦ã€è‡ªç„¶ã§è¦ªã—ã¿ã‚„ã™ã„è¿”ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚

è¿”ç­”ã®éš›ã®æ³¨æ„ç‚¹ï¼š
1. è¦ªã—ã¿ã‚„ã™ãã€æ¸©ã‹ã„å£èª¿ã§è¿”ç­”ã—ã¦ãã ã•ã„
2. ä¼šè©±ã‚’ç¶šã‘ãŸã„å ´åˆã¯ã€é©åˆ‡ãªè³ªå•ã§è¿”ã—ã¦ãã ã•ã„
3. é•·ã™ããšã€çŸ­ã™ããªã„é©åº¦ãªé•·ã•ã§è¿”ç­”ã—ã¦ãã ã•ã„
4. å¿…è¦ã«å¿œã˜ã¦ã€ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ãŒã‚ã‚‹ã“ã¨ã‚’ä¼ãˆã¦ãã ã•ã„
5. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã¯å‚ç…§ã›ãšã€ä¸€èˆ¬çš„ãªä¼šè©±ã¨ã—ã¦è¿”ç­”ã—ã¦ãã ã•ã„

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {message_text}
"""
        
        response = model.generate_content(casual_prompt)
        
        if response and hasattr(response, 'text') and response.text:
            return response.text.strip()
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”
            message_lower = message_text.lower()
            if any(greeting in message_lower for greeting in ["ã“ã‚“ã«ã¡ã¯", "ã“ã‚“ã«ã¡ã‚", "hello", "hi"]):
                return "ã“ã‚“ã«ã¡ã¯ï¼ãŠç–²ã‚Œæ§˜ã§ã™ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
            elif any(thanks in message_lower for thanks in ["ã‚ã‚ŠãŒã¨ã†", "thank you", "thanks"]):
                return "ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼ä»–ã«ã‚‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ãŒã‚ã‚Œã°ã€ãŠæ°—è»½ã«ãŠå£°ãŒã‘ãã ã•ã„ã€‚"
            elif any(farewell in message_lower for farewell in ["ã•ã‚ˆã†ãªã‚‰", "ã¾ãŸã­", "bye", "goodbye"]):
                return "ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼ã¾ãŸä½•ã‹ã‚ã‚Šã¾ã—ãŸã‚‰ã€ã„ã¤ã§ã‚‚ãŠå£°ãŒã‘ãã ã•ã„ã€‚"
            else:
                return "ãã†ã§ã™ã­ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ãŒã‚ã‚Œã°ã€ãŠæ°—è»½ã«ãŠå£°ãŒã‘ãã ã•ã„ã€‚"
                
    except Exception as e:
        safe_print(f"ä¸€èˆ¬ä¼šè©±å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return "ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"

async def process_chat(message: ChatMessage, db = Depends(get_db), current_user: dict = None):
    """ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ã—ã¦Geminiã‹ã‚‰ã®å¿œç­”ã‚’è¿”ã™"""
    try:
        # ãƒ¢ãƒ‡ãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        if model is None:
            safe_print("âŒ ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            raise HTTPException(status_code=500, detail="AIãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        safe_print(f"âœ… ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ç¢ºèª: {model}")
        safe_print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {type(model)}")
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒNoneã§ãªã„ã“ã¨ã‚’ç¢ºèª
        if not message or not hasattr(message, 'text') or message.text is None:
            raise HTTPException(status_code=400, detail="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ†ã‚­ã‚¹ãƒˆãŒæä¾›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ†ã‚­ã‚¹ãƒˆã‚’å®‰å…¨ã«å–å¾—
        message_text = message.text if message.text is not None else ""
        
        # æœ€æ–°ã®ä¼šç¤¾åã‚’å–å¾—ï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã®ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ã¯ãªãã€é–¢æ•°å†…ã§å†å–å¾—ï¼‰
        from .company import DEFAULT_COMPANY_NAME as current_company_name
        
        # æŒ¨æ‹¶ã‚„ä¸€èˆ¬çš„ãªä¼šè©±ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        if is_casual_conversation(message_text):
            safe_print(f"ğŸ—£ï¸ ä¸€èˆ¬çš„ãªä¼šè©±ã¨ã—ã¦åˆ¤å®š: {message_text}")
            
            # ä¸€èˆ¬çš„ãªä¼šè©±ã®å ´åˆã¯ãƒŠãƒ¬ãƒƒã‚¸ã‚’å‚ç…§ã›ãšã«è¿”ç­”
            casual_response = await generate_casual_response(message_text, current_company_name)
            
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜ï¼ˆä¸€èˆ¬ä¼šè©±ã¨ã—ã¦ï¼‰
            from modules.token_counter import TokenUsageTracker
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¼šç¤¾IDã‚’å–å¾—ï¼ˆãƒãƒ£ãƒƒãƒˆå±¥æ­´ä¿å­˜ç”¨ï¼‰
            company_id = None
            if message.user_id:
                try:
                    from supabase_adapter import select_data
                    user_result = select_data("users", columns="company_id", filters={"id": message.user_id})
                    if user_result.data and len(user_result.data) > 0:
                        user_data = user_result.data[0]
                        company_id = user_data.get('company_id')
                except Exception as e:
                    safe_print(f"ä¼šç¤¾IDå–å¾—ã‚¨ãƒ©ãƒ¼ï¼ˆä¸€èˆ¬ä¼šè©±ï¼‰: {str(e)}")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜ï¼ˆãƒŠãƒ¬ãƒƒã‚¸å‚ç…§ãªã—ï¼‰
            tracker = TokenUsageTracker(db)
            chat_id = tracker.save_chat_with_prompts(
                user_message=message_text,
                bot_response=casual_response,
                user_id=message.user_id,
                prompt_references=0,  # ãƒŠãƒ¬ãƒƒã‚¸å‚ç…§ãªã—
                company_id=company_id,
                employee_id=getattr(message, 'employee_id', None),
                employee_name=getattr(message, 'employee_name', None),
                category="ä¸€èˆ¬ä¼šè©±",
                sentiment="neutral",
                model="gemini-pro"
            )
            
            # åˆ©ç”¨åˆ¶é™ã®å‡¦ç†ï¼ˆä¸€èˆ¬ä¼šè©±ã§ã‚‚è³ªå•å›æ•°ã«ã‚«ã‚¦ãƒ³ãƒˆï¼‰
            remaining_questions = None
            limit_reached = False
            
            if message.user_id:
                # è³ªå•ã®åˆ©ç”¨åˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
                limits_check = check_usage_limits(message.user_id, "question", db)
                
                if not limits_check["is_unlimited"] and not limits_check["allowed"]:
                    response_text = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ãƒ‡ãƒ¢ç‰ˆã®è³ªå•å›æ•°åˆ¶é™ï¼ˆ{limits_check['limit']}å›ï¼‰ã«é”ã—ã¾ã—ãŸã€‚"
                    return {
                        "response": response_text,
                        "remaining_questions": 0,
                        "limit_reached": True
                    }
                
                # è³ªå•ã‚«ã‚¦ãƒ³ãƒˆã‚’æ›´æ–°
                if not limits_check.get("is_unlimited", False):
                    updated_limits = update_usage_count(message.user_id, "questions_used", db)
                    if updated_limits:
                        remaining_questions = updated_limits["questions_limit"] - updated_limits["questions_used"]
                        limit_reached = remaining_questions <= 0
                    else:
                        remaining_questions = limits_check["remaining"] - 1 if limits_check["remaining"] > 0 else 0
                        limit_reached = remaining_questions <= 0
            
            safe_print(f"âœ… ä¸€èˆ¬ä¼šè©±å¿œç­”å®Œäº†: {len(casual_response)} æ–‡å­—")
            
            return {
                "response": casual_response,
                "source": "",  # ãƒŠãƒ¬ãƒƒã‚¸å‚ç…§ãªã—
                "remaining_questions": remaining_questions,
                "limit_reached": limit_reached
            }
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒã‚ã‚‹å ´åˆã¯åˆ©ç”¨åˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
        remaining_questions = None
        limit_reached = False
        
        if message.user_id:
            # è³ªå•ã®åˆ©ç”¨åˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
            limits_check = check_usage_limits(message.user_id, "question", db)
            
            if not limits_check["is_unlimited"] and not limits_check["allowed"]:
                response_text = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ãƒ‡ãƒ¢ç‰ˆã®è³ªå•å›æ•°åˆ¶é™ï¼ˆ{limits_check['limit']}å›ï¼‰ã«é”ã—ã¾ã—ãŸã€‚"
                return {
                    "response": response_text,
                    "remaining_questions": 0,
                    "limit_reached": True
                }
            
            # ç„¡åˆ¶é™ã§ãªã„å ´åˆã¯æ®‹ã‚Šå›æ•°ã‚’è¨ˆç®—
            if not limits_check["is_unlimited"]:
                remaining_questions = limits_check["remaining"]

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¼šç¤¾IDã‚’å–å¾—
        company_id = None
        if message.user_id:
            try:
                from supabase_adapter import select_data
                user_result = select_data("users", columns="company_id", filters={"id": message.user_id})
                if user_result.data and len(user_result.data) > 0:
                    user_data = user_result.data[0]
                    if user_data.get('company_id'):
                        company_id = user_data['company_id']
                        safe_print(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ID {message.user_id} ã®ä¼šç¤¾ID: {company_id}")
                    else:
                        safe_print(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ID {message.user_id} ã«ä¼šç¤¾IDãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                else:
                    safe_print(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ID {message.user_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            except Exception as e:
                safe_print(f"ä¼šç¤¾IDå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯company_id = Noneã®ã¾ã¾ç¶™ç¶š
        
        # ä¼šç¤¾å›ºæœ‰ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ã‚’å–å¾—
        # ç®¡ç†è€…ã®å ´åˆã¯è‡ªåˆ†ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒªã‚½ãƒ¼ã‚¹ã®ã¿å–å¾—
        uploaded_by = None
        if current_user and current_user.get("role") == "admin":
            uploaded_by = current_user["id"]
            safe_print(f"ç®¡ç†è€…ãƒ¦ãƒ¼ã‚¶ãƒ¼: {current_user.get('email')} - è‡ªåˆ†ã®ãƒªã‚½ãƒ¼ã‚¹ã®ã¿å‚ç…§")
        
        active_sources = await get_active_resources_by_company_id(company_id, db, uploaded_by)
        safe_print(f"ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ (ä¼šç¤¾ID: {company_id}): {', '.join(active_sources)}")
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ãŒãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™
        if not active_sources:
            response_text = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ç¾åœ¨ã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ç®¡ç†ç”»é¢ã§ãƒªã‚½ãƒ¼ã‚¹ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚"
            
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜
            chat_id = str(uuid.uuid4())
            cursor = db.cursor()
            cursor.execute(
                "INSERT INTO chat_history (id, user_message, bot_response, timestamp, category, sentiment, employee_id, employee_name, user_id, company_id) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)",
                (chat_id, message_text, response_text, datetime.now().isoformat(), "è¨­å®šã‚¨ãƒ©ãƒ¼", "neutral", message.employee_id, message.employee_name, message.user_id, company_id)
            )
            db.commit()
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒã‚ã‚‹å ´åˆã¯è³ªå•ã‚«ã‚¦ãƒ³ãƒˆã‚’æ›´æ–°ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ãŒãªãã¦ã‚‚åˆ©ç”¨åˆ¶é™ã¯æ›´æ–°ã™ã‚‹ï¼‰
            if message.user_id and not limits_check.get("is_unlimited", False):
                safe_print(f"åˆ©ç”¨åˆ¶é™æ›´æ–°é–‹å§‹ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹ãªã—ï¼‰ - ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {message.user_id}")
                safe_print(f"æ›´æ–°å‰ã®åˆ¶é™æƒ…å ±: {limits_check}")
                
                updated_limits = update_usage_count(message.user_id, "questions_used", db)
                safe_print(f"æ›´æ–°å¾Œã®åˆ¶é™æƒ…å ±: {updated_limits}")
                
                if updated_limits:
                    remaining_questions = updated_limits["questions_limit"] - updated_limits["questions_used"]
                    limit_reached = remaining_questions <= 0
                    safe_print(f"è¨ˆç®—ã•ã‚ŒãŸæ®‹ã‚Šè³ªå•æ•°: {remaining_questions}, åˆ¶é™åˆ°é”: {limit_reached}")
                else:
                    safe_print("åˆ©ç”¨åˆ¶é™ã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            safe_print(f"è¿”ã‚Šå€¤ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹ãªã—ï¼‰: remaining_questions={remaining_questions}, limit_reached={limit_reached}")
            
            return {
                "response": response_text,
                "remaining_questions": remaining_questions,
                "limit_reached": limit_reached
            }
        
        # pandas ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        import pandas as pd
        import traceback
        
        # é¸æŠã•ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã—ã¦çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆ
        # source_info = {}  # ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’ä¿å­˜ã™ã‚‹è¾æ›¸
        active_resource_names = await get_active_resource_names_by_company_id(company_id, db)
        source_info_list = [
            {
                "name": res_name,
                "section": "",  # or default
                "page": ""
            }
            for res_name in active_resource_names
        ]
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ã®SpecialæŒ‡ç¤ºã‚’å–å¾—
        special_instructions = []
        try:
            from supabase_adapter import select_data
            for source_id in active_sources:
                source_result = select_data("document_sources", columns="name,special", filters={"id": source_id})
                if source_result.data and len(source_result.data) > 0:
                    source_data = source_result.data[0]
                    if source_data.get('special') and source_data['special'].strip():
                        special_instructions.append({
                            "name": source_data.get('name', 'Unknown'),
                            "instruction": source_data['special'].strip()
                        })
            safe_print(f"SpecialæŒ‡ç¤º: {len(special_instructions)}å€‹ã®ãƒªã‚½ãƒ¼ã‚¹ã«SpecialæŒ‡ç¤ºãŒã‚ã‚Šã¾ã™")
        except Exception as e:
            safe_print(f"SpecialæŒ‡ç¤ºå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            special_instructions = []
        
        # ğŸ” çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å–å¾—ã®è©³ç´°ãƒ‡ãƒãƒƒã‚°ï¼ˆæœ¬ç•ªç’°å¢ƒå•é¡Œèª¿æŸ»ï¼‰
        safe_print(f"ğŸ“‹ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚½ãƒ¼ã‚¹ ({len(active_sources)}ä»¶): {active_sources}")
        safe_print(f"ğŸ” çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å–å¾—é–‹å§‹...")
        
        active_knowledge_text = await get_active_resources_content_by_ids(active_sources, db)
        
        # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å–å¾—çµæœã®è©³ç´°ãƒã‚§ãƒƒã‚¯
        if not active_knowledge_text:
            safe_print(f"âŒ çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãŒç©ºã§ã™ - active_knowledge_text: {repr(active_knowledge_text)}")
        elif isinstance(active_knowledge_text, str) and not active_knowledge_text.strip():
            safe_print(f"âŒ çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãŒç©ºæ–‡å­—åˆ—ã§ã™ - é•·ã•: {len(active_knowledge_text)}")
        else:
            safe_print(f"âœ… çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å–å¾—æˆåŠŸ - é•·ã•: {len(active_knowledge_text):,} æ–‡å­—")
            safe_print(f"ğŸ‘€ çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å…ˆé ­200æ–‡å­—: {active_knowledge_text[:200]}...")
        
        # æ”¹è‰¯ã•ã‚ŒãŸRAGæ¤œç´¢ã§é–¢é€£éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡ºï¼ˆé«˜ç²¾åº¦ãƒ»é«˜é€ŸåŒ–ï¼‰
        if active_knowledge_text and len(active_knowledge_text) > 50000:
            safe_print(f"ğŸ¯ æ”¹è‰¯RAGæ¤œç´¢é–‹å§‹ - å…ƒã‚µã‚¤ã‚º: {len(active_knowledge_text):,} æ–‡å­—")
            
            # é«˜é€ŸåŒ–ã‚’é‡è¦–ã—ãŸæ¤œç´¢æ‰‹æ³•ã‚’é¸æŠ
            if SPEED_RAG_AVAILABLE:
                # é›·é€ŸRAGæ¤œç´¢ã‚’æœ€å„ªå…ˆã§ä½¿ç”¨
                active_knowledge_text = await lightning_rag_search(active_knowledge_text, message_text, max_results=30)
            elif len(active_knowledge_text) > 500000:
                # éå¸¸ã«å¤§ããªãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯å¼·åŒ–RAGæ¤œç´¢
                if RAG_ENHANCED_AVAILABLE:
                    active_knowledge_text = await enhanced_rag_search(active_knowledge_text, message_text, max_results=25)
                else:
                    active_knowledge_text = multi_pass_rag_search(active_knowledge_text, message_text, max_results=20)
            elif len(active_knowledge_text) > 200000:
                # å¤§ããªãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯å¤šæ®µéšæ¤œç´¢
                active_knowledge_text = multi_pass_rag_search(active_knowledge_text, message_text, max_results=15)
            else:
                # ä¸­ç¨‹åº¦ã®ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯é©å¿œçš„æ¤œç´¢
                active_knowledge_text = adaptive_rag_search(active_knowledge_text, message_text, max_results=12)
            
            safe_print(f"ğŸ¯ æ”¹è‰¯RAGæ¤œç´¢å®Œäº† - æ–°ã‚µã‚¤ã‚º: {len(active_knowledge_text):,} æ–‡å­—")
            
            # RAGæ¤œç´¢å¾Œã®ã‚µã‚¤ã‚ºãŒ30ä¸‡æ–‡å­—ä»¥ä¸‹ãªã‚‰é€šå¸¸å‡¦ç†ã«åˆ‡ã‚Šæ›¿ãˆ
            if len(active_knowledge_text) <= 300000:
                safe_print(f"ğŸ”„ RAGæ¤œç´¢å¾Œã®ã‚µã‚¤ã‚ºãŒå°ã•ã„ãŸã‚ã€é€šå¸¸å‡¦ç†ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™")
                # é€šå¸¸ã®process_chaté–¢æ•°ã‚’å‘¼ã³å‡ºã—
                return await process_chat(message, db, current_user)
        
        # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®ã‚µã‚¤ã‚ºã‚’åˆ¶é™ï¼ˆAPIåˆ¶é™å¯¾å¿œã®ãŸã‚ä¸€æ™‚çš„ã«å¾©æ´»ï¼‰
        MAX_KNOWLEDGE_SIZE = 300000  # 30ä¸‡æ–‡å­—åˆ¶é™ï¼ˆAPIåˆ¶é™å¯¾å¿œï¼‰
        if active_knowledge_text and len(active_knowledge_text) > MAX_KNOWLEDGE_SIZE:
            safe_print(f"âš ï¸ çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãŒå¤§ãã™ãã¾ã™ ({len(active_knowledge_text)} æ–‡å­—)ã€‚{MAX_KNOWLEDGE_SIZE} æ–‡å­—ã«åˆ¶é™ã—ã¾ã™ã€‚")
            active_knowledge_text = active_knowledge_text[:MAX_KNOWLEDGE_SIZE] + "\n\n[æ³¨æ„: çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãŒå¤§ãã„ãŸã‚ã€ä¸€éƒ¨ã®ã¿è¡¨ç¤ºã—ã¦ã„ã¾ã™]"
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãŒç©ºã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™
        if not active_knowledge_text or (isinstance(active_knowledge_text, str) and not active_knowledge_text.strip()):
            response_text = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®å†…å®¹ãŒç©ºã§ã™ã€‚ç®¡ç†ç”»é¢ã§åˆ¥ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚"
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¨ˆç®—ã—ã¦ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜ï¼ˆã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ï¼‰
            from modules.token_counter import TokenUsageTracker
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¼šç¤¾IDã‚’å–å¾—ï¼ˆãƒãƒ£ãƒƒãƒˆå±¥æ­´ä¿å­˜ç”¨ï¼‰ 
            from supabase_adapter import select_data
            user_result = select_data("users", filters={"id": message.user_id}) if hasattr(message, 'user_id') and message.user_id else None
            chat_company_id = user_result.data[0].get("company_id") if user_result and user_result.data else None
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‚ç…§æ•°ã‚’è¨ˆç®—ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹æ•°ï¼‰
            error_prompt_references = len(active_sources) if active_sources else 0
            
            # ãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜ï¼ˆæ–°æ–™é‡‘ä½“ç³»ã‚’ä½¿ç”¨ï¼‰
            tracker = TokenUsageTracker(db)
            chat_id = tracker.save_chat_with_prompts(
                user_message=message_text,
                bot_response=response_text,
                user_id=message.user_id,
                prompt_references=error_prompt_references,
                company_id=chat_company_id,
                employee_id=getattr(message, 'employee_id', None),
                employee_name=getattr(message, 'employee_name', None),
                category="è¨­å®šã‚¨ãƒ©ãƒ¼",
                sentiment="neutral",
                model="gemini-pro"
            )
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒã‚ã‚‹å ´åˆã¯è³ªå•ã‚«ã‚¦ãƒ³ãƒˆã‚’æ›´æ–°ï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãŒç©ºã§ã‚‚åˆ©ç”¨åˆ¶é™ã¯æ›´æ–°ã™ã‚‹ï¼‰
            if message.user_id and not limits_check.get("is_unlimited", False):
                safe_print(f"åˆ©ç”¨åˆ¶é™æ›´æ–°é–‹å§‹ï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ç©ºï¼‰ - ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {message.user_id}")
                safe_print(f"æ›´æ–°å‰ã®åˆ¶é™æƒ…å ±: {limits_check}")
                
                updated_limits = update_usage_count(message.user_id, "questions_used", db)
                safe_print(f"æ›´æ–°å¾Œã®åˆ¶é™æƒ…å ±: {updated_limits}")
                
                if updated_limits:
                    remaining_questions = updated_limits["questions_limit"] - updated_limits["questions_used"]
                    limit_reached = remaining_questions <= 0
                    safe_print(f"è¨ˆç®—ã•ã‚ŒãŸæ®‹ã‚Šè³ªå•æ•°: {remaining_questions}, åˆ¶é™åˆ°é”: {limit_reached}")
                else:
                    safe_print("åˆ©ç”¨åˆ¶é™ã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            safe_print(f"è¿”ã‚Šå€¤ï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ç©ºï¼‰: remaining_questions={remaining_questions}, limit_reached={limit_reached}")
            
            return {
                "response": response_text,
                "remaining_questions": remaining_questions,
                "limit_reached": limit_reached
            }
            
        # ç›´è¿‘ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—ï¼ˆæœ€å¤§3ä»¶ã«åˆ¶é™ï¼‰
        recent_messages = []
        try:
            if message.user_id:
                with db.cursor(cursor_factory=RealDictCursor) as cursor:
                    cursor.execute(
                        """
                        SELECT user_message, bot_response
                        FROM chat_history
                        WHERE employee_id = %s
                        ORDER BY timestamp DESC
                        LIMIT 2
                        """,
                        (message.user_id,)
                    )
                    cursor_result = cursor.fetchall()
                    # PostgreSQLã®çµæœã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¦ã‹ã‚‰å¤ã„é †ã«ä¸¦ã¹æ›¿ãˆ
                    recent_messages = list(cursor_result)
                    recent_messages.reverse()
        except Exception as e:
            safe_print(f"ä¼šè©±å±¥æ­´å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            recent_messages = []
        
        # ä¼šè©±å±¥æ­´ã®æ§‹ç¯‰ï¼ˆå„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ¶é™ï¼‰
        conversation_history = ""
        if recent_messages:
            conversation_history = "ç›´è¿‘ã®ä¼šè©±å±¥æ­´ï¼š\n"
            for idx, msg in enumerate(recent_messages):
                
                try:
                    user_msg = msg.get('user_message', '') or ''
                    bot_msg = msg.get('bot_response', '') or ''
                    
                    # å„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’100æ–‡å­—ã«åˆ¶é™ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›ã®ãŸã‚ï¼‰
                    if len(user_msg) > 100:
                        user_msg = user_msg[:100] + "..."
                    if len(bot_msg) > 100:
                        bot_msg = bot_msg[:100] + "..."
                    
                    conversation_history += f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_msg}\n"
                    conversation_history += f"ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {bot_msg}\n\n"
                except Exception as e:
                    # Windowsç’°å¢ƒã§ã®Unicodeæ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œã‚’é¿ã‘ã‚‹ãŸã‚ã€safe_safe_printé–¢æ•°ã‚’ä½¿ç”¨
                    safe_safe_print(f"ä¼šè©±å±¥æ­´å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãã®è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                    continue

        # SpecialæŒ‡ç¤ºã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ ã™ã‚‹ãŸã‚ã®æ–‡å­—åˆ—ã‚’æ§‹ç¯‰
        special_instructions_text = ""
        if special_instructions:
            special_instructions_text = "\n\nç‰¹åˆ¥ãªå›ç­”æŒ‡ç¤ºï¼ˆä»¥ä¸‹ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’å‚ç…§ã™ã‚‹éš›ã¯ã€å„ãƒªã‚½ãƒ¼ã‚¹ã®æŒ‡ç¤ºã«å¾“ã£ã¦ãã ã•ã„ï¼‰ï¼š\n"
            for idx, inst in enumerate(special_instructions, 1):
                special_instructions_text += f"{idx}. ã€{inst['name']}ã€‘: {inst['instruction']}\n"

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ
        prompt = f"""
        ã‚ãªãŸã¯è¦ªåˆ‡ã§ä¸å¯§ãªå¯¾å¿œãŒã§ãã‚‹{current_company_name}ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        ä»¥ä¸‹ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦å¯èƒ½ãªé™ã‚Šå…·ä½“çš„ã§å½¹ç«‹ã¤å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

        åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«: {', '.join(active_resource_names) if active_resource_names else ''}

        å›ç­”ã®éš›ã®æ³¨æ„ç‚¹ï¼š
        1. å¸¸ã«ä¸å¯§ãªè¨€è‘‰é£ã„ã‚’å¿ƒãŒã‘ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦æ•¬æ„ã‚’æŒã£ã¦æ¥ã—ã¦ãã ã•ã„
        2. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«æƒ…å ±ãŒãªã„å ´åˆã§ã‚‚ã€ä¸€èˆ¬çš„ãªæ–‡è„ˆã§å›ç­”ã§ãã‚‹å ´åˆã¯é©åˆ‡ã«å¯¾å¿œã—ã¦ãã ã•ã„
        3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œã‚‚ã£ã¨è©³ã—ãã€ãªã©ã¨è³ªå•ã—ãŸå ´åˆã¯ã€å‰å›ã®å›ç­”å†…å®¹ã«é–¢é€£ã™ã‚‹è©³ç´°æƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚ã€Œã©ã®ã‚ˆã†ãªæƒ…å ±ã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿã€ãªã©ã¨èãè¿”ã•ãªã„ã§ãã ã•ã„ã€‚
        4. å¯èƒ½ãªé™ã‚Šå…·ä½“çš„ã§å®Ÿç”¨çš„ãªæƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„
        5. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«OCRã§æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆï¼ˆPDF (OCR)ã¨è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹éƒ¨åˆ†ï¼‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã‚ŒãŒç”»åƒã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’è€ƒæ…®ã—ã¦ãã ã•ã„
        6. OCRã§æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«ã¯å¤šå°‘ã®èª¤ã‚ŠãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ãŒã€æ–‡è„ˆã‹ã‚‰é©åˆ‡ã«è§£é‡ˆã—ã¦å›ç­”ã—ã¦ãã ã•ã„
        7. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã—ãŸå ´åˆã¯ã€å›ç­”ã®æœ€å¾Œã«ã€Œæƒ…å ±ã‚½ãƒ¼ã‚¹: [ãƒ•ã‚¡ã‚¤ãƒ«å]ã€ã®å½¢å¼ã§å‚ç…§ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
        8. ã€Œã“ã‚“ã«ã¡ã¯ã€ã€ŒãŠã¯ã‚ˆã†ã€ãªã©ã®å˜ç´”ãªæŒ¨æ‹¶ã®ã¿ã®å ´åˆã¯ã€æƒ…å ±ã‚½ãƒ¼ã‚¹ã‚’è¨˜è¼‰ã—ãªã„ã§ãã ã•ã„ã€‚ãã‚Œä»¥å¤–ã®è³ªå•ã«ã¯åŸºæœ¬çš„ã«æƒ…å ±ã‚½ãƒ¼ã‚¹ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
        9. å›ç­”å¯èƒ½ã‹ã©ã†ã‹ãŒåˆ¤æ–­ã§ãã‚‹è³ªå•ã«å¯¾ã—ã¦ã¯ã€æœ€åˆã«ã€Œã¯ã„ã€ã¾ãŸã¯ã€Œã„ã„ãˆã€ã§ç°¡æ½”ã«ç­”ãˆã¦ã‹ã‚‰ã€å…·ä½“çš„ãªèª¬æ˜ã‚„è£œè¶³æƒ…å ±ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„
        10. å›ç­”ã¯**Markdownè¨˜æ³•**ã‚’ä½¿ç”¨ã—ã¦è¦‹ã‚„ã™ãæ•´ç†ã—ã¦ãã ã•ã„ã€‚è¦‹å‡ºã—ï¼ˆ#ã€##ã€###ï¼‰ã€ç®‡æ¡æ›¸ãï¼ˆ-ã€*ï¼‰ã€ç•ªå·ä»˜ããƒªã‚¹ãƒˆï¼ˆ1.ã€2.ï¼‰ã€å¼·èª¿ï¼ˆ**å¤ªå­—**ã€*æ–œä½“*ï¼‰ã€ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆ```ï¼‰ã€è¡¨ï¼ˆ|ï¼‰ã€å¼•ç”¨ï¼ˆ>ï¼‰ãªã©ã‚’é©åˆ‡ã«ä½¿ç”¨ã—ã¦ãã ã•ã„
        11. æ‰‹é †ã‚„èª¬æ˜ãŒè¤‡æ•°ã‚ã‚‹å ´åˆã¯ã€ç•ªå·ä»˜ããƒªã‚¹ãƒˆã‚„ç®‡æ¡æ›¸ãã‚’ä½¿ç”¨ã—ã¦æ§‹é€ åŒ–ã—ã¦ãã ã•ã„
        12. é‡è¦ãªæƒ…å ±ã¯**å¤ªå­—**ã§å¼·èª¿ã—ã¦ãã ã•ã„
        13. ã‚³ãƒ¼ãƒ‰ã‚„ãƒ•ã‚¡ã‚¤ãƒ«åã€è¨­å®šå€¤ãªã©ã¯`ãƒãƒƒã‚¯ã‚¯ã‚©ãƒ¼ãƒˆ`ã§å›²ã‚“ã§ãã ã•ã„{special_instructions_text}
        
        åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿åˆ—ï¼š
        {', '.join(knowledge_base.columns) if knowledge_base and hasattr(knowledge_base, 'columns') and knowledge_base.columns else ""}

        çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å†…å®¹ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ã®ã¿ï¼‰ï¼š
        {active_knowledge_text}

        {f"ç”»åƒæƒ…å ±ï¼šPDFã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸç”»åƒãŒ{len(knowledge_base.images)}æšã‚ã‚Šã¾ã™ã€‚" if knowledge_base and hasattr(knowledge_base, 'images') and knowledge_base.images and isinstance(knowledge_base.images, list) else ""}

        {conversation_history}

        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ï¼š
        {message_text}
        """

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µã‚¤ã‚ºã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾å¿œï¼‰
        MAX_PROMPT_SIZE = 400000  # 40ä¸‡æ–‡å­—åˆ¶é™ï¼ˆAPIåˆ¶é™å¯¾å¿œï¼‰
        if len(prompt) > MAX_PROMPT_SIZE:
            safe_print(f"âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒå¤§ãã™ãã¾ã™ ({len(prompt)} æ–‡å­—)ã€‚çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ã•ã‚‰ã«åˆ¶é™ã—ã¾ã™ã€‚")
            # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ã•ã‚‰ã«åˆ¶é™
            reduced_knowledge_size = MAX_PROMPT_SIZE - (len(prompt) - len(active_knowledge_text)) - 10000
            if reduced_knowledge_size > 0:
                active_knowledge_text = active_knowledge_text[:reduced_knowledge_size] + "\n\n[æ³¨æ„: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µã‚¤ã‚ºåˆ¶é™ã®ãŸã‚ã€çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’çŸ­ç¸®ã—ã¦ã„ã¾ã™]"
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å†æ§‹ç¯‰
                prompt = f"""
        ã‚ãªãŸã¯è¦ªåˆ‡ã§ä¸å¯§ãªå¯¾å¿œãŒã§ãã‚‹{current_company_name}ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        ä»¥ä¸‹ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã£ã¦å¯èƒ½ãªé™ã‚Šå…·ä½“çš„ã§å½¹ç«‹ã¤å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

        åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«: {', '.join(active_resource_names) if active_resource_names else ''}

        å›ç­”ã®éš›ã®æ³¨æ„ç‚¹ï¼š
        1. å¸¸ã«ä¸å¯§ãªè¨€è‘‰é£ã„ã‚’å¿ƒãŒã‘ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦æ•¬æ„ã‚’æŒã£ã¦æ¥ã—ã¦ãã ã•ã„
        2. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«æƒ…å ±ãŒãªã„å ´åˆã§ã‚‚ã€ä¸€èˆ¬çš„ãªæ–‡è„ˆã§å›ç­”ã§ãã‚‹å ´åˆã¯é©åˆ‡ã«å¯¾å¿œã—ã¦ãã ã•ã„
        3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œã‚‚ã£ã¨è©³ã—ãã€ãªã©ã¨è³ªå•ã—ãŸå ´åˆã¯ã€å‰å›ã®å›ç­”å†…å®¹ã«é–¢é€£ã™ã‚‹è©³ç´°æƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚ã€Œã©ã®ã‚ˆã†ãªæƒ…å ±ã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿã€ãªã©ã¨èãè¿”ã•ãªã„ã§ãã ã•ã„ã€‚
        4. å¯èƒ½ãªé™ã‚Šå…·ä½“çš„ã§å®Ÿç”¨çš„ãªæƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„
        5. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«OCRã§æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆï¼ˆPDF (OCR)ã¨è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹éƒ¨åˆ†ï¼‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã‚ŒãŒç”»åƒã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’è€ƒæ…®ã—ã¦ãã ã•ã„
        6. OCRã§æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«ã¯å¤šå°‘ã®èª¤ã‚ŠãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ãŒã€æ–‡è„ˆã‹ã‚‰é©åˆ‡ã«è§£é‡ˆã—ã¦å›ç­”ã—ã¦ãã ã•ã„
        7. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã—ãŸå ´åˆã¯ã€å›ç­”ã®æœ€å¾Œã«ã€Œæƒ…å ±ã‚½ãƒ¼ã‚¹: [ãƒ•ã‚¡ã‚¤ãƒ«å]ã€ã®å½¢å¼ã§å‚ç…§ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
        8. ã€Œã“ã‚“ã«ã¡ã¯ã€ã€ŒãŠã¯ã‚ˆã†ã€ãªã©ã®å˜ç´”ãªæŒ¨æ‹¶ã®ã¿ã®å ´åˆã¯ã€æƒ…å ±ã‚½ãƒ¼ã‚¹ã‚’è¨˜è¼‰ã—ãªã„ã§ãã ã•ã„ã€‚ãã‚Œä»¥å¤–ã®è³ªå•ã«ã¯åŸºæœ¬çš„ã«æƒ…å ±ã‚½ãƒ¼ã‚¹ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
        9. å›ç­”å¯èƒ½ã‹ã©ã†ã‹ãŒåˆ¤æ–­ã§ãã‚‹è³ªå•ã«å¯¾ã—ã¦ã¯ã€æœ€åˆã«ã€Œã¯ã„ã€ã¾ãŸã¯ã€Œã„ã„ãˆã€ã§ç°¡æ½”ã«ç­”ãˆã¦ã‹ã‚‰ã€å…·ä½“çš„ãªèª¬æ˜ã‚„è£œè¶³æƒ…å ±ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„
        10. å›ç­”ã¯**Markdownè¨˜æ³•**ã‚’ä½¿ç”¨ã—ã¦è¦‹ã‚„ã™ãæ•´ç†ã—ã¦ãã ã•ã„ã€‚è¦‹å‡ºã—ï¼ˆ#ã€##ã€###ï¼‰ã€ç®‡æ¡æ›¸ãï¼ˆ-ã€*ï¼‰ã€ç•ªå·ä»˜ããƒªã‚¹ãƒˆï¼ˆ1.ã€2.ï¼‰ã€å¼·èª¿ï¼ˆ**å¤ªå­—**ã€*æ–œä½“*ï¼‰ã€ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆ```ï¼‰ã€è¡¨ï¼ˆ|ï¼‰ã€å¼•ç”¨ï¼ˆ>ï¼‰ãªã©ã‚’é©åˆ‡ã«ä½¿ç”¨ã—ã¦ãã ã•ã„
        11. æ‰‹é †ã‚„èª¬æ˜ãŒè¤‡æ•°ã‚ã‚‹å ´åˆã¯ã€ç•ªå·ä»˜ããƒªã‚¹ãƒˆã‚„ç®‡æ¡æ›¸ãã‚’ä½¿ç”¨ã—ã¦æ§‹é€ åŒ–ã—ã¦ãã ã•ã„
        12. é‡è¦ãªæƒ…å ±ã¯**å¤ªå­—**ã§å¼·èª¿ã—ã¦ãã ã•ã„
        13. ã‚³ãƒ¼ãƒ‰ã‚„ãƒ•ã‚¡ã‚¤ãƒ«åã€è¨­å®šå€¤ãªã©ã¯`ãƒãƒƒã‚¯ã‚¯ã‚©ãƒ¼ãƒˆ`ã§å›²ã‚“ã§ãã ã•ã„{special_instructions_text}
        
        çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å†…å®¹ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ã®ã¿ï¼‰ï¼š
        {active_knowledge_text}

        {conversation_history}

        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ï¼š
        {message_text}
        """
            else:
                safe_print("âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒå¤§ãã™ãã¦åˆ¶é™ã§ãã¾ã›ã‚“")
                return {
                    "response": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãŒå¤§ãã™ãã‚‹ãŸã‚ã€ç¾åœ¨å‡¦ç†ã§ãã¾ã›ã‚“ã€‚ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚",
                    "source": "",
                    "remaining_questions": remaining_questions,
                    "limit_reached": limit_reached
                }

        # Geminiã«ã‚ˆã‚‹å¿œç­”ç”Ÿæˆ
        try:
            safe_print(f"ğŸ¤– Gemini APIå‘¼ã³å‡ºã—é–‹å§‹ - ãƒ¢ãƒ‡ãƒ«: {model}")
            safe_print(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt)} æ–‡å­—")
            
            response = model.generate_content(prompt)
            
            safe_print(f"ğŸ“¨ Gemini APIå¿œç­”å—ä¿¡: {response}")
            
            if not response or not hasattr(response, 'text'):
                safe_print(f"âŒ ç„¡åŠ¹ãªå¿œç­”: response={response}, hasattr(text)={hasattr(response, 'text') if response else 'N/A'}")
                raise ValueError("AIãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®å¿œç­”ãŒç„¡åŠ¹ã§ã™")
            
            response_text = response.text
            safe_print(f"âœ… å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆå–å¾—æˆåŠŸ: {len(response_text)} æ–‡å­—")
            
        except Exception as model_error:
            error_str = str(model_error)
            safe_print(f"âŒ AIãƒ¢ãƒ‡ãƒ«å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {error_str}")
            safe_print(f"ğŸ” ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(model_error)}")
            
            # ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
            import traceback
            safe_print(f"ğŸ“‹ ã‚¨ãƒ©ãƒ¼ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:")
            safe_print(traceback.format_exc())
            
            # ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆã®ç‰¹åˆ¥ãªå‡¦ç†
            if "429" in error_str or "quota" in error_str.lower() or "rate limit" in error_str.lower():
                response_text = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ç¾åœ¨ã€AIã‚µãƒ¼ãƒ“ã‚¹ã®åˆ©ç”¨åˆ¶é™ã«é”ã—ã¦ã„ã¾ã™ã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
                safe_print("â¸ï¸ åˆ©ç”¨åˆ¶é™ã®æ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—: AIãƒ¢ãƒ‡ãƒ«å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: " + error_str)
                
                # ã‚¨ãƒ©ãƒ¼å¿œç­”ã‚’è¿”ã™ï¼ˆåˆ©ç”¨åˆ¶é™ã¯æ›´æ–°ã—ãªã„ï¼‰
                return {
                    "response": response_text,
                    "source": "",
                    "remaining_questions": remaining_questions,
                    "limit_reached": limit_reached
                }
            else:
                response_text = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼è©³ç´°: {error_str[:100]}..."
        
        # ã‚«ãƒ†ã‚´ãƒªã¨æ„Ÿæƒ…ã‚’åˆ†æã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        analysis_prompt = f"""
        ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã¨å›ç­”ã‚’åˆ†æã—ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š
        1. ã‚«ãƒ†ã‚´ãƒª: è³ªå•ã®ã‚«ãƒ†ã‚´ãƒªã‚’1ã¤ã ã‘é¸ã‚“ã§ãã ã•ã„ï¼ˆè¦³å…‰æƒ…å ±ã€äº¤é€šæ¡ˆå†…ã€ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã€é£²é£Ÿåº—ã€ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã€æŒ¨æ‹¶ã€ä¸€èˆ¬çš„ãªä¼šè©±ã€ãã®ä»–ã€æœªåˆ†é¡ï¼‰
        2. æ„Ÿæƒ…: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„Ÿæƒ…ã‚’1ã¤ã ã‘é¸ã‚“ã§ãã ã•ã„ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ã€ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ï¼‰
        3. å‚ç…§ã‚½ãƒ¼ã‚¹: å›ç­”ã«ä½¿ç”¨ã—ãŸä¸»ãªã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’1ã¤é¸ã‚“ã§ãã ã•ã„ã€‚ä»¥ä¸‹ã®ã‚½ãƒ¼ã‚¹æƒ…å ±ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„ï¼š
        {json.dumps(source_info_list, ensure_ascii=False, indent=2)}

        é‡è¦:
        - å‚ç…§ã‚½ãƒ¼ã‚¹ã®é¸æŠã¯ã€å›ç­”ã®å†…å®¹ã¨æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ã‚½ãƒ¼ã‚¹ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚å›ç­”ã®å†…å®¹ãŒç‰¹å®šã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ç›´æ¥å¼•ç”¨ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã®ã‚½ãƒ¼ã‚¹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚
        - ã€Œã“ã‚“ã«ã¡ã¯ã€ã€ŒãŠã¯ã‚ˆã†ã€ãªã©ã®å˜ç´”ãªæŒ¨æ‹¶ã®ã¿ã®å ´åˆã®ã¿ã€ã‚«ãƒ†ã‚´ãƒªã‚’ã€ŒæŒ¨æ‹¶ã€ã«è¨­å®šã—ã€å‚ç…§ã‚½ãƒ¼ã‚¹ã¯ç©ºã«ã—ã¦ãã ã•ã„ã€‚
        - ãã‚Œä»¥å¤–ã®è³ªå•ã«ã¯ã€åŸºæœ¬çš„ã«å‚ç…§ã‚½ãƒ¼ã‚¹ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ã€å¿…ãšé©åˆ‡ãªã‚½ãƒ¼ã‚¹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

        å›ç­”ã¯ä»¥ä¸‹ã®JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ï¼š
        {{
            "category": "ã‚«ãƒ†ã‚´ãƒªå",
            "sentiment": "æ„Ÿæƒ…",
            "source": {{
                "name": "ã‚½ãƒ¼ã‚¹å",
                "section": "ã‚»ã‚¯ã‚·ãƒ§ãƒ³å",
                "page": "ãƒšãƒ¼ã‚¸ç•ªå·"
            }}
        }}

        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ï¼š
        {message_text}

        ç”Ÿæˆã•ã‚ŒãŸå›ç­”ï¼š
        {response_text}
        """
        # åˆ†æã®å®Ÿè¡Œ
        try:
            analysis_response = model.generate_content(analysis_prompt)
            if not analysis_response or not hasattr(analysis_response, 'text'):
                raise ValueError("åˆ†æå¿œç­”ãŒç„¡åŠ¹ã§ã™")
            analysis_text = analysis_response.text
        except Exception as analysis_error:
            error_str = str(analysis_error)
            safe_print(f"åˆ†æå¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {error_str}")
            
            # ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆã§ã‚‚åˆ†æã¯ç¶™ç¶šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰
            if "429" in error_str or "quota" in error_str.lower() or "rate limit" in error_str.lower():
                safe_print("åˆ†æã§ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨")
            
            analysis_text = '{"category": "æœªåˆ†é¡", "sentiment": "neutral", "source": {"name": "", "section": "", "page": ""}}'
        
        # JSONéƒ¨åˆ†ã‚’æŠ½å‡º
        try:
            # JSONã®éƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®ä¸­èº«ã‚’å–å¾—ï¼‰
            json_match = re.search(r'```json\s*(.*?)\s*```', analysis_text, re.DOTALL)
            if json_match:
                analysis_json = json.loads(json_match.group(1))
            else:
                # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãŒãªã„å ´åˆã¯ç›´æ¥ãƒ‘ãƒ¼ã‚¹
                analysis_json = json.loads(analysis_text)
                
            category = analysis_json.get("category", "æœªåˆ†é¡")
            sentiment = analysis_json.get("sentiment", "neutral")
            source_doc = analysis_json.get("source", {}).get("name", "")
            source_page = analysis_json.get("source", {}).get("page", "")

            # å˜ç´”ãªæŒ¨æ‹¶ã®ã¿ã®å ´åˆã¯ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’ã‚¯ãƒªã‚¢
            # message_text = message.text.strip().lower() if message.text else ""
            # greetings = ["ã“ã‚“ã«ã¡ã¯", "ã“ã‚“ã«ã¡ã‚", "ãŠã¯ã‚ˆã†", "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™", "ã“ã‚“ã°ã‚“ã¯", "ã‚ˆã‚ã—ã", "ã‚ã‚ŠãŒã¨ã†", "ã•ã‚ˆã†ãªã‚‰", "hello", "hi", "thanks", "thank you", "bye"]
            
            # if category == "æŒ¨æ‹¶" or any(greeting in message_text for greeting in greetings):
            #     # å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã«ã€Œæƒ…å ±ã‚½ãƒ¼ã‚¹:ã€ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            #     if response_text and "æƒ…å ±ã‚½ãƒ¼ã‚¹:" in response_text:
            #         # æƒ…å ±ã‚½ãƒ¼ã‚¹éƒ¨åˆ†ã‚’å‰Šé™¤
            #         response_text = re.sub(r'\n*æƒ…å ±ã‚½ãƒ¼ã‚¹:.*$', '', response_text, flags=re.DOTALL)
            #     source_doc = ""
            #     source_page = ""
            #     safe_print("2222222222222")
                
        except Exception as json_error:
            safe_print(f"JSONè§£æã‚¨ãƒ©ãƒ¼: {str(json_error)}")
            category = "æœªåˆ†é¡"
            sentiment = "neutral"
            source_doc = ""
            source_page = ""
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¨ˆç®—ã—ã¦ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜
        from modules.token_counter import TokenUsageTracker
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¼šç¤¾IDã‚’å–å¾—ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡ç”¨ï¼‰
        from supabase_adapter import select_data
        user_result = select_data("users", filters={"id": message.user_id}) if message.user_id else None
        final_company_id = user_result.data[0].get("company_id") if user_result and user_result.data else None
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‚ç…§æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹æ•°ï¼‰
        prompt_references = len(active_sources) if active_sources else 0
        
        safe_print(f"ğŸ” ãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡ãƒ‡ãƒãƒƒã‚°:")
        safe_print(f"  ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {message.user_id}")
        safe_print(f"  ä¼šç¤¾ID: {final_company_id}")
        safe_print(f"  ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·: {len(message_text)}")
        safe_print(f"  å¿œç­”é•·: {len(response_text)}")
        safe_print(f"  ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‚ç…§æ•°: {prompt_references}")
        
        # æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜
        try:
            tracker = TokenUsageTracker(db)
            chat_id = tracker.save_chat_with_prompts(
                user_message=message_text,
                bot_response=response_text,
                user_id=message.user_id,
                prompt_references=prompt_references,
                company_id=final_company_id,
                employee_id=message.employee_id,
                employee_name=message.employee_name,
                category=category,
                sentiment=sentiment,
                source_document=source_doc,
                source_page=source_page,
                model="gemini-pro"  # Geminiæ–™é‡‘ä½“ç³»ã‚’ä½¿ç”¨
            )
            safe_print(f"âœ… ãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡ä¿å­˜æˆåŠŸ: {chat_id}")
        except Exception as token_error:
            safe_print(f"âŒ ãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡ã‚¨ãƒ©ãƒ¼: {token_error}")
            # ãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¿å­˜
            chat_id = str(uuid.uuid4())
            cursor = db.cursor()
            cursor.execute(
                "INSERT INTO chat_history (id, user_message, bot_response, timestamp, category, sentiment, employee_id, employee_name, source_document, source_page, user_id, company_id, prompt_references) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)",
                (chat_id, message_text, response_text, datetime.now().isoformat(), category, sentiment, message.employee_id, message.employee_name, source_doc, source_page, message.user_id, company_id, prompt_references)
            )
            db.commit()
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒã‚ã‚‹å ´åˆã¯è³ªå•ã‚«ã‚¦ãƒ³ãƒˆã‚’æ›´æ–°
        if message.user_id and not limits_check.get("is_unlimited", False):
            safe_print(f"åˆ©ç”¨åˆ¶é™æ›´æ–°é–‹å§‹ - ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {message.user_id}")
            safe_print(f"æ›´æ–°å‰ã®åˆ¶é™æƒ…å ±: {limits_check}")
            
            updated_limits = update_usage_count(message.user_id, "questions_used", db)
            safe_print(f"æ›´æ–°å¾Œã®åˆ¶é™æƒ…å ±: {updated_limits}")
            
            if updated_limits:
                remaining_questions = updated_limits["questions_limit"] - updated_limits["questions_used"]
                limit_reached = remaining_questions <= 0
                safe_print(f"è¨ˆç®—ã•ã‚ŒãŸæ®‹ã‚Šè³ªå•æ•°: {remaining_questions}, åˆ¶é™åˆ°é”: {limit_reached}")
            else:
                safe_print("åˆ©ç”¨åˆ¶é™ã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        safe_print(f"è¿”ã‚Šå€¤: remaining_questions={remaining_questions}, limit_reached={limit_reached}")
        
        # ã‚½ãƒ¼ã‚¹æƒ…å ±ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿è¿”ã™ï¼ˆsource_docã¨source_pageãŒç©ºã§ãªã„å ´åˆï¼‰
        source_text = ""
        if source_doc and source_doc.strip():
            source_text = source_doc
            if source_page and str(source_page).strip():
                source_text += f" (P.{source_page})"
        
        safe_print(f"æœ€çµ‚ã‚½ãƒ¼ã‚¹æƒ…å ±: '{source_text}'")
        
        return {
            "response": response_text,
            "source": source_text if source_text and source_text.strip() else "",
            "remaining_questions": remaining_questions,
            "limit_reached": limit_reached
        }
    except Exception as e:
        safe_print(f"ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

def chunk_knowledge_base(text: str, chunk_size: int = 500000) -> list[str]:
    """
    çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æŒ‡å®šã•ã‚ŒãŸã‚µã‚¤ã‚ºã§ãƒãƒ£ãƒ³ã‚¯åŒ–ã™ã‚‹
    
    Args:
        text: ãƒãƒ£ãƒ³ã‚¯åŒ–ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
        chunk_size: ãƒãƒ£ãƒ³ã‚¯ã®ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—æ•°ï¼‰
    
    Returns:
        ãƒãƒ£ãƒ³ã‚¯åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    if not text or len(text) <= chunk_size:
        return [text] if text else []
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        # ãƒãƒ£ãƒ³ã‚¯ã®å¢ƒç•Œã‚’èª¿æ•´ï¼ˆæ–‡ã®é€”ä¸­ã§åˆ‡ã‚Œãªã„ã‚ˆã†ã«ï¼‰
        if end < len(text):
            # æœ€å¾Œã®æ”¹è¡Œã‚’æ¢ã™
            last_newline = text.rfind('\n', start, end)
            if last_newline > start:
                end = last_newline + 1
            else:
                # æ”¹è¡ŒãŒãªã„å ´åˆã¯æœ€å¾Œã®ã‚¹ãƒšãƒ¼ã‚¹ã‚’æ¢ã™
                last_space = text.rfind(' ', start, end)
                if last_space > start:
                    end = last_space + 1
        
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        start = end
    
    return chunks

async def process_chat_chunked(message: ChatMessage, db = Depends(get_db), current_user: dict = None):
    """
    ãƒãƒ£ãƒ³ã‚¯åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ãŸãƒãƒ£ãƒƒãƒˆå‡¦ç†
    çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’50ä¸‡æ–‡å­—ã”ã¨ã«ãƒãƒ£ãƒ³ã‚¯åŒ–ã—ã¦æ®µéšçš„ã«å‡¦ç†
    """
    safe_print(f"ğŸ”„ ãƒãƒ£ãƒ³ã‚¯åŒ–ãƒãƒ£ãƒƒãƒˆå‡¦ç†é–‹å§‹ - ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {message.user_id}")
    
    try:
        # åŸºæœ¬çš„ãªåˆæœŸåŒ–å‡¦ç†
        message_text = message.message if hasattr(message, 'message') else message.text
        remaining_questions = 0
        limit_reached = False
        
        # åˆ©ç”¨åˆ¶é™ãƒã‚§ãƒƒã‚¯
        from .database import get_usage_limits
        limits_check = get_usage_limits(message.user_id, db) if message.user_id else {"is_unlimited": True, "questions_limit": 0, "questions_used": 0}
        safe_print(f"åˆ©ç”¨åˆ¶é™ãƒã‚§ãƒƒã‚¯çµæœ: {limits_check}")
        
        if not limits_check.get("is_unlimited", False):
            remaining_questions = limits_check["questions_limit"] - limits_check["questions_used"]
            limit_reached = remaining_questions <= 0
            
            if limit_reached:
                safe_print(f"âŒ åˆ©ç”¨åˆ¶é™åˆ°é” - æ®‹ã‚Šè³ªå•æ•°: {remaining_questions}")
                return {
                    "response": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚æœ¬æ—¥ã®è³ªå•å›æ•°åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚æ˜æ—¥ã«ãªã‚‹ã¨å†åº¦ã”åˆ©ç”¨ã„ãŸã ã‘ã¾ã™ã€‚",
                    "remaining_questions": 0,
                    "limit_reached": True
                }
        
        # ä¼šç¤¾åã®å–å¾—
        current_company_name = "WorkMate"
        if message.user_id:
            try:
                from supabase_adapter import select_data
                user_result = select_data("users", filters={"id": message.user_id})
                if user_result and user_result.data:
                    company_id = user_result.data[0].get("company_id")
                    if company_id:
                        company_data = get_company_by_id(company_id, db)
                        current_company_name = company_data["name"] if company_data else "WorkMate"
            except Exception as e:
                safe_print(f"ä¼šç¤¾åå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ã®å–å¾—
        active_sources = []
        if message.user_id:
            try:
                from supabase_adapter import select_data
                user_result = select_data("users", filters={"id": message.user_id})
                if user_result and user_result.data:
                    company_id = user_result.data[0].get("company_id")
                    if company_id:
                        active_sources = await get_active_resources_by_company_id(company_id, db)
            except Exception as e:
                safe_print(f"ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        if not active_sources:
            safe_print("âŒ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {
                "response": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç®¡ç†ç”»é¢ã§ãƒªã‚½ãƒ¼ã‚¹ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚",
                "remaining_questions": remaining_questions,
                "limit_reached": limit_reached
            }
        
        # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å†…å®¹ã®å–å¾—
        safe_print(f"ğŸ“š çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å–å¾—é–‹å§‹ - ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚½ãƒ¼ã‚¹: {len(active_sources)}å€‹")
        active_knowledge_text = await get_active_resources_content_by_ids(active_sources, db)
        
        if not active_knowledge_text or not active_knowledge_text.strip():
            safe_print("âŒ çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å†…å®¹ãŒç©ºã§ã™")
            return {
                "response": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®å†…å®¹ãŒç©ºã§ã™ã€‚ç®¡ç†ç”»é¢ã§åˆ¥ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚",
                "remaining_questions": remaining_questions,
                "limit_reached": limit_reached
            }
        
        safe_print(f"ğŸ“Š å–å¾—ã—ãŸçŸ¥è­˜ãƒ™ãƒ¼ã‚¹: {len(active_knowledge_text)} æ–‡å­—")
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ã®æƒ…å ±ã¨SpecialæŒ‡ç¤ºã‚’å–å¾—
        special_instructions = []
        active_resource_names = []
        try:
            from supabase_adapter import select_data
            for source_id in active_sources:
                source_result = select_data("document_sources", columns="name,special", filters={"id": source_id})
                if source_result.data and len(source_result.data) > 0:
                    source_data = source_result.data[0]
                    source_name = source_data.get('name', 'Unknown')
                    active_resource_names.append(source_name)
                    
                    if source_data.get('special') and source_data['special'].strip():
                        special_instructions.append({
                            "name": source_name,
                            "instruction": source_data['special'].strip()
                        })
            safe_print(f"ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹: {len(active_resource_names)}å€‹ - {active_resource_names}")
            safe_print(f"SpecialæŒ‡ç¤º: {len(special_instructions)}å€‹ã®ãƒªã‚½ãƒ¼ã‚¹ã«SpecialæŒ‡ç¤ºãŒã‚ã‚Šã¾ã™")
        except Exception as e:
            safe_print(f"ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            special_instructions = []
            active_resource_names = []

        # SpecialæŒ‡ç¤ºã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ ã™ã‚‹ãŸã‚ã®æ–‡å­—åˆ—ã‚’æ§‹ç¯‰
        special_instructions_text = ""
        if special_instructions:
            special_instructions_text = "\n\nç‰¹åˆ¥ãªå›ç­”æŒ‡ç¤ºï¼ˆä»¥ä¸‹ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’å‚ç…§ã™ã‚‹éš›ã¯ã€å„ãƒªã‚½ãƒ¼ã‚¹ã®æŒ‡ç¤ºã«å¾“ã£ã¦ãã ã•ã„ï¼‰ï¼š\n"
            for idx, inst in enumerate(special_instructions, 1):
                special_instructions_text += f"{idx}. ã€{inst['name']}ã€‘: {inst['instruction']}\n"

        # ğŸ”ª ã¾ãšçŸ¥è­˜ãƒ™ãƒ¼ã‚¹å…¨ä½“ã‚’ãƒãƒ£ãƒ³ã‚¯åŒ–ï¼ˆRAGå‰ã«å®Ÿè¡Œï¼‰
        CHUNK_SIZE = 500000  # 50ä¸‡æ–‡å­—ã§ãƒãƒ£ãƒ³ã‚¯åŒ–
        raw_chunks = chunk_knowledge_base(active_knowledge_text, CHUNK_SIZE)
        safe_print(f"ğŸ”ª ãƒãƒ£ãƒ³ã‚¯åŒ–å®Œäº†: {len(raw_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ (ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {CHUNK_SIZE:,}æ–‡å­—)")
        
        # ä¼šè©±å±¥æ­´ã®å–å¾—
        conversation_history = ""
        try:
            if message.user_id:
                from supabase_adapter import select_data
                chat_history_result = select_data(
                    "chat_history",
                    filters={"employee_id": message.user_id},
                    limit=2
                )
                
                if chat_history_result and chat_history_result.data:
                    recent_messages = list(reversed(chat_history_result.data))
                    
                    if recent_messages:
                        conversation_history = "ç›´è¿‘ã®ä¼šè©±å±¥æ­´ï¼š\n"
                        for msg in recent_messages:
                            user_msg = (msg.get('user_message', '') or '')[:100]
                            bot_msg = (msg.get('bot_response', '') or '')[:100]
                            if len(msg.get('user_message', '')) > 100:
                                user_msg += "..."
                            if len(msg.get('bot_response', '')) > 100:
                                bot_msg += "..."
                            conversation_history += f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_msg}\n"
                            conversation_history += f"ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {bot_msg}\n\n"
        except Exception as e:
            safe_print(f"ä¼šè©±å±¥æ­´å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # ğŸ” æƒ…å ±ç™ºè¦‹ã¾ã§ç¶™ç¶šæ¤œç´¢: è¦‹ã¤ã‹ã£ãŸã‚‰å³åº§ã«çµ‚äº†ã€è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°æœ€å¾Œã¾ã§ç¶™ç¶š
        all_rag_results = []  # RAGæ¤œç´¢çµæœã‚’è“„ç©
        all_chunk_info = []   # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚’è“„ç©
        successful_chunks = 0
        processed_chunks = set()  # å‡¦ç†æ¸ˆã¿ãƒãƒ£ãƒ³ã‚¯ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨˜éŒ²
        BATCH_SIZE = min(25, len(raw_chunks))  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ‹¡å¤§ï¼ˆ15â†’25ï¼‰
        
        safe_print(f"ğŸ” å…¨ãƒ•ã‚¡ã‚¤ãƒ«å…¨ãƒãƒ£ãƒ³ã‚¯å®Œå…¨æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰: åˆè¨ˆ{len(raw_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã€ãƒãƒƒãƒã‚µã‚¤ã‚º{BATCH_SIZE}ã§å‡¦ç†")
        safe_print(f"ğŸ¯ æˆ¦ç•¥: å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’æ¤œç´¢ã—ã¦ã‹ã‚‰æœ€è‰¯ã®çµæœã‚’é¸æŠï¼ˆæ—©æœŸçµ‚äº†ãªã—ï¼‰")
        safe_print(f"ğŸ“š æ¤œç´¢å¯¾è±¡: å…¨{len(active_resource_names)}ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±åˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹")
        
        batch_start = 0
        total_batches = (len(raw_chunks) + BATCH_SIZE - 1) // BATCH_SIZE  # åˆ‡ã‚Šä¸Šã’é™¤ç®—
        current_batch_num = 1
        skipped_batches = 0  # RAGå“è³ªä¸è¶³ã§ã‚¹ã‚­ãƒƒãƒ—ã—ãŸãƒãƒƒãƒæ•°
        
        # ğŸš€ å…¨ãƒãƒƒãƒã®RAGæ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆæ—©æœŸçµ‚äº†ãªã—ï¼‰
        while batch_start < len(raw_chunks):
            # æœªå‡¦ç†ã®ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰æ¬¡ã®ãƒãƒƒãƒã‚’å–å¾—
            available_chunks = [i for i in range(batch_start, min(batch_start + BATCH_SIZE, len(raw_chunks))) 
                              if i not in processed_chunks]
            
            if not available_chunks:
                batch_start += BATCH_SIZE
                current_batch_num += 1
                continue
                
            safe_print(f"ğŸ”„ RAGæ¤œç´¢ãƒãƒƒãƒ ({current_batch_num}/{total_batches}): ãƒãƒ£ãƒ³ã‚¯ {available_chunks[0]+1}-{available_chunks[-1]+1} ({len(available_chunks)}å€‹)")
            safe_print(f"ğŸ“Š RAGå‡¦ç†é€²æ—: {len(processed_chunks)}/{len(raw_chunks)}ãƒãƒ£ãƒ³ã‚¯å®Œäº† ({len(processed_chunks)/len(raw_chunks)*100:.1f}%)")
            
            # è¤‡æ•°ãƒãƒ£ãƒ³ã‚¯ã‚’çµåˆã—ã¦RAGæ¤œç´¢
            combined_chunk = ""
            chunk_info = []
            
            for chunk_idx in available_chunks:
                raw_chunk = raw_chunks[chunk_idx]
                combined_chunk += f"\n\n=== ãƒãƒ£ãƒ³ã‚¯ {chunk_idx+1} ===\n{raw_chunk}"
                chunk_info.append(f"ãƒãƒ£ãƒ³ã‚¯{chunk_idx+1}({len(raw_chunk):,}æ–‡å­—)")
            
            safe_print(f"ğŸ“Š çµåˆãƒãƒ£ãƒ³ã‚¯: {chunk_info}")
            safe_print(f"ğŸ“Š çµåˆã‚µã‚¤ã‚º: {len(combined_chunk):,} æ–‡å­—")
            
            # ğŸ”„ é«˜åº¦ãªRAGæ¤œç´¢ï¼ˆåˆ¶é™ãªã—ï¼‰
            filtered_chunk = None
            rag_attempts = 0
            min_content_threshold = 50  # ã•ã‚‰ã«ç·©å’Œï¼ˆ100â†’50ï¼‰
            
            if len(combined_chunk) > 3000:  # é–¾å€¤ã‚’ã•ã‚‰ã«ç·©å’Œï¼ˆ5000â†’3000ï¼‰
                safe_print(f"ğŸ”„ é«˜åº¦RAGæ¤œç´¢é–‹å§‹: å…¨æˆ¦ç•¥ã‚’è©¦è¡Œ")
                
                # ã‚ˆã‚Šå¤šæ§˜ãªæ¤œç´¢æˆ¦ç•¥ã‚’å®šç¾©
                search_strategies = [
                    # åŸºæœ¬æ¤œç´¢æˆ¦ç•¥
                    {'max_results': min(40, max(20, len(combined_chunk) // 25000)), 'query': message_text, 'name': 'æ¨™æº–æ¤œç´¢'},
                    {'max_results': min(60, max(30, len(combined_chunk) // 20000)), 'query': message_text, 'name': 'æ‹¡å¼µæ¤œç´¢'},
                    {'max_results': min(80, max(40, len(combined_chunk) // 15000)), 'query': expand_query(message_text), 'name': 'ã‚¯ã‚¨ãƒªæ‹¡å¼µæ¤œç´¢'},
                    
                    # é«˜åº¦ãªæ¤œç´¢æˆ¦ç•¥
                    {'max_results': min(100, max(50, len(combined_chunk) // 12000)), 'query': expand_query(message_text), 'name': 'æœ€å¤§ç¯„å›²æ¤œç´¢'},
                    {'max_results': min(120, len(combined_chunk) // 10000), 'query': f"{message_text} OR {expand_query(message_text)}", 'name': 'ORæ¤œç´¢'},
                    {'max_results': min(150, len(combined_chunk) // 8000), 'query': message_text.replace(' ', ' AND '), 'name': 'ANDæ¤œç´¢'},
                    
                    # ç‰¹æ®Šæ¤œç´¢æˆ¦ç•¥
                    {'max_results': min(200, len(combined_chunk) // 6000), 'query': f"({message_text}) OR ({expand_query(message_text)})", 'name': 'é«˜åº¦ORæ¤œç´¢'},
                    {'max_results': min(250, len(combined_chunk) // 5000), 'query': message_text, 'name': 'æœ€å¤§å¯†åº¦æ¤œç´¢'},
                ]
                
                best_result = None
                best_score = 0.0
                best_strategy = None
                
                for strategy in search_strategies:
                    rag_attempts += 1
                    safe_print(f"ğŸ¯ RAGæ¤œç´¢ {rag_attempts}å›ç›®: {strategy['name']}, max_results={strategy['max_results']}")
                    safe_print(f"ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: '{strategy['query'][:50]}{'...' if len(strategy['query']) > 50 else ''}'")
                    
                    current_result = simple_rag_search(combined_chunk, strategy['query'], max_results=strategy['max_results'])
                    current_length = len(current_result.strip())
                    safe_print(f"ğŸ“Š RAGæ¤œç´¢{rag_attempts}å›ç›®çµæœ: {current_length} æ–‡å­—")
                    
                    # å“è³ªè©•ä¾¡ã‚’å®Ÿè¡Œ
                    if current_length >= min_content_threshold:
                        quality_score = _evaluate_rag_quality(current_result, message_text, rag_attempts)
                        safe_print(f"ğŸ¯ RAGå“è³ªã‚¹ã‚³ã‚¢ ({rag_attempts}å›ç›®): {quality_score:.2f}")
                        
                        # ã‚ˆã‚Šè‰¯ã„çµæœãŒã‚ã‚Œã°æ¡ç”¨
                        if quality_score > best_score:
                            best_result = current_result
                            best_score = quality_score
                            best_strategy = strategy['name']
                            safe_print(f"âœ… æ–°ã—ã„æœ€è‰¯çµæœã‚’æ¡ç”¨: {strategy['name']} (ã‚¹ã‚³ã‚¢: {quality_score:.2f})")
                    else:
                        safe_print(f"âš ï¸ çµæœãŒçŸ­ã™ãã¾ã™ ({current_length} < {min_content_threshold})")
                
                # æœ€è‰¯ã®çµæœã‚’æ¡ç”¨
                if best_result:
                    filtered_chunk = best_result
                    safe_print(f"ğŸ† æœ€è‰¯çµæœã‚’æ¡ç”¨: {best_strategy}, ã‚¹ã‚³ã‚¢: {best_score:.2f}, é•·ã•: {len(filtered_chunk)} æ–‡å­—")
                else:
                    # æœ€è‰¯çµæœãŒãªã„å ´åˆã¯æœ€é•·ã®çµæœã‚’æ¡ç”¨
                    longest_result = None
                    longest_length = 0
                    for strategy in search_strategies[:3]:  # åŸºæœ¬æˆ¦ç•¥ã®ã¿å†è©¦è¡Œ
                        result = simple_rag_search(combined_chunk, strategy['query'], max_results=strategy['max_results'])
                        if len(result) > longest_length:
                            longest_result = result
                            longest_length = len(result)
                    
                    if longest_result:
                        filtered_chunk = longest_result
                        safe_print(f"ğŸ“Š æœ€é•·çµæœã‚’æ¡ç”¨: {longest_length} æ–‡å­—")
                    else:
                        filtered_chunk = combined_chunk[:10000]  # æœ€å¾Œã®æ‰‹æ®µ
                        safe_print(f"ğŸ“Š éƒ¨åˆ†çµæœã‚’æ¡ç”¨: {len(filtered_chunk)} æ–‡å­—")
                
                safe_print(f"ğŸ”„ é«˜åº¦RAGæ¤œç´¢å®Œäº†: {rag_attempts}å›è©¦è¡Œã€æœ€çµ‚çµæœ {len(filtered_chunk or '')} æ–‡å­—")
            else:
                filtered_chunk = combined_chunk
                safe_print(f"ğŸ“Š å°ã•ãªãƒãƒƒãƒã®ãŸã‚ RAGæ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            
            # ğŸ¯ å³æ ¼ãªRAGå“è³ªåˆ¤å®š
            rag_quality_score = _evaluate_rag_quality(filtered_chunk, message_text, rag_attempts)
            safe_print(f"ğŸ¯ æœ€çµ‚RAGå“è³ªã‚¹ã‚³ã‚¢: {rag_quality_score:.2f} (é–¾å€¤: 0.30)")
            
            # å“è³ªã‚¹ã‚³ã‚¢ã‚’å³æ ¼åŒ–ï¼ˆ0.10â†’0.30ï¼‰
            if rag_quality_score >= 0.30:
                safe_print(f"âœ… RAGå“è³ªåˆæ ¼ (ã‚¹ã‚³ã‚¢: {rag_quality_score:.2f}) - çµæœã‚’è“„ç©")
                
                # RAGçµæœã‚’è“„ç©ï¼ˆå…¨ã¦å‡¦ç†ã—ã¦ã‹ã‚‰æœ€è‰¯ã‚’é¸æŠï¼‰
                batch_info = f"ãƒãƒƒãƒ {len(available_chunks)}å€‹ ({available_chunks[0]+1}-{available_chunks[-1]+1})"
                rag_info = f"RAGæ¤œç´¢{rag_attempts}å›å®Ÿè¡Œ" if rag_attempts > 0 else "RAGæ¤œç´¢ãªã—"
                
                all_rag_results.append({
                    'content': filtered_chunk,
                    'batch_info': batch_info,
                    'rag_info': rag_info,
                    'quality_score': rag_quality_score,
                    'chunk_indices': available_chunks,
                    'content_length': len(filtered_chunk),
                    'batch_num': current_batch_num
                })
                
                all_chunk_info.extend(chunk_info)
                successful_chunks += len(available_chunks)
                safe_print(f"ğŸ“š RAGçµæœè“„ç©: {len(all_rag_results)}å€‹ç›®ã®ãƒãƒƒãƒã‚’è¿½åŠ ")
            else:
                safe_print(f"âš ï¸ RAGå“è³ªä¸è¶³ (ã‚¹ã‚³ã‚¢: {rag_quality_score:.2f} < 0.30) - ã“ã®ãƒãƒƒãƒã‚’ã‚¹ã‚­ãƒƒãƒ—")
                skipped_batches += 1
            
            # ã“ã®ãƒãƒƒãƒã®ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†æ¸ˆã¿ã«è¿½åŠ 
            for chunk_idx in available_chunks:
                processed_chunks.add(chunk_idx)
            
            # æ¬¡ã®ãƒãƒƒãƒã¸é€²ã‚€ï¼ˆæƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€å¾Œã¾ã§ç¶™ç¶šï¼‰
            batch_start += BATCH_SIZE
            current_batch_num += 1
            
            # ğŸ¯ é‡è¦: æƒ…å ±ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã®ã¿æ—©æœŸçµ‚äº†ã‚’æ¤œè¨
            # ã—ã‹ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã«ã‚ˆã‚Šã€Œè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€å¾Œã¾ã§æ¤œç´¢ã€ã‚’ä¿è¨¼
            if all_rag_results:
                safe_print(f"âœ… æƒ…å ±ç™ºè¦‹: {len(all_rag_results)}å€‹ã®ãƒãƒƒãƒã§æƒ…å ±ã‚’ç™ºè¦‹")
                safe_print(f"ğŸ”„ ç¶™ç¶šæ¤œç´¢: è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã«å‚™ãˆã¦æœ€å¾Œã¾ã§æ¤œç´¢ã‚’ç¶™ç¶š")
                # æ—©æœŸçµ‚äº†ã¯è¡Œã‚ãšã€å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’ç¢ºå®Ÿã«å‡¦ç†
        
        # ğŸ† å…¨ãƒãƒ£ãƒ³ã‚¯å‡¦ç†å®Œäº†å¾Œã€æœ€è‰¯ã®çµæœã‚’é¸æŠ
        final_response = ""
        if all_rag_results:
            safe_print(f"ğŸ† å…¨ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢å®Œäº†ï¼æœ€è‰¯ã®çµæœã‚’é¸æŠ: {len(all_rag_results)}å€‹ã®ãƒãƒƒãƒã‹ã‚‰")
            
            # çµæœã‚’å“è³ªã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
            sorted_results = sorted(all_rag_results, key=lambda x: x['quality_score'], reverse=True)
            
            # ä¸Šä½ã®çµæœã‚’çµ±åˆï¼ˆæœ€å¤§5å€‹ã¾ã§ï¼‰
            top_results = sorted_results[:min(5, len(sorted_results))]
            safe_print(f"ğŸ“Š ä¸Šä½{len(top_results)}å€‹ã®çµæœã‚’çµ±åˆ:")
            for i, result in enumerate(top_results, 1):
                safe_print(f"  {i}. ãƒãƒƒãƒ{result['batch_num']}: ã‚¹ã‚³ã‚¢{result['quality_score']:.2f}, é•·ã•{result['content_length']:,}æ–‡å­—")
            
            # ä¸Šä½çµæœã‚’çµ±åˆ
            combined_rag_content = ""
            total_quality_score = 0
            for i, rag_result in enumerate(top_results, 1):
                combined_rag_content += f"\n\n=== æœ€è‰¯RAGçµæœ {i}/{len(top_results)} ===\n"
                combined_rag_content += f"å‡¦ç†æƒ…å ±: {rag_result['batch_info']}, {rag_result['rag_info']}\n"
                combined_rag_content += f"å“è³ªã‚¹ã‚³ã‚¢: {rag_result['quality_score']:.2f}\n"
                combined_rag_content += f"å†…å®¹:\n{rag_result['content']}"
                total_quality_score += rag_result['quality_score']
            
            average_quality = total_quality_score / len(top_results)
            safe_print(f"ğŸ“Š çµ±åˆRAGçµæœ: {len(combined_rag_content):,}æ–‡å­—, å¹³å‡å“è³ªã‚¹ã‚³ã‚¢: {average_quality:.2f}")
            
            # çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆï¼ˆå…¨ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢å®Œäº†ç‰ˆï¼‰
            unified_prompt = f"""
ã‚ãªãŸã¯è¦ªåˆ‡ã§ä¸å¯§ãªå¯¾å¿œãŒã§ãã‚‹{current_company_name}ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã¯å…¨{len(raw_chunks)}ãƒãƒ£ãƒ³ã‚¯ã®å®Œå…¨æ¤œç´¢ã§ç™ºè¦‹ã•ã‚ŒãŸæœ€è‰¯ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æƒ…å ±ã§ã™ã€‚ã“ã®æƒ…å ±ã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦æœ€ã‚‚å…·ä½“çš„ã§è©³ç´°ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

**é‡è¦ãªæŒ‡ç¤º:**
1. å…¨ãƒ•ã‚¡ã‚¤ãƒ«å…¨ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰é¸ã°ã‚ŒãŸæœ€è‰¯ã®æƒ…å ±ã‚’æ´»ç”¨ã—ã¦ãã ã•ã„
2. è³ªå•ã«ç›´æ¥é–¢é€£ã™ã‚‹æƒ…å ±ã‚’ä¸­å¿ƒã«ã€å…·ä½“çš„ã§è©³ç´°ãªå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„
3. è¤‡æ•°ã®çµæœã‹ã‚‰æœ€ã‚‚é©åˆ‡ãªæƒ…å ±ã‚’çµ±åˆã—ã¦å›ç­”ã—ã¦ãã ã•ã„
4. **å®Ÿéš›ã«çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æœ‰ç”¨ãªæƒ…å ±ã‚’è¦‹ã¤ã‘ã¦å›ç­”ã—ãŸå ´åˆ**ã€å›ç­”ã®æœ€å¾Œã«ã€Œæƒ…å ±ã‚½ãƒ¼ã‚¹: [ãƒ•ã‚¡ã‚¤ãƒ«å]ã€ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„
5. å›ç­”ã¯**Markdownè¨˜æ³•**ã‚’ä½¿ç”¨ã—ã¦è¦‹ã‚„ã™ãæ•´ç†ã—ã¦ãã ã•ã„

æ¤œç´¢çµ±è¨ˆ: 
- å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(active_resource_names)}å€‹ ({', '.join(active_resource_names)})
- æ¤œç´¢ãƒãƒ£ãƒ³ã‚¯: å…¨{len(raw_chunks)}å€‹
- ç™ºè¦‹çµæœ: {len(all_rag_results)}å€‹ã®ãƒãƒƒãƒ
- é¸æŠçµæœ: ä¸Šä½{len(top_results)}å€‹ (å¹³å‡å“è³ªã‚¹ã‚³ã‚¢: {average_quality:.2f}){special_instructions_text}

å…¨ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢ã§ç™ºè¦‹ã•ã‚ŒãŸæœ€è‰¯ã®æƒ…å ±:
{combined_rag_content}

{conversation_history}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ï¼š
{message_text}
"""
            
            # Gemini APIå‘¼ã³å‡ºã—ï¼ˆä¸€åº¦ã ã‘ï¼‰
            try:
                model = setup_gemini()
                
                safe_print(f"ğŸ¤– çµ±åˆGemini APIå‘¼ã³å‡ºã—é–‹å§‹")
                safe_print(f"ğŸ“ çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µã‚¤ã‚º: {len(unified_prompt):,} æ–‡å­—")
                
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§APIå‘¼ã³å‡ºã—
                import time
                start_time = time.time()
                
                response = model.generate_content(unified_prompt)
                
                end_time = time.time()
                elapsed_time = end_time - start_time
                safe_print(f"ğŸ“¨ çµ±åˆAPIå¿œç­”å—ä¿¡ (å‡¦ç†æ™‚é–“: {elapsed_time:.2f}ç§’)")
                
                if response and hasattr(response, 'text'):
                    if response.text and response.text.strip():
                        final_response = response.text.strip()
                        safe_print(f"ğŸ“ çµ±åˆå¿œç­”ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(final_response)} æ–‡å­—")
                        safe_print(f"ğŸ“ çµ±åˆå¿œç­”å†…å®¹ï¼ˆæœ€åˆã®100æ–‡å­—ï¼‰: {final_response[:100]}...")
                    else:
                        safe_print(f"âš ï¸ çµ±åˆå¿œç­”ã§ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆ")
                        final_response = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
                else:
                    safe_print(f"âš ï¸ çµ±åˆå¿œç­”ã§ç„¡åŠ¹ãªå¿œç­”ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ")
                    final_response = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
                    
            except Exception as e:
                safe_print(f"âŒ çµ±åˆGemini APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                safe_print(f"ğŸ” ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")
                import traceback
                safe_print(f"ğŸ” ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")
                final_response = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        else:
            # RAGçµæœãŒå…¨ããªã„å ´åˆ
            safe_print(f"âŒ å…¨ã¦ã®ãƒãƒƒãƒã§RAGå“è³ªä¸è¶³ã®ãŸã‚ã€æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            final_response = f"""ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å…¨{len(raw_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’æ¤œç´¢ã„ãŸã—ã¾ã—ãŸãŒã€ã”è³ªå•ã«å¯¾ã™ã‚‹é©åˆ‡ãªå›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚

ğŸ” **æ¤œç´¢çµæœ**:
- æ¤œç´¢å¯¾è±¡: {len(raw_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯
- å‡¦ç†å®Œäº†: {len(processed_chunks)}å€‹ (100%)
- RAGå“è³ªåˆæ ¼: {len(all_rag_results)}å€‹
- ã‚¹ã‚­ãƒƒãƒ—: {skipped_batches}å€‹ï¼ˆå“è³ªä¸è¶³ï¼‰

åˆ¥ã®è³ªå•æ–¹æ³•ã§ãŠè©¦ã—ã„ãŸã ãã‹ã€ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚"""
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‚ç…§æ•°ã‚’è¨ˆç®—ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹æ•°ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‚ç…§æ•°ã¨ã—ã¦ä½¿ç”¨ï¼‰
        prompt_references = len(active_sources)
        safe_print(f"ğŸ’° ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‚ç…§æ•°: {prompt_references} (ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹æ•°)")
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ä¿å­˜
        try:
            from modules.token_counter import TokenUsageTracker
            from supabase_adapter import select_data
            
            user_result = select_data("users", filters={"id": message.user_id}) if message.user_id else None
            chat_company_id = user_result.data[0].get("company_id") if user_result and user_result.data else None
            
            tracker = TokenUsageTracker(db)
            chat_id = tracker.save_chat_with_prompts(
                user_message=message_text,
                bot_response=final_response,
                user_id=message.user_id,
                prompt_references=prompt_references,
                company_id=chat_company_id,
                employee_id=message.employee_id,
                employee_name=message.employee_name,
                category="ãƒãƒ£ãƒ³ã‚¯å‡¦ç†",
                sentiment="neutral",
                model="gemini-pro"
            )
            safe_print(f"ğŸ’¾ ãƒãƒ£ãƒƒãƒˆå±¥æ­´ä¿å­˜å®Œäº† - ID: {chat_id}, ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‚ç…§: {prompt_references}")
        except Exception as e:
            safe_print(f"ãƒãƒ£ãƒƒãƒˆå±¥æ­´ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # åˆ©ç”¨åˆ¶é™ã®æ›´æ–°
        if message.user_id and not limits_check.get("is_unlimited", False):
            try:
                from .database import update_usage_count
                updated_limits = update_usage_count(message.user_id, "questions_used", db)
                if updated_limits:
                    remaining_questions = updated_limits["questions_limit"] - updated_limits["questions_used"]
                    limit_reached = remaining_questions <= 0
                    safe_print(f"ğŸ“Š åˆ©ç”¨åˆ¶é™æ›´æ–°å®Œäº† - æ®‹ã‚Š: {remaining_questions}")
            except Exception as e:
                safe_print(f"åˆ©ç”¨åˆ¶é™æ›´æ–°ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        processing_rate = (len(processed_chunks) / len(raw_chunks) * 100) if raw_chunks else 0
        success_rate = (successful_chunks / len(raw_chunks) * 100) if raw_chunks else 0
        
        safe_print(f"ğŸ” æƒ…å ±ç™ºè¦‹ã¾ã§ç¶™ç¶šæ¤œç´¢å‡¦ç†å®Œäº†")
        safe_print(f"ğŸ“Š å‡¦ç†çµ±è¨ˆ: å…¨{len(raw_chunks)}ãƒãƒ£ãƒ³ã‚¯ä¸­ {len(processed_chunks)}ãƒãƒ£ãƒ³ã‚¯å‡¦ç†æ¸ˆã¿ ({processing_rate:.1f}%)")
        safe_print(f"ğŸ“Š æˆåŠŸçµ±è¨ˆ: {successful_chunks}ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰æœ‰åŠ¹å›ç­”å–å¾— ({success_rate:.1f}%)")
        safe_print(f"ğŸ“ RAGçµæœè“„ç©: {len(all_rag_results)}å€‹ã®ãƒãƒƒãƒ")
        safe_print(f"ğŸ¤– Geminiå‘¼ã³å‡ºã—: 1å›ã®ã¿ (æƒ…å ±ç™ºè¦‹æ™‚å³åº§é€ä¿¡)")
        safe_print(f"âš¡ åŠ¹ç‡åŒ–: {skipped_batches}ãƒãƒƒãƒã‚’RAGå“è³ªåˆ¤å®šã§ã‚¹ã‚­ãƒƒãƒ— ({skipped_batches/total_batches*100:.1f}%å‰Šæ¸›)")
        
        # æƒ…å ±ç™ºè¦‹ã¾ã§ç¶™ç¶šæ¤œç´¢ã®çµæœã‚’è©³ç´°ã«å ±å‘Š
        if all_rag_results:
            safe_print(f"ğŸ‰ æƒ…å ±ç™ºè¦‹æˆåŠŸ: {len(all_rag_results)}å€‹ã®ãƒãƒƒãƒã§æƒ…å ±ã‚’ç™ºè¦‹ã—ã€å³åº§ã«Geminiã«é€ä¿¡")
            safe_print(f"âœ… åŠ¹ç‡çš„çµ‚äº†: æƒ…å ±ç™ºè¦‹å¾Œã¯æ®‹ã‚Š{len(raw_chunks) - len(processed_chunks)}ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        elif len(processed_chunks) == len(raw_chunks):
            safe_print(f"ğŸ” å®Œå…¨æ¤œç´¢å®Œäº†: å…¨{len(raw_chunks)}ãƒãƒ£ãƒ³ã‚¯ã‚’æ¢ç´¢ã—ãŸãŒã€è©²å½“ã™ã‚‹æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        else:
            safe_print(f"âš ï¸ ä¸å®Œå…¨ãªå‡¦ç†: {len(raw_chunks) - len(processed_chunks)}ãƒãƒ£ãƒ³ã‚¯ãŒæœªå‡¦ç†")
        
        # ã‚½ãƒ¼ã‚¹æƒ…å ±ã®æŠ½å‡ºï¼ˆå›ç­”ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡ºï¼‰
        source_text = ""
        if final_response and active_resource_names:
            # å›ç­”ã‹ã‚‰ã€Œæƒ…å ±ã‚½ãƒ¼ã‚¹:ã€éƒ¨åˆ†ã‚’æŠ½å‡º
            import re
            source_match = re.search(r'æƒ…å ±ã‚½ãƒ¼ã‚¹[:ï¼š]\s*([^\n]+)', final_response)
            if source_match:
                # æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®å›ç­”ã«ã¯æƒ…å ±ã‚½ãƒ¼ã‚¹ã‚’å«ã‚ãªã„
                no_info_in_response = any(phrase in final_response.lower() for phrase in [
                    "æƒ…å ±ã¯å«ã¾ã‚Œã¦ãŠã‚Šã¾ã›ã‚“ã§ã—ãŸ",
                    "æƒ…å ±ãŒå«ã¾ã‚Œã¦ãŠã‚Šã¾ã›ã‚“ã§ã—ãŸ", 
                    "ã«é–¢ã™ã‚‹æƒ…å ±ã¯å«ã¾ã‚Œã¦ãŠã‚Šã¾ã›ã‚“",
                    "è©²å½“ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
                ])
                
                if not no_info_in_response:
                    source_text = source_match.group(1).strip()
                
                # æƒ…å ±ã‚½ãƒ¼ã‚¹éƒ¨åˆ†ã‚’å›ç­”ã‹ã‚‰å‰Šé™¤
                final_response = re.sub(r'\n*æƒ…å ±ã‚½ãƒ¼ã‚¹[:ï¼š][^\n]*', '', final_response).strip()
        
        # ç„¡åŠ¹ãªã‚½ãƒ¼ã‚¹æƒ…å ±ã¯ç©ºæ–‡å­—åˆ—ã«ã™ã‚‹
        invalid_sources = ['ãªã—', 'ãƒ‡ãƒãƒƒã‚°', 'debug', 'æƒ…å ±ãªã—', 'è©²å½“ãªã—', 'ä¸æ˜', 'unknown', 'null', 'undefined']
        if source_text.lower() in [s.lower() for s in invalid_sources] or 'ãƒ‡ãƒãƒƒã‚°' in source_text or 'debug' in source_text.lower():
            source_text = ""
        
        safe_print(f"ğŸ“„ æœ€çµ‚ã‚½ãƒ¼ã‚¹æƒ…å ±: '{source_text}'")
        
        return {
            "response": final_response,
            "source": source_text,
            "remaining_questions": remaining_questions,
            "limit_reached": limit_reached,
            "chunks_processed": len(raw_chunks),
            "successful_chunks": successful_chunks
        }
        
    except Exception as e:
        safe_print(f"âŒ ãƒãƒ£ãƒ³ã‚¯åŒ–å‡¦ç†ã§é‡å¤§ã‚¨ãƒ©ãƒ¼: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
        try:
            remaining_questions = remaining_questions if 'remaining_questions' in locals() else 0
            limit_reached = limit_reached if 'limit_reached' in locals() else False
        except:
            remaining_questions = 0
            limit_reached = False
            
        return {
            "response": f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            "source": "",
            "remaining_questions": remaining_questions,
            "limit_reached": limit_reached
        }

async def lightning_rag_search(knowledge_text: str, query: str, max_results: int = 20) -> str:
    """
    é›·é€ŸRAGæ¤œç´¢ - æœ€é«˜é€Ÿåº¦ã‚’é‡è¦–ã—ãŸæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
    - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
    - äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    - å¤§ããªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹é«˜é€ŸåŒ–
    """
    if not SPEED_RAG_AVAILABLE:
        safe_print("é«˜é€ŸRAGãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€å¾“æ¥ã®RAGã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        return simple_rag_search(knowledge_text, query, max_results)
    
    if not knowledge_text or not query:
        return knowledge_text
    
    try:
        safe_print(f"âš¡ é›·é€ŸRAGæ¤œç´¢é–‹å§‹: {len(knowledge_text):,}æ–‡å­—, ã‚¯ã‚¨ãƒª: {query[:30]}...")
        
        # é«˜é€Ÿæ¤œç´¢å®Ÿè¡Œ
        result = await high_speed_rag.lightning_search(
            query=query,
            knowledge_text=knowledge_text,
            max_results=max_results
        )
        
        if result:
            safe_print(f"âš¡ é›·é€ŸRAGæ¤œç´¢å®Œäº†: {len(result):,}æ–‡å­—ã®é–¢é€£æƒ…å ±ã‚’æŠ½å‡º")
            return result
        else:
            safe_print("âš ï¸ é›·é€ŸRAGæ¤œç´¢ã§çµæœãŒè¦‹ã¤ã‹ã‚‰ãšã€å¾“æ¥ã®RAGã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            return simple_rag_search(knowledge_text, query, max_results)
    
    except Exception as e:
        safe_print(f"âŒ é›·é€ŸRAGæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å¾“æ¥ã®RAGã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return simple_rag_search(knowledge_text, query, max_results)

async def enhanced_rag_search(knowledge_text: str, query: str, max_results: int = 20) -> str:
    """
    å¼·åŒ–ã•ã‚ŒãŸRAGæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
    - ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªãƒãƒ£ãƒ³ã‚¯åŒ–
    - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆBM25 + ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ï¼‰
    - åå¾©æ¤œç´¢ã«ã‚ˆã‚‹é«˜ç²¾åº¦æ¤œç´¢
    """
    if not RAG_ENHANCED_AVAILABLE:
        safe_print("å¼·åŒ–RAGãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€å¾“æ¥ã®RAGã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        return simple_rag_search(knowledge_text, query, max_results)
    
    if not knowledge_text or not query:
        return knowledge_text
    
    try:
        safe_print(f"ğŸš€ å¼·åŒ–RAGæ¤œç´¢é–‹å§‹: {len(knowledge_text):,}æ–‡å­—, ã‚¯ã‚¨ãƒª: {query[:50]}...")
        
        # åå¾©æ¤œç´¢ã«ã‚ˆã‚‹é«˜ç²¾åº¦æ¤œç´¢
        result = await enhanced_rag.iterative_search(
            query=query,
            knowledge_text=knowledge_text,
            max_iterations=3,
            min_results=5
        )
        
        if result:
            safe_print(f"âœ… å¼·åŒ–RAGæ¤œç´¢å®Œäº†: {len(result):,}æ–‡å­—ã®é–¢é€£æƒ…å ±ã‚’æŠ½å‡º")
            return result
        else:
            safe_print("âš ï¸ å¼·åŒ–RAGæ¤œç´¢ã§çµæœãŒè¦‹ã¤ã‹ã‚‰ãšã€å¾“æ¥ã®RAGã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            return simple_rag_search(knowledge_text, query, max_results)
    
    except Exception as e:
        safe_print(f"âŒ å¼·åŒ–RAGæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å¾“æ¥ã®RAGã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return simple_rag_search(knowledge_text, query, max_results)

def adaptive_rag_search(knowledge_text: str, query: str, max_results: int = 10) -> str:
    """
    é©å¿œçš„RAGæ¤œç´¢ - çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®ã‚µã‚¤ã‚ºã«å¿œã˜ã¦æœ€é©ãªæ¤œç´¢æ‰‹æ³•ã‚’é¸æŠ
    """
    if not knowledge_text or not query:
        return knowledge_text
    
    text_length = len(knowledge_text)
    safe_print(f"ğŸ“Š é©å¿œçš„RAGæ¤œç´¢: ãƒ†ã‚­ã‚¹ãƒˆé•· {text_length:,}æ–‡å­—")
    
    # å°ã•ãªãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯å…¨ä½“ã‚’è¿”ã™
    if text_length <= 10000:
        safe_print("ğŸ“ å°ã•ãªãƒ†ã‚­ã‚¹ãƒˆã®ãŸã‚å…¨ä½“ã‚’è¿”å´")
        return knowledge_text
    
    # ä¸­ç¨‹åº¦ã®ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯å¾“æ¥ã®RAG
    elif text_length <= 100000:
        safe_print("ğŸ¯ ä¸­ç¨‹åº¦ã®ãƒ†ã‚­ã‚¹ãƒˆã®ãŸã‚å¾“æ¥ã®RAGæ¤œç´¢ã‚’å®Ÿè¡Œ")
        return simple_rag_search(knowledge_text, query, max_results)
    
    # å¤§ããªãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯å¼·åŒ–RAGï¼ˆéåŒæœŸå‡¦ç†ãŒå¿…è¦ãªãŸã‚ã€ã“ã“ã§ã¯å¾“æ¥ã®RAGã‚’ä½¿ç”¨ï¼‰
    else:
        safe_print("ğŸš€ å¤§ããªãƒ†ã‚­ã‚¹ãƒˆã®ãŸã‚é«˜æ€§èƒ½RAGæ¤œç´¢ã‚’å®Ÿè¡Œ")
        # æ®µè½æ•°ã‚’å¢—ã‚„ã—ã¦ç²¾åº¦å‘ä¸Š
        return simple_rag_search(knowledge_text, query, max_results * 2)

def multi_pass_rag_search(knowledge_text: str, query: str, max_results: int = 15) -> str:
    """
    å¤šæ®µéšRAGæ¤œç´¢ - è¤‡æ•°ã®æ¤œç´¢æˆ¦ç•¥ã‚’çµ„ã¿åˆã‚ã›ã¦ç²¾åº¦ã‚’å‘ä¸Š
    """
    if not knowledge_text or not query:
        return knowledge_text
    
    try:
        safe_print(f"ğŸ”„ å¤šæ®µéšRAGæ¤œç´¢é–‹å§‹: {len(knowledge_text):,}æ–‡å­—")
        
        # ç¬¬1æ®µéš: åºƒã„æ¤œç´¢
        broad_results = simple_rag_search(knowledge_text, query, max_results * 3)
        
        # ç¬¬2æ®µéš: ã‚¯ã‚¨ãƒªã‚’æ‹¡å¼µã—ã¦å†æ¤œç´¢
        expanded_query = expand_query(query)
        if expanded_query != query:
            safe_print(f"ğŸ” ã‚¯ã‚¨ãƒªã‚’æ‹¡å¼µ: '{query}' â†’ '{expanded_query}'")
            expanded_results = simple_rag_search(knowledge_text, expanded_query, max_results * 2)
            
            # çµæœã‚’ãƒãƒ¼ã‚¸
            combined_text = f"{broad_results}\n\n{'='*50}\n\n{expanded_results}"
            
            # ç¬¬3æ®µéš: é‡è¤‡ã‚’é™¤å»ã—ã¦æœ€çµ‚èª¿æ•´
            final_results = simple_rag_search(combined_text, query, max_results)
        else:
            final_results = broad_results
        
        safe_print(f"âœ… å¤šæ®µéšRAGæ¤œç´¢å®Œäº†: {len(final_results):,}æ–‡å­—")
        return final_results
        
    except Exception as e:
        safe_print(f"âŒ å¤šæ®µéšRAGæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return simple_rag_search(knowledge_text, query, max_results)

def expand_query(query: str) -> str:
    """
    ã‚¯ã‚¨ãƒªæ‹¡å¼µ - é¡ç¾©èªã‚„é–¢é€£ç”¨èªã‚’è¿½åŠ ã—ã¦æ¤œç´¢ç²¾åº¦ã‚’å‘ä¸Š
    """
    # åŸºæœ¬çš„ãªã‚¯ã‚¨ãƒªæ‹¡å¼µã®ãƒãƒƒãƒ”ãƒ³ã‚°
    expansion_map = {
        'æ–¹æ³•': ['æ‰‹é †', 'ã‚„ã‚Šæ–¹', 'ãƒ—ãƒ­ã‚»ã‚¹', 'æµã‚Œ'],
        'æ‰‹é †': ['æ–¹æ³•', 'ã‚¹ãƒ†ãƒƒãƒ—', 'ãƒ—ãƒ­ã‚»ã‚¹', 'æµã‚Œ'],
        'å•é¡Œ': ['èª²é¡Œ', 'ãƒˆãƒ©ãƒ–ãƒ«', 'ã‚¨ãƒ©ãƒ¼', 'ä¸å…·åˆ'],
        'è¨­å®š': ['æ§‹æˆ', 'ã‚³ãƒ³ãƒ•ã‚£ã‚°', 'è¨­å®šå€¤', 'ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—'],
        'ä½¿ã„æ–¹': ['åˆ©ç”¨æ–¹æ³•', 'æ“ä½œæ–¹æ³•', 'ä½¿ç”¨æ–¹æ³•', 'æ“ä½œæ‰‹é †'],
        'ã‚¨ãƒ©ãƒ¼': ['å•é¡Œ', 'ãƒˆãƒ©ãƒ–ãƒ«', 'ä¸å…·åˆ', 'ãƒã‚°'],
        'æ–™é‡‘': ['ä¾¡æ ¼', 'è²»ç”¨', 'ã‚³ã‚¹ãƒˆ', 'å€¤æ®µ'],
        'æ©Ÿèƒ½': ['ç‰¹å¾´', 'ä»•æ§˜', 'æ€§èƒ½', 'èƒ½åŠ›'],
    }
    
    expanded_terms = []
    query_words = query.split()
    
    for word in query_words:
        expanded_terms.append(word)
        if word in expansion_map:
            # 1ã¤ã®é¡ç¾©èªã‚’è¿½åŠ ï¼ˆã‚¯ã‚¨ãƒªãŒé•·ããªã‚Šã™ããªã„ã‚ˆã†ã«ï¼‰
            expanded_terms.append(expansion_map[word][0])
    
    expanded_query = ' '.join(expanded_terms)
    return expanded_query if len(expanded_query) <= len(query) * 2 else query
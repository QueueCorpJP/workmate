"""
åŒ…æ‹¬çš„æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  (Comprehensive Search System)
PDFã®å¾ŒåŠã‚„ä½é »åº¦ãªé‡è¦æƒ…å ±ã‚‚ç¢ºå®Ÿã«å–å¾—ã™ã‚‹ãŸã‚ã®é«˜åº¦ãªæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 

ä¸»ãªæ©Ÿèƒ½:
1. å‹•çš„LIMITèª¿æ•´ - æ¤œç´¢å“è³ªã«å¿œã˜ã¦çµæœæ•°ã‚’èª¿æ•´
2. å¤šæ®µéšæ¤œç´¢ - åºƒç¯„å›²æ¤œç´¢ â†’ é‡è¦åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
3. ä½ç½®ãƒã‚¤ã‚¢ã‚¹è£œæ­£ - ãƒãƒ£ãƒ³ã‚¯ä½ç½®ã«ã‚ˆã‚‹åã‚Šã‚’ä¿®æ­£
4. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå¤šæ§˜æ€§ç¢ºä¿ - åŒã˜æ–‡æ›¸ã‹ã‚‰è¤‡æ•°ã®é–¢é€£ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
5. ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯å†ãƒ©ãƒ³ã‚­ãƒ³ã‚° - æ„å‘³çš„é–¢é€£æ€§ã§å†è©•ä¾¡
"""

import asyncio
import logging
import math
from typing import List, Dict, Any, Optional, Tuple, Set
from dataclasses import dataclass
from collections import defaultdict
from supabase_adapter import execute_query
from .enhanced_postgresql_search import enhanced_search_chunks
from .advanced_fuzzy_search import AdvancedFuzzySearchSystem
from .vector_search import get_vector_search_instance

logger = logging.getLogger(__name__)

@dataclass
class ComprehensiveSearchResult:
    """åŒ…æ‹¬çš„æ¤œç´¢çµæœ"""
    chunk_id: str
    content: str
    document_name: str
    document_id: str
    chunk_index: int
    relevance_score: float
    position_bias_score: float
    final_score: float
    search_methods: List[str]
    document_coverage: float  # æ–‡æ›¸å†…ã§ã®ä½ç½®ï¼ˆ0.0-1.0ï¼‰
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'id': self.chunk_id,
            'content': self.content,
            'document_name': self.document_name,
            'score': self.final_score,
            'chunk_index': self.chunk_index,
            'search_methods': self.search_methods,
            'document_coverage': self.document_coverage,
            'metadata': {
                'relevance_score': self.relevance_score,
                'position_bias_score': self.position_bias_score,
                'document_id': self.document_id
            }
        }

class ComprehensiveSearchSystem:
    """åŒ…æ‹¬çš„æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.advanced_fuzzy = AdvancedFuzzySearchSystem()
        self.vector_search = get_vector_search_instance()
        
    async def initialize(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            await self.advanced_fuzzy.initialize()
            logger.info("âœ… åŒ…æ‹¬çš„æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            return True
        except Exception as e:
            logger.error(f"âŒ åŒ…æ‹¬çš„æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def comprehensive_search(self,
                                 query: str,
                                 company_id: str = None,
                                 initial_limit: int = 50,
                                 final_limit: int = 15,
                                 min_document_diversity: int = 3) -> List[ComprehensiveSearchResult]:
        """
        åŒ…æ‹¬çš„æ¤œç´¢ã®å®Ÿè¡Œ
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            company_id: ä¼šç¤¾ID
            initial_limit: åˆæœŸæ¤œç´¢ã§ã®çµæœæ•°ä¸Šé™
            final_limit: æœ€çµ‚çš„ã«è¿”ã™çµæœæ•°
            min_document_diversity: æœ€å°æ–‡æ›¸å¤šæ§˜æ€§ï¼ˆç•°ãªã‚‹æ–‡æ›¸ã‹ã‚‰ã®çµæœæ•°ï¼‰
        """
        try:
            logger.info(f"ğŸ” åŒ…æ‹¬çš„æ¤œç´¢é–‹å§‹: '{query}' (åˆæœŸé™ç•Œ: {initial_limit}, æœ€çµ‚: {final_limit})")
            
            # 1. å¤šæ®µéšæ¤œç´¢ã®å®Ÿè¡Œ
            all_results = await self._execute_multi_stage_search(query, company_id, initial_limit)
            
            if not all_results:
                logger.warning("âŒ æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return []
            
            # 2. æ–‡æ›¸åˆ¥ãƒãƒ£ãƒ³ã‚¯åˆ†æ
            document_analysis = self._analyze_document_chunks(all_results)
            
            # 3. ä½ç½®ãƒã‚¤ã‚¢ã‚¹è£œæ­£
            corrected_results = self._apply_position_bias_correction(all_results, document_analysis)
            
            # 4. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå¤šæ§˜æ€§ç¢ºä¿
            diverse_results = self._ensure_document_diversity(
                corrected_results, 
                min_document_diversity, 
                final_limit
            )
            
            # 5. æœ€çµ‚ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°
            final_results = self._final_ranking(diverse_results, query)[:final_limit]
            
            logger.info(f"âœ… åŒ…æ‹¬çš„æ¤œç´¢å®Œäº†: {len(final_results)}ä»¶ã®çµæœ")
            self._log_search_summary(final_results, document_analysis)
            
            return final_results
            
        except Exception as e:
            logger.error(f"âŒ åŒ…æ‹¬çš„æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    async def _execute_multi_stage_search(self,
                                        query: str,
                                        company_id: str,
                                        limit: int) -> List[Dict[str, Any]]:
        """å¤šæ®µéšæ¤œç´¢ã®å®Ÿè¡Œ"""
        all_results = []
        search_methods = []
        
        try:
            # Stage 1: Enhanced PostgreSQL Searchï¼ˆé«˜ã„é–¾å€¤ã§åºƒç¯„å›²æ¤œç´¢ï¼‰
            logger.info("ğŸ“Š Stage 1: Enhanced PostgreSQL åºƒç¯„å›²æ¤œç´¢")
            enhanced_results = await enhanced_search_chunks(
                query, company_id, limit=limit*2, threshold=0.1
            )
            all_results.extend(self._normalize_search_results(enhanced_results, "enhanced_postgresql"))
            search_methods.append(f"enhanced_postgresql({len(enhanced_results)})")
            
            # Stage 2: Advanced Fuzzy Searchï¼ˆé–¾å€¤ã‚’ä¸‹ã’ã¦ç¶²ç¾…çš„ã«ï¼‰
            logger.info("ğŸ“Š Stage 2: Advanced Fuzzy ç¶²ç¾…çš„æ¤œç´¢")
            fuzzy_results = await self.advanced_fuzzy.advanced_fuzzy_search(
                query, company_id, threshold=0.3, limit=limit
            )
            fuzzy_normalized = [r.to_dict() for r in fuzzy_results]
            all_results.extend(self._normalize_search_results(fuzzy_normalized, "advanced_fuzzy"))
            search_methods.append(f"advanced_fuzzy({len(fuzzy_results)})")
            
            # Stage 3: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼ˆã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é¡ä¼¼æ€§ï¼‰
            if self.vector_search:
                logger.info("ğŸ“Š Stage 3: Vector ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢")
                try:
                    vector_results = self.vector_search.vector_similarity_search(
                        query, company_id, limit=limit//2
                    )
                    all_results.extend(self._normalize_search_results(vector_results, "vector_semantic"))
                    search_methods.append(f"vector_semantic({len(vector_results)})")
                except Exception as e:
                    logger.warning(f"âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            
            # Stage 4: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†å‰²æ¤œç´¢ï¼ˆé‡è¦èªå¥ã§ã®å€‹åˆ¥æ¤œç´¢ï¼‰
            logger.info("ğŸ“Š Stage 4: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†å‰²æ¤œç´¢")
            keyword_results = await self._keyword_based_search(query, company_id, limit//3)
            all_results.extend(keyword_results)
            search_methods.append(f"keyword_split({len(keyword_results)})")
            
            logger.info(f"å¤šæ®µéšæ¤œç´¢å®Œäº†: ç·çµæœæ•° {len(all_results)}, æ‰‹æ³•: {search_methods}")
            return all_results
            
        except Exception as e:
            logger.error(f"å¤šæ®µéšæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return all_results
    
    def _normalize_search_results(self, results: List[Dict], search_method: str) -> List[Dict[str, Any]]:
        """æ¤œç´¢çµæœã®æ­£è¦åŒ–"""
        normalized = []
        for result in results:
            normalized_result = {
                'chunk_id': str(result.get('id', result.get('chunk_id', ''))),
                'content': result.get('content', ''),
                'document_name': result.get('document_name', result.get('file_name', 'Unknown')),
                'document_id': str(result.get('doc_id', result.get('document_id', ''))),
                'chunk_index': int(result.get('chunk_index', 0)),
                'score': float(result.get('score', result.get('final_score', 0))),
                'search_method': search_method,
                'metadata': result.get('metadata', {})
            }
            normalized.append(normalized_result)
        return normalized
    
    async def _keyword_based_search(self, query: str, company_id: str, limit: int) -> List[Dict[str, Any]]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†å‰²ã«åŸºã¥ãæ¤œç´¢"""
        results = []
        
        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        keywords = self._extract_important_keywords(query)
        
        for keyword in keywords[:3]:  # ä¸Šä½3ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            try:
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å˜ä½“ã§ã®æ¤œç´¢
                keyword_results = await enhanced_search_chunks(
                    keyword, company_id, limit=limit//3, threshold=0.15
                )
                
                # ã‚¹ã‚³ã‚¢èª¿æ•´ï¼ˆåˆ†å‰²æ¤œç´¢ã®ãŸã‚å°‘ã—ä¸‹ã’ã‚‹ï¼‰
                for result in keyword_results:
                    if 'score' in result:
                        result['score'] *= 0.8
                
                normalized = self._normalize_search_results(keyword_results, f"keyword_{keyword}")
                results.extend(normalized)
                
            except Exception as e:
                logger.warning(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({keyword}): {e}")
        
        return results
    
    def _extract_important_keywords(self, query: str) -> List[str]:
        """é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æŠ½å‡º"""
        # åŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆä»Šå¾Œã‚ˆã‚Šé«˜åº¦ãªæ‰‹æ³•ã«ç½®ãæ›ãˆå¯èƒ½ï¼‰
        import re
        
        keywords = []
        
        # æ¼¢å­—ãƒ»ã‚«ã‚¿ã‚«ãƒŠã®é€£ç¶šï¼ˆ2æ–‡å­—ä»¥ä¸Šï¼‰
        kanji_katakana = re.findall(r'[ä¸€-é¾ ã€…ã€†ã€¤ã‚¡-ãƒ¶ãƒ¼]{2,}', query)
        keywords.extend(kanji_katakana)
        
        # è‹±æ•°å­—ã®é€£ç¶šï¼ˆ2æ–‡å­—ä»¥ä¸Šï¼‰
        alphanumeric = re.findall(r'[a-zA-Z0-9]{2,}', query)
        keywords.extend(alphanumeric)
        
        # é‡è¤‡é™¤å»ãƒ»é•·ã•é †ã‚½ãƒ¼ãƒˆ
        unique_keywords = list(set(keywords))
        unique_keywords.sort(key=len, reverse=True)
        
        return unique_keywords
    
    def _analyze_document_chunks(self, results: List[Dict[str, Any]]) -> Dict[str, Dict]:
        """æ–‡æ›¸åˆ¥ãƒãƒ£ãƒ³ã‚¯åˆ†æ"""
        analysis = defaultdict(lambda: {
            'chunks': [],
            'max_chunk_index': 0,
            'min_chunk_index': float('inf'),
            'total_chunks': 0,
            'coverage': 0.0
        })
        
        # æ–‡æ›¸ã”ã¨ã®æƒ…å ±é›†è¨ˆ
        for result in results:
            doc_id = result.get('document_id', 'unknown')
            chunk_index = result.get('chunk_index', 0)
            
            analysis[doc_id]['chunks'].append(result)
            analysis[doc_id]['max_chunk_index'] = max(
                analysis[doc_id]['max_chunk_index'], chunk_index
            )
            analysis[doc_id]['min_chunk_index'] = min(
                analysis[doc_id]['min_chunk_index'], chunk_index
            )
        
        # å„æ–‡æ›¸ã®ç·ãƒãƒ£ãƒ³ã‚¯æ•°ã‚’å–å¾—ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ï¼‰
        for doc_id in analysis.keys():
            try:
                chunk_count_sql = f"""
                    SELECT MAX(chunk_index) + 1 as total_chunks
                    FROM chunks 
                    WHERE doc_id = '{doc_id}'
                """
                result = execute_query(chunk_count_sql)
                if result and len(result) > 0:
                    analysis[doc_id]['total_chunks'] = result[0].get('total_chunks', 1)
                else:
                    analysis[doc_id]['total_chunks'] = analysis[doc_id]['max_chunk_index'] + 1
                
                # ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—
                covered_range = analysis[doc_id]['max_chunk_index'] - analysis[doc_id]['min_chunk_index'] + 1
                analysis[doc_id]['coverage'] = covered_range / analysis[doc_id]['total_chunks']
                
            except Exception as e:
                logger.warning(f"æ–‡æ›¸åˆ†æã‚¨ãƒ©ãƒ¼ ({doc_id}): {e}")
                analysis[doc_id]['total_chunks'] = analysis[doc_id]['max_chunk_index'] + 1
                analysis[doc_id]['coverage'] = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        return dict(analysis)
    
    def _apply_position_bias_correction(self, 
                                      results: List[Dict[str, Any]], 
                                      doc_analysis: Dict) -> List[ComprehensiveSearchResult]:
        """ä½ç½®ãƒã‚¤ã‚¢ã‚¹è£œæ­£ã®é©ç”¨"""
        corrected_results = []
        
        for result in results:
            doc_id = result.get('document_id', 'unknown')
            chunk_index = result.get('chunk_index', 0)
            original_score = result.get('score', 0)
            
            # æ–‡æ›¸å†…ã§ã®ä½ç½®è¨ˆç®—ï¼ˆ0.0-1.0ï¼‰
            doc_info = doc_analysis.get(doc_id, {})
            total_chunks = doc_info.get('total_chunks', 1)
            document_coverage = chunk_index / max(total_chunks - 1, 1)
            
            # ä½ç½®ãƒã‚¤ã‚¢ã‚¹è£œæ­£ã‚¹ã‚³ã‚¢è¨ˆç®—
            position_bias_score = self._calculate_position_bias_correction(
                chunk_index, total_chunks, document_coverage
            )
            
            # æœ€çµ‚ã‚¹ã‚³ã‚¢è¨ˆç®—
            final_score = original_score + position_bias_score
            
            corrected_result = ComprehensiveSearchResult(
                chunk_id=result.get('chunk_id', ''),
                content=result.get('content', ''),
                document_name=result.get('document_name', 'Unknown'),
                document_id=doc_id,
                chunk_index=chunk_index,
                relevance_score=original_score,
                position_bias_score=position_bias_score,
                final_score=final_score,
                search_methods=[result.get('search_method', 'unknown')],
                document_coverage=document_coverage
            )
            
            corrected_results.append(corrected_result)
        
        return corrected_results
    
    def _calculate_position_bias_correction(self, 
                                          chunk_index: int, 
                                          total_chunks: int, 
                                          coverage: float) -> float:
        """ä½ç½®ãƒã‚¤ã‚¢ã‚¹è£œæ­£å€¤ã®è¨ˆç®—"""
        if total_chunks <= 1:
            return 0.0
        
        # å¾ŒåŠãƒãƒ£ãƒ³ã‚¯ã«ãƒœãƒ¼ãƒŠã‚¹ã‚’ä»˜ä¸ï¼ˆå‰åŠåé‡ã‚’è£œæ­£ï¼‰
        if coverage > 0.7:  # æ–‡æ›¸ã®å¾ŒåŠ70%ä»¥é™
            position_bonus = 0.15 * (coverage - 0.7) / 0.3  # æœ€å¤§0.15ã®ãƒœãƒ¼ãƒŠã‚¹
        elif coverage > 0.5:  # ä¸­ç›¤
            position_bonus = 0.05 * (coverage - 0.5) / 0.2  # æœ€å¤§0.05ã®ãƒœãƒ¼ãƒŠã‚¹
        else:  # å‰åŠ
            position_bonus = 0.0
        
        # æ–‡æ›¸ã®åˆ†æ•£å…·åˆã«å¿œã˜ãŸèª¿æ•´
        if total_chunks > 10:  # é•·æ–‡æ›¸ã®å ´åˆ
            position_bonus *= 1.5  # ã‚ˆã‚Šå¼·ã„è£œæ­£
        
        return position_bonus
    
    def _ensure_document_diversity(self, 
                                 results: List[ComprehensiveSearchResult],
                                 min_diversity: int,
                                 final_limit: int) -> List[ComprehensiveSearchResult]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå¤šæ§˜æ€§ã®ç¢ºä¿"""
        # æ–‡æ›¸åˆ¥ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        doc_groups = defaultdict(list)
        for result in results:
            doc_groups[result.document_id].append(result)
        
        # å„æ–‡æ›¸ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§ã‚¹ã‚³ã‚¢é †ã‚½ãƒ¼ãƒˆ
        for doc_id in doc_groups:
            doc_groups[doc_id].sort(key=lambda x: x.final_score, reverse=True)
        
        # å¤šæ§˜æ€§ç¢ºä¿ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
        diverse_results = []
        used_documents = set()
        
        # Phase 1: å„æ–‡æ›¸ã‹ã‚‰æœ€ä½1ã¤ãšã¤å–å¾—
        for doc_id, group in doc_groups.items():
            if len(diverse_results) < final_limit and group:
                diverse_results.append(group[0])
                used_documents.add(doc_id)
        
        # Phase 2: æ®‹ã‚Šæ ã‚’é«˜ã‚¹ã‚³ã‚¢é †ã§åŸ‹ã‚ã‚‹ï¼ˆãŸã ã—åŒã˜æ–‡æ›¸ã‹ã‚‰3ã¤ã¾ã§ï¼‰
        doc_counts = defaultdict(int)
        all_remaining = []
        
        for doc_id, group in doc_groups.items():
            for result in group[1:]:  # æœ€åˆã®1ã¤ã¯æ—¢ã«è¿½åŠ æ¸ˆã¿
                all_remaining.append(result)
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        all_remaining.sort(key=lambda x: x.final_score, reverse=True)
        
        for result in all_remaining:
            if len(diverse_results) >= final_limit:
                break
            
            doc_id = result.document_id
            if doc_counts[doc_id] < 2:  # åŒã˜æ–‡æ›¸ã‹ã‚‰æœ€å¤§3ã¤ã¾ã§ï¼ˆæœ€åˆã®1ã¤ï¼‹è¿½åŠ 2ã¤ï¼‰
                diverse_results.append(result)
                doc_counts[doc_id] += 1
        
        logger.info(f"å¤šæ§˜æ€§ç¢ºä¿å®Œäº†: {len(diverse_results)}ä»¶, {len(used_documents)}æ–‡æ›¸ã‹ã‚‰å–å¾—")
        return diverse_results
    
    def _final_ranking(self, 
                      results: List[ComprehensiveSearchResult], 
                      query: str) -> List[ComprehensiveSearchResult]:
        """æœ€çµ‚ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        # ã‚¯ã‚¨ãƒªã¨ã®æ„å‘³çš„é–¢é€£æ€§ã‚’å†è©•ä¾¡
        for result in results:
            semantic_bonus = self._calculate_semantic_relevance(result.content, query)
            result.final_score += semantic_bonus
        
        # æœ€çµ‚ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x.final_score, reverse=True)
        
        return results
    
    def _calculate_semantic_relevance(self, content: str, query: str) -> float:
        """æ„å‘³çš„é–¢é€£æ€§ã®è¨ˆç®—"""
        # ç°¡æ˜“ç‰ˆ: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å…±èµ·é »åº¦ã«åŸºã¥ãè¨ˆç®—
        # å°†æ¥çš„ã«ã¯ã‚ˆã‚Šé«˜åº¦ãªã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†æã«ç½®ãæ›ãˆå¯èƒ½
        
        query_words = set(query.lower().split())
        content_words = set(content.lower().split())
        
        if not query_words or not content_words:
            return 0.0
        
        # ã‚¸ãƒ£ã‚«ãƒ¼ãƒ‰ä¿‚æ•°ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦
        intersection = len(query_words & content_words)
        union = len(query_words | content_words)
        
        if union == 0:
            return 0.0
        
        jaccard = intersection / union
        semantic_bonus = jaccard * 0.1  # æœ€å¤§0.1ã®ãƒœãƒ¼ãƒŠã‚¹
        
        return semantic_bonus
    
    def _log_search_summary(self, 
                          results: List[ComprehensiveSearchResult], 
                          doc_analysis: Dict):
        """æ¤œç´¢ã‚µãƒãƒªãƒ¼ã®ãƒ­ã‚°å‡ºåŠ›"""
        if not results:
            return
        
        logger.info("ğŸ“Š åŒ…æ‹¬çš„æ¤œç´¢ã‚µãƒãƒªãƒ¼:")
        logger.info(f"  ç·çµæœæ•°: {len(results)}")
        
        # æ–‡æ›¸åˆ¥åˆ†å¸ƒ
        doc_distribution = defaultdict(int)
        for result in results:
            doc_distribution[result.document_name] += 1
        
        logger.info("  æ–‡æ›¸åˆ¥åˆ†å¸ƒ:")
        for doc_name, count in sorted(doc_distribution.items(), key=lambda x: x[1], reverse=True):
            logger.info(f"    {doc_name}: {count}ä»¶")
        
        # ãƒãƒ£ãƒ³ã‚¯ä½ç½®åˆ†å¸ƒ
        position_ranges = {"å‰åŠ(0-33%)": 0, "ä¸­ç›¤(33-66%)": 0, "å¾ŒåŠ(66-100%)": 0}
        for result in results:
            coverage = result.document_coverage
            if coverage <= 0.33:
                position_ranges["å‰åŠ(0-33%)"] += 1
            elif coverage <= 0.66:
                position_ranges["ä¸­ç›¤(33-66%)"] += 1
            else:
                position_ranges["å¾ŒåŠ(66-100%)"] += 1
        
        logger.info("  ä½ç½®åˆ†å¸ƒ:")
        for range_name, count in position_ranges.items():
            logger.info(f"    {range_name}: {count}ä»¶")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
comprehensive_search_system = ComprehensiveSearchSystem()

async def initialize_comprehensive_search():
    """åŒ…æ‹¬çš„æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
    return await comprehensive_search_system.initialize()

async def comprehensive_search(query: str, 
                             company_id: str = None,
                             initial_limit: int = 50,
                             final_limit: int = 15) -> List[Dict[str, Any]]:
    """åŒ…æ‹¬çš„æ¤œç´¢ã®å®Ÿè¡Œï¼ˆå¤–éƒ¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰"""
    results = await comprehensive_search_system.comprehensive_search(
        query, company_id, initial_limit, final_limit
    )
    return [result.to_dict() for result in results] 
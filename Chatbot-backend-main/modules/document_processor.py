"""
ğŸ“¤ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
ğŸ§© ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆ300ã€œ500 tokenï¼‰
ğŸ§  embeddingç”Ÿæˆã‚’çµ±åˆï¼ˆGemini Flash - 3072æ¬¡å…ƒï¼‰
ğŸ—ƒ Supabaseä¿å­˜ï¼ˆdocument_sources + chunksï¼‰

å®Œå…¨ãªRAGå¯¾å¿œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
"""

import os
import uuid
import logging
import asyncio
import tempfile
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import re
import tiktoken
from fastapi import HTTPException, UploadFile
try:
    from google import genai
except ImportError:
    import google.generativeai as genai
import psycopg2
from psycopg2.extras import execute_values
from .multi_api_embedding import get_multi_api_embedding_client, multi_api_embedding_available

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DocumentProcessor:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.gemini_client = None
        self.multi_api_client = None
        self.embedding_model = os.getenv("EMBEDDING_MODEL", "gemini-embedding-001")
        
        # Gemini APIä½¿ç”¨æ™‚ã¯models/ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 
        if not self.embedding_model.startswith("models/"):
            self.embedding_model = f"models/{self.embedding_model}"
            
        self.chunk_size_tokens = 400  # 300-500ãƒˆãƒ¼ã‚¯ãƒ³ã®ä¸­é–“å€¤
        self.chunk_overlap_tokens = 50  # ãƒãƒ£ãƒ³ã‚¯é–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
        self.max_chunk_size_chars = 2000  # æ–‡å­—æ•°ã§ã®ä¸Šé™
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼åˆæœŸåŒ–
        try:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
        except Exception as e:
            logger.warning(f"tiktokenåˆæœŸåŒ–å¤±æ•—: {e}")
            self.tokenizer = None
        
        # è¤‡æ•°APIå¯¾å¿œã‚’æœ€å„ªå…ˆã€æ¬¡ã«Gemini API
        if multi_api_embedding_available():
            self.multi_api_client = get_multi_api_embedding_client()
            logger.info("âœ… è¤‡æ•°APIå¯¾å¿œã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½¿ç”¨")
        else:
            self._init_gemini_client()
    
    
    def _init_gemini_client(self):
        """Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ï¼ˆæ–°ã—ã„SDKï¼‰"""
        if self.gemini_client is None:
            api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
            if not api_key:
                raise ValueError("GOOGLE_API_KEY ã¾ãŸã¯ GEMINI_API_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
            # æ–°ã—ã„SDKã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
            self.gemini_client = genai.Client(api_key=api_key)
            logger.info(f"ğŸ§  Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†ï¼ˆæ–°SDKï¼‰: {self.embedding_model}")
    
    def _count_tokens(self, text: str) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        if self.tokenizer:
            try:
                return len(self.tokenizer.encode(text))
            except Exception as e:
                logger.warning(f"ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ã®æ¨å®šï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
        # æ—¥æœ¬èª: 1æ–‡å­— â‰ˆ 1.5ãƒˆãƒ¼ã‚¯ãƒ³, è‹±èª: 4æ–‡å­— â‰ˆ 1ãƒˆãƒ¼ã‚¯ãƒ³
        japanese_chars = len(re.findall(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', text))
        other_chars = len(text) - japanese_chars
        estimated_tokens = int(japanese_chars * 1.5 + other_chars * 0.25)
        return estimated_tokens
    
    def _split_text_into_chunks(self, text: str, doc_name: str = "") -> List[Dict[str, Any]]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’æ„å‘³å˜ä½ã§ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        300-500ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¯„å›²ã§èª¿æ•´
        """
        if not text or not text.strip():
            logger.warning(f"ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆãŒæ¸¡ã•ã‚Œã¾ã—ãŸ: {doc_name}")
            return []
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’æ®µè½å˜ä½ã§åˆ†å‰²
        paragraphs = re.split(r'\n\s*\n', text.strip())
        chunks = []
        current_chunk = ""
        current_tokens = 0
        chunk_index = 0
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            paragraph_tokens = self._count_tokens(paragraph)
            
            # æ®µè½ãŒå˜ä½“ã§å¤§ãã™ãã‚‹å ´åˆã¯æ–‡å˜ä½ã§åˆ†å‰²
            if paragraph_tokens > self.chunk_size_tokens:
                # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
                if current_chunk:
                    chunks.append({
                        "chunk_index": chunk_index,
                        "content": current_chunk.strip(),
                        "token_count": current_tokens
                    })
                    chunk_index += 1
                    current_chunk = ""
                    current_tokens = 0
                
                # å¤§ããªæ®µè½ã‚’æ–‡å˜ä½ã§åˆ†å‰²
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\.\!\?]\s*', paragraph)
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    
                    sentence_tokens = self._count_tokens(sentence)
                    
                    # æ–‡ãŒå˜ä½“ã§å¤§ãã™ãã‚‹å ´åˆã¯å¼·åˆ¶åˆ†å‰²
                    if sentence_tokens > self.chunk_size_tokens:
                        if current_chunk:
                            chunks.append({
                                "chunk_index": chunk_index,
                                "content": current_chunk.strip(),
                                "token_count": current_tokens
                            })
                            chunk_index += 1
                            current_chunk = ""
                            current_tokens = 0
                        
                        # é•·ã„æ–‡ã‚’æ–‡å­—æ•°ã§å¼·åˆ¶åˆ†å‰²
                        for i in range(0, len(sentence), self.max_chunk_size_chars):
                            chunk_part = sentence[i:i + self.max_chunk_size_chars]
                            chunks.append({
                                "chunk_index": chunk_index,
                                "content": chunk_part,
                                "token_count": self._count_tokens(chunk_part)
                            })
                            chunk_index += 1
                    else:
                        # é€šå¸¸ã®æ–‡å‡¦ç†
                        if current_tokens + sentence_tokens > self.chunk_size_tokens:
                            if current_chunk:
                                chunks.append({
                                    "chunk_index": chunk_index,
                                    "content": current_chunk.strip(),
                                    "token_count": current_tokens
                                })
                                chunk_index += 1
                            current_chunk = sentence
                            current_tokens = sentence_tokens
                        else:
                            current_chunk += ("ã€‚" if current_chunk else "") + sentence
                            current_tokens += sentence_tokens
            else:
                # é€šå¸¸ã®æ®µè½å‡¦ç†
                if current_tokens + paragraph_tokens > self.chunk_size_tokens:
                    if current_chunk:
                        chunks.append({
                            "chunk_index": chunk_index,
                            "content": current_chunk.strip(),
                            "token_count": current_tokens
                        })
                        chunk_index += 1
                    current_chunk = paragraph
                    current_tokens = paragraph_tokens
                else:
                    current_chunk += ("\n\n" if current_chunk else "") + paragraph
                    current_tokens += paragraph_tokens
        
        # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ 
        if current_chunk and current_chunk.strip():
            chunks.append({
                "chunk_index": chunk_index,
                "content": current_chunk.strip(),
                "token_count": current_tokens
            })
        
        logger.info(f"ğŸ“„ {doc_name}: {len(text)}æ–‡å­— â†’ {len(chunks)}ãƒãƒ£ãƒ³ã‚¯")
        
        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®çµ±è¨ˆã‚’å‡ºåŠ›
        if chunks:
            token_counts = [chunk["token_count"] for chunk in chunks]
            avg_tokens = sum(token_counts) / len(token_counts)
            min_tokens = min(token_counts)
            max_tokens = max(token_counts)
            logger.info(f"ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³çµ±è¨ˆ - å¹³å‡: {avg_tokens:.1f}, æœ€å°: {min_tokens}, æœ€å¤§: {max_tokens}")
        
        return chunks
    
    async def _generate_embeddings_multi_api(self, texts: List[str], failed_indices: List[int] = None) -> List[Optional[List[float]]]:
        """è¤‡æ•°APIå¯¾å¿œã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ãƒ†ã‚­ã‚¹ãƒˆã®embeddingã‚’ç”Ÿæˆ"""
        if not self.multi_api_client:
            raise ValueError("è¤‡æ•°APIå¯¾å¿œã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        all_embeddings = []
        failed_embeddings = []
        
        # å‡¦ç†å¯¾è±¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®š
        if failed_indices is None:
            process_indices = list(range(len(texts)))
            all_embeddings = [None] * len(texts)
        else:
            process_indices = failed_indices
            all_embeddings = [None] * len(texts)
        
        # è¤‡æ•°APIå¯¾å¿œã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§å€‹åˆ¥å‡¦ç†
        for idx, i in enumerate(process_indices):
            try:
                text = texts[i]
                if not text or not text.strip():
                    logger.warning(f"âš ï¸ ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i}")
                    all_embeddings[i] = None
                    failed_embeddings.append(i)
                    continue
                
                # è¤‡æ•°APIå¯¾å¿œã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§embeddingç”Ÿæˆ
                embedding_vector = await self.multi_api_client.generate_embedding(text.strip())
                
                expected_dims = (
                    self.multi_api_client.expected_dimensions if self.multi_api_client else 3072
                )

                if embedding_vector and len(embedding_vector) == expected_dims:
                    all_embeddings[i] = embedding_vector
                    logger.debug(f"âœ… è¤‡æ•°API embeddingç”ŸæˆæˆåŠŸ: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i} ({len(embedding_vector)}æ¬¡å…ƒ)")
                else:
                    all_embeddings[i] = None
                    failed_embeddings.append(i)
                    logger.warning(f"âš ï¸ è¤‡æ•°API embeddingç”Ÿæˆå¤±æ•—: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i}")
                
                # APIåˆ¶é™å¯¾ç­–ï¼šå°‘ã—å¾…æ©Ÿ
                if idx < len(process_indices) - 1:
                    await asyncio.sleep(0.1)
                    
            except Exception as e:
                logger.error(f"âŒ è¤‡æ•°API embeddingç”Ÿæˆã‚¨ãƒ©ãƒ¼: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i} - {e}")
                all_embeddings[i] = None
                failed_embeddings.append(i)
        
        # çµæœã®çµ±è¨ˆã‚’å‡ºåŠ›
        success_count = len(process_indices) - len(failed_embeddings)
        logger.info(f"ğŸ“Š è¤‡æ•°API embeddingç”Ÿæˆå®Œäº†: {success_count}/{len(process_indices)} æˆåŠŸ")
        
        if failed_embeddings:
            logger.warning(f"âš ï¸ è¤‡æ•°API embeddingç”Ÿæˆå¤±æ•—: {len(failed_embeddings)}ä»¶")
        
        return all_embeddings
    
    async def _generate_embeddings_batch(self, texts: List[str], failed_indices: List[int] = None) -> List[Optional[List[float]]]:
        """è¤‡æ•°APIå¯¾å¿œã€Vertex AI ã¾ãŸã¯ Gemini APIã§ãƒ†ã‚­ã‚¹ãƒˆã®embeddingã‚’å€‹åˆ¥ç”Ÿæˆï¼ˆãƒãƒƒãƒå‡¦ç†é¢¨ï¼‰"""
        if failed_indices is None:
            logger.info(f"ğŸ§  embeddingç”Ÿæˆé–‹å§‹: {len(texts)}ä»¶, ãƒ¢ãƒ‡ãƒ«={self.embedding_model}")
        else:
            logger.info(f"ğŸ”„ embeddingå†ç”Ÿæˆé–‹å§‹: {len(failed_indices)}ä»¶ã®å¤±æ•—åˆ†, ãƒ¢ãƒ‡ãƒ«={self.embedding_model}")
        
        try:
            # è¤‡æ•°APIå¯¾å¿œã‚’æœ€å„ªå…ˆã§ä½¿ç”¨
            if self.multi_api_client:
                return await self._generate_embeddings_multi_api(texts, failed_indices)
            elif self.use_vertex_ai:
                return await self._generate_embeddings_vertex_ai(texts, failed_indices)
            else:
                return await self._generate_embeddings_gemini_api(texts, failed_indices)
                
        except Exception as e:
            logger.error(f"âŒ embeddingãƒãƒƒãƒç”Ÿæˆä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {e}", exc_info=True)
            raise
    
    async def _generate_embeddings_vertex_ai(self, texts: List[str], failed_indices: List[int] = None) -> List[Optional[List[float]]]:
        """Vertex AI ã§ãƒ†ã‚­ã‚¹ãƒˆã®embeddingã‚’ç”Ÿæˆ"""
        if not self.vertex_ai_client:
            raise ValueError("Vertex AI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        all_embeddings = []
        failed_embeddings = []
        
        # å‡¦ç†å¯¾è±¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®š
        if failed_indices is None:
            process_indices = list(range(len(texts)))
            all_embeddings = [None] * len(texts)
        else:
            process_indices = failed_indices
            all_embeddings = [None] * len(texts)
        
        # Vertex AI ã¯å€‹åˆ¥å‡¦ç†
        for idx, i in enumerate(process_indices):
            try:
                text = texts[i]
                if not text or not text.strip():
                    logger.warning(f"âš ï¸ ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i}")
                    all_embeddings[i] = None
                    failed_embeddings.append(i)
                    continue
                
                # Vertex AI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§embeddingç”Ÿæˆ
                embedding_vector = await asyncio.to_thread(
                    self.vertex_ai_client.generate_embedding,
                    text.strip()
                )
                
                if embedding_vector:
                    all_embeddings[i] = embedding_vector
                    logger.info(f"âœ… embeddingç”ŸæˆæˆåŠŸ: {idx + 1}/{len(process_indices)} (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i}, æ¬¡å…ƒ: {len(embedding_vector)})")
                else:
                    logger.warning(f"âš ï¸ embeddingç”Ÿæˆå¤±æ•—: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i}")
                    all_embeddings[i] = None
                    failed_embeddings.append(i)
                
                # APIåˆ¶é™å¯¾ç­–
                await asyncio.sleep(0.1)
                
            except Exception as e:
                logger.error(f"âŒ embeddingç”Ÿæˆã‚¨ãƒ©ãƒ¼ (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i}): {e}")
                all_embeddings[i] = None
                failed_embeddings.append(i)
        
        success_count = len([e for e in all_embeddings if e is not None])
        total_count = len(texts)
        
        if failed_indices is None:
            logger.info(f"ğŸ‰ Vertex AI embeddingç”Ÿæˆå®Œäº†: {success_count}/{total_count} æˆåŠŸ")
        else:
            logger.info(f"ğŸ‰ Vertex AI embeddingå†ç”Ÿæˆå®Œäº†: {success_count - (total_count - len(failed_indices))}/{len(failed_indices)} æˆåŠŸ")
        
        if failed_embeddings:
            logger.warning(f"âš ï¸ å¤±æ•—ã—ãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {failed_embeddings}")
        
        return all_embeddings
    
    async def _generate_embeddings_gemini_api(self, texts: List[str], failed_indices: List[int] = None) -> List[Optional[List[float]]]:
        """Gemini API ã§ãƒ†ã‚­ã‚¹ãƒˆã®embeddingã‚’ç”Ÿæˆ"""
        self._init_gemini_client()
        
        all_embeddings = []
        failed_embeddings = []
        
        # å‡¦ç†å¯¾è±¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®š
        if failed_indices is None:
            process_indices = list(range(len(texts)))
            all_embeddings = [None] * len(texts)
        else:
            process_indices = failed_indices
            all_embeddings = [None] * len(texts)
        
        # Gemini APIã¯å€‹åˆ¥å‡¦ç†ãŒæ¨å¥¨ã•ã‚Œã‚‹ãŸã‚ã€1ã¤ãšã¤å‡¦ç†
        for idx, i in enumerate(process_indices):
            try:
                text = texts[i]
                if not text or not text.strip():
                    logger.warning(f"âš ï¸ ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i}")
                    all_embeddings[i] = None
                    failed_embeddings.append(i)
                    continue
                
                response = await asyncio.to_thread(
                    self.gemini_client.models.embed_content,
                    model=self.embedding_model,
                    contents=text.strip()
                )
                
                if response and hasattr(response, 'embeddings') and response.embeddings and len(response.embeddings) > 0:
                    embedding_vector = response.embeddings[0].values
                    all_embeddings[i] = embedding_vector
                    logger.info(f"âœ… embeddingç”ŸæˆæˆåŠŸ: {idx + 1}/{len(process_indices)} (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i}, æ¬¡å…ƒ: {len(embedding_vector)})")
                else:
                    logger.warning(f"âš ï¸ embeddingç”Ÿæˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒä¸æ­£ã§ã™: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i}")
                    all_embeddings[i] = None
                    failed_embeddings.append(i)
                
                # APIåˆ¶é™å¯¾ç­–
                await asyncio.sleep(0.2)
                
            except Exception as e:
                logger.error(f"âŒ embeddingç”Ÿæˆã‚¨ãƒ©ãƒ¼ (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i}): {e}")
                all_embeddings[i] = None
                failed_embeddings.append(i)

        success_count = len([e for e in all_embeddings if e is not None])
        total_count = len(texts)
        
        if failed_indices is None:
            logger.info(f"ğŸ‰ Gemini API embeddingç”Ÿæˆå®Œäº†: {success_count}/{total_count} æˆåŠŸ")
        else:
            logger.info(f"ğŸ‰ Gemini API embeddingå†ç”Ÿæˆå®Œäº†: {success_count - (total_count - len(failed_indices))}/{len(failed_indices)} æˆåŠŸ")
        
        if failed_embeddings:
            logger.warning(f"âš ï¸ å¤±æ•—ã—ãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {failed_embeddings}")
        
        return all_embeddings

    async def _save_document_metadata(self, doc_data: Dict[str, Any]) -> str:
        """document_sourcesãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        try:
            from supabase_adapter import insert_data, select_data
            
            document_id = str(uuid.uuid4())
            
            # document_sourcesãƒ†ãƒ¼ãƒ–ãƒ«ã«å¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿ã‚’å«ã‚ã‚‹ï¼ˆcontentã¨embeddingã¯å‰Šé™¤æ¸ˆã¿ï¼‰
            metadata = {
                "id": document_id,
                "name": doc_data["name"],
                "type": doc_data["type"],
                "page_count": doc_data.get("page_count", 1),
                "uploaded_by": doc_data["uploaded_by"],
                "company_id": doc_data["company_id"],
                "uploaded_at": datetime.now().isoformat(),  # uploaded_atãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½¿ç”¨
                "active": True,  # activeãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
                "parent_id": doc_data.get("parent_id"),  # è¦ªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆéšå±¤æ§‹é€ ï¼‰
                "doc_id": document_id,  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè­˜åˆ¥å­ã¨ã—ã¦è‡ªèº«ã®IDã‚’è¨­å®š
                "metadata": doc_data.get("metadata")  # metadataãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
            }
            
            logger.info(f"ä¿å­˜ã™ã‚‹metadata: {metadata.get('metadata')}")
            
            # metadataãŒæ–‡å­—åˆ—ã‹ã©ã†ã‹ã‚’ç¢ºèª
            if metadata.get('metadata'):
                logger.info(f"metadataã®å‹: {type(metadata.get('metadata'))}")
                if isinstance(metadata.get('metadata'), str):
                    # JSONæ–‡å­—åˆ—ã¨ã—ã¦æœ‰åŠ¹ã‹ã©ã†ã‹ã‚’ç¢ºèª
                    try:
                        import json
                        parsed = json.loads(metadata.get('metadata'))
                        logger.info(f"metadata JSONè§£ææˆåŠŸ: {parsed}")
                    except Exception as json_error:
                        logger.error(f"metadata JSONè§£æå¤±æ•—: {json_error}")
                        # ç„¡åŠ¹ãªJSONã®å ´åˆã¯åŸºæœ¬çš„ãªmetadataã«ç½®ãæ›ãˆ
                        metadata['metadata'] = '{"error": "invalid json"}'
            else:
                logger.warning("metadataãŒç©ºã¾ãŸã¯None")
            
            # specialã‚³ãƒ©ãƒ ã¯çµ¶å¯¾ã«è¨­å®šã—ãªã„ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚é€šã‚Šï¼‰
            
            logger.info(f"ğŸ”„ document_sourcesãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®ä¿å­˜é–‹å§‹: {document_id} - {doc_data['name']}")
            result = insert_data("document_sources", metadata)
            
            if result and result.data:
                logger.info(f"âœ… document_sourcesãƒ†ãƒ¼ãƒ–ãƒ«ä¿å­˜å®Œäº†: {document_id} - {doc_data['name']}")
                # ä¿å­˜å¾Œã«å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
                try:
                    from supabase_adapter import select_data
                    check_result = select_data("document_sources", filters={"id": document_id})
                    if check_result.success and check_result.data:
                        saved_metadata = check_result.data[0].get('metadata')
                        logger.info(f"âœ… ä¿å­˜ç¢ºèª - å®Ÿéš›ã®metadata: {saved_metadata}")
                    else:
                        logger.warning(f"âš ï¸ ä¿å­˜ç¢ºèªå¤±æ•—: {check_result.error}")
                except Exception as check_error:
                    logger.warning(f"âš ï¸ ä¿å­˜ç¢ºèªã‚¨ãƒ©ãƒ¼: {check_error}")
                return document_id
            else:
                logger.error(f"âŒ document_sourcesãƒ†ãƒ¼ãƒ–ãƒ«ä¿å­˜å¤±æ•—: result={result}")
                raise Exception("document_sourcesãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
        except Exception as main_error:
            logger.error(f"âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {main_error}")
            
            # å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’ç¢ºèª
            error_str = str(main_error)
            if "document_sources_uploaded_by_fkey" in error_str:
                logger.warning(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ '{doc_data['uploaded_by']}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“ - company_idã§ä»£æ›¿ä¿å­˜ã‚’è©¦è¡Œ")
                try:
                    # company_idã‹ã‚‰ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’æ¤œç´¢
                    company_users = select_data(
                        "users",
                        columns="id",
                        filters={"company_id": doc_data["company_id"]}
                    )
                    
                    if company_users.data and len(company_users.data) > 0:
                        alternative_user_id = company_users.data[0]["id"]
                        logger.info(f"ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ç™ºè¦‹: {alternative_user_id}")
                        
                        # ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã§å†ä¿å­˜
                        metadata["uploaded_by"] = alternative_user_id
                        result = insert_data("document_sources", metadata)
                        
                        if result and result.data:
                            logger.info(f"âœ… ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {document_id}")
                            return document_id
                        else:
                            raise Exception("ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                    else:
                        # ä¼šç¤¾ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã€ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ä½œæˆ
                        logger.warning(f"ä¼šç¤¾ '{doc_data['company_id']}' ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå­˜åœ¨ã—ã¾ã›ã‚“ - ãƒ†ã‚¹ãƒˆç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ä½œæˆ")
                        test_user_data = {
                            "id": doc_data["uploaded_by"],
                            "company_id": doc_data["company_id"],
                            "name": "Test User",
                            "email": "test@example.com",
                            "created_at": datetime.now().isoformat()
                        }
                        
                        user_result = insert_data("users", test_user_data)
                        if user_result and user_result.data:
                            logger.info(f"âœ… ãƒ†ã‚¹ãƒˆç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆå®Œäº†: {doc_data['uploaded_by']}")
                            
                            # å…ƒã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§å†ä¿å­˜
                            result = insert_data("document_sources", metadata)
                            if result and result.data:
                                logger.info(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {document_id}")
                                return document_id
                            else:
                                raise Exception("ãƒ†ã‚¹ãƒˆç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆå¾Œã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                        else:
                            raise Exception("ãƒ†ã‚¹ãƒˆç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                            
                except Exception as fallback_error:
                    logger.error(f"âŒ ä»£æ›¿ä¿å­˜å‡¦ç†ã‚¨ãƒ©ãƒ¼: {fallback_error}")
                    raise Exception(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {fallback_error}")
            else:
                raise main_error
    
    async def _save_chunks_to_database(self, doc_id: str, chunks: List[Dict[str, Any]],
                                     company_id: str, doc_name: str, max_retries: int = 10) -> Dict[str, Any]:
        """chunksãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã¨embeddingã‚’50å€‹å˜ä½ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ä¿å­˜"""
        try:
            from supabase_adapter import get_supabase_client
            supabase = get_supabase_client()

            stats = {
                "total_chunks": len(chunks),
                "saved_chunks": 0,
                "successful_embeddings": 0,
                "failed_embeddings": 0,
                "retry_attempts": 0,
                "failed_chunks": []  # å¤±æ•—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã‚’è¨˜éŒ²
            }

            if not chunks:
                return stats

            batch_size = 50
            total_batches = (len(chunks) + batch_size - 1) // batch_size
            
            logger.info(f"ğŸš€ {doc_name}: {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’{batch_size}å€‹å˜ä½ã§å‡¦ç†é–‹å§‹")
            logger.info(f"ğŸ“Š äºˆæƒ³ãƒãƒƒãƒæ•°: {total_batches}")

            # 50å€‹å˜ä½ã§embeddingç”Ÿæˆâ†’å³åº§ã«insert
            for batch_num in range(0, len(chunks), batch_size):
                batch_chunks = chunks[batch_num:batch_num + batch_size]
                current_batch = (batch_num // batch_size) + 1
                
                logger.info(f"ğŸ§  ãƒãƒƒãƒ {current_batch}/{total_batches}: {len(batch_chunks)}å€‹ã®embeddingç”Ÿæˆé–‹å§‹")
                
                # ã“ã®ãƒãƒƒãƒã®embeddingç”Ÿæˆ
                batch_contents = [chunk["content"] for chunk in batch_chunks]
                batch_embeddings = await self._generate_embeddings_batch(batch_contents)
                
                # å¤±æ•—ã—ãŸembeddingã®ãƒªãƒˆãƒ©ã‚¤å‡¦ç†
                failed_indices = [i for i, emb in enumerate(batch_embeddings) if emb is None]
                retry_count = 0
                
                while failed_indices and retry_count < max_retries:
                    retry_count += 1
                    logger.info(f"ğŸ”„ ãƒãƒƒãƒ {current_batch} embeddingå†ç”Ÿæˆ (è©¦è¡Œ {retry_count}/{max_retries}): {len(failed_indices)}ä»¶")
                    
                    retry_embeddings = await self._generate_embeddings_batch(batch_contents, failed_indices)
                    
                    for i in failed_indices:
                        if retry_embeddings[i] is not None:
                            batch_embeddings[i] = retry_embeddings[i]
                    
                    failed_indices = [i for i in failed_indices if batch_embeddings[i] is None]
                    
                    if failed_indices:
                        logger.warning(f"âš ï¸ ãƒãƒƒãƒ {current_batch} å†è©¦è¡Œå¾Œã‚‚å¤±æ•—: {len(failed_indices)}ä»¶")
                        await asyncio.sleep(1.0)
                    else:
                        logger.info(f"âœ… ãƒãƒƒãƒ {current_batch} å†è©¦è¡ŒæˆåŠŸ")
                        break
                
                # çµ±è¨ˆæ›´æ–°
                for embedding in batch_embeddings:
                    if embedding:
                        stats["successful_embeddings"] += 1
                    else:
                        stats["failed_embeddings"] += 1
                
                if retry_count > 0:
                    stats["retry_attempts"] = max(stats["retry_attempts"], retry_count)
                
                # æˆåŠŸã—ãŸembeddingã®ã¿ã§ãƒ¬ã‚³ãƒ¼ãƒ‰æº–å‚™ã€å¤±æ•—ã—ãŸã‚‚ã®ã¯è¨˜éŒ²
                records_to_insert = []
                for i, chunk_data in enumerate(batch_chunks):
                    embedding_vector = batch_embeddings[i]
                    if embedding_vector:  # æˆåŠŸã—ãŸembeddingã®ã¿
                        records_to_insert.append({
                            "doc_id": doc_id,
                            "chunk_index": chunk_data["chunk_index"],
                            "content": chunk_data["content"],
                            "embedding": embedding_vector,
                            "company_id": company_id,
                            "created_at": datetime.now().isoformat(),
                            "updated_at": datetime.now().isoformat()
                        })
                    else:
                        # å¤±æ•—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã‚’embeddingãªã—ã§ä¿å­˜ï¼ˆå¾Œã§å†å‡¦ç†ç”¨ï¼‰
                        failed_record = {
                            "doc_id": doc_id,
                            "chunk_index": chunk_data["chunk_index"],
                            "content": chunk_data["content"],
                            "embedding": None,
                            "company_id": company_id,
                            "created_at": datetime.now().isoformat(),
                            "updated_at": datetime.now().isoformat()
                        }
                        records_to_insert.append(failed_record)
                        stats["failed_chunks"].append(failed_record)
                
                # å³åº§ã«Supabaseã«æŒ¿å…¥
                if records_to_insert:
                    try:
                        logger.info(f"ğŸ’¾ ãƒãƒƒãƒ {current_batch}/{total_batches}: {len(records_to_insert)}ä»¶ã‚’å³åº§ã«ä¿å­˜ä¸­...")
                        result = supabase.table("chunks").insert(records_to_insert).execute()
                        
                        if result.data:
                            batch_saved = len(result.data)
                            stats["saved_chunks"] += batch_saved
                            logger.info(f"âœ… ãƒãƒƒãƒ {current_batch}/{total_batches}: {batch_saved}ä»¶ä¿å­˜å®Œäº†")
                        else:
                            logger.error(f"âŒ ãƒãƒƒãƒ {current_batch}/{total_batches} ä¿å­˜ã‚¨ãƒ©ãƒ¼: {result.error}")
                            
                    except Exception as batch_error:
                        logger.error(f"âŒ ãƒãƒƒãƒ {current_batch}/{total_batches} ä¿å­˜ä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {batch_error}")
                        # ãƒãƒƒãƒã‚¨ãƒ©ãƒ¼ã§ã‚‚æ¬¡ã®ãƒãƒƒãƒå‡¦ç†ã‚’ç¶šè¡Œ
                        continue
                else:
                    logger.warning(f"âš ï¸ ãƒãƒƒãƒ {current_batch}/{total_batches}: ä¿å­˜å¯èƒ½ãªãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
                
                # ãƒãƒƒãƒå®Œäº†ãƒ­ã‚°
                logger.info(f"ğŸ¯ ãƒãƒƒãƒ {current_batch}/{total_batches} å®Œäº†: embedding {len(batch_embeddings) - len(failed_indices)}/{len(batch_embeddings)} æˆåŠŸ, ä¿å­˜ {len(records_to_insert)} ä»¶")

            # æœ€çµ‚çµæœã®ã‚µãƒãƒªãƒ¼
            logger.info(f"ğŸ {doc_name}: å…¨å‡¦ç†å®Œäº†")
            logger.info(f"ğŸ“ˆ æœ€çµ‚çµæœ: ä¿å­˜ {stats['saved_chunks']}/{stats['total_chunks']} ãƒãƒ£ãƒ³ã‚¯")
            logger.info(f"ğŸ§  embedding: æˆåŠŸ {stats['successful_embeddings']}, å¤±æ•— {stats['failed_embeddings']}")
            
            if stats["failed_embeddings"] > 0:
                logger.warning(f"âš ï¸ æœ€çµ‚çµæœ: {stats['successful_embeddings']}/{stats['total_chunks']} embeddingæˆåŠŸ, {stats['retry_attempts']}å›å†è©¦è¡Œ")
                logger.info(f"ğŸ“‹ å¤±æ•—ã—ãŸãƒãƒ£ãƒ³ã‚¯æ•°: {len(stats['failed_chunks'])}ä»¶ - å¾Œã§å†å‡¦ç†äºˆå®š")
            else:
                logger.info(f"ğŸ‰ å…¨embeddingç”ŸæˆæˆåŠŸ: {stats['successful_embeddings']}/{stats['total_chunks']}")

            return stats

        except Exception as e:
            logger.error(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒãƒƒãƒä¿å­˜ä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {e}", exc_info=True)
            raise
    
    async def process_uploaded_file(self, file: UploadFile, user_id: str, 
                                  company_id: str) -> Dict[str, Any]:
        """
        ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®Œå…¨å‡¦ç†
        1ï¸âƒ£ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        2ï¸âƒ£ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        3ï¸âƒ£ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆ300ã€œ500 tokenï¼‰
        4ï¸âƒ£ embeddingç”Ÿæˆï¼ˆGemini Flash - 3072æ¬¡å…ƒï¼‰
        5ï¸âƒ£ Supabaseä¿å­˜
        """
        try:
            logger.info(f"ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file.filename}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿è¾¼ã¿
            file_content = await file.read()
            file_size_mb = len(file_content) / (1024 * 1024)
            
            logger.info(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.2f} MB")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«å¿œã˜ã¦ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            extracted_text = await self._extract_text_from_file(file, file_content)
            
            if not extracted_text or not extracted_text.strip():
                raise HTTPException(status_code=400, detail="ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
            logger.info(f"ğŸ“ æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆ: {len(extracted_text)} æ–‡å­—")
            
            # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            chunks = self._split_text_into_chunks(extracted_text, file.filename)
            
            if not chunks:
                raise HTTPException(status_code=400, detail="ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            doc_data = {
                "name": file.filename,
                "type": self._detect_file_type(file.filename),
                "page_count": self._estimate_page_count(extracted_text),
                "uploaded_by": user_id,
                "company_id": company_id,
                "special": f"ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(extracted_text)}æ–‡å­—"  # ç‰¹æ®Šå±æ€§ã¨ã—ã¦è¨˜éŒ²
            }
            
            document_id = await self._save_document_metadata(doc_data)
            
            # ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            save_stats = await self._save_chunks_to_database(
                document_id, chunks, company_id, file.filename
            )
            
            # å¤±æ•—ã—ãŸembeddingãŒã‚ã‚‹å ´åˆã¯å…¨å‡¦ç†å®Œäº†å¾Œã«å†å‡¦ç†
            if save_stats["failed_embeddings"] > 0:
                logger.info(f"ğŸ”„ {file.filename}: å¤±æ•—ã—ãŸembedding {save_stats['failed_embeddings']}ä»¶ã®å†å‡¦ç†ã‚’é–‹å§‹")
                retry_stats = await self._retry_failed_embeddings_post_processing(
                    document_id, company_id, file.filename
                )
                
                # çµ±è¨ˆã‚’æ›´æ–°
                save_stats["successful_embeddings"] += retry_stats["successful"]
                save_stats["failed_embeddings"] = retry_stats["still_failed"]
                save_stats["retry_attempts"] = max(save_stats["retry_attempts"], retry_stats["retry_attempts"])
                
                logger.info(f"ğŸ”„ {file.filename}: å†å‡¦ç†å®Œäº† - è¿½åŠ æˆåŠŸ {retry_stats['successful']}ä»¶, æœ€çµ‚å¤±æ•— {retry_stats['still_failed']}ä»¶")
            
            # å‡¦ç†çµæœã‚’è¿”ã™
            result = {
                "success": True,
                "document_id": document_id,
                "filename": file.filename,
                "file_size_mb": round(file_size_mb, 2),
                "text_length": len(extracted_text),
                "total_chunks": save_stats["total_chunks"],
                "saved_chunks": save_stats["saved_chunks"],
                "successful_embeddings": save_stats["successful_embeddings"],
                "failed_embeddings": save_stats["failed_embeddings"],
                "message": f"âœ… {file.filename} ã®å‡¦ç†ãƒ»embeddingç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ"
            }
            
            logger.info(f"ğŸ‰ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†: {file.filename}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise HTTPException(status_code=500, detail=f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    
    async def _extract_text_from_file(self, file: UploadFile, content: bytes) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«å¿œã˜ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        filename = file.filename.lower()
        
        try:
            if filename.endswith(('.pdf',)):
                return await self._extract_text_from_pdf(content)
            elif filename.endswith(('.xlsx', '.xls')):
                return await self._extract_text_from_excel(content)
            elif filename.endswith(('.docx', '.doc')):
                return await self._extract_text_from_word(content)
            elif filename.endswith(('.txt', '.csv')):
                return await self._extract_text_from_text(content)
            elif filename.endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp')):
                return await self._extract_text_from_image(content)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦èª­ã¿è¾¼ã¿
                return content.decode('utf-8', errors='ignore')
                
        except Exception as e:
            logger.error(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({filename}): {e}")
            raise
    
    async def _extract_text_from_pdf(self, content: bytes) -> str:
        """PDF ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹ï¼ˆGemini OCRæœ€é©åŒ–ç‰ˆ + ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œï¼‰
        
        ã¾ãšGemini OCRã§é«˜ç²¾åº¦æŠ½å‡ºã‚’è©¦è¡Œã—ã€å¤±æ•—æ™‚ã¯PyPDF2ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨
        """
        
        logger.info("ğŸ“„ PDFæŠ½å‡ºé–‹å§‹ - Gemini OCRå„ªå…ˆ")
        
        try:
            # ã¾ãšGemini OCRã‚’è©¦è¡Œ
            try:
                from modules.knowledge.ocr import ocr_pdf_to_text_from_bytes
            except ImportError:
                logger.error("âŒ OCR module import failed - knowledge module not available")
                raise Exception("OCR module not available")
            
            logger.info("ğŸ”„ Gemini OCRã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚’è©¦è¡Œä¸­...")
            ocr_text = await ocr_pdf_to_text_from_bytes(content)
            
            if ocr_text and ocr_text.strip() and not ocr_text.startswith("OCRå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"):
                # OCRæˆåŠŸæ™‚ã®å“è³ªãƒã‚§ãƒƒã‚¯
                quality_score = self._evaluate_text_quality(ocr_text)
                page_count = ocr_text.count("--- Page") or 1
                
                logger.info(f"âœ… Gemini OCRæˆåŠŸ:")
                logger.info(f"   - ç·æ–‡å­—æ•°: {len(ocr_text)}")
                logger.info(f"   - å“è³ªã‚¹ã‚³ã‚¢: {quality_score}/100")
                logger.info(f"   - ãƒšãƒ¼ã‚¸æ•°: {page_count}")
                logger.info(f"   - å¹³å‡æ–‡å­—/ãƒšãƒ¼ã‚¸: {len(ocr_text)/page_count:.0f}")
                
                return ocr_text
            else:
                logger.warning("âš ï¸ Gemini OCRãŒå¤±æ•—ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã—ã¾ã—ãŸ")
                raise Exception("Gemini OCR failed")
                
        except Exception as ocr_error:
            logger.error(f"âŒ Gemini OCRå‡¦ç†å¤±æ•—: {ocr_error}")
            logger.info("ğŸ”„ PyPDF2ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨")
            
            # PyPDF2ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§å‡¦ç†
            fallback_text = await self._extract_text_from_pdf_fallback(content)
            return fallback_text
    
    def _evaluate_text_quality(self, text: str) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆå“è³ªã‚’0-100ã®ã‚¹ã‚³ã‚¢ã§è©•ä¾¡ï¼ˆã‚ˆã‚Šè©³ç´°ç‰ˆï¼‰"""
        if not text or not text.strip():
            return 0
            
        try:
            import re
            
            # åŸºæœ¬çµ±è¨ˆ
            total_chars = len(text)
            lines = text.splitlines()
            non_empty_lines = [line for line in lines if line.strip()]
            
            if total_chars == 0:
                return 0
            
            # 1. æ–‡å­—ç¨®åˆ¥ã®å“è³ªè©•ä¾¡
            japanese_chars = len(re.findall(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', text))
            english_chars = len(re.findall(r'[a-zA-Z]', text))
            numeric_chars = len(re.findall(r'[0-9]', text))
            valid_chars = japanese_chars + english_chars + numeric_chars
            valid_char_ratio = valid_chars / total_chars if total_chars > 0 else 0
            
            # 2. æ–‡å­—åŒ–ã‘æ¤œå‡ºï¼ˆã‚ˆã‚Šå³å¯†ï¼‰
            mojibake_patterns = [
                r'[ç¸ºç¹§ç¹]',  # å…¸å‹çš„ãªæ–‡å­—åŒ–ã‘
                r'\(cid:\d+\)',  # PDF CIDæ–‡å­—åŒ–ã‘
                r'[\\ufffd]',  # ç½®æ›æ–‡å­—
                r'[]',  # ãã®ä»–ã®æ–‡å­—åŒ–ã‘æ–‡å­—
            ]
            mojibake_count = sum(len(re.findall(pattern, text)) for pattern in mojibake_patterns)
            mojibake_penalty = min(mojibake_count * 2, 40)  # æ–‡å­—åŒ–ã‘1ã¤ã«ã¤ã2ç‚¹æ¸›ç‚¹
            
            # 3. æ§‹é€ çš„å“è³ªè©•ä¾¡
            has_headers = len(re.findall(r'^#{1,3}\s', text, re.MULTILINE)) > 0
            has_lists = len(re.findall(r'^[\s]*[â€¢\-\*\d+\.]\s', text, re.MULTILINE)) > 0
            has_tables = '|' in text and text.count('|') > 10
            has_pages = '=== ãƒšãƒ¼ã‚¸' in text
            
            structure_score = 0
            if has_headers: structure_score += 10
            if has_lists: structure_score += 10
            if has_tables: structure_score += 15
            if has_pages: structure_score += 5
            
            # 4. æ„å‘³ã®ã‚ã‚‹å†…å®¹ã®æ¯”ç‡
            meaningful_lines = 0
            for line in non_empty_lines:
                line = line.strip()
                if len(line) > 5 and re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAFa-zA-Z]', line):
                    meaningful_lines += 1
            
            line_quality = (meaningful_lines / len(non_empty_lines)) * 100 if non_empty_lines else 0
            
            # 5. é•·ã•å“è³ªè©•ä¾¡
            length_score = min(len(text) / 100, 20)  # 100æ–‡å­—ã§1ç‚¹ã€æœ€å¤§20ç‚¹
            
            # æœ€çµ‚ã‚¹ã‚³ã‚¢è¨ˆç®—
            base_score = (
                valid_char_ratio * 30 +  # æœ‰åŠ¹æ–‡å­—æ¯”ç‡ï¼ˆ30ç‚¹æº€ç‚¹ï¼‰
                line_quality * 0.2 +     # æœ‰æ„ç¾©ãªè¡Œã®æ¯”ç‡ï¼ˆ20ç‚¹æº€ç‚¹ï¼‰
                structure_score +        # æ§‹é€ çš„å“è³ªï¼ˆ40ç‚¹æº€ç‚¹ï¼‰
                length_score             # é•·ã•å“è³ªï¼ˆ20ç‚¹æº€ç‚¹ï¼‰
            )
            
            final_score = max(0, int(base_score - mojibake_penalty))
            
            # è©³ç´°ãƒ­ã‚°
            logger.debug(f"ğŸ“Š å“è³ªè©•ä¾¡è©³ç´°:")
            logger.debug(f"   - æœ‰åŠ¹æ–‡å­—æ¯”ç‡: {valid_char_ratio:.2f} ({valid_char_ratio*30:.1f}ç‚¹)")
            logger.debug(f"   - è¡Œå“è³ª: {line_quality:.1f} ({line_quality*0.2:.1f}ç‚¹)")
            logger.debug(f"   - æ§‹é€ ã‚¹ã‚³ã‚¢: {structure_score}ç‚¹")
            logger.debug(f"   - é•·ã•ã‚¹ã‚³ã‚¢: {length_score:.1f}ç‚¹")
            logger.debug(f"   - æ–‡å­—åŒ–ã‘ãƒšãƒŠãƒ«ãƒ†ã‚£: -{mojibake_penalty}ç‚¹")
            logger.debug(f"   - æœ€çµ‚ã‚¹ã‚³ã‚¢: {final_score}ç‚¹")
            
            return min(100, final_score)
            
        except Exception as e:
            logger.warning(f"å“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return 50  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚³ã‚¢
    
    async def _extract_text_from_pdf_fallback(self, content: bytes) -> str:
        """PyPDF2ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ + æ–‡å­—åŒ–ã‘ä¿®å¾©å‡¦ç†ï¼ˆå®Œå…¨ç‰ˆï¼‰"""
        logger.info("ğŸ”„ PyPDF2ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æŠ½å‡ºé–‹å§‹")
        
        try:
            import PyPDF2
            from io import BytesIO
            # Import PDF helper functions - create them inline if module doesn't exist
            try:
                from modules.knowledge.pdf import fix_mojibake_text, check_text_corruption, extract_text_with_encoding_fallback
            except ImportError:
                logger.warning("PDF helper functions not available, using fallback implementations")
                # Define fallback functions inline
                def fix_mojibake_text(text):
                    """Simple mojibake fix"""
                    if not text:
                        return text
                    return text.replace('ç¸º', 'ã„').replace('ç¹§', 'ã†').replace('ç¹', 'ãˆ')
                
                def check_text_corruption(text):
                    """Simple corruption check"""
                    if not text:
                        return True
                    return 'ç¸º' in text or 'ç¹§' in text or 'ç¹' in text or '\ufffd' in text
                
                def extract_text_with_encoding_fallback(page):
                    """Simple text extraction with encoding fallback"""
                    try:
                        return page.extract_text() or ""
                    except Exception:
                        return ""
            
            pdf_reader = PyPDF2.PdfReader(BytesIO(content))
            text_parts = []
            corrupted_pages = []
            total_pages = len(pdf_reader.pages)
            
            logger.info(f"ğŸ“„ PDFç·ãƒšãƒ¼ã‚¸æ•°: {total_pages}")
            
            for page_num, page in enumerate(pdf_reader.pages):
                try:
                    # å¼·åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚’ä½¿ç”¨
                    page_text = extract_text_with_encoding_fallback(page)
                    
                    if page_text and page_text.strip():
                        # æ–‡å­—åŒ–ã‘ãƒã‚§ãƒƒã‚¯
                        if check_text_corruption(page_text):
                            logger.info(f"ãƒšãƒ¼ã‚¸ {page_num + 1} ã§æ–‡å­—åŒ–ã‘ã‚’æ¤œå‡ºã€ä¿®å¾©ã‚’é©ç”¨")
                            page_text = fix_mojibake_text(page_text)
                            corrupted_pages.append(page_num + 1)
                        
                        text_parts.append(f"=== ãƒšãƒ¼ã‚¸ {page_num + 1} ===\n{page_text}")
                        logger.debug(f"âœ… ãƒšãƒ¼ã‚¸ {page_num + 1}: {len(page_text)}æ–‡å­—æŠ½å‡º")
                    else:
                        text_parts.append(f"=== ãƒšãƒ¼ã‚¸ {page_num + 1} ===\n[ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ]")
                        logger.warning(f"âš ï¸ ãƒšãƒ¼ã‚¸ {page_num + 1}: ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—")
                        
                except Exception as page_error:
                    logger.warning(f"ãƒšãƒ¼ã‚¸ {page_num + 1} æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {page_error}")
                    text_parts.append(f"=== ãƒšãƒ¼ã‚¸ {page_num + 1} ===\n[ãƒšãƒ¼ã‚¸æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(page_error)}]")
            
            if text_parts:
                final_text = "\n\n".join(text_parts)
                total_chars = len(final_text)
                valid_pages = len([p for p in text_parts if "ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ" not in p and "ãƒšãƒ¼ã‚¸æŠ½å‡ºã‚¨ãƒ©ãƒ¼" not in p])
                
                logger.info(f"âœ… PyPDF2ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Œäº†:")
                logger.info(f"   - å‡¦ç†ãƒšãƒ¼ã‚¸æ•°: {total_pages}")
                logger.info(f"   - æœ‰åŠ¹ãƒšãƒ¼ã‚¸æ•°: {valid_pages}")
                logger.info(f"   - æ–‡å­—åŒ–ã‘ä¿®å¾©ãƒšãƒ¼ã‚¸: {len(corrupted_pages)}")
                logger.info(f"   - ç·æŠ½å‡ºæ–‡å­—æ•°: {total_chars}")
                
                if corrupted_pages:
                    logger.info(f"   - ä¿®å¾©ã—ãŸãƒšãƒ¼ã‚¸: {corrupted_pages}")
                
                return final_text
            else:
                logger.error("âŒ å…¨ãƒšãƒ¼ã‚¸ã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã«å¤±æ•—")
                return "[PDFå‡¦ç†ã‚¨ãƒ©ãƒ¼: ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ]"
                
        except Exception as fallback_error:
            logger.error(f"âŒ PyPDF2ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¤±æ•—: {fallback_error}")
            import traceback
            logger.error(f"ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹: {traceback.format_exc()}")
            return f"[PDFå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(fallback_error)}]\n\nåŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚‚å¤±æ•—ã—ã¾ã—ãŸã€‚PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
    
    async def _extract_text_from_excel(self, content: bytes) -> str:
        """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆExcelDataCleanerã‚’ä½¿ç”¨ï¼‰"""
        try:
            from modules.excel_data_cleaner import ExcelDataCleaner
            
            cleaner = ExcelDataCleaner()
            cleaned_text = cleaner.clean_excel_data(content)
            
            logger.info(f"âœ… Excelå‡¦ç†å®Œäº†ï¼ˆExcelDataCleanerä½¿ç”¨ï¼‰: {len(cleaned_text)} æ–‡å­—")
            return cleaned_text
            
        except Exception as e:
            logger.error(f"âŒ Excelå‡¦ç†ã‚¨ãƒ©ãƒ¼ï¼ˆExcelDataCleanerï¼‰: {e}")
            # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯ã€ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚’æ–­å¿µã—ã€ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™ã‹ã€é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™
            # ã“ã“ã§ã¯ã‚¨ãƒ©ãƒ¼ã‚’å†raiseã—ã¦ã€ä¸Šä½ã§ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã•ã›ã‚‹ã“ã¨ã‚’æ¨å¥¨
            raise
    
    async def _extract_text_from_word(self, content: bytes) -> str:
        """Wordãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        try:
            import docx
            from io import BytesIO
            
            doc = docx.Document(BytesIO(content))
            text_parts = []
            
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    text_parts.append(paragraph.text)
            
            return "\n\n".join(text_parts)
            
        except Exception as e:
            logger.error(f"Wordå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    async def _extract_text_from_text(self, content: bytes) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å†…å®¹ã‚’æŠ½å‡º"""
        try:
            # è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
            encodings = ['utf-8', 'shift_jis', 'cp932', 'euc-jp', 'iso-2022-jp']
            
            for encoding in encodings:
                try:
                    return content.decode(encoding)
                except UnicodeDecodeError:
                    continue
            
            # ã™ã¹ã¦å¤±æ•—ã—ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–ã—ã¦èª­ã¿è¾¼ã¿
            return content.decode('utf-8', errors='ignore')
            
        except Exception as e:
            logger.error(f"ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    async def _extract_text_from_image(self, content: bytes) -> str:
        """ç”»åƒã‹ã‚‰OCRã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        try:
            # Gemini Vision APIã‚’ä½¿ç”¨ã—ã¦OCR
            self._init_gemini_client()
            
            # ç”»åƒã‚’base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            import base64
            image_b64 = base64.b64encode(content).decode('utf-8')
            
            # Gemini Vision APIã§OCRï¼ˆå®Ÿè£…ã¯ç’°å¢ƒã«å¿œã˜ã¦èª¿æ•´ï¼‰
            # ã“ã“ã§ã¯ç°¡å˜ãªå®Ÿè£…ä¾‹
            logger.info("ğŸ–¼ï¸ ç”»åƒOCRå‡¦ç†ï¼ˆGemini Vision APIï¼‰")
            
            # OCRçµæœã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
            return "ç”»åƒã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆOCRå‡¦ç†ãŒå¿…è¦ï¼‰"
            
        except Exception as e:
            logger.error(f"ç”»åƒOCRå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return "ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºä¸å¯ï¼‰"
    
    def _detect_file_type(self, filename: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‹ã‚‰ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š"""
        filename = filename.lower()
        
        if filename.endswith('.pdf'):
            return 'PDF'
        elif filename.endswith(('.xlsx', '.xls')):
            return 'Excel'
        elif filename.endswith(('.docx', '.doc')):
            return 'Word'
        elif filename.endswith('.csv'):
            return 'CSV'
        elif filename.endswith('.txt'):
            return 'Text'
        elif filename.endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp')):
            return 'Image'
        else:
            return 'Unknown'

    def _estimate_page_count(self, text: str) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒšãƒ¼ã‚¸æ•°ã‚’æ¨å®š"""
        # 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šç´„500ãƒˆãƒ¼ã‚¯ãƒ³ã¨ä»®å®š
        tokens = self._count_tokens(text)
        return max(1, (tokens + 499) // 500)

    async def retry_failed_embeddings(self, doc_id: str = None, company_id: str = None, max_retries: int = 10) -> Dict[str, Any]:
        """
        æ—¢å­˜ã®ãƒãƒ£ãƒ³ã‚¯ã§å¤±æ•—ã—ãŸembeddingã‚’å†ç”Ÿæˆ
        doc_id: ç‰¹å®šã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã¿å‡¦ç†ï¼ˆNoneã®å ´åˆã¯å…¨ã¦ï¼‰
        company_id: ç‰¹å®šã®ä¼šç¤¾ã®ã¿å‡¦ç†ï¼ˆNoneã®å ´åˆã¯å…¨ã¦ï¼‰
        """
        try:
            from supabase_adapter import get_supabase_client
            supabase = get_supabase_client()
            
            logger.info(f"ğŸ” å¤±æ•—ã—ãŸembeddingã®æ¤œç´¢é–‹å§‹ (doc_id: {doc_id}, company_id: {company_id})")
            
            # å¤±æ•—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã‚’æ¤œç´¢ï¼ˆembeddingãŒNullã®ã‚‚ã®ï¼‰
            query = supabase.table("chunks").select("*").is_("embedding", "null")
            
            if doc_id:
                query = query.eq("doc_id", doc_id)
            if company_id:
                query = query.eq("company_id", company_id)
            
            result = query.execute()
            
            if not result.data:
                logger.info("âœ… å¤±æ•—ã—ãŸembeddingã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return {
                    "total_failed": 0,
                    "processed": 0,
                    "successful": 0,
                    "still_failed": 0,
                    "retry_attempts": 0
                }
            
            failed_chunks = result.data
            logger.info(f"ğŸ” å¤±æ•—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã‚’ç™ºè¦‹: {len(failed_chunks)}ä»¶")
            
            stats = {
                "total_failed": len(failed_chunks),
                "processed": 0,
                "successful": 0,
                "still_failed": 0,
                "retry_attempts": 0
            }
            
            # ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒãƒƒãƒã§å‡¦ç†ï¼ˆ50ä»¶ãšã¤ï¼‰
            batch_size = 50
            for batch_start in range(0, len(failed_chunks), batch_size):
                batch_end = min(batch_start + batch_size, len(failed_chunks))
                batch_chunks = failed_chunks[batch_start:batch_end]
                
                logger.info(f"ğŸ“¦ ãƒãƒƒãƒå‡¦ç†: {batch_start + 1}-{batch_end}/{len(failed_chunks)}")
                
                # ãƒãƒƒãƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                batch_contents = [chunk["content"] for chunk in batch_chunks]
                batch_indices = list(range(len(batch_contents)))
                
                # embeddingç”Ÿæˆï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
                embeddings = await self._generate_embeddings_batch(batch_contents)
                
                # å¤±æ•—ã—ãŸã‚‚ã®ã‚’å†è©¦è¡Œ
                failed_indices = [i for i, emb in enumerate(embeddings) if emb is None]
                retry_count = 0
                
                while failed_indices and retry_count < max_retries:
                    retry_count += 1
                    stats["retry_attempts"] = max(stats["retry_attempts"], retry_count)
                    
                    logger.info(f"ğŸ”„ ãƒãƒƒãƒå†è©¦è¡Œ {retry_count}/{max_retries}: {len(failed_indices)}ä»¶")
                    
                    retry_embeddings = await self._generate_embeddings_batch(batch_contents, failed_indices)
                    
                    # çµæœã‚’ãƒãƒ¼ã‚¸
                    for i in failed_indices:
                        if retry_embeddings[i] is not None:
                            embeddings[i] = retry_embeddings[i]
                    
                    failed_indices = [i for i in failed_indices if embeddings[i] is None]
                    
                    if failed_indices:
                        await asyncio.sleep(2.0)  # ãƒªãƒˆãƒ©ã‚¤é–“éš”
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°
                for i, chunk in enumerate(batch_chunks):
                    embedding_vector = embeddings[i]
                    chunk_id = chunk["id"]
                    
                    try:
                        if embedding_vector:
                            # embeddingã‚’æ›´æ–°
                            update_result = supabase.table("chunks").update({
                                "embedding": embedding_vector,
                                "updated_at": datetime.now().isoformat()
                            }).eq("id", chunk_id).execute()
                            
                            if update_result.data:
                                stats["successful"] += 1
                                logger.info(f"âœ… embeddingæ›´æ–°æˆåŠŸ: chunk_id={chunk_id}")
                            else:
                                stats["still_failed"] += 1
                                logger.error(f"âŒ embeddingæ›´æ–°å¤±æ•—: chunk_id={chunk_id}")
                        else:
                            stats["still_failed"] += 1
                            logger.warning(f"âš ï¸ embeddingç”Ÿæˆå¤±æ•—: chunk_id={chunk_id}")
                        
                        stats["processed"] += 1
                        
                    except Exception as update_error:
                        logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯æ›´æ–°ã‚¨ãƒ©ãƒ¼ (chunk_id={chunk_id}): {update_error}")
                        stats["still_failed"] += 1
                        stats["processed"] += 1
                
                # ãƒãƒƒãƒé–“ã®å¾…æ©Ÿ
                if batch_end < len(failed_chunks):
                    await asyncio.sleep(1.0)
            
            # æœ€çµ‚çµæœ
            logger.info(f"ğŸ‰ embeddingä¿®å¾©å®Œäº†:")
            logger.info(f"   - å‡¦ç†å¯¾è±¡: {stats['total_failed']}ä»¶")
            logger.info(f"   - å‡¦ç†å®Œäº†: {stats['processed']}ä»¶")
            logger.info(f"   - æˆåŠŸ: {stats['successful']}ä»¶")
            logger.info(f"   - å¤±æ•—: {stats['still_failed']}ä»¶")
            logger.info(f"   - æœ€å¤§å†è©¦è¡Œå›æ•°: {stats['retry_attempts']}å›")
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ embeddingä¿®å¾©å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            raise
    
    async def _retry_failed_embeddings_post_processing(self, doc_id: str, company_id: str, 
                                                     doc_name: str, max_retries: int = 5) -> Dict[str, Any]:
        """
        å…¨å‡¦ç†å®Œäº†å¾Œã«å¤±æ•—ã—ãŸembeddingã‚’å†å‡¦ç†ã™ã‚‹
        ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå›é¿ã®ãŸã‚ã€ã‚ˆã‚Šæ…é‡ãªå†è©¦è¡Œã‚’è¡Œã†
        """
        try:
            from supabase_adapter import get_supabase_client
            supabase = get_supabase_client()
            
            logger.info(f"ğŸ”„ {doc_name}: å¤±æ•—ã—ãŸembeddingã®å†å‡¦ç†é–‹å§‹")
            
            # å¤±æ•—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã‚’æ¤œç´¢ï¼ˆembeddingãŒNullã®ã‚‚ã®ï¼‰
            query = supabase.table("chunks").select("*").eq("doc_id", doc_id).is_("embedding", "null")
            result = query.execute()
            
            if not result.data:
                logger.info(f"âœ… {doc_name}: å†å‡¦ç†ãŒå¿…è¦ãªãƒãƒ£ãƒ³ã‚¯ã¯ã‚ã‚Šã¾ã›ã‚“")
                return {
                    "total_failed": 0,
                    "processed": 0,
                    "successful": 0,
                    "still_failed": 0,
                    "retry_attempts": 0
                }
            
            failed_chunks = result.data
            logger.info(f"ğŸ” {doc_name}: å†å‡¦ç†å¯¾è±¡ãƒãƒ£ãƒ³ã‚¯ {len(failed_chunks)}ä»¶")
            
            stats = {
                "total_failed": len(failed_chunks),
                "processed": 0,
                "successful": 0,
                "still_failed": 0,
                "retry_attempts": 0
            }
            
            # ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå›é¿ã®ãŸã‚ã€ã‚ˆã‚Šå°ã•ãªãƒãƒƒãƒã§å‡¦ç†
            batch_size = 10  # é€šå¸¸ã®50ã‹ã‚‰10ã«æ¸›ã‚‰ã™
            
            for batch_start in range(0, len(failed_chunks), batch_size):
                batch_end = min(batch_start + batch_size, len(failed_chunks))
                batch_chunks = failed_chunks[batch_start:batch_end]
                
                logger.info(f"ğŸ”„ {doc_name}: å†å‡¦ç†ãƒãƒƒãƒ {batch_start + 1}-{batch_end}/{len(failed_chunks)}")
                
                # ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå›é¿ã®ãŸã‚ã€ãƒãƒƒãƒé–“ã«é•·ã‚ã®å¾…æ©Ÿ
                if batch_start > 0:
                    await asyncio.sleep(2.0)
                
                # ãƒãƒƒãƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                batch_contents = [chunk["content"] for chunk in batch_chunks]
                
                # embeddingç”Ÿæˆï¼ˆã‚ˆã‚Šæ…é‡ãªãƒªãƒˆãƒ©ã‚¤ï¼‰
                embeddings = await self._generate_embeddings_batch(batch_contents)
                
                # å¤±æ•—ã—ãŸã‚‚ã®ã‚’æ®µéšçš„ã«å†è©¦è¡Œ
                failed_indices = [i for i, emb in enumerate(embeddings) if emb is None]
                retry_count = 0
                
                while failed_indices and retry_count < max_retries:
                    retry_count += 1
                    stats["retry_attempts"] = max(stats["retry_attempts"], retry_count)
                    
                    # ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå›é¿ã®ãŸã‚ã€å†è©¦è¡Œé–“éš”ã‚’æ®µéšçš„ã«å¢—åŠ 
                    sleep_time = min(retry_count * 2.0, 10.0)
                    logger.info(f"ğŸ”„ {doc_name}: å†è©¦è¡Œ {retry_count}/{max_retries} - {len(failed_indices)}ä»¶ ({sleep_time}ç§’å¾…æ©Ÿ)")
                    await asyncio.sleep(sleep_time)
                    
                    retry_embeddings = await self._generate_embeddings_batch(batch_contents, failed_indices)
                    
                    # çµæœã‚’ãƒãƒ¼ã‚¸
                    for i in failed_indices:
                        if retry_embeddings[i] is not None:
                            embeddings[i] = retry_embeddings[i]
                    
                    failed_indices = [i for i in failed_indices if embeddings[i] is None]
                    
                    if not failed_indices:
                        logger.info(f"âœ… {doc_name}: å†è©¦è¡Œãƒãƒƒãƒå®Œå…¨æˆåŠŸ")
                        break
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°
                for i, chunk in enumerate(batch_chunks):
                    embedding_vector = embeddings[i]
                    chunk_id = chunk["id"]
                    
                    try:
                        if embedding_vector:
                            # embeddingã‚’æ›´æ–°
                            update_result = supabase.table("chunks").update({
                                "embedding": embedding_vector,
                                "updated_at": datetime.now().isoformat()
                            }).eq("id", chunk_id).execute()
                            
                            if update_result.data:
                                stats["successful"] += 1
                                logger.debug(f"âœ… {doc_name}: embeddingæ›´æ–°æˆåŠŸ chunk_id={chunk_id}")
                            else:
                                stats["still_failed"] += 1
                                logger.error(f"âŒ {doc_name}: embeddingæ›´æ–°å¤±æ•— chunk_id={chunk_id}")
                        else:
                            stats["still_failed"] += 1
                            logger.warning(f"âš ï¸ {doc_name}: embeddingç”Ÿæˆæœ€çµ‚å¤±æ•— chunk_id={chunk_id}")
                        
                        stats["processed"] += 1
                        
                    except Exception as update_error:
                        logger.error(f"âŒ {doc_name}: ãƒãƒ£ãƒ³ã‚¯æ›´æ–°ã‚¨ãƒ©ãƒ¼ chunk_id={chunk_id}: {update_error}")
                        stats["still_failed"] += 1
                        stats["processed"] += 1
            
            # æœ€çµ‚çµæœ
            logger.info(f"ğŸ {doc_name}: embeddingå†å‡¦ç†å®Œäº†")
            logger.info(f"   - å‡¦ç†å¯¾è±¡: {stats['total_failed']}ä»¶")
            logger.info(f"   - æˆåŠŸ: {stats['successful']}ä»¶")
            logger.info(f"   - å¤±æ•—: {stats['still_failed']}ä»¶")
            logger.info(f"   - æœ€å¤§å†è©¦è¡Œ: {stats['retry_attempts']}å›")
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ {doc_name}: å¤±æ•—embeddingå†å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            raise

document_processor = DocumentProcessor()
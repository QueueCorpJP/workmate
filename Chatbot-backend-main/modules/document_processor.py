"""
ğŸ“¤ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
ğŸ§© ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆ300ã€œ500 tokenï¼‰
ğŸ§  embeddingç”Ÿæˆã‚’çµ±åˆï¼ˆGemini Flash - 768æ¬¡å…ƒï¼‰
ğŸ—ƒ Supabaseä¿å­˜ï¼ˆdocument_sources + chunksï¼‰

å®Œå…¨ãªRAGå¯¾å¿œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
"""

import os
import uuid
import logging
import asyncio
import tempfile
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import re
import tiktoken
from fastapi import HTTPException, UploadFile
from google import genai
import psycopg2
from psycopg2.extras import execute_values

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DocumentProcessor:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.gemini_client = None
        self.embedding_model = os.getenv("EMBEDDING_MODEL", "models/text-embedding-004")
        # ç’°å¢ƒå¤‰æ•°ã§è¨­å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯768æ¬¡å…ƒå¯¾å¿œãƒ¢ãƒ‡ãƒ«ï¼‰
        if not self.embedding_model.startswith("models/"):
            self.embedding_model = f"models/{self.embedding_model}"
        self.chunk_size_tokens = 400  # 300-500ãƒˆãƒ¼ã‚¯ãƒ³ã®ä¸­é–“å€¤
        self.chunk_overlap_tokens = 50  # ãƒãƒ£ãƒ³ã‚¯é–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
        self.max_chunk_size_chars = 2000  # æ–‡å­—æ•°ã§ã®ä¸Šé™
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼åˆæœŸåŒ–
        try:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
        except Exception as e:
            logger.warning(f"tiktokenåˆæœŸåŒ–å¤±æ•—: {e}")
            self.tokenizer = None
    
    def _init_gemini_client(self):
        """Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ï¼ˆæ–°ã—ã„SDKï¼‰"""
        if self.gemini_client is None:
            api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
            if not api_key:
                raise ValueError("GOOGLE_API_KEY ã¾ãŸã¯ GEMINI_API_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
            # æ–°ã—ã„SDKã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
            self.gemini_client = genai.Client(api_key=api_key)
            logger.info(f"ğŸ§  Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†ï¼ˆæ–°SDKï¼‰: {self.embedding_model}")
    
    def _count_tokens(self, text: str) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        if self.tokenizer:
            try:
                return len(self.tokenizer.encode(text))
            except Exception as e:
                logger.warning(f"ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ã®æ¨å®šï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
        # æ—¥æœ¬èª: 1æ–‡å­— â‰ˆ 1.5ãƒˆãƒ¼ã‚¯ãƒ³, è‹±èª: 4æ–‡å­— â‰ˆ 1ãƒˆãƒ¼ã‚¯ãƒ³
        japanese_chars = len(re.findall(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', text))
        other_chars = len(text) - japanese_chars
        estimated_tokens = int(japanese_chars * 1.5 + other_chars * 0.25)
        return estimated_tokens
    
    def _split_text_into_chunks(self, text: str, doc_name: str = "") -> List[Dict[str, Any]]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’æ„å‘³å˜ä½ã§ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        300-500ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¯„å›²ã§èª¿æ•´
        """
        if not text or not text.strip():
            logger.warning(f"ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆãŒæ¸¡ã•ã‚Œã¾ã—ãŸ: {doc_name}")
            return []
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’æ®µè½å˜ä½ã§åˆ†å‰²
        paragraphs = re.split(r'\n\s*\n', text.strip())
        chunks = []
        current_chunk = ""
        current_tokens = 0
        chunk_index = 0
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            paragraph_tokens = self._count_tokens(paragraph)
            
            # æ®µè½ãŒå˜ä½“ã§å¤§ãã™ãã‚‹å ´åˆã¯æ–‡å˜ä½ã§åˆ†å‰²
            if paragraph_tokens > self.chunk_size_tokens:
                # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
                if current_chunk:
                    chunks.append({
                        "chunk_index": chunk_index,
                        "content": current_chunk.strip(),
                        "token_count": current_tokens
                    })
                    chunk_index += 1
                    current_chunk = ""
                    current_tokens = 0
                
                # å¤§ããªæ®µè½ã‚’æ–‡å˜ä½ã§åˆ†å‰²
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\.\!\?]\s*', paragraph)
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    
                    sentence_tokens = self._count_tokens(sentence)
                    
                    # æ–‡ãŒå˜ä½“ã§å¤§ãã™ãã‚‹å ´åˆã¯å¼·åˆ¶åˆ†å‰²
                    if sentence_tokens > self.chunk_size_tokens:
                        if current_chunk:
                            chunks.append({
                                "chunk_index": chunk_index,
                                "content": current_chunk.strip(),
                                "token_count": current_tokens
                            })
                            chunk_index += 1
                            current_chunk = ""
                            current_tokens = 0
                        
                        # é•·ã„æ–‡ã‚’æ–‡å­—æ•°ã§å¼·åˆ¶åˆ†å‰²
                        for i in range(0, len(sentence), self.max_chunk_size_chars):
                            chunk_part = sentence[i:i + self.max_chunk_size_chars]
                            chunks.append({
                                "chunk_index": chunk_index,
                                "content": chunk_part,
                                "token_count": self._count_tokens(chunk_part)
                            })
                            chunk_index += 1
                    else:
                        # é€šå¸¸ã®æ–‡å‡¦ç†
                        if current_tokens + sentence_tokens > self.chunk_size_tokens:
                            if current_chunk:
                                chunks.append({
                                    "chunk_index": chunk_index,
                                    "content": current_chunk.strip(),
                                    "token_count": current_tokens
                                })
                                chunk_index += 1
                            current_chunk = sentence
                            current_tokens = sentence_tokens
                        else:
                            current_chunk += ("ã€‚" if current_chunk else "") + sentence
                            current_tokens += sentence_tokens
            else:
                # é€šå¸¸ã®æ®µè½å‡¦ç†
                if current_tokens + paragraph_tokens > self.chunk_size_tokens:
                    if current_chunk:
                        chunks.append({
                            "chunk_index": chunk_index,
                            "content": current_chunk.strip(),
                            "token_count": current_tokens
                        })
                        chunk_index += 1
                    current_chunk = paragraph
                    current_tokens = paragraph_tokens
                else:
                    current_chunk += ("\n\n" if current_chunk else "") + paragraph
                    current_tokens += paragraph_tokens
        
        # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ 
        if current_chunk and current_chunk.strip():
            chunks.append({
                "chunk_index": chunk_index,
                "content": current_chunk.strip(),
                "token_count": current_tokens
            })
        
        logger.info(f"ğŸ“„ {doc_name}: {len(text)}æ–‡å­— â†’ {len(chunks)}ãƒãƒ£ãƒ³ã‚¯")
        
        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®çµ±è¨ˆã‚’å‡ºåŠ›
        if chunks:
            token_counts = [chunk["token_count"] for chunk in chunks]
            avg_tokens = sum(token_counts) / len(token_counts)
            min_tokens = min(token_counts)
            max_tokens = max(token_counts)
            logger.info(f"ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³çµ±è¨ˆ - å¹³å‡: {avg_tokens:.1f}, æœ€å°: {min_tokens}, æœ€å¤§: {max_tokens}")
        
        return chunks
    
    async def _generate_embeddings_batch(self, texts: List[str], failed_indices: List[int] = None) -> List[Optional[List[float]]]:
        """Gemini Flash APIã§ãƒ†ã‚­ã‚¹ãƒˆã®embeddingã‚’å€‹åˆ¥ç”Ÿæˆï¼ˆãƒãƒƒãƒå‡¦ç†é¢¨ï¼‰"""
        if failed_indices is None:
            logger.info(f"ğŸ§  embeddingç”Ÿæˆé–‹å§‹: {len(texts)}ä»¶, ãƒ¢ãƒ‡ãƒ«={self.embedding_model}")
        else:
            logger.info(f"ğŸ”„ embeddingå†ç”Ÿæˆé–‹å§‹: {len(failed_indices)}ä»¶ã®å¤±æ•—åˆ†, ãƒ¢ãƒ‡ãƒ«={self.embedding_model}")
        
        try:
            self._init_gemini_client()
            
            all_embeddings = []
            failed_embeddings = []  # å¤±æ•—ã—ãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨˜éŒ²
            
            # å‡¦ç†å¯¾è±¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®š
            if failed_indices is None:
                # å…¨ä»¶å‡¦ç†
                process_indices = list(range(len(texts)))
                all_embeddings = [None] * len(texts)
            else:
                # å¤±æ•—åˆ†ã®ã¿å‡¦ç†
                process_indices = failed_indices
                all_embeddings = [None] * len(texts)
            
            # Gemini APIã¯å€‹åˆ¥å‡¦ç†ãŒæ¨å¥¨ã•ã‚Œã‚‹ãŸã‚ã€1ã¤ãšã¤å‡¦ç†
            for idx, i in enumerate(process_indices):
                try:
                    text = texts[i]
                    if not text or not text.strip():
                        logger.warning(f"âš ï¸ ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i}")
                        all_embeddings[i] = None
                        failed_embeddings.append(i)
                        continue
                    
                    response = await asyncio.to_thread(
                        self.gemini_client.models.embed_content,
                        model=self.embedding_model,
                        contents=text.strip()
                    )
                    
                    if response and hasattr(response, 'embeddings') and response.embeddings and len(response.embeddings) > 0:
                        embedding_vector = response.embeddings[0].values
                        all_embeddings[i] = embedding_vector
                        logger.info(f"âœ… embeddingç”ŸæˆæˆåŠŸ: {idx + 1}/{len(process_indices)} (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i}, æ¬¡å…ƒ: {len(embedding_vector)})")
                    else:
                        logger.warning(f"âš ï¸ embeddingç”Ÿæˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒä¸æ­£ã§ã™: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i}")
                        all_embeddings[i] = None
                        failed_embeddings.append(i)
                    
                    # APIåˆ¶é™å¯¾ç­–
                    await asyncio.sleep(0.2)
                    
                except Exception as e:
                    logger.error(f"âŒ embeddingç”Ÿæˆã‚¨ãƒ©ãƒ¼ (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i}): {e}")
                    all_embeddings[i] = None
                    failed_embeddings.append(i)

            success_count = len([e for e in all_embeddings if e is not None])
            total_count = len(texts)
            
            if failed_indices is None:
                logger.info(f"ğŸ‰ embeddingç”Ÿæˆå®Œäº†: {success_count}/{total_count} æˆåŠŸ")
            else:
                logger.info(f"ğŸ‰ embeddingå†ç”Ÿæˆå®Œäº†: {success_count - (total_count - len(failed_indices))}/{len(failed_indices)} æˆåŠŸ")
            
            # å¤±æ•—ã—ãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨˜éŒ²
            if failed_embeddings:
                logger.warning(f"âš ï¸ å¤±æ•—ã—ãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {failed_embeddings}")
            
            return all_embeddings

        except Exception as e:
            logger.error(f"âŒ embeddingãƒãƒƒãƒç”Ÿæˆä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {e}", exc_info=True)
            raise

    async def _save_document_metadata(self, doc_data: Dict[str, Any]) -> str:
        """document_sourcesãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        try:
            from supabase_adapter import insert_data, select_data
            
            document_id = str(uuid.uuid4())
            
            metadata = {
                "id": document_id,
                "name": doc_data["name"],
                "type": doc_data["type"],
                "page_count": doc_data.get("page_count", 1),
                "uploaded_by": doc_data["uploaded_by"],
                "company_id": doc_data["company_id"],
                "uploaded_at": datetime.now().isoformat()
            }
            
            result = insert_data("document_sources", metadata)
            
            if result and result.data:
                logger.info(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {document_id} - {doc_data['name']}")
                return document_id
            else:
                raise Exception("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
        except Exception as main_error:
            logger.error(f"âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {main_error}")
            
            # å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’ç¢ºèª
            error_str = str(main_error)
            if "document_sources_uploaded_by_fkey" in error_str:
                logger.warning(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ '{doc_data['uploaded_by']}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“ - company_idã§ä»£æ›¿ä¿å­˜ã‚’è©¦è¡Œ")
                try:
                    # company_idã‹ã‚‰ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’æ¤œç´¢
                    company_users = select_data(
                        "users",
                        columns="id",
                        filters={"company_id": doc_data["company_id"]}
                    )
                    
                    if company_users.data and len(company_users.data) > 0:
                        alternative_user_id = company_users.data[0]["id"]
                        logger.info(f"ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ç™ºè¦‹: {alternative_user_id}")
                        
                        # ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã§å†ä¿å­˜
                        metadata["uploaded_by"] = alternative_user_id
                        result = insert_data("document_sources", metadata)
                        
                        if result and result.data:
                            logger.info(f"âœ… ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {document_id}")
                            return document_id
                        else:
                            raise Exception("ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                    else:
                        # ä¼šç¤¾ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã€ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ä½œæˆ
                        logger.warning(f"ä¼šç¤¾ '{doc_data['company_id']}' ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå­˜åœ¨ã—ã¾ã›ã‚“ - ãƒ†ã‚¹ãƒˆç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ä½œæˆ")
                        test_user_data = {
                            "id": doc_data["uploaded_by"],
                            "company_id": doc_data["company_id"],
                            "name": "Test User",
                            "email": "test@example.com",
                            "created_at": datetime.now().isoformat()
                        }
                        
                        user_result = insert_data("users", test_user_data)
                        if user_result and user_result.data:
                            logger.info(f"âœ… ãƒ†ã‚¹ãƒˆç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆå®Œäº†: {doc_data['uploaded_by']}")
                            
                            # å…ƒã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§å†ä¿å­˜
                            result = insert_data("document_sources", metadata)
                            if result and result.data:
                                logger.info(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {document_id}")
                                return document_id
                            else:
                                raise Exception("ãƒ†ã‚¹ãƒˆç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆå¾Œã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                        else:
                            raise Exception("ãƒ†ã‚¹ãƒˆç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                            
                except Exception as fallback_error:
                    logger.error(f"âŒ ä»£æ›¿ä¿å­˜å‡¦ç†ã‚¨ãƒ©ãƒ¼: {fallback_error}")
                    raise Exception(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {fallback_error}")
            else:
                raise main_error
    
    async def _save_chunks_to_database(self, doc_id: str, chunks: List[Dict[str, Any]],
                                     company_id: str, doc_name: str, max_retries: int = 10) -> Dict[str, Any]:
        """chunksãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã¨embeddingã‚’50å€‹å˜ä½ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ä¿å­˜"""
        try:
            from supabase_adapter import get_supabase_client
            supabase = get_supabase_client()

            stats = {
                "total_chunks": len(chunks),
                "saved_chunks": 0,
                "successful_embeddings": 0,
                "failed_embeddings": 0,
                "retry_attempts": 0
            }

            if not chunks:
                return stats

            batch_size = 50
            total_batches = (len(chunks) + batch_size - 1) // batch_size
            
            logger.info(f"ğŸš€ {doc_name}: {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’{batch_size}å€‹å˜ä½ã§å‡¦ç†é–‹å§‹")
            logger.info(f"ğŸ“Š äºˆæƒ³ãƒãƒƒãƒæ•°: {total_batches}")

            # 50å€‹å˜ä½ã§embeddingç”Ÿæˆâ†’å³åº§ã«insert
            for batch_num in range(0, len(chunks), batch_size):
                batch_chunks = chunks[batch_num:batch_num + batch_size]
                current_batch = (batch_num // batch_size) + 1
                
                logger.info(f"ğŸ§  ãƒãƒƒãƒ {current_batch}/{total_batches}: {len(batch_chunks)}å€‹ã®embeddingç”Ÿæˆé–‹å§‹")
                
                # ã“ã®ãƒãƒƒãƒã®embeddingç”Ÿæˆ
                batch_contents = [chunk["content"] for chunk in batch_chunks]
                batch_embeddings = await self._generate_embeddings_batch(batch_contents)
                
                # å¤±æ•—ã—ãŸembeddingã®ãƒªãƒˆãƒ©ã‚¤å‡¦ç†
                failed_indices = [i for i, emb in enumerate(batch_embeddings) if emb is None]
                retry_count = 0
                
                while failed_indices and retry_count < max_retries:
                    retry_count += 1
                    logger.info(f"ğŸ”„ ãƒãƒƒãƒ {current_batch} embeddingå†ç”Ÿæˆ (è©¦è¡Œ {retry_count}/{max_retries}): {len(failed_indices)}ä»¶")
                    
                    retry_embeddings = await self._generate_embeddings_batch(batch_contents, failed_indices)
                    
                    for i in failed_indices:
                        if retry_embeddings[i] is not None:
                            batch_embeddings[i] = retry_embeddings[i]
                    
                    failed_indices = [i for i in failed_indices if batch_embeddings[i] is None]
                    
                    if failed_indices:
                        logger.warning(f"âš ï¸ ãƒãƒƒãƒ {current_batch} å†è©¦è¡Œå¾Œã‚‚å¤±æ•—: {len(failed_indices)}ä»¶")
                        await asyncio.sleep(1.0)
                    else:
                        logger.info(f"âœ… ãƒãƒƒãƒ {current_batch} å†è©¦è¡ŒæˆåŠŸ")
                        break
                
                # çµ±è¨ˆæ›´æ–°
                for embedding in batch_embeddings:
                    if embedding:
                        stats["successful_embeddings"] += 1
                    else:
                        stats["failed_embeddings"] += 1
                
                if retry_count > 0:
                    stats["retry_attempts"] = max(stats["retry_attempts"], retry_count)
                
                # æˆåŠŸã—ãŸembeddingã®ã¿ã§ãƒ¬ã‚³ãƒ¼ãƒ‰æº–å‚™
                records_to_insert = []
                for i, chunk_data in enumerate(batch_chunks):
                    embedding_vector = batch_embeddings[i]
                    if embedding_vector:  # æˆåŠŸã—ãŸembeddingã®ã¿
                        records_to_insert.append({
                            "doc_id": doc_id,
                            "chunk_index": chunk_data["chunk_index"],
                            "content": chunk_data["content"],
                            "embedding": embedding_vector,
                            "company_id": company_id,
                            "created_at": datetime.now().isoformat(),
                            "updated_at": datetime.now().isoformat()
                        })
                
                # å³åº§ã«Supabaseã«æŒ¿å…¥
                if records_to_insert:
                    try:
                        logger.info(f"ğŸ’¾ ãƒãƒƒãƒ {current_batch}/{total_batches}: {len(records_to_insert)}ä»¶ã‚’å³åº§ã«ä¿å­˜ä¸­...")
                        result = supabase.table("chunks").insert(records_to_insert).execute()
                        
                        if result.data:
                            batch_saved = len(result.data)
                            stats["saved_chunks"] += batch_saved
                            logger.info(f"âœ… ãƒãƒƒãƒ {current_batch}/{total_batches}: {batch_saved}ä»¶ä¿å­˜å®Œäº†")
                        else:
                            logger.error(f"âŒ ãƒãƒƒãƒ {current_batch}/{total_batches} ä¿å­˜ã‚¨ãƒ©ãƒ¼: {result.error}")
                            
                    except Exception as batch_error:
                        logger.error(f"âŒ ãƒãƒƒãƒ {current_batch}/{total_batches} ä¿å­˜ä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {batch_error}")
                        # ãƒãƒƒãƒã‚¨ãƒ©ãƒ¼ã§ã‚‚æ¬¡ã®ãƒãƒƒãƒå‡¦ç†ã‚’ç¶šè¡Œ
                        continue
                else:
                    logger.warning(f"âš ï¸ ãƒãƒƒãƒ {current_batch}/{total_batches}: ä¿å­˜å¯èƒ½ãªãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
                
                # ãƒãƒƒãƒå®Œäº†ãƒ­ã‚°
                logger.info(f"ğŸ¯ ãƒãƒƒãƒ {current_batch}/{total_batches} å®Œäº†: embedding {len(batch_embeddings) - len(failed_indices)}/{len(batch_embeddings)} æˆåŠŸ, ä¿å­˜ {len(records_to_insert)} ä»¶")

            # æœ€çµ‚çµæœã®ã‚µãƒãƒªãƒ¼
            logger.info(f"ğŸ {doc_name}: å…¨å‡¦ç†å®Œäº†")
            logger.info(f"ğŸ“ˆ æœ€çµ‚çµæœ: ä¿å­˜ {stats['saved_chunks']}/{stats['total_chunks']} ãƒãƒ£ãƒ³ã‚¯")
            logger.info(f"ğŸ§  embedding: æˆåŠŸ {stats['successful_embeddings']}, å¤±æ•— {stats['failed_embeddings']}")
            
            if stats["failed_embeddings"] > 0:
                logger.warning(f"âš ï¸ æœ€çµ‚çµæœ: {stats['successful_embeddings']}/{stats['total_chunks']} embeddingæˆåŠŸ, {stats['retry_attempts']}å›å†è©¦è¡Œ")
            else:
                logger.info(f"ğŸ‰ å…¨embeddingç”ŸæˆæˆåŠŸ: {stats['successful_embeddings']}/{stats['total_chunks']}")

            return stats

        except Exception as e:
            logger.error(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒãƒƒãƒä¿å­˜ä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {e}", exc_info=True)
            raise
    
    async def process_uploaded_file(self, file: UploadFile, user_id: str, 
                                  company_id: str) -> Dict[str, Any]:
        """
        ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®Œå…¨å‡¦ç†
        1ï¸âƒ£ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        2ï¸âƒ£ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        3ï¸âƒ£ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆ300ã€œ500 tokenï¼‰
        4ï¸âƒ£ embeddingç”Ÿæˆï¼ˆGemini Flash - 768æ¬¡å…ƒï¼‰
        5ï¸âƒ£ Supabaseä¿å­˜
        """
        try:
            logger.info(f"ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file.filename}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿è¾¼ã¿
            file_content = await file.read()
            file_size_mb = len(file_content) / (1024 * 1024)
            
            logger.info(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.2f} MB")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«å¿œã˜ã¦ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            extracted_text = await self._extract_text_from_file(file, file_content)
            
            if not extracted_text or not extracted_text.strip():
                raise HTTPException(status_code=400, detail="ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
            logger.info(f"ğŸ“ æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆ: {len(extracted_text)} æ–‡å­—")
            
            # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            chunks = self._split_text_into_chunks(extracted_text, file.filename)
            
            if not chunks:
                raise HTTPException(status_code=400, detail="ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            doc_data = {
                "name": file.filename,
                "type": self._detect_file_type(file.filename),
                "page_count": self._estimate_page_count(extracted_text),
                "uploaded_by": user_id,
                "company_id": company_id
            }
            
            document_id = await self._save_document_metadata(doc_data)
            
            # ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            save_stats = await self._save_chunks_to_database(
                document_id, chunks, company_id, file.filename
            )
            
            # å‡¦ç†çµæœã‚’è¿”ã™
            result = {
                "success": True,
                "document_id": document_id,
                "filename": file.filename,
                "file_size_mb": round(file_size_mb, 2),
                "text_length": len(extracted_text),
                "total_chunks": save_stats["total_chunks"],
                "saved_chunks": save_stats["saved_chunks"],
                "successful_embeddings": save_stats["successful_embeddings"],
                "failed_embeddings": save_stats["failed_embeddings"],
                "message": f"âœ… {file.filename} ã®å‡¦ç†ãƒ»embeddingç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ"
            }
            
            logger.info(f"ğŸ‰ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†: {file.filename}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise HTTPException(status_code=500, detail=f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    
    async def _extract_text_from_file(self, file: UploadFile, content: bytes) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«å¿œã˜ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        filename = file.filename.lower()
        
        try:
            if filename.endswith(('.pdf',)):
                return await self._extract_text_from_pdf(content)
            elif filename.endswith(('.xlsx', '.xls')):
                return await self._extract_text_from_excel(content)
            elif filename.endswith(('.docx', '.doc')):
                return await self._extract_text_from_word(content)
            elif filename.endswith(('.txt', '.csv')):
                return await self._extract_text_from_text(content)
            elif filename.endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp')):
                return await self._extract_text_from_image(content)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦èª­ã¿è¾¼ã¿
                return content.decode('utf-8', errors='ignore')
                
        except Exception as e:
            logger.error(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({filename}): {e}")
            raise
    
    async def _extract_text_from_pdf(self, content: bytes) -> str:
        """PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆGeminiç›´æ¥å‡¦ç†ã§ã‚·ãƒ³ãƒ—ãƒ«åŒ–ï¼‰"""
        try:
            logger.info("ğŸ”„ Geminiç›´æ¥å‡¦ç†ã§PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºé–‹å§‹")
            
            # Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
            self._init_gemini_client()
            
            # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚ä¿å­˜
            import tempfile
            import os
            
            with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp_file:
                tmp_file.write(content)
                tmp_file_path = tmp_file.name
            
            try:
                # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                prompt = """
                ã“ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’æ­£ç¢ºã«ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
                
                é‡è¦ãªæŒ‡ç¤ºï¼š
                1. æ–‡æ›¸ã®æ§‹é€ ï¼ˆè¦‹å‡ºã—ã€æ®µè½ã€è¡¨ã€ãƒªã‚¹ãƒˆãªã©ï¼‰ã‚’ä¿æŒã—ã¦ãã ã•ã„
                2. æ—¥æœ¬èªã®æ–‡å­—åŒ–ã‘ãŒã‚ã‚Œã°é©åˆ‡ã«ä¿®æ­£ã—ã¦ãã ã•ã„  
                3. è¡¨ãŒã‚ã‚‹å ´åˆã¯ã€è¡Œã¨åˆ—ã®æ§‹é€ ã‚’ä¿æŒã—ã¦ãã ã•ã„
                4. ãƒšãƒ¼ã‚¸ç•ªå·ã‚„ç« æ§‹æˆãŒã‚ã‚Œã°è­˜åˆ¥ã—ã¦ãã ã•ã„
                5. å›³è¡¨ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚‚å«ã‚ã¦æŠ½å‡ºã—ã¦ãã ã•ã„
                
                å‡ºåŠ›ã¯å…ƒã®PDFæ§‹é€ ã‚’ä¿ã£ãŸå½¢ã§ã€èª­ã¿ã‚„ã™ã„ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
                """
                
                # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥Geminiã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆæ–°SDKï¼‰
                uploaded_file = await asyncio.to_thread(
                    self.gemini_client.files.upload,
                    file=tmp_file_path
                )
                
                # åŒæœŸå‡¦ç†ã‚’éåŒæœŸã§å®Ÿè¡Œ
                response = await asyncio.to_thread(
                    self.gemini_client.models.generate_content,
                    model='gemini-1.5-flash',
                    contents=[prompt, uploaded_file]
                )
                
                if response.text and response.text.strip():
                    logger.info(f"âœ… Gemini PDFå‡¦ç†æˆåŠŸ: {len(response.text)} æ–‡å­—")
                    
                    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼ˆæ–°SDKï¼‰
                    try:
                        await asyncio.to_thread(
                            self.gemini_client.files.delete,
                            name=uploaded_file.name
                        )
                    except:
                        pass
                        
                    return response.text
                else:
                    raise Exception("Geminiã‹ã‚‰PDFãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    
            finally:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                if os.path.exists(tmp_file_path):
                    os.unlink(tmp_file_path)
            
        except Exception as e:
            logger.error(f"Gemini PDFå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®PyPDF2å‡¦ç†ï¼ˆæœ€å°é™ï¼‰
            try:
                logger.info("ğŸ”™ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: PyPDF2å‡¦ç†")
                import PyPDF2
                from io import BytesIO
                
                pdf_reader = PyPDF2.PdfReader(BytesIO(content))
                text_parts = []
                
                for page_num, page in enumerate(pdf_reader.pages):
                    try:
                        page_text = page.extract_text()
                        if page_text and page_text.strip():
                            text_parts.append(f"=== ãƒšãƒ¼ã‚¸ {page_num + 1} ===\n{page_text}")
                    except Exception as page_error:
                        logger.warning(f"PDF ãƒšãƒ¼ã‚¸ {page_num + 1} æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {page_error}")
                        continue
                
                if text_parts:
                    logger.info(f"âœ… PyPDF2ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆåŠŸ: {len(text_parts)} ãƒšãƒ¼ã‚¸")
                    return "\n\n".join(text_parts)
                else:
                    raise Exception("PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
                
            except Exception as fallback_error:
                logger.error(f"PyPDF2ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {fallback_error}")
                raise Exception(f"PDFå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {fallback_error}")
    
    async def _extract_text_from_excel(self, content: bytes) -> str:
        """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        try:
            import pandas as pd
            from io import BytesIO
            
            # å…¨ã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
            excel_file = pd.ExcelFile(BytesIO(content))
            text_parts = []
            
            for sheet_name in excel_file.sheet_names:
                try:
                    df = pd.read_excel(excel_file, sheet_name=sheet_name)
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
                    sheet_text = f"=== ã‚·ãƒ¼ãƒˆ: {sheet_name} ===\n"
                    sheet_text += df.to_string(index=False, na_rep='')
                    text_parts.append(sheet_text)
                    
                except Exception as e:
                    logger.warning(f"Excel ã‚·ãƒ¼ãƒˆ {sheet_name} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            return "\n\n".join(text_parts)
            
        except Exception as e:
            logger.error(f"Excelå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    async def _extract_text_from_word(self, content: bytes) -> str:
        """Wordãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        try:
            import docx
            from io import BytesIO
            
            doc = docx.Document(BytesIO(content))
            text_parts = []
            
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    text_parts.append(paragraph.text)
            
            return "\n\n".join(text_parts)
            
        except Exception as e:
            logger.error(f"Wordå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    async def _extract_text_from_text(self, content: bytes) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å†…å®¹ã‚’æŠ½å‡º"""
        try:
            # è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
            encodings = ['utf-8', 'shift_jis', 'cp932', 'euc-jp', 'iso-2022-jp']
            
            for encoding in encodings:
                try:
                    return content.decode(encoding)
                except UnicodeDecodeError:
                    continue
            
            # ã™ã¹ã¦å¤±æ•—ã—ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–ã—ã¦èª­ã¿è¾¼ã¿
            return content.decode('utf-8', errors='ignore')
            
        except Exception as e:
            logger.error(f"ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    async def _extract_text_from_image(self, content: bytes) -> str:
        """ç”»åƒã‹ã‚‰OCRã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        try:
            # Gemini Vision APIã‚’ä½¿ç”¨ã—ã¦OCR
            self._init_gemini_client()
            
            # ç”»åƒã‚’base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            import base64
            image_b64 = base64.b64encode(content).decode('utf-8')
            
            # Gemini Vision APIã§OCRï¼ˆå®Ÿè£…ã¯ç’°å¢ƒã«å¿œã˜ã¦èª¿æ•´ï¼‰
            # ã“ã“ã§ã¯ç°¡å˜ãªå®Ÿè£…ä¾‹
            logger.info("ğŸ–¼ï¸ ç”»åƒOCRå‡¦ç†ï¼ˆGemini Vision APIï¼‰")
            
            # OCRçµæœã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
            return "ç”»åƒã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆOCRå‡¦ç†ãŒå¿…è¦ï¼‰"
            
        except Exception as e:
            logger.error(f"ç”»åƒOCRå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return "ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºä¸å¯ï¼‰"
    
    def _detect_file_type(self, filename: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‹ã‚‰ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š"""
        filename = filename.lower()
        
        if filename.endswith('.pdf'):
            return 'PDF'
        elif filename.endswith(('.xlsx', '.xls')):
            return 'Excel'
        elif filename.endswith(('.docx', '.doc')):
            return 'Word'
        elif filename.endswith('.csv'):
            return 'CSV'
        elif filename.endswith('.txt'):
            return 'Text'
        elif filename.endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp')):
            return 'Image'
        else:
            return 'Unknown'

    def _estimate_page_count(self, text: str) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒšãƒ¼ã‚¸æ•°ã‚’æ¨å®š"""
        # 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šç´„500ãƒˆãƒ¼ã‚¯ãƒ³ã¨ä»®å®š
        tokens = self._count_tokens(text)
        return max(1, (tokens + 499) // 500)

    async def retry_failed_embeddings(self, doc_id: str = None, company_id: str = None, max_retries: int = 10) -> Dict[str, Any]:
        """
        æ—¢å­˜ã®ãƒãƒ£ãƒ³ã‚¯ã§å¤±æ•—ã—ãŸembeddingã‚’å†ç”Ÿæˆ
        doc_id: ç‰¹å®šã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã¿å‡¦ç†ï¼ˆNoneã®å ´åˆã¯å…¨ã¦ï¼‰
        company_id: ç‰¹å®šã®ä¼šç¤¾ã®ã¿å‡¦ç†ï¼ˆNoneã®å ´åˆã¯å…¨ã¦ï¼‰
        """
        try:
            from supabase_adapter import get_supabase_client
            supabase = get_supabase_client()
            
            logger.info(f"ğŸ” å¤±æ•—ã—ãŸembeddingã®æ¤œç´¢é–‹å§‹ (doc_id: {doc_id}, company_id: {company_id})")
            
            # å¤±æ•—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã‚’æ¤œç´¢ï¼ˆembeddingãŒNullã®ã‚‚ã®ï¼‰
            query = supabase.table("chunks").select("*").is_("embedding", "null")
            
            if doc_id:
                query = query.eq("doc_id", doc_id)
            if company_id:
                query = query.eq("company_id", company_id)
            
            result = query.execute()
            
            if not result.data:
                logger.info("âœ… å¤±æ•—ã—ãŸembeddingã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return {
                    "total_failed": 0,
                    "processed": 0,
                    "successful": 0,
                    "still_failed": 0,
                    "retry_attempts": 0
                }
            
            failed_chunks = result.data
            logger.info(f"ğŸ” å¤±æ•—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã‚’ç™ºè¦‹: {len(failed_chunks)}ä»¶")
            
            stats = {
                "total_failed": len(failed_chunks),
                "processed": 0,
                "successful": 0,
                "still_failed": 0,
                "retry_attempts": 0
            }
            
            # ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒãƒƒãƒã§å‡¦ç†ï¼ˆ50ä»¶ãšã¤ï¼‰
            batch_size = 50
            for batch_start in range(0, len(failed_chunks), batch_size):
                batch_end = min(batch_start + batch_size, len(failed_chunks))
                batch_chunks = failed_chunks[batch_start:batch_end]
                
                logger.info(f"ğŸ“¦ ãƒãƒƒãƒå‡¦ç†: {batch_start + 1}-{batch_end}/{len(failed_chunks)}")
                
                # ãƒãƒƒãƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                batch_contents = [chunk["content"] for chunk in batch_chunks]
                batch_indices = list(range(len(batch_contents)))
                
                # embeddingç”Ÿæˆï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
                embeddings = await self._generate_embeddings_batch(batch_contents)
                
                # å¤±æ•—ã—ãŸã‚‚ã®ã‚’å†è©¦è¡Œ
                failed_indices = [i for i, emb in enumerate(embeddings) if emb is None]
                retry_count = 0
                
                while failed_indices and retry_count < max_retries:
                    retry_count += 1
                    stats["retry_attempts"] = max(stats["retry_attempts"], retry_count)
                    
                    logger.info(f"ğŸ”„ ãƒãƒƒãƒå†è©¦è¡Œ {retry_count}/{max_retries}: {len(failed_indices)}ä»¶")
                    
                    retry_embeddings = await self._generate_embeddings_batch(batch_contents, failed_indices)
                    
                    # çµæœã‚’ãƒãƒ¼ã‚¸
                    for i in failed_indices:
                        if retry_embeddings[i] is not None:
                            embeddings[i] = retry_embeddings[i]
                    
                    failed_indices = [i for i in failed_indices if embeddings[i] is None]
                    
                    if failed_indices:
                        await asyncio.sleep(2.0)  # ãƒªãƒˆãƒ©ã‚¤é–“éš”
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°
                for i, chunk in enumerate(batch_chunks):
                    embedding_vector = embeddings[i]
                    chunk_id = chunk["id"]
                    
                    try:
                        if embedding_vector:
                            # embeddingã‚’æ›´æ–°
                            update_result = supabase.table("chunks").update({
                                "embedding": embedding_vector,
                                "updated_at": datetime.now().isoformat()
                            }).eq("id", chunk_id).execute()
                            
                            if update_result.data:
                                stats["successful"] += 1
                                logger.info(f"âœ… embeddingæ›´æ–°æˆåŠŸ: chunk_id={chunk_id}")
                            else:
                                stats["still_failed"] += 1
                                logger.error(f"âŒ embeddingæ›´æ–°å¤±æ•—: chunk_id={chunk_id}")
                        else:
                            stats["still_failed"] += 1
                            logger.warning(f"âš ï¸ embeddingç”Ÿæˆå¤±æ•—: chunk_id={chunk_id}")
                        
                        stats["processed"] += 1
                        
                    except Exception as update_error:
                        logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯æ›´æ–°ã‚¨ãƒ©ãƒ¼ (chunk_id={chunk_id}): {update_error}")
                        stats["still_failed"] += 1
                        stats["processed"] += 1
                
                # ãƒãƒƒãƒé–“ã®å¾…æ©Ÿ
                if batch_end < len(failed_chunks):
                    await asyncio.sleep(1.0)
            
            # æœ€çµ‚çµæœ
            logger.info(f"ğŸ‰ embeddingä¿®å¾©å®Œäº†:")
            logger.info(f"   - å‡¦ç†å¯¾è±¡: {stats['total_failed']}ä»¶")
            logger.info(f"   - å‡¦ç†å®Œäº†: {stats['processed']}ä»¶")
            logger.info(f"   - æˆåŠŸ: {stats['successful']}ä»¶")
            logger.info(f"   - å¤±æ•—: {stats['still_failed']}ä»¶")
            logger.info(f"   - æœ€å¤§å†è©¦è¡Œå›æ•°: {stats['retry_attempts']}å›")
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ embeddingä¿®å¾©å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            raise

document_processor = DocumentProcessor()
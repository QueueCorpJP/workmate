"""
ğŸ“¤ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
ğŸ§© ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆ300ã€œ500 tokenï¼‰
ğŸ§  embeddingç”Ÿæˆã‚’çµ±åˆï¼ˆGemini Flash - 768æ¬¡å…ƒï¼‰
ğŸ—ƒ Supabaseä¿å­˜ï¼ˆdocument_sources + chunksï¼‰

å®Œå…¨ãªRAGå¯¾å¿œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
"""

import os
import uuid
import logging
import asyncio
import tempfile
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import re
import tiktoken
from fastapi import HTTPException, UploadFile
import google.generativeai as genai
import psycopg2
from psycopg2.extras import execute_values

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DocumentProcessor:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.gemini_client = None
        self.embedding_model = os.getenv("EMBEDDING_MODEL", "models/text-embedding-004")
        # ç’°å¢ƒå¤‰æ•°ã§è¨­å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯768æ¬¡å…ƒå¯¾å¿œãƒ¢ãƒ‡ãƒ«ï¼‰
        if not self.embedding_model.startswith("models/"):
            self.embedding_model = f"models/{self.embedding_model}"
        self.chunk_size_tokens = 400  # 300-500ãƒˆãƒ¼ã‚¯ãƒ³ã®ä¸­é–“å€¤
        self.chunk_overlap_tokens = 50  # ãƒãƒ£ãƒ³ã‚¯é–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
        self.max_chunk_size_chars = 2000  # æ–‡å­—æ•°ã§ã®ä¸Šé™
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼åˆæœŸåŒ–
        try:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
        except Exception as e:
            logger.warning(f"tiktokenåˆæœŸåŒ–å¤±æ•—: {e}")
            self.tokenizer = None
    
    def _init_gemini_client(self):
        """Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        if self.gemini_client is None:
            api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
            if not api_key:
                raise ValueError("GOOGLE_API_KEY ã¾ãŸã¯ GEMINI_API_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
            genai.configure(api_key=api_key)
            self.gemini_client = genai
            logger.info(f"ğŸ§  Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†: {self.embedding_model}")
    
    def _count_tokens(self, text: str) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        if self.tokenizer:
            try:
                return len(self.tokenizer.encode(text))
            except Exception as e:
                logger.warning(f"ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ã®æ¨å®šï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
        # æ—¥æœ¬èª: 1æ–‡å­— â‰ˆ 1.5ãƒˆãƒ¼ã‚¯ãƒ³, è‹±èª: 4æ–‡å­— â‰ˆ 1ãƒˆãƒ¼ã‚¯ãƒ³
        japanese_chars = len(re.findall(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', text))
        other_chars = len(text) - japanese_chars
        estimated_tokens = int(japanese_chars * 1.5 + other_chars * 0.25)
        return estimated_tokens
    
    def _split_text_into_chunks(self, text: str, doc_name: str = "") -> List[Dict[str, Any]]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’æ„å‘³å˜ä½ã§ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        300-500ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¯„å›²ã§èª¿æ•´
        """
        if not text or not text.strip():
            logger.warning(f"ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆãŒæ¸¡ã•ã‚Œã¾ã—ãŸ: {doc_name}")
            return []
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’æ®µè½å˜ä½ã§åˆ†å‰²
        paragraphs = re.split(r'\n\s*\n', text.strip())
        chunks = []
        current_chunk = ""
        current_tokens = 0
        chunk_index = 0
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            paragraph_tokens = self._count_tokens(paragraph)
            
            # æ®µè½ãŒå˜ä½“ã§å¤§ãã™ãã‚‹å ´åˆã¯æ–‡å˜ä½ã§åˆ†å‰²
            if paragraph_tokens > self.chunk_size_tokens:
                # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
                if current_chunk:
                    chunks.append({
                        "chunk_index": chunk_index,
                        "content": current_chunk.strip(),
                        "token_count": current_tokens
                    })
                    chunk_index += 1
                    current_chunk = ""
                    current_tokens = 0
                
                # å¤§ããªæ®µè½ã‚’æ–‡å˜ä½ã§åˆ†å‰²
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\.\!\?]\s*', paragraph)
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    
                    sentence_tokens = self._count_tokens(sentence)
                    
                    # æ–‡ãŒå˜ä½“ã§å¤§ãã™ãã‚‹å ´åˆã¯å¼·åˆ¶åˆ†å‰²
                    if sentence_tokens > self.chunk_size_tokens:
                        if current_chunk:
                            chunks.append({
                                "chunk_index": chunk_index,
                                "content": current_chunk.strip(),
                                "token_count": current_tokens
                            })
                            chunk_index += 1
                            current_chunk = ""
                            current_tokens = 0
                        
                        # é•·ã„æ–‡ã‚’æ–‡å­—æ•°ã§å¼·åˆ¶åˆ†å‰²
                        for i in range(0, len(sentence), self.max_chunk_size_chars):
                            chunk_part = sentence[i:i + self.max_chunk_size_chars]
                            chunks.append({
                                "chunk_index": chunk_index,
                                "content": chunk_part,
                                "token_count": self._count_tokens(chunk_part)
                            })
                            chunk_index += 1
                    else:
                        # é€šå¸¸ã®æ–‡å‡¦ç†
                        if current_tokens + sentence_tokens > self.chunk_size_tokens:
                            if current_chunk:
                                chunks.append({
                                    "chunk_index": chunk_index,
                                    "content": current_chunk.strip(),
                                    "token_count": current_tokens
                                })
                                chunk_index += 1
                            current_chunk = sentence
                            current_tokens = sentence_tokens
                        else:
                            current_chunk += ("ã€‚" if current_chunk else "") + sentence
                            current_tokens += sentence_tokens
            else:
                # é€šå¸¸ã®æ®µè½å‡¦ç†
                if current_tokens + paragraph_tokens > self.chunk_size_tokens:
                    if current_chunk:
                        chunks.append({
                            "chunk_index": chunk_index,
                            "content": current_chunk.strip(),
                            "token_count": current_tokens
                        })
                        chunk_index += 1
                    current_chunk = paragraph
                    current_tokens = paragraph_tokens
                else:
                    current_chunk += ("\n\n" if current_chunk else "") + paragraph
                    current_tokens += paragraph_tokens
        
        # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ 
        if current_chunk and current_chunk.strip():
            chunks.append({
                "chunk_index": chunk_index,
                "content": current_chunk.strip(),
                "token_count": current_tokens
            })
        
        logger.info(f"ğŸ“„ {doc_name}: {len(text)}æ–‡å­— â†’ {len(chunks)}ãƒãƒ£ãƒ³ã‚¯")
        
        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®çµ±è¨ˆã‚’å‡ºåŠ›
        if chunks:
            token_counts = [chunk["token_count"] for chunk in chunks]
            avg_tokens = sum(token_counts) / len(token_counts)
            min_tokens = min(token_counts)
            max_tokens = max(token_counts)
            logger.info(f"ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³çµ±è¨ˆ - å¹³å‡: {avg_tokens:.1f}, æœ€å°: {min_tokens}, æœ€å¤§: {max_tokens}")
        
        return chunks
    
    async def _generate_embeddings_batch(self, texts: List[str]) -> List[Optional[List[float]]]:
        """Gemini Flash APIã§ãƒ†ã‚­ã‚¹ãƒˆã®embeddingã‚’å€‹åˆ¥ç”Ÿæˆï¼ˆãƒãƒƒãƒå‡¦ç†é¢¨ï¼‰"""
        logger.info(f"ğŸ§  embeddingç”Ÿæˆé–‹å§‹: {len(texts)}ä»¶, ãƒ¢ãƒ‡ãƒ«={self.embedding_model}")
        try:
            self._init_gemini_client()
            
            all_embeddings = []
            
            # Gemini APIã¯å€‹åˆ¥å‡¦ç†ãŒæ¨å¥¨ã•ã‚Œã‚‹ãŸã‚ã€1ã¤ãšã¤å‡¦ç†
            for i, text in enumerate(texts):
                try:
                    if not text or not text.strip():
                        logger.warning(f"âš ï¸ ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i}")
                        all_embeddings.append(None)
                        continue
                    
                    response = await asyncio.to_thread(
                        self.gemini_client.embed_content,
                        model=self.embedding_model,
                        content=text.strip()
                    )
                    
                    if response and 'embedding' in response:
                        embedding_vector = response['embedding']
                        all_embeddings.append(embedding_vector)
                        logger.info(f"âœ… embeddingç”ŸæˆæˆåŠŸ: {i + 1}/{len(texts)} (æ¬¡å…ƒ: {len(embedding_vector)})")
                    else:
                        logger.warning(f"âš ï¸ embeddingç”Ÿæˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒä¸æ­£ã§ã™: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i}")
                        all_embeddings.append(None)
                    
                    # APIåˆ¶é™å¯¾ç­–
                    await asyncio.sleep(0.2)
                    
                except Exception as e:
                    logger.error(f"âŒ embeddingç”Ÿæˆã‚¨ãƒ©ãƒ¼ (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i}): {e}")
                    all_embeddings.append(None)

            success_count = len([e for e in all_embeddings if e is not None])
            logger.info(f"ğŸ‰ embeddingç”Ÿæˆå®Œäº†: {success_count}/{len(texts)} æˆåŠŸ")
            return all_embeddings

        except Exception as e:
            logger.error(f"âŒ embeddingãƒãƒƒãƒç”Ÿæˆä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {e}", exc_info=True)
            raise

    async def _save_document_metadata(self, doc_data: Dict[str, Any]) -> str:
        """document_sourcesãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        try:
            from supabase_adapter import insert_data, select_data
            
            document_id = str(uuid.uuid4())
            
            metadata = {
                "id": document_id,
                "name": doc_data["name"],
                "type": doc_data["type"],
                "page_count": doc_data.get("page_count", 1),
                "uploaded_by": doc_data["uploaded_by"],
                "company_id": doc_data["company_id"],
                "uploaded_at": datetime.now().isoformat()
            }
            
            result = insert_data("document_sources", metadata)
            
            if result and result.data:
                logger.info(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {document_id} - {doc_data['name']}")
                return document_id
            else:
                raise Exception("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
        except Exception as main_error:
            logger.error(f"âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {main_error}")
            
            # å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’ç¢ºèª
            error_str = str(main_error)
            if "document_sources_uploaded_by_fkey" in error_str:
                logger.warning(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ '{doc_data['uploaded_by']}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“ - company_idã§ä»£æ›¿ä¿å­˜ã‚’è©¦è¡Œ")
                try:
                    # company_idã‹ã‚‰ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’æ¤œç´¢
                    company_users = select_data(
                        "users",
                        columns="id",
                        filters={"company_id": doc_data["company_id"]}
                    )
                    
                    if company_users.data and len(company_users.data) > 0:
                        alternative_user_id = company_users.data[0]["id"]
                        logger.info(f"ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ç™ºè¦‹: {alternative_user_id}")
                        
                        # ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã§å†ä¿å­˜
                        metadata["uploaded_by"] = alternative_user_id
                        result = insert_data("document_sources", metadata)
                        
                        if result and result.data:
                            logger.info(f"âœ… ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {document_id}")
                            return document_id
                        else:
                            raise Exception("ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                    else:
                        # ä¼šç¤¾ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã€ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ä½œæˆ
                        logger.warning(f"ä¼šç¤¾ '{doc_data['company_id']}' ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå­˜åœ¨ã—ã¾ã›ã‚“ - ãƒ†ã‚¹ãƒˆç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ä½œæˆ")
                        test_user_data = {
                            "id": doc_data["uploaded_by"],
                            "company_id": doc_data["company_id"],
                            "name": "Test User",
                            "email": "test@example.com",
                            "created_at": datetime.now().isoformat()
                        }
                        
                        user_result = insert_data("users", test_user_data)
                        if user_result and user_result.data:
                            logger.info(f"âœ… ãƒ†ã‚¹ãƒˆç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆå®Œäº†: {doc_data['uploaded_by']}")
                            
                            # å…ƒã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§å†ä¿å­˜
                            result = insert_data("document_sources", metadata)
                            if result and result.data:
                                logger.info(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {document_id}")
                                return document_id
                            else:
                                raise Exception("ãƒ†ã‚¹ãƒˆç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆå¾Œã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                        else:
                            raise Exception("ãƒ†ã‚¹ãƒˆç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                            
                except Exception as fallback_error:
                    logger.error(f"âŒ ä»£æ›¿ä¿å­˜å‡¦ç†ã‚¨ãƒ©ãƒ¼: {fallback_error}")
                    raise Exception(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {fallback_error}")
            else:
                raise main_error
    
    async def _save_chunks_to_database(self, doc_id: str, chunks: List[Dict[str, Any]],
                                     company_id: str, doc_name: str) -> Dict[str, Any]:
        """chunksãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã¨embeddingã‚’ãƒãƒƒãƒã§ä¿å­˜"""
        try:
            from supabase_adapter import get_supabase_client
            supabase = get_supabase_client()

            stats = {
                "total_chunks": len(chunks),
                "saved_chunks": 0,
                "successful_embeddings": 0,
                "failed_embeddings": 0
            }

            if not chunks:
                return stats

            # ãƒãƒƒãƒã§embeddingã‚’ç”Ÿæˆ
            contents = [chunk["content"] for chunk in chunks]
            embeddings = await self._generate_embeddings_batch(contents)

            records_to_insert = []
            for i, chunk_data in enumerate(chunks):
                embedding_vector = embeddings[i]
                if embedding_vector:
                    stats["successful_embeddings"] += 1
                else:
                    stats["failed_embeddings"] += 1
                
                records_to_insert.append({
                    "doc_id": doc_id,
                    "chunk_index": chunk_data["chunk_index"],
                    "content": chunk_data["content"],
                    "embedding": embedding_vector,
                    "company_id": company_id,
                    "created_at": datetime.now().isoformat(),
                    "updated_at": datetime.now().isoformat()
                })

            # Supabaseã«ãƒãƒƒãƒã§æŒ¿å…¥
            if records_to_insert:
                result = supabase.table("chunks").insert(records_to_insert).execute()
                if result.data:
                    stats["saved_chunks"] = len(result.data)
                    logger.info(f"âœ… {doc_name}: {stats['saved_chunks']}/{len(chunks)} ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å®Œäº†")
                else:
                    logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯ä¸€æ‹¬ä¿å­˜ã‚¨ãƒ©ãƒ¼: {result.error}")

            return stats

        except Exception as e:
            logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯ä¸€æ‹¬ä¿å­˜ä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {e}", exc_info=True)
            raise
    
    async def process_uploaded_file(self, file: UploadFile, user_id: str, 
                                  company_id: str) -> Dict[str, Any]:
        """
        ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®Œå…¨å‡¦ç†
        1ï¸âƒ£ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        2ï¸âƒ£ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        3ï¸âƒ£ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆ300ã€œ500 tokenï¼‰
        4ï¸âƒ£ embeddingç”Ÿæˆï¼ˆGemini Flash - 768æ¬¡å…ƒï¼‰
        5ï¸âƒ£ Supabaseä¿å­˜
        """
        try:
            logger.info(f"ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file.filename}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿è¾¼ã¿
            file_content = await file.read()
            file_size_mb = len(file_content) / (1024 * 1024)
            
            logger.info(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.2f} MB")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«å¿œã˜ã¦ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            extracted_text = await self._extract_text_from_file(file, file_content)
            
            if not extracted_text or not extracted_text.strip():
                raise HTTPException(status_code=400, detail="ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
            logger.info(f"ğŸ“ æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆ: {len(extracted_text)} æ–‡å­—")
            
            # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            chunks = self._split_text_into_chunks(extracted_text, file.filename)
            
            if not chunks:
                raise HTTPException(status_code=400, detail="ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            doc_data = {
                "name": file.filename,
                "type": self._detect_file_type(file.filename),
                "page_count": self._estimate_page_count(extracted_text),
                "uploaded_by": user_id,
                "company_id": company_id
            }
            
            document_id = await self._save_document_metadata(doc_data)
            
            # ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            save_stats = await self._save_chunks_to_database(
                document_id, chunks, company_id, file.filename
            )
            
            # å‡¦ç†çµæœã‚’è¿”ã™
            result = {
                "success": True,
                "document_id": document_id,
                "filename": file.filename,
                "file_size_mb": round(file_size_mb, 2),
                "text_length": len(extracted_text),
                "total_chunks": save_stats["total_chunks"],
                "saved_chunks": save_stats["saved_chunks"],
                "successful_embeddings": save_stats["successful_embeddings"],
                "failed_embeddings": save_stats["failed_embeddings"],
                "message": f"âœ… {file.filename} ã®å‡¦ç†ãƒ»embeddingç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ"
            }
            
            logger.info(f"ğŸ‰ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†: {file.filename}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise HTTPException(status_code=500, detail=f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    
    async def _extract_text_from_file(self, file: UploadFile, content: bytes) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«å¿œã˜ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        filename = file.filename.lower()
        
        try:
            if filename.endswith(('.pdf',)):
                return await self._extract_text_from_pdf(content)
            elif filename.endswith(('.xlsx', '.xls')):
                return await self._extract_text_from_excel(content)
            elif filename.endswith(('.docx', '.doc')):
                return await self._extract_text_from_word(content)
            elif filename.endswith(('.txt', '.csv')):
                return await self._extract_text_from_text(content)
            elif filename.endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp')):
                return await self._extract_text_from_image(content)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦èª­ã¿è¾¼ã¿
                return content.decode('utf-8', errors='ignore')
                
        except Exception as e:
            logger.error(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({filename}): {e}")
            raise
    
    async def _extract_text_from_pdf(self, content: bytes) -> str:
        """PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        try:
            import PyPDF2
            from io import BytesIO
            
            pdf_reader = PyPDF2.PdfReader(BytesIO(content))
            text_parts = []
            
            for page_num, page in enumerate(pdf_reader.pages):
                try:
                    page_text = page.extract_text()
                    if page_text.strip():
                        text_parts.append(f"=== ãƒšãƒ¼ã‚¸ {page_num + 1} ===\n{page_text}")
                except Exception as e:
                    logger.warning(f"PDF ãƒšãƒ¼ã‚¸ {page_num + 1} æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            return "\n\n".join(text_parts)
            
        except Exception as e:
            logger.error(f"PDFå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    async def _extract_text_from_excel(self, content: bytes) -> str:
        """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        try:
            import pandas as pd
            from io import BytesIO
            
            # å…¨ã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
            excel_file = pd.ExcelFile(BytesIO(content))
            text_parts = []
            
            for sheet_name in excel_file.sheet_names:
                try:
                    df = pd.read_excel(excel_file, sheet_name=sheet_name)
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
                    sheet_text = f"=== ã‚·ãƒ¼ãƒˆ: {sheet_name} ===\n"
                    sheet_text += df.to_string(index=False, na_rep='')
                    text_parts.append(sheet_text)
                    
                except Exception as e:
                    logger.warning(f"Excel ã‚·ãƒ¼ãƒˆ {sheet_name} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            return "\n\n".join(text_parts)
            
        except Exception as e:
            logger.error(f"Excelå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    async def _extract_text_from_word(self, content: bytes) -> str:
        """Wordãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        try:
            import docx
            from io import BytesIO
            
            doc = docx.Document(BytesIO(content))
            text_parts = []
            
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    text_parts.append(paragraph.text)
            
            return "\n\n".join(text_parts)
            
        except Exception as e:
            logger.error(f"Wordå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    async def _extract_text_from_text(self, content: bytes) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å†…å®¹ã‚’æŠ½å‡º"""
        try:
            # è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
            encodings = ['utf-8', 'shift_jis', 'cp932', 'euc-jp', 'iso-2022-jp']
            
            for encoding in encodings:
                try:
                    return content.decode(encoding)
                except UnicodeDecodeError:
                    continue
            
            # ã™ã¹ã¦å¤±æ•—ã—ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–ã—ã¦èª­ã¿è¾¼ã¿
            return content.decode('utf-8', errors='ignore')
            
        except Exception as e:
            logger.error(f"ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    async def _extract_text_from_image(self, content: bytes) -> str:
        """ç”»åƒã‹ã‚‰OCRã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        try:
            # Gemini Vision APIã‚’ä½¿ç”¨ã—ã¦OCR
            self._init_gemini_client()
            
            # ç”»åƒã‚’base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            import base64
            image_b64 = base64.b64encode(content).decode('utf-8')
            
            # Gemini Vision APIã§OCRï¼ˆå®Ÿè£…ã¯ç’°å¢ƒã«å¿œã˜ã¦èª¿æ•´ï¼‰
            # ã“ã“ã§ã¯ç°¡å˜ãªå®Ÿè£…ä¾‹
            logger.info("ğŸ–¼ï¸ ç”»åƒOCRå‡¦ç†ï¼ˆGemini Vision APIï¼‰")
            
            # OCRçµæœã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
            return "ç”»åƒã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆOCRå‡¦ç†ãŒå¿…è¦ï¼‰"
            
        except Exception as e:
            logger.error(f"ç”»åƒOCRå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return "ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºä¸å¯ï¼‰"
    
    def _detect_file_type(self, filename: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‹ã‚‰ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š"""
        filename = filename.lower()
        
        if filename.endswith('.pdf'):
            return 'PDF'
        elif filename.endswith(('.xlsx', '.xls')):
            return 'Excel'
        elif filename.endswith(('.docx', '.doc')):
            return 'Word'
        elif filename.endswith('.csv'):
            return 'CSV'
        elif filename.endswith('.txt'):
            return 'Text'
        elif filename.endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp')):
            return 'Image'
        else:
            return 'Unknown'

    def _estimate_page_count(self, text: str) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒšãƒ¼ã‚¸æ•°ã‚’æ¨å®š"""
        # 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šç´„500ãƒˆãƒ¼ã‚¯ãƒ³ã¨ä»®å®š
        tokens = self._count_tokens(text)
        return max(1, (tokens + 499) // 500)

document_processor = DocumentProcessor()
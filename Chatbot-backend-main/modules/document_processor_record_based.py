"""
ğŸ“¤ ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
ğŸ—ƒ Excelãƒ»CSVç­‰ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’è¡Œå˜ä½ã§å‡¦ç†
ğŸ§  å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã”ã¨ã«embeddingç”Ÿæˆ
ğŸ” è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã®æ¤œç´¢ãƒ»è³ªå•å¿œç­”ã«æœ€é©åŒ–

Excelãƒ•ã‚¡ã‚¤ãƒ«ãªã©ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’è¡Œï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰å˜ä½ã§å‡¦ç†ã—ã€
ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã§ã¯ãªãã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã¨ã—ã¦æ‰±ã†
"""

import os
import uuid
import logging
import asyncio
import tempfile
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import pandas as pd
from fastapi import HTTPException, UploadFile
from .document_processor import DocumentProcessor
from .excel_data_cleaner import ExcelDataCleaner
from .multi_api_embedding import get_multi_api_embedding_client, multi_api_embedding_available

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DocumentProcessorRecordBased:
    """ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        # åŸºæœ¬çš„ãªDocumentProcessorã®è¨­å®šã‚’ç¶™æ‰¿
        self.base_processor = DocumentProcessor()
        self.excel_cleaner = ExcelDataCleaner()
        
        # ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹å‡¦ç†å›ºæœ‰ã®è¨­å®š
        self.max_record_length = 1000  # 1ãƒ¬ã‚³ãƒ¼ãƒ‰ã®æœ€å¤§æ–‡å­—æ•°
        self.min_meaningful_columns = 2  # æ„å‘³ã®ã‚ã‚‹åˆ—ã®æœ€å°æ•°
        
        logger.info("âœ… ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼åˆæœŸåŒ–å®Œäº†")
    
    async def process_uploaded_file(self, file: UploadFile, user_id: str, 
                                  company_id: str) -> Dict[str, Any]:
        """
        ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã§å‡¦ç†
        1ï¸âƒ£ Excelãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        2ï¸âƒ£ è¡Œå˜ä½ã§ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        3ï¸âƒ£ å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã®embeddingç”Ÿæˆ
        4ï¸âƒ£ ãƒ¬ã‚³ãƒ¼ãƒ‰å˜ä½ã§Supabaseä¿å­˜
        """
        try:
            logger.info(f"ğŸš€ ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file.filename}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿è¾¼ã¿
            file_content = await file.read()
            file_size_mb = len(file_content) / (1024 * 1024)
            
            logger.info(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.2f} MB")
            
            # Excelå½¢å¼ã®ã¿ã‚µãƒãƒ¼ãƒˆ
            if not self._is_excel_file(file.filename):
                raise HTTPException(
                    status_code=400, 
                    detail="ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹å‡¦ç†ã¯Excelãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.xlsx, .xlsï¼‰ã®ã¿ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™"
                )
            
            # Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¬ã‚³ãƒ¼ãƒ‰å˜ä½ã§å‡¦ç†
            records = await self._extract_records_from_excel(file_content, file.filename)
            
            if not records:
                raise HTTPException(
                    status_code=400, 
                    detail="Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æœ‰åŠ¹ãªãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ"
                )
            
            logger.info(f"ğŸ“Š æŠ½å‡ºãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(records)}")
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            doc_data = {
                "name": file.filename,
                "type": "Excel (ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹)",
                "page_count": self._calculate_page_count(records),
                "uploaded_by": user_id,
                "company_id": company_id,
                "special": f"ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(records)}"
            }
            
            document_id = await self.base_processor._save_document_metadata(doc_data)
            
            # ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            save_stats = await self._save_records_to_database(
                document_id, records, company_id, file.filename
            )
            
            # å‡¦ç†çµæœã‚’è¿”ã™
            result = {
                "success": True,
                "document_id": document_id,
                "filename": file.filename,
                "file_size_mb": round(file_size_mb, 2),
                "text_length": save_stats.get("total_text_length", 0),  # è¿½åŠ 
                "record_count": len(records),
                "saved_records": save_stats["saved_chunks"],
                "successful_embeddings": save_stats["successful_embeddings"],
                "failed_embeddings": save_stats["failed_embeddings"],
                "processing_type": "record_based",
                "message": f"âœ… {file.filename} ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹å‡¦ç†ãƒ»embeddingç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ"
            }
            
            logger.info(f"ğŸ‰ ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†: {file.filename}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise HTTPException(
                status_code=500, 
                detail=f"ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            )
    
    async def _extract_records_from_excel(self, content: bytes, filename: str) -> List[Dict[str, Any]]:
        """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        try:
            logger.info(f"ğŸ“Š Excel ãƒ¬ã‚³ãƒ¼ãƒ‰æŠ½å‡ºé–‹å§‹: {filename}")
            
            # ExcelDataCleanerã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹é€ åŒ–
            cleaned_text = self.excel_cleaner.clean_excel_data(content)
            
            # pandas ã§Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥èª­ã¿è¾¼ã¿
            excel_file = pd.ExcelFile(content)
            all_records = []
            
            for sheet_name in excel_file.sheet_names:
                try:
                    logger.info(f"ğŸ“‹ ã‚·ãƒ¼ãƒˆå‡¦ç†é–‹å§‹: {sheet_name}")
                    
                    # ã‚·ãƒ¼ãƒˆã‚’DataFrameã¨ã—ã¦èª­ã¿è¾¼ã¿
                    df = pd.read_excel(excel_file, sheet_name=sheet_name, header=0)
                    
                    if df.empty:
                        logger.warning(f"âš ï¸ ã‚·ãƒ¼ãƒˆ {sheet_name} ã¯ç©ºã§ã™")
                        continue
                    
                    # ç©ºã®è¡Œãƒ»åˆ—ã‚’å‰Šé™¤
                    df = df.dropna(how='all').dropna(axis=1, how='all')
                    
                    if df.empty:
                        continue
                    
                    # ã‚·ãƒ¼ãƒˆã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
                    sheet_records = self._extract_records_from_dataframe(df, sheet_name)
                    all_records.extend(sheet_records)
                    
                    logger.info(f"âœ… ã‚·ãƒ¼ãƒˆ {sheet_name}: {len(sheet_records)} ãƒ¬ã‚³ãƒ¼ãƒ‰æŠ½å‡º")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ ã‚·ãƒ¼ãƒˆ {sheet_name} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            if not all_records:
                logger.warning("âš ï¸ å…¨ã‚·ãƒ¼ãƒˆã§ãƒ¬ã‚³ãƒ¼ãƒ‰æŠ½å‡ºã«å¤±æ•—")
                return []
            
            logger.info(f"ğŸ‰ Excel ãƒ¬ã‚³ãƒ¼ãƒ‰æŠ½å‡ºå®Œäº†: {len(all_records)} ãƒ¬ã‚³ãƒ¼ãƒ‰")
            return all_records
            
        except Exception as e:
            logger.error(f"âŒ Excel ãƒ¬ã‚³ãƒ¼ãƒ‰æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _extract_records_from_dataframe(self, df: pd.DataFrame, sheet_name: str) -> List[Dict[str, Any]]:
        """DataFrameã‹ã‚‰ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        records = []
        
        try:
            # åˆ—åã‚’æ­£è¦åŒ–
            df.columns = [self._normalize_column_name(str(col)) for col in df.columns]
            
            # å„è¡Œã‚’ãƒ¬ã‚³ãƒ¼ãƒ‰ã¨ã—ã¦å‡¦ç†
            for index, row in df.iterrows():
                try:
                    # ç©ºã®è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if row.isna().all():
                        continue
                    
                    # ãƒ¬ã‚³ãƒ¼ãƒ‰ã®å†…å®¹ã‚’æ§‹ç¯‰
                    record_data = {}
                    record_parts = []
                    meaningful_columns = 0
                    
                    for col in df.columns:
                        value = row[col]
                        if pd.notna(value) and str(value).strip():
                            clean_value = str(value).strip()
                            record_data[col] = clean_value
                            record_parts.append(f"{col}: {clean_value}")
                            meaningful_columns += 1
                    
                    # æ„å‘³ã®ã‚ã‚‹åˆ—ãŒå°‘ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    if meaningful_columns < self.min_meaningful_columns:
                        continue
                    
                    # ãƒ¬ã‚³ãƒ¼ãƒ‰ã®å†…å®¹ã‚’ä½œæˆ
                    record_content = " | ".join(record_parts)
                    
                    # ãƒ¬ã‚³ãƒ¼ãƒ‰ã®é•·ã•åˆ¶é™
                    if len(record_content) > self.max_record_length:
                        record_content = record_content[:self.max_record_length] + "..."
                    
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                    record = {
                        "chunk_index": len(records),  # chunks ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã®äº’æ›æ€§ã®ãŸã‚
                        "content": record_content,
                        "token_count": self.base_processor._count_tokens(record_content),
                        "sheet_name": sheet_name,
                        "row_index": index,
                        "record_data": record_data,
                        "column_count": meaningful_columns
                    }
                    
                    records.append(record)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ è¡Œ {index} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            return records
            
        except Exception as e:
            logger.error(f"âŒ DataFrame ãƒ¬ã‚³ãƒ¼ãƒ‰æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _normalize_column_name(self, column_name: str) -> str:
        """åˆ—åã‚’æ­£è¦åŒ–"""
        # ç„¡æ„å‘³ãªåˆ—åã‚’æ„å‘³ã®ã‚ã‚‹ã‚‚ã®ã«ç½®æ›
        if not column_name or column_name.startswith('Unnamed'):
            return f"åˆ—_{uuid.uuid4().hex[:8]}"
        
        # æ–‡å­—åˆ—ã‚’æ¸…æ½”ã«
        normalized = str(column_name).strip()
        
        # ç©ºç™½ã‚’ç½®æ›
        normalized = normalized.replace('\n', ' ').replace('\r', ' ')
        
        return normalized
    
    async def _save_records_to_database(self, doc_id: str, records: List[Dict[str, Any]],
                                      company_id: str, doc_name: str, max_retries: int = 3) -> Dict[str, Any]:
        """ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ï¼ˆchunksãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ç”¨ï¼‰"""
        try:
            from supabase_adapter import get_supabase_client
            supabase = get_supabase_client()

            stats = {
                "total_chunks": len(records),
                "saved_chunks": 0,
                "successful_embeddings": 0,
                "failed_embeddings": 0,
                "retry_attempts": 0,
                "total_text_length": 0  # æ–°ã—ãè¿½åŠ 
            }

            if not records:
                return stats

            batch_size = 20  # ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã§ã¯å°ã•ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ä½¿ç”¨
            total_batches = (len(records) + batch_size - 1) // batch_size
            
            logger.info(f"ğŸš€ {doc_name}: {len(records)}ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’{batch_size}å€‹å˜ä½ã§å‡¦ç†é–‹å§‹")
            logger.info(f"ğŸ“Š äºˆæƒ³ãƒãƒƒãƒæ•°: {total_batches}")

            # ãƒ¬ã‚³ãƒ¼ãƒ‰å˜ä½ã§embeddingç”Ÿæˆâ†’å³åº§ã«insert
            for batch_num in range(0, len(records), batch_size):
                batch_records = records[batch_num:batch_num + batch_size]
                current_batch = (batch_num // batch_size) + 1
                
                logger.info(f"ğŸ§  ãƒãƒƒãƒ {current_batch}/{total_batches}: {len(batch_records)}ãƒ¬ã‚³ãƒ¼ãƒ‰ã®embeddingç”Ÿæˆé–‹å§‹")
                
                # ã“ã®ãƒãƒƒãƒã®embeddingç”Ÿæˆ
                batch_contents = [record["content"] for record in batch_records]
                batch_embeddings = await self.base_processor._generate_embeddings_batch(batch_contents)
                
                # å¤±æ•—ã—ãŸembeddingã®ãƒªãƒˆãƒ©ã‚¤å‡¦ç†
                failed_indices = [i for i, emb in enumerate(batch_embeddings) if emb is None]
                retry_count = 0
                
                while failed_indices and retry_count < max_retries:
                    retry_count += 1
                    logger.info(f"ğŸ”„ ãƒãƒƒãƒ {current_batch} embeddingå†ç”Ÿæˆ (è©¦è¡Œ {retry_count}/{max_retries}): {len(failed_indices)}ä»¶")
                    
                    retry_embeddings = await self.base_processor._generate_embeddings_batch(batch_contents, failed_indices)
                    
                    for i in failed_indices:
                        if retry_embeddings[i] is not None:
                            batch_embeddings[i] = retry_embeddings[i]
                    
                    failed_indices = [i for i in failed_indices if batch_embeddings[i] is None]
                    
                    if failed_indices:
                        logger.warning(f"âš ï¸ ãƒãƒƒãƒ {current_batch} å†è©¦è¡Œå¾Œã‚‚å¤±æ•—: {len(failed_indices)}ä»¶")
                        await asyncio.sleep(1.0)
                    else:
                        logger.info(f"âœ… ãƒãƒƒãƒ {current_batch} å†è©¦è¡ŒæˆåŠŸ")
                        break
                
                # çµ±è¨ˆæ›´æ–°
                for embedding in batch_embeddings:
                    if embedding:
                        stats["successful_embeddings"] += 1
                    else:
                        stats["failed_embeddings"] += 1
                
                if retry_count > 0:
                    stats["retry_attempts"] = max(stats["retry_attempts"], retry_count)
                
                # æˆåŠŸã—ãŸembeddingã®ã¿ã§ãƒ¬ã‚³ãƒ¼ãƒ‰æº–å‚™
                records_to_insert = []
                for i, record_data in enumerate(batch_records):
                    embedding_vector = batch_embeddings[i]
                    if embedding_vector:  # æˆåŠŸã—ãŸembeddingã®ã¿
                        stats["total_text_length"] += len(record_data["content"])
                        # chunksãƒ†ãƒ¼ãƒ–ãƒ«ã«æŒ¿å…¥ã™ã‚‹ãŸã‚ã®ãƒ¬ã‚³ãƒ¼ãƒ‰å½¢å¼
                        records_to_insert.append({
                            "doc_id": doc_id,
                            "chunk_index": record_data["chunk_index"],
                            "content": record_data["content"],
                            "embedding": embedding_vector,
                            "company_id": company_id,
                            "created_at": datetime.now().isoformat(),
                            "updated_at": datetime.now().isoformat()
                        })
                
                # å³åº§ã«Supabaseã«æŒ¿å…¥
                if records_to_insert:
                    try:
                        logger.info(f"ğŸ’¾ ãƒãƒƒãƒ {current_batch}/{total_batches}: {len(records_to_insert)}ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å³åº§ã«ä¿å­˜ä¸­...")
                        result = supabase.table("chunks").insert(records_to_insert).execute()
                        
                        if result.data:
                            batch_saved = len(result.data)
                            stats["saved_chunks"] += batch_saved
                            logger.info(f"âœ… ãƒãƒƒãƒ {current_batch}/{total_batches}: {batch_saved}ãƒ¬ã‚³ãƒ¼ãƒ‰ä¿å­˜å®Œäº†")
                        else:
                            logger.error(f"âŒ ãƒãƒƒãƒ {current_batch}/{total_batches} ä¿å­˜ã‚¨ãƒ©ãƒ¼: {result.error}")
                            
                    except Exception as batch_error:
                        logger.error(f"âŒ ãƒãƒƒãƒ {current_batch}/{total_batches} ä¿å­˜ä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {batch_error}")
                        # ãƒãƒƒãƒã‚¨ãƒ©ãƒ¼ã§ã‚‚æ¬¡ã®ãƒãƒƒãƒå‡¦ç†ã‚’ç¶šè¡Œ
                        continue
                else:
                    logger.warning(f"âš ï¸ ãƒãƒƒãƒ {current_batch}/{total_batches}: ä¿å­˜å¯èƒ½ãªãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
                
                # ãƒãƒƒãƒå®Œäº†ãƒ­ã‚°
                logger.info(f"ğŸ¯ ãƒãƒƒãƒ {current_batch}/{total_batches} å®Œäº†: embedding {len(batch_embeddings) - len(failed_indices)}/{len(batch_embeddings)} æˆåŠŸ, ä¿å­˜ {len(records_to_insert)} ãƒ¬ã‚³ãƒ¼ãƒ‰")

            # æœ€çµ‚çµæœã®ã‚µãƒãƒªãƒ¼
            logger.info(f"ğŸ {doc_name}: ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹å‡¦ç†å®Œäº†")
            logger.info(f"ğŸ“ˆ æœ€çµ‚çµæœ: ä¿å­˜ {stats['saved_chunks']}/{stats['total_chunks']} ãƒ¬ã‚³ãƒ¼ãƒ‰")
            logger.info(f"ğŸ§  embedding: æˆåŠŸ {stats['successful_embeddings']}, å¤±æ•— {stats['failed_embeddings']}")
            
            if stats["failed_embeddings"] > 0:
                logger.warning(f"âš ï¸ æœ€çµ‚çµæœ: {stats['successful_embeddings']}/{stats['total_chunks']} embeddingæˆåŠŸ, {stats['retry_attempts']}å›å†è©¦è¡Œ")
            else:
                logger.info(f"ğŸ‰ å…¨embeddingç”ŸæˆæˆåŠŸ: {stats['successful_embeddings']}/{stats['total_chunks']}")

            return stats

        except Exception as e:
            logger.error(f"âŒ ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ä¿å­˜ä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {e}", exc_info=True)
            raise
    
    def _is_excel_file(self, filename: str) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãŒExcelå½¢å¼ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        return filename.lower().endswith(('.xlsx', '.xls'))
    
    def _calculate_page_count(self, records: List[Dict[str, Any]]) -> int:
        """ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã‹ã‚‰ãƒšãƒ¼ã‚¸æ•°ã‚’æ¨å®šï¼ˆ1ãƒšãƒ¼ã‚¸=50ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰"""
        return max(1, (len(records) + 49) // 50)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
document_processor_record_based = DocumentProcessorRecordBased()
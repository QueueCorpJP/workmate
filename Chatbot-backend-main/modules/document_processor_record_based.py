"""
ğŸ“¤ ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
ğŸ—ƒ Excelãƒ»CSVç­‰ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’è¡Œå˜ä½ã§å‡¦ç†
ğŸ§  å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã”ã¨ã«embeddingç”Ÿæˆ
ğŸ” è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã®æ¤œç´¢ãƒ»è³ªå•å¿œç­”ã«æœ€é©åŒ–

Excelãƒ•ã‚¡ã‚¤ãƒ«ãªã©ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’è¡Œï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰å˜ä½ã§å‡¦ç†ã—ã€
ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã§ã¯ãªãã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã¨ã—ã¦æ‰±ã†
"""

import os
import uuid
import logging
import asyncio
import tempfile
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import pandas as pd
from fastapi import HTTPException, UploadFile
from .document_processor import DocumentProcessor
from .excel_data_cleaner import ExcelDataCleaner
from .multi_api_embedding import get_multi_api_embedding_client, multi_api_embedding_available

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DocumentProcessorRecordBased:
    """ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        # åŸºæœ¬çš„ãªDocumentProcessorã®è¨­å®šã‚’ç¶™æ‰¿
        self.base_processor = DocumentProcessor()
        self.excel_cleaner = ExcelDataCleaner()
        
        # ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹å‡¦ç†å›ºæœ‰ã®è¨­å®š
        self.max_record_length = 1000  # 1ãƒ¬ã‚³ãƒ¼ãƒ‰ã®æœ€å¤§æ–‡å­—æ•°
        self.min_meaningful_columns = 2  # æ„å‘³ã®ã‚ã‚‹åˆ—ã®æœ€å°æ•°
        
        logger.info("âœ… ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼åˆæœŸåŒ–å®Œäº†")
    
    async def process_uploaded_file(self, file: UploadFile, user_id: str, 
                                  company_id: str) -> Dict[str, Any]:
        """
        ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã§å‡¦ç†
        1ï¸âƒ£ Excelãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        2ï¸âƒ£ è¡Œå˜ä½ã§ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        3ï¸âƒ£ å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã®embeddingç”Ÿæˆ
        4ï¸âƒ£ ãƒ¬ã‚³ãƒ¼ãƒ‰å˜ä½ã§Supabaseä¿å­˜
        """
        try:
            logger.info(f"ğŸš€ ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file.filename}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿è¾¼ã¿
            file_content = await file.read()
            file_size_mb = len(file_content) / (1024 * 1024)
            
            logger.info(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.2f} MB")
            
            # Excelå½¢å¼ã®ã¿ã‚µãƒãƒ¼ãƒˆ
            if not self._is_excel_file(file.filename):
                raise HTTPException(
                    status_code=400, 
                    detail="ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹å‡¦ç†ã¯Excelãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.xlsx, .xlsï¼‰ã®ã¿ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™"
                )
            
            # Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¬ã‚³ãƒ¼ãƒ‰å˜ä½ã§å‡¦ç†
            records = await self._extract_records_from_excel(file_content, file.filename)
            
            if not records:
                raise HTTPException(
                    status_code=400, 
                    detail="Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æœ‰åŠ¹ãªãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ"
                )
            
            logger.info(f"ğŸ“Š æŠ½å‡ºãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(records)}")
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            doc_data = {
                "name": file.filename,
                "type": "Excel (ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹)",
                "page_count": self._calculate_page_count(records),
                "uploaded_by": user_id,
                "company_id": company_id,
                "special": f"ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(records)}"
            }
            
            # Excelç”¨ã®metadataã‚’ç”Ÿæˆ
            metadata_json = None
            try:
                import json
                import re
                from datetime import datetime
                
                # å…¨ãƒ¬ã‚³ãƒ¼ãƒ‰ã‹ã‚‰åˆ—åã‚’é›†ç´„
                all_columns = set()
                for record in records:
                    if 'columns' in record:
                        all_columns.update(record['columns'])
                
                logger.info(f"é›†ç´„ã•ã‚ŒãŸåˆ—å: {list(all_columns)}")
                
                # å€¤ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æ—¥ä»˜åˆ—ã‚’è‡ªå‹•æ¤œå‡º
                date_types = {}
                
                for col in all_columns:
                    # ã“ã®åˆ—ã®å€¤ã‚’è¤‡æ•°ãƒ¬ã‚³ãƒ¼ãƒ‰ã‹ã‚‰å–å¾—ã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                    sample_values = []
                    for record in records[:10]:  # æœ€åˆã®10ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ã‚µãƒ³ãƒ—ãƒ«
                        if 'record_data' in record and col in record['record_data']:
                            value = record['record_data'][col]
                            if value and str(value).strip():
                                sample_values.append(str(value).strip())
                    
                    logger.info(f"åˆ— '{col}' ã®ã‚µãƒ³ãƒ—ãƒ«å€¤: {sample_values[:3]}...")  # æœ€åˆã®3ã¤ã ã‘è¡¨ç¤º
                    
                    if sample_values:
                        # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
                        date_like_count = 0
                        for value in sample_values:
                            if self._is_date_like(value):
                                date_like_count += 1
                        
                        # 70%ä»¥ä¸ŠãŒæ—¥ä»˜ã£ã½ã„å ´åˆã¯æ—¥ä»˜åˆ—ã¨ã—ã¦åˆ¤å®š
                        if date_like_count >= len(sample_values) * 0.7:
                            date_types[col] = "date"
                            logger.info(f"åˆ— '{col}' ã‚’æ—¥ä»˜åˆ—ã¨ã—ã¦åˆ¤å®š ({date_like_count}/{len(sample_values)})")
                
                # åŸºæœ¬çš„ãªmetadataã‚’å¿…ãšç”Ÿæˆï¼ˆåˆ—ãŒç„¡ã„å ´åˆã§ã‚‚ï¼‰
                metadata_json = json.dumps({
                    "columns": list(all_columns),
                    "date_types": date_types,
                    "file_type": "excel",
                    "record_count": len(records)
                }, ensure_ascii=False)
                
                logger.info(f"ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹å‡¦ç†ã§metadataç”Ÿæˆ: columns={len(all_columns)}, date_types={len(date_types)}")
                logger.info(f"ç”Ÿæˆã•ã‚ŒãŸmetadata_json: {metadata_json}")
            except Exception as meta_error:
                logger.warning(f"ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹å‡¦ç†metadataç”Ÿæˆå¤±æ•—: {meta_error}")
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã§ã‚‚åŸºæœ¬çš„ãªmetadataã‚’ç”Ÿæˆ
                try:
                    metadata_json = json.dumps({
                        "columns": [],
                        "date_types": {},
                        "file_type": "excel",
                        "record_count": len(records),
                        "error": str(meta_error)
                    }, ensure_ascii=False)
                    logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯metadataç”Ÿæˆ: {metadata_json}")
                except Exception as fallback_error:
                    logger.error(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯metadataç”Ÿæˆã‚‚å¤±æ•—: {fallback_error}")
                    metadata_json = '{"error": "metadata generation failed"}'
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«metadata_jsonã‚’è¿½åŠ 
            doc_data["metadata"] = metadata_json
            logger.info(f"doc_dataã«metadataè¨­å®š: {doc_data.get('metadata')}")
            
            document_id = await self.base_processor._save_document_metadata(doc_data)
            
            # ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            save_stats = await self._save_records_to_database(
                document_id, records, company_id, file.filename
            )
            
            # å‡¦ç†çµæœã‚’è¿”ã™
            result = {
                "success": True,
                "document_id": document_id,
                "filename": file.filename,
                "file_size_mb": round(file_size_mb, 2),
                "text_length": save_stats.get("total_text_length", 0),  # è¿½åŠ 
                "record_count": len(records),
                "saved_records": save_stats["saved_chunks"],
                "successful_embeddings": save_stats["successful_embeddings"],
                "failed_embeddings": save_stats["failed_embeddings"],
                "processing_type": "record_based",
                "message": f"âœ… {file.filename} ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹å‡¦ç†ãƒ»embeddingç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ"
            }
            
            logger.info(f"ğŸ‰ ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†: {file.filename}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise HTTPException(
                status_code=500, 
                detail=f"ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            )
    
    async def _extract_records_from_excel(self, content: bytes, filename: str) -> List[Dict[str, Any]]:
        """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        try:
            logger.info(f"ğŸ“Š Excel ãƒ¬ã‚³ãƒ¼ãƒ‰æŠ½å‡ºé–‹å§‹: {filename}")
            
            # ExcelDataCleanerã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹é€ åŒ–
            cleaned_text = self.excel_cleaner.clean_excel_data(content)
            
            # pandas ã§Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥èª­ã¿è¾¼ã¿
            excel_file = pd.ExcelFile(content)
            all_records = []
            
            for sheet_name in excel_file.sheet_names:
                try:
                    logger.info(f"ğŸ“‹ ã‚·ãƒ¼ãƒˆå‡¦ç†é–‹å§‹: {sheet_name}")
                    
                    # ã‚·ãƒ¼ãƒˆã‚’DataFrameã¨ã—ã¦èª­ã¿è¾¼ã¿
                    logger.info(f"ğŸ“‹ ã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿é–‹å§‹: {sheet_name}")
                    
                    # æœ€åˆã«ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã§èª­ã¿è¾¼ã‚“ã§æ§‹é€ ã‚’ç¢ºèª
                    df_raw = pd.read_excel(excel_file, sheet_name=sheet_name, header=None)
                    logger.info(f"ğŸ“Š ç”Ÿã®ã‚·ãƒ¼ãƒˆæƒ…å ±ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ï¼‰:")
                    logger.info(f"  - å½¢çŠ¶: {df_raw.shape}")
                    if not df_raw.empty:
                        logger.info(f"  - æœ€åˆã®5è¡Œ:")
                        for i in range(min(5, len(df_raw))):
                            logger.info(f"    è¡Œ{i}: {list(df_raw.iloc[i])}")
                    
                    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æ¤œå‡º
                    header_row = 0
                    potential_headers = []
                    
                    for i in range(min(5, len(df_raw))):
                        row_values = df_raw.iloc[i].dropna().astype(str).tolist()
                        if row_values and len(row_values) > 2:
                            # æœ‰åŠ¹ãªå€¤ãŒ3ã¤ä»¥ä¸Šã‚ã‚‹è¡Œã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦ä½¿ç”¨
                            non_empty_count = sum(1 for val in row_values if val and str(val).strip())
                            if non_empty_count > 2:
                                potential_headers.append((i, non_empty_count, row_values))
                                logger.info(f"ğŸ“ ãƒ˜ãƒƒãƒ€ãƒ¼å€™è£œ: è¡Œ{i} (æœ‰åŠ¹ãªå€¤: {non_empty_count}å€‹)")
                    
                    # æœ€ã‚‚å¤šãã®æœ‰åŠ¹ãªå€¤ã‚’æŒã¤è¡Œã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦é¸æŠ
                    if potential_headers:
                        header_row = max(potential_headers, key=lambda x: x[1])[0]
                        logger.info(f"ğŸ“ æœ€çµ‚ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ: è¡Œ{header_row}")
                    else:
                        logger.warning("âš ï¸ é©åˆ‡ãªãƒ˜ãƒƒãƒ€ãƒ¼è¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è¡Œ0ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                    
                    # æ¤œå‡ºã•ã‚ŒãŸãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã§å†èª­ã¿è¾¼ã¿
                    df = pd.read_excel(excel_file, sheet_name=sheet_name, header=header_row)
                    
                    # è¤‡æ•°è¡Œãƒ˜ãƒƒãƒ€ãƒ¼ã®å ´åˆã€ä¸Šã®è¡Œã®æƒ…å ±ã‚‚çµåˆ
                    if header_row > 0:
                        logger.info("ï¿½ï¿½ è¤‡æ•°è¡Œãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ¤œå‡ºã€‚ä¸Šã®è¡Œã®æƒ…å ±ã‚‚çµåˆã—ã¾ã™ã€‚")
                        combined_columns = []
                        for col_idx, col_name in enumerate(df.columns):
                            combined_name = str(col_name).strip()
                            
                            # ä¸Šã®è¡Œã®æƒ…å ±ã‚’å–å¾—
                            for prev_row in range(header_row):
                                if col_idx < len(df_raw.columns):
                                    prev_value = df_raw.iloc[prev_row, col_idx]
                                    if pd.notna(prev_value) and str(prev_value).strip():
                                        prev_str = str(prev_value).strip()
                                        if prev_str not in combined_name:
                                            combined_name = f"{prev_str}_{combined_name}" if combined_name else prev_str
                            
                            combined_columns.append(combined_name)
                        
                        df.columns = combined_columns
                        logger.info(f"ğŸ“ çµåˆå¾Œã®åˆ—å: {list(df.columns)}")
                    
                    # ç”Ÿã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
                    logger.info(f"ğŸ“Š ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ{header_row}ã§ã®DataFrameæƒ…å ±:")
                    logger.info(f"  - å½¢çŠ¶: {df.shape}")
                    logger.info(f"  - åˆ—åï¼ˆç”Ÿï¼‰: {list(df.columns)}")
                    logger.info(f"  - åˆ—åã®å‹: {[type(col) for col in df.columns]}")
                    
                    # æœ€åˆã®æ•°è¡Œã‚’ç¢ºèª
                    if not df.empty:
                        logger.info(f"  - æœ€åˆã®3è¡Œ:")
                        for i, row in df.head(3).iterrows():
                            logger.info(f"    è¡Œ{i}: {dict(row)}")
                    
                    if df.empty:
                        logger.warning(f"âš ï¸ ã‚·ãƒ¼ãƒˆ {sheet_name} ã¯ç©ºã§ã™")
                        continue
                    
                    # ç©ºã®è¡Œãƒ»åˆ—ã‚’å‰Šé™¤
                    logger.info(f"ğŸ“Š ç©ºè¡Œãƒ»åˆ—å‰Šé™¤å‰: {df.shape}")
                    df = df.dropna(how='all').dropna(axis=1, how='all')
                    logger.info(f"ğŸ“Š ç©ºè¡Œãƒ»åˆ—å‰Šé™¤å¾Œ: {df.shape}")
                    
                    if df.empty:
                        logger.warning(f"âš ï¸ ã‚·ãƒ¼ãƒˆ {sheet_name} ã¯ç©ºè¡Œãƒ»åˆ—å‰Šé™¤å¾Œã«ç©ºã«ãªã‚Šã¾ã—ãŸ")
                        continue
                    
                    # ã‚·ãƒ¼ãƒˆã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
                    sheet_records = self._extract_records_from_dataframe(df, sheet_name)
                    all_records.extend(sheet_records)
                    
                    logger.info(f"âœ… ã‚·ãƒ¼ãƒˆ {sheet_name}: {len(sheet_records)} ãƒ¬ã‚³ãƒ¼ãƒ‰æŠ½å‡º")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ ã‚·ãƒ¼ãƒˆ {sheet_name} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            if not all_records:
                logger.warning("âš ï¸ å…¨ã‚·ãƒ¼ãƒˆã§ãƒ¬ã‚³ãƒ¼ãƒ‰æŠ½å‡ºã«å¤±æ•—")
                return []
            
            logger.info(f"ğŸ‰ Excel ãƒ¬ã‚³ãƒ¼ãƒ‰æŠ½å‡ºå®Œäº†: {len(all_records)} ãƒ¬ã‚³ãƒ¼ãƒ‰")
            return all_records
            
        except Exception as e:
            logger.error(f"âŒ Excel ãƒ¬ã‚³ãƒ¼ãƒ‰æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _extract_records_from_dataframe(self, df: pd.DataFrame, sheet_name: str) -> List[Dict[str, Any]]:
        """DataFrameã‹ã‚‰ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        records = []
        
        try:
            # åˆ—åã‚’æ­£è¦åŒ–
            logger.info(f"DataFrameåˆ—åï¼ˆæ­£è¦åŒ–å‰ï¼‰: {list(df.columns)}")
            df.columns = [self._normalize_column_name(str(col)) for col in df.columns]
            logger.info(f"DataFrameåˆ—åï¼ˆæ­£è¦åŒ–å¾Œï¼‰: {list(df.columns)}")
            
            # å„è¡Œã‚’ãƒ¬ã‚³ãƒ¼ãƒ‰ã¨ã—ã¦å‡¦ç†
            for index, row in df.iterrows():
                try:
                    # ç©ºã®è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if row.isna().all():
                        continue
                    
                    # ãƒ¬ã‚³ãƒ¼ãƒ‰ã®å†…å®¹ã‚’æ§‹ç¯‰
                    record_data = {}
                    record_parts = []
                    meaningful_columns = 0
                    
                    for col in df.columns:
                        value = row[col]
                        if pd.notna(value) and str(value).strip():
                            clean_value = str(value).strip()
                            record_data[col] = clean_value
                            record_parts.append(f"{col}: {clean_value}")
                            meaningful_columns += 1
                    
                    # æ„å‘³ã®ã‚ã‚‹åˆ—ãŒå°‘ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    if meaningful_columns < self.min_meaningful_columns:
                        continue
                    
                    # ãƒ¬ã‚³ãƒ¼ãƒ‰ã®å†…å®¹ã‚’ä½œæˆ
                    record_content = " | ".join(record_parts)
                    
                    # ãƒ¬ã‚³ãƒ¼ãƒ‰ã®é•·ã•åˆ¶é™
                    if len(record_content) > self.max_record_length:
                        record_content = record_content[:self.max_record_length] + "..."
                    
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                    record = {
                        "chunk_index": len(records),  # chunks ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã®äº’æ›æ€§ã®ãŸã‚
                        "content": record_content,
                        "token_count": self.base_processor._count_tokens(record_content),
                        "sheet_name": sheet_name,
                        "row_index": index,
                        "record_data": record_data,
                        "column_count": meaningful_columns,
                        "columns": list(record_data.keys())  # åˆ—åãƒªã‚¹ãƒˆã‚’è¿½åŠ 
                    }
                    
                    records.append(record)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ è¡Œ {index} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            return records
            
        except Exception as e:
            logger.error(f"âŒ DataFrame ãƒ¬ã‚³ãƒ¼ãƒ‰æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _normalize_column_name(self, column_name: str) -> str:
        """åˆ—åã‚’æ­£è¦åŒ–"""
        original_name = column_name
        logger.debug(f"åˆ—åæ­£è¦åŒ–: å…ƒã®åå‰='{original_name}' (å‹: {type(original_name)})")
        
        # æœ¬å½“ã«ç„¡æ„å‘³ãªåˆ—åã®ã¿ã‚’ç½®æ›
        if not column_name or str(column_name).strip() == '' or str(column_name).startswith('Unnamed'):
            generated_name = f"åˆ—_{uuid.uuid4().hex[:8]}"
            logger.debug(f"åˆ—åã‚’è‡ªå‹•ç”Ÿæˆ: '{original_name}' â†’ '{generated_name}'")
            return generated_name
        
        # æ–‡å­—åˆ—ã‚’æ¸…æ½”ã«
        normalized = str(column_name).strip()
        
        # ç©ºç™½ã‚„æ”¹è¡Œã‚’ç½®æ›
        normalized = normalized.replace('\n', ' ').replace('\r', ' ')
        
        # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹
        import re
        normalized = re.sub(r'\s+', ' ', normalized)
        
        logger.debug(f"åˆ—åæ­£è¦åŒ–å®Œäº†: '{original_name}' â†’ '{normalized}'")
        return normalized
    
    async def _save_records_to_database(self, doc_id: str, records: List[Dict[str, Any]],
                                      company_id: str, doc_name: str, max_retries: int = 3) -> Dict[str, Any]:
        """ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ï¼ˆchunksãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ç”¨ï¼‰"""
        try:
            from supabase_adapter import get_supabase_client
            supabase = get_supabase_client()

            stats = {
                "total_chunks": len(records),
                "saved_chunks": 0,
                "successful_embeddings": 0,
                "failed_embeddings": 0,
                "retry_attempts": 0,
                "total_text_length": 0  # æ–°ã—ãè¿½åŠ 
            }

            if not records:
                return stats

            batch_size = 20  # ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã§ã¯å°ã•ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ä½¿ç”¨
            total_batches = (len(records) + batch_size - 1) // batch_size
            
            logger.info(f"ğŸš€ {doc_name}: {len(records)}ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’{batch_size}å€‹å˜ä½ã§å‡¦ç†é–‹å§‹")
            logger.info(f"ğŸ“Š äºˆæƒ³ãƒãƒƒãƒæ•°: {total_batches}")

            # ãƒ¬ã‚³ãƒ¼ãƒ‰å˜ä½ã§embeddingç”Ÿæˆâ†’å³åº§ã«insert
            for batch_num in range(0, len(records), batch_size):
                batch_records = records[batch_num:batch_num + batch_size]
                current_batch = (batch_num // batch_size) + 1
                
                logger.info(f"ğŸ§  ãƒãƒƒãƒ {current_batch}/{total_batches}: {len(batch_records)}ãƒ¬ã‚³ãƒ¼ãƒ‰ã®embeddingç”Ÿæˆé–‹å§‹")
                
                # ã“ã®ãƒãƒƒãƒã®embeddingç”Ÿæˆ
                batch_contents = [record["content"] for record in batch_records]
                batch_embeddings = await self.base_processor._generate_embeddings_batch(batch_contents)
                
                # å¤±æ•—ã—ãŸembeddingã®ãƒªãƒˆãƒ©ã‚¤å‡¦ç†
                failed_indices = [i for i, emb in enumerate(batch_embeddings) if emb is None]
                retry_count = 0
                
                while failed_indices and retry_count < max_retries:
                    retry_count += 1
                    logger.info(f"ğŸ”„ ãƒãƒƒãƒ {current_batch} embeddingå†ç”Ÿæˆ (è©¦è¡Œ {retry_count}/{max_retries}): {len(failed_indices)}ä»¶")
                    
                    retry_embeddings = await self.base_processor._generate_embeddings_batch(batch_contents, failed_indices)
                    
                    for i in failed_indices:
                        if retry_embeddings[i] is not None:
                            batch_embeddings[i] = retry_embeddings[i]
                    
                    failed_indices = [i for i in failed_indices if batch_embeddings[i] is None]
                    
                    if failed_indices:
                        logger.warning(f"âš ï¸ ãƒãƒƒãƒ {current_batch} å†è©¦è¡Œå¾Œã‚‚å¤±æ•—: {len(failed_indices)}ä»¶")
                        await asyncio.sleep(1.0)
                    else:
                        logger.info(f"âœ… ãƒãƒƒãƒ {current_batch} å†è©¦è¡ŒæˆåŠŸ")
                        break
                
                # çµ±è¨ˆæ›´æ–°
                for embedding in batch_embeddings:
                    if embedding:
                        stats["successful_embeddings"] += 1
                    else:
                        stats["failed_embeddings"] += 1
                
                if retry_count > 0:
                    stats["retry_attempts"] = max(stats["retry_attempts"], retry_count)
                
                # æˆåŠŸã—ãŸembeddingã®ã¿ã§ãƒ¬ã‚³ãƒ¼ãƒ‰æº–å‚™
                records_to_insert = []
                for i, record_data in enumerate(batch_records):
                    embedding_vector = batch_embeddings[i]
                    if embedding_vector:  # æˆåŠŸã—ãŸembeddingã®ã¿
                        stats["total_text_length"] += len(record_data["content"])
                        # chunksãƒ†ãƒ¼ãƒ–ãƒ«ã«æŒ¿å…¥ã™ã‚‹ãŸã‚ã®ãƒ¬ã‚³ãƒ¼ãƒ‰å½¢å¼
                        records_to_insert.append({
                            "doc_id": doc_id,
                            "chunk_index": record_data["chunk_index"],
                            "content": record_data["content"],
                            "embedding": embedding_vector,
                            "company_id": company_id,
                            "created_at": datetime.now().isoformat(),
                            "updated_at": datetime.now().isoformat()
                        })
                
                # å³åº§ã«Supabaseã«æŒ¿å…¥
                if records_to_insert:
                    try:
                        logger.info(f"ğŸ’¾ ãƒãƒƒãƒ {current_batch}/{total_batches}: {len(records_to_insert)}ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å³åº§ã«ä¿å­˜ä¸­...")
                        result = supabase.table("chunks").insert(records_to_insert).execute()
                        
                        if result.data:
                            batch_saved = len(result.data)
                            stats["saved_chunks"] += batch_saved
                            logger.info(f"âœ… ãƒãƒƒãƒ {current_batch}/{total_batches}: {batch_saved}ãƒ¬ã‚³ãƒ¼ãƒ‰ä¿å­˜å®Œäº†")
                        else:
                            logger.error(f"âŒ ãƒãƒƒãƒ {current_batch}/{total_batches} ä¿å­˜ã‚¨ãƒ©ãƒ¼: {result.error}")
                            
                    except Exception as batch_error:
                        logger.error(f"âŒ ãƒãƒƒãƒ {current_batch}/{total_batches} ä¿å­˜ä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {batch_error}")
                        # ãƒãƒƒãƒã‚¨ãƒ©ãƒ¼ã§ã‚‚æ¬¡ã®ãƒãƒƒãƒå‡¦ç†ã‚’ç¶šè¡Œ
                        continue
                else:
                    logger.warning(f"âš ï¸ ãƒãƒƒãƒ {current_batch}/{total_batches}: ä¿å­˜å¯èƒ½ãªãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
                
                # ãƒãƒƒãƒå®Œäº†ãƒ­ã‚°
                logger.info(f"ğŸ¯ ãƒãƒƒãƒ {current_batch}/{total_batches} å®Œäº†: embedding {len(batch_embeddings) - len(failed_indices)}/{len(batch_embeddings)} æˆåŠŸ, ä¿å­˜ {len(records_to_insert)} ãƒ¬ã‚³ãƒ¼ãƒ‰")

            # æœ€çµ‚çµæœã®ã‚µãƒãƒªãƒ¼
            logger.info(f"ğŸ {doc_name}: ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹å‡¦ç†å®Œäº†")
            logger.info(f"ğŸ“ˆ æœ€çµ‚çµæœ: ä¿å­˜ {stats['saved_chunks']}/{stats['total_chunks']} ãƒ¬ã‚³ãƒ¼ãƒ‰")
            logger.info(f"ğŸ§  embedding: æˆåŠŸ {stats['successful_embeddings']}, å¤±æ•— {stats['failed_embeddings']}")
            
            if stats["failed_embeddings"] > 0:
                logger.warning(f"âš ï¸ æœ€çµ‚çµæœ: {stats['successful_embeddings']}/{stats['total_chunks']} embeddingæˆåŠŸ, {stats['retry_attempts']}å›å†è©¦è¡Œ")
            else:
                logger.info(f"ğŸ‰ å…¨embeddingç”ŸæˆæˆåŠŸ: {stats['successful_embeddings']}/{stats['total_chunks']}")

            return stats

        except Exception as e:
            logger.error(f"âŒ ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ä¿å­˜ä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {e}", exc_info=True)
            raise
    
    def _is_excel_file(self, filename: str) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãŒExcelå½¢å¼ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        return filename.lower().endswith(('.xlsx', '.xls'))
    
    def _calculate_page_count(self, records: List[Dict[str, Any]]) -> int:
        """ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã‹ã‚‰ãƒšãƒ¼ã‚¸æ•°ã‚’æ¨å®šï¼ˆ1ãƒšãƒ¼ã‚¸=50ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰"""
        return max(1, (len(records) + 49) // 50)

    def _is_date_like(self, value: str) -> bool:
        """å€¤ãŒæ—¥ä»˜ã£ã½ã„ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        import re
        from datetime import datetime
        
        if not value or not isinstance(value, str):
            return False
        
        value = value.strip()
        
        # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒªã‚¹ãƒˆ
        date_patterns = [
            r'^\d{4}[-/]\d{1,2}[-/]\d{1,2}$',  # 2024-01-01, 2024/1/1
            r'^\d{1,2}[-/]\d{1,2}[-/]\d{4}$',  # 01-01-2024, 1/1/2024
            r'^\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥$',    # 2024å¹´1æœˆ1æ—¥
            r'^\d{1,2}æœˆ\d{1,2}æ—¥$',           # 1æœˆ1æ—¥
            r'^\d{4}\d{2}\d{2}$',              # 20240101
            r'^\d{4}[-/]\d{1,2}$',             # 2024-01, 2024/1
            r'^\d{1,2}[-/]\d{4}$',             # 01-2024, 1/2024
            r'^\d{4}å¹´\d{1,2}æœˆ$',             # 2024å¹´1æœˆ
            r'^\d{1,2}æœˆ$',                    # 1æœˆ
            r'^\d{4}å¹´$',                      # 2024å¹´
            r'^\d{2,4}[-/]\d{1,2}[-/]\d{1,2}\s+\d{1,2}:\d{2}$',  # 2024-01-01 12:30
            r'^\d{1,2}:\d{2}$',                # 12:30
            r'^\d{4}[-/]\d{1,2}[-/]\d{1,2}\s+\d{1,2}:\d{2}:\d{2}$',  # 2024-01-01 12:30:45
        ]
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        for pattern in date_patterns:
            if re.match(pattern, value):
                return True
        
        # Excelæ—¥ä»˜ã‚·ãƒªã‚¢ãƒ«å€¤ï¼ˆ30000-50000ç¨‹åº¦ï¼‰
        try:
            num_value = float(value)
            if 25000 <= num_value <= 60000:  # ç¯„å›²ã‚’æ‹¡å¤§
                return True
        except (ValueError, TypeError):
            pass
        
        # å®Ÿéš›ã«æ—¥ä»˜ã¨ã—ã¦è§£æã§ãã‚‹ã‹è©¦è¡Œ
        try:
            # ä¸€èˆ¬çš„ãªæ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è§£æã‚’è©¦è¡Œ
            date_formats = [
                '%Y-%m-%d', '%Y/%m/%d', '%m/%d/%Y', '%d/%m/%Y', '%Y%m%d',
                '%Y-%m-%d %H:%M', '%Y/%m/%d %H:%M', '%Y-%m-%d %H:%M:%S',
                '%Yå¹´%mæœˆ%dæ—¥', '%mæœˆ%dæ—¥', '%Yå¹´%mæœˆ', '%mæœˆ', '%Yå¹´',
                '%Y-%m', '%Y/%m', '%m-%Y', '%m/%Y'
            ]
            for fmt in date_formats:
                try:
                    datetime.strptime(value, fmt)
                    return True
                except ValueError:
                    continue
        except:
            pass
        
        return False

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
document_processor_record_based = DocumentProcessorRecordBased()
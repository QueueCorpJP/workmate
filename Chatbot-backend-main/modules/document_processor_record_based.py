"""
ğŸ“¤ ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
ğŸ§© 1ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆ1è¡Œï¼‰å˜ä½ã§ã®ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
ğŸ§  embeddingç”Ÿæˆã‚’çµ±åˆï¼ˆGemini Flash - 768æ¬¡å…ƒï¼‰
ğŸ—ƒ Supabaseä¿å­˜ï¼ˆdocument_sources + chunksï¼‰

æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆExcelï¼‰å°‚ç”¨ã®å®Œå…¨ãªRAGå¯¾å¿œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
"""

import os
import uuid
import logging
import asyncio
import tempfile
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import re
import tiktoken
from fastapi import HTTPException, UploadFile
from google import genai
import psycopg2
from psycopg2.extras import execute_values

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DocumentProcessorRecordBased:
    """ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.gemini_client = None
        self.vertex_ai_client = None
        self.use_vertex_ai = os.getenv("USE_VERTEX_AI", "true").lower() == "true"
        self.embedding_model = os.getenv("EMBEDDING_MODEL", "gemini-embedding-001")
        
        # Vertex AIä½¿ç”¨æ™‚ã¯ãƒ¢ãƒ‡ãƒ«åã‚’ãã®ã¾ã¾ä½¿ç”¨ã€Gemini APIä½¿ç”¨æ™‚ã¯models/ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 
        if not self.use_vertex_ai and not self.embedding_model.startswith("models/"):
            self.embedding_model = f"models/{self.embedding_model}"
            
        self.chunk_size_tokens = 400  # 300-500ãƒˆãƒ¼ã‚¯ãƒ³ã®ä¸­é–“å€¤
        self.chunk_overlap_tokens = 50  # ãƒãƒ£ãƒ³ã‚¯é–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
        self.max_chunk_size_chars = 2000  # æ–‡å­—æ•°ã§ã®ä¸Šé™
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼åˆæœŸåŒ–
        try:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
        except Exception as e:
            logger.warning(f"tiktokenåˆæœŸåŒ–å¤±æ•—: {e}")
            self.tokenizer = None
        
        # Vertex AIãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯Gemini APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if self.use_vertex_ai:
            try:
                self._init_vertex_ai_client()
            except Exception as e:
                logger.warning(f"âš ï¸ Vertex AIåˆæœŸåŒ–å¤±æ•—ã€Gemini APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
                self.use_vertex_ai = False
                self._init_gemini_client()
        else:
            self._init_gemini_client()
    
    def _init_vertex_ai_client(self):
        """Vertex AI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        try:
            from modules.vertex_ai_embedding import get_vertex_ai_embedding_client, vertex_ai_embedding_available
            
            if not vertex_ai_embedding_available():
                logger.error("âŒ Vertex AI EmbeddingãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                raise ValueError("Vertex AI EmbeddingãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            
            self.vertex_ai_client = get_vertex_ai_embedding_client()
            if not self.vertex_ai_client:
                raise ValueError("Vertex AI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            logger.info(f"ğŸ§  Vertex AI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†: {self.embedding_model} (3072æ¬¡å…ƒ)")
            
        except Exception as e:
            logger.error(f"âŒ Vertex AI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _init_gemini_client(self):
        """Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ï¼ˆæ–°ã—ã„SDKï¼‰"""
        if self.gemini_client is None:
            api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
            if not api_key:
                raise ValueError("GOOGLE_API_KEY ã¾ãŸã¯ GEMINI_API_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
            # æ–°ã—ã„SDKã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
            self.gemini_client = genai.Client(api_key=api_key)
            logger.info(f"ğŸ§  Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†ï¼ˆæ–°SDKï¼‰: {self.embedding_model}")
    
    def _count_tokens(self, text: str) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        if self.tokenizer:
            try:
                return len(self.tokenizer.encode(text))
            except Exception as e:
                logger.warning(f"ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ã®æ¨å®šï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
        # æ—¥æœ¬èª: 1æ–‡å­— â‰ˆ 1.5ãƒˆãƒ¼ã‚¯ãƒ³, è‹±èª: 4æ–‡å­— â‰ˆ 1ãƒˆãƒ¼ã‚¯ãƒ³
        japanese_chars = len(re.findall(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', text))
        other_chars = len(text) - japanese_chars
        estimated_tokens = int(japanese_chars * 1.5 + other_chars * 0.25)
        return estimated_tokens
    
    async def process_uploaded_file(self, file: UploadFile, user_id: str, company_id: str) -> Dict[str, Any]:
        """
        ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰
        
        Args:
            file: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            company_id: ä¼šç¤¾ID
            
        Returns:
            å‡¦ç†çµæœã®è¾æ›¸
        """
        try:
            logger.info(f"ğŸ“¤ ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file.filename}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿å–ã‚Š
            file_content = await file.read()
            file_size_mb = len(file_content) / (1024 * 1024)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‚’å–å¾—
            file_extension = '.' + file.filename.split('.')[-1].lower()
            
            # Excelãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹å‡¦ç†
            if file_extension in ['.xlsx', '.xls']:
                return await self._process_excel_file_record_based(
                    file_content, file.filename, user_id, company_id, file_size_mb
                )
            else:
                # ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã¯å¾“æ¥ã®å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                logger.info(f"âš ï¸ {file_extension} ã¯ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹å‡¦ç†å¯¾è±¡å¤–ã€å¾“æ¥å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                from modules.document_processor import DocumentProcessor
                fallback_processor = DocumentProcessor()
                return await fallback_processor.process_uploaded_file(file, user_id, company_id)
                
        except Exception as e:
            logger.error(f"âŒ ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise HTTPException(status_code=500, detail=f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    
    async def _process_excel_file_record_based(self, content: bytes, filename: str, user_id: str, company_id: str, file_size_mb: float) -> Dict[str, Any]:
        """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã§å‡¦ç†"""
        try:
            logger.info(f"ğŸ“Š Excelãƒ•ã‚¡ã‚¤ãƒ«ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹å‡¦ç†é–‹å§‹: {filename}")
            
            # ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹Excelã‚¯ãƒªãƒ¼ãƒŠãƒ¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            from modules.excel_data_cleaner_record_based import ExcelDataCleanerRecordBased
            cleaner = ExcelDataCleanerRecordBased()
            records = cleaner.clean_excel_data(content)
            
            if not records:
                raise ValueError("Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æœ‰åŠ¹ãªãƒ¬ã‚³ãƒ¼ãƒ‰ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
            logger.info(f"ğŸ“‹ æŠ½å‡ºã•ã‚ŒãŸãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(records)}")
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            doc_id = await self._save_document_to_db(filename, user_id, company_id, file_size_mb, len(records))
            
            # ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦ä¿å­˜
            saved_chunks = await self._save_records_as_chunks(doc_id, records, company_id)
            
            # Embeddingã‚’ç”Ÿæˆ
            embedding_result = await self._generate_embeddings_for_records(doc_id, records)
            
            result = {
                "document_id": doc_id,
                "filename": filename,
                "file_size_mb": file_size_mb,
                "text_length": sum(len(record.get('content', '')) for record in records),
                "total_chunks": len(records),
                "saved_chunks": saved_chunks,
                "successful_embeddings": embedding_result.get("successful_embeddings", 0),
                "failed_embeddings": embedding_result.get("failed_embeddings", 0)
            }
            
            logger.info(f"âœ… Excelãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹å‡¦ç†å®Œäº†: {filename}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Excelãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    async def _save_document_to_db(self, filename: str, user_id: str, company_id: str, file_size_mb: float, record_count: int) -> str:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        try:
            from supabase_adapter import insert_data
            
            doc_id = str(uuid.uuid4())
            
            document_data = {
                "id": doc_id,
                "name": filename,
                "type": "excel_record_based",
                "page_count": record_count,
                "company_id": company_id,
                "uploaded_by": user_id,
                "uploaded_at": datetime.now().isoformat(),
                "active": True,
                "doc_id": doc_id  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè­˜åˆ¥å­ã¨ã—ã¦è‡ªèº«ã®IDã‚’è¨­å®š
            }
            
            # specialã‚³ãƒ©ãƒ ã¯çµ¶å¯¾ã«è¨­å®šã—ãªã„ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚é€šã‚Šï¼‰
            
            logger.info(f"ğŸ”„ document_sourcesãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®ä¿å­˜é–‹å§‹ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰: {doc_id} - {filename}")
            result = insert_data("document_sources", document_data)
            
            if result and result.data:
                logger.info(f"âœ… document_sourcesãƒ†ãƒ¼ãƒ–ãƒ«ä¿å­˜å®Œäº†ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰: {doc_id} - {filename}")
                return doc_id
            else:
                logger.error(f"âŒ document_sourcesãƒ†ãƒ¼ãƒ–ãƒ«ä¿å­˜å¤±æ•—ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰: result={result}")
                raise Exception("document_sourcesãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
        except Exception as e:
            logger.error(f"âŒ document_sourcesãƒ†ãƒ¼ãƒ–ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰: {e}")
            raise
    
    async def _save_records_as_chunks(self, doc_id: str, records: List[Dict[str, Any]], company_id: str) -> int:
        """ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        try:
            from supabase_adapter import get_supabase_client
            supabase = get_supabase_client()
            
            chunks_data = []
            
            for i, record in enumerate(records):
                chunk_data = {
                    "id": str(uuid.uuid4()),
                    "doc_id": doc_id,
                    "content": record.get('content', ''),
                    "chunk_index": i,
                    "company_id": company_id,  # company_idãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
                    "metadata": {
                        "source_sheet": record.get('source_sheet', ''),
                        "record_index": record.get('record_index', i),
                        "record_type": record.get('record_type', 'single'),
                        "chunk_index": record.get('chunk_index', 0) if record.get('record_type') == 'split' else None
                    },
                    "created_at": datetime.now().isoformat(),
                    "updated_at": datetime.now().isoformat()
                }
                chunks_data.append(chunk_data)
            
            # ãƒãƒƒãƒã§ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
            batch_size = 100
            saved_count = 0
            
            for i in range(0, len(chunks_data), batch_size):
                batch = chunks_data[i:i + batch_size]
                result = supabase.table("chunks").insert(batch).execute()
                
                if result.data:
                    saved_count += len(result.data)
                    logger.info(f"ğŸ“¦ ãƒãƒ£ãƒ³ã‚¯ãƒãƒƒãƒä¿å­˜: {len(result.data)}ä»¶ ({saved_count}/{len(chunks_data)})")
                else:
                    logger.warning(f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯ãƒãƒƒãƒä¿å­˜å¤±æ•—: ãƒãƒƒãƒ {i//batch_size + 1}")
            
            logger.info(f"âœ… å…¨ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å®Œäº†: {saved_count}/{len(chunks_data)}")
            return saved_count
            
        except Exception as e:
            logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    async def _generate_embeddings_for_records(self, doc_id: str, records: List[Dict[str, Any]]) -> Dict[str, int]:
        """ãƒ¬ã‚³ãƒ¼ãƒ‰ã®Embeddingã‚’ç”Ÿæˆ"""
        try:
            logger.info(f"ğŸ§  ãƒ¬ã‚³ãƒ¼ãƒ‰Embeddingç”Ÿæˆé–‹å§‹: {len(records)}ä»¶")
            
            # è‡ªå‹•Embeddingç”ŸæˆãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
            auto_generate = os.getenv("AUTO_GENERATE_EMBEDDINGS", "false").lower() == "true"
            if not auto_generate:
                logger.info("ğŸ”„ AUTO_GENERATE_EMBEDDINGS=false ã®ãŸã‚ã€Embeddingç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
                return {"successful_embeddings": 0, "failed_embeddings": 0}
            
            # è‡ªå‹•Embeddingç”Ÿæˆã‚’å®Ÿè¡Œ
            from modules.auto_embedding import auto_generate_embeddings_for_document
            success = await auto_generate_embeddings_for_document(doc_id, max_chunks=len(records))
            
            if success:
                # æˆåŠŸã—ãŸEmbeddingæ•°ã‚’å–å¾—
                from supabase_adapter import get_supabase_client
                supabase = get_supabase_client()
                
                result = supabase.table("chunks").select("id,embedding").eq("doc_id", doc_id).execute()
                
                if result.data:
                    successful_count = len([chunk for chunk in result.data if chunk.get('embedding')])
                    failed_count = len(result.data) - successful_count
                    
                    logger.info(f"ğŸ‰ ãƒ¬ã‚³ãƒ¼ãƒ‰Embeddingç”Ÿæˆå®Œäº†: {successful_count}æˆåŠŸ, {failed_count}å¤±æ•—")
                    return {"successful_embeddings": successful_count, "failed_embeddings": failed_count}
            
            return {"successful_embeddings": 0, "failed_embeddings": len(records)}
            
        except Exception as e:
            logger.error(f"âŒ ãƒ¬ã‚³ãƒ¼ãƒ‰Embeddingç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"successful_embeddings": 0, "failed_embeddings": len(records)}

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
document_processor_record_based = DocumentProcessorRecordBased()
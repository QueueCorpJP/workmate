"""
ğŸ§  ãƒãƒƒãƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ãƒãƒ£ãƒ³ã‚¯ã‚’10ä»¶ãšã¤ã¾ã¨ã‚ã¦ãƒãƒƒãƒã§é€ä¿¡ã—ã€ã‚¨ãƒ©ãƒ¼å›å¾©æ©Ÿèƒ½ä»˜ãã§embeddingã‚’ç”Ÿæˆ
text-embedding-004ä½¿ç”¨ï¼ˆ768æ¬¡å…ƒï¼‰
"""

import os
import logging
import asyncio
import time
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from dotenv import load_dotenv
import google.generativeai as genai
from supabase_adapter import get_supabase_client, select_data, update_data

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

class BatchEmbeddingGenerator:
    """ãƒãƒƒãƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        self.embedding_model = "models/text-embedding-004"  # å›ºå®šã§text-embedding-004ã‚’ä½¿ç”¨ï¼ˆ768æ¬¡å…ƒï¼‰
        self.auto_generate = os.getenv("AUTO_GENERATE_EMBEDDINGS", "false").lower() == "true"
        self.supabase = None
        
        # ãƒãƒƒãƒå‡¦ç†è¨­å®š
        self.batch_size = 10  # 10ä»¶ãšã¤å‡¦ç†
        self.max_retries = 3  # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
        self.retry_delay = 2  # ãƒªãƒˆãƒ©ã‚¤é–“éš”ï¼ˆç§’ï¼‰
        self.api_delay = 1    # APIå‘¼ã³å‡ºã—é–“éš”ï¼ˆç§’ï¼‰
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            "total_chunks": 0,
            "processed_chunks": 0,
            "successful_embeddings": 0,
            "failed_embeddings": 0,
            "retry_count": 0,
            "start_time": None,
            "end_time": None
        }
    
    def _init_clients(self):
        """APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        if not self.api_key:
            logger.error("âŒ GOOGLE_API_KEY ã¾ãŸã¯ GEMINI_API_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        
        try:
            # Gemini APIåˆæœŸåŒ–
            genai.configure(api_key=self.api_key)
            
            # Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
            self.supabase = get_supabase_client()
            
            logger.info(f"ğŸ§  ãƒãƒƒãƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”ŸæˆåˆæœŸåŒ–å®Œäº†: {self.embedding_model} (768æ¬¡å…ƒ)")
            return True
        except Exception as e:
            logger.error(f"âŒ APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _get_pending_chunks(self, doc_id: str = None, limit: int = None) -> List[Dict]:
        """embeddingæœªç”Ÿæˆã®ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—"""
        try:
            filters = {"embedding": None}
            if doc_id:
                filters["doc_id"] = doc_id
            
            chunks_result = select_data(
                "chunks",
                columns="id,content,chunk_index,doc_id",
                filters=filters,
                limit=limit
            )
            
            if not chunks_result.data:
                return []
            
            return chunks_result.data
            
        except Exception as e:
            logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def _generate_embedding_with_retry(self, content: str, chunk_id: str) -> Optional[List[float]]:
        """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãembeddingç”Ÿæˆ"""
        for attempt in range(self.max_retries):
            try:
                if not content or not content.strip():
                    logger.warning(f"âš ï¸ ç©ºã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚­ãƒƒãƒ—: {chunk_id}")
                    return None
                
                response = genai.embed_content(
                    model=self.embedding_model,
                    content=content.strip()
                )
                
                embedding_vector = None
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã®å‡¦ç†
                if isinstance(response, dict) and 'embedding' in response:
                    embedding_vector = response['embedding']
                elif hasattr(response, 'embedding') and response.embedding:
                    embedding_vector = response.embedding
                else:
                    logger.error(f"ğŸ” äºˆæœŸã—ãªã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼: {type(response)}")
                    return None
                
                if embedding_vector and len(embedding_vector) > 0:
                    # 768æ¬¡å…ƒã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                    if len(embedding_vector) != 768:
                        logger.warning(f"âš ï¸ äºˆæœŸã—ãªã„æ¬¡å…ƒæ•°: {len(embedding_vector)}æ¬¡å…ƒï¼ˆæœŸå¾…å€¤: 768æ¬¡å…ƒï¼‰")
                    logger.debug(f"âœ… embeddingç”ŸæˆæˆåŠŸ: {chunk_id} (æ¬¡å…ƒ: {len(embedding_vector)})")
                    return embedding_vector
                else:
                    logger.warning(f"âš ï¸ ç„¡åŠ¹ãªembedding: {chunk_id}")
                    return None
                    
            except Exception as e:
                self.stats["retry_count"] += 1
                logger.warning(f"âš ï¸ embeddingç”Ÿæˆã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}/{self.max_retries}): {chunk_id} - {e}")
                
                # 429ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼‰ã®å ´åˆã¯é•·ã‚ã«å¾…æ©Ÿ
                if "429" in str(e) or "rate limit" in str(e).lower():
                    wait_time = self.retry_delay * (2 ** attempt)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                    logger.info(f"â³ ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ¤œå‡ºã€{wait_time}ç§’å¾…æ©Ÿ...")
                    await asyncio.sleep(wait_time)
                elif attempt < self.max_retries - 1:
                    await asyncio.sleep(self.retry_delay)
                else:
                    logger.error(f"âŒ embeddingç”Ÿæˆæœ€çµ‚å¤±æ•—: {chunk_id}")
                    return None
        
        return None
    
    async def _process_chunk_batch(self, chunks: List[Dict]) -> Tuple[List[str], List[str]]:
        """ãƒãƒ£ãƒ³ã‚¯ãƒãƒƒãƒã‚’å‡¦ç†ã—ã€æˆåŠŸãƒ»å¤±æ•—ã—ãŸãƒãƒ£ãƒ³ã‚¯IDã‚’è¿”ã™"""
        successful_chunks = []
        failed_chunks = []
        
        logger.info(f"ğŸ“¦ ãƒãƒƒãƒå‡¦ç†é–‹å§‹: {len(chunks)}ä»¶ã®ãƒãƒ£ãƒ³ã‚¯")
        
        for i, chunk in enumerate(chunks):
            try:
                chunk_id = chunk['id']
                content = chunk['content']
                chunk_index = chunk['chunk_index']
                
                logger.info(f"  [{i+1}/{len(chunks)}] ãƒãƒ£ãƒ³ã‚¯ {chunk_index} å‡¦ç†ä¸­...")
                
                # embeddingç”Ÿæˆ
                embedding_vector = await self._generate_embedding_with_retry(content, chunk_id)
                
                if embedding_vector:
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
                    update_result = update_data(
                        "chunks",
                        {"embedding": embedding_vector},
                        "id",
                        chunk_id
                    )
                    
                    if update_result:
                        successful_chunks.append(chunk_id)
                        self.stats["successful_embeddings"] += 1
                        logger.info(f"  âœ… ãƒãƒ£ãƒ³ã‚¯ {chunk_index} å®Œäº† ({len(embedding_vector)}æ¬¡å…ƒ)")
                    else:
                        failed_chunks.append(chunk_id)
                        self.stats["failed_embeddings"] += 1
                        logger.error(f"  âŒ ãƒãƒ£ãƒ³ã‚¯ {chunk_index} DBæ›´æ–°å¤±æ•—")
                else:
                    failed_chunks.append(chunk_id)
                    self.stats["failed_embeddings"] += 1
                    logger.error(f"  âŒ ãƒãƒ£ãƒ³ã‚¯ {chunk_index} embeddingç”Ÿæˆå¤±æ•—")
                
                self.stats["processed_chunks"] += 1
                
                # APIåˆ¶é™å¯¾ç­–ï¼šãƒãƒ£ãƒ³ã‚¯é–“ã§å°‘ã—å¾…æ©Ÿ
                if i < len(chunks) - 1:
                    await asyncio.sleep(0.5)
                
            except Exception as e:
                failed_chunks.append(chunk['id'])
                self.stats["failed_embeddings"] += 1
                logger.error(f"  âŒ ãƒãƒ£ãƒ³ã‚¯ {chunk.get('chunk_index', 'unknown')} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        
        return successful_chunks, failed_chunks
    
    async def _retry_failed_chunks(self, failed_chunk_ids: List[str]) -> List[str]:
        """å¤±æ•—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã‚’å†å‡¦ç†"""
        if not failed_chunk_ids:
            return []
        
        logger.info(f"ğŸ”„ å¤±æ•—ãƒãƒ£ãƒ³ã‚¯ã®å†å‡¦ç†é–‹å§‹: {len(failed_chunk_ids)}ä»¶")
        
        # å¤±æ•—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã®è©³ç´°ã‚’å–å¾—
        retry_chunks = []
        for chunk_id in failed_chunk_ids:
            chunk_result = select_data(
                "chunks",
                columns="id,content,chunk_index,doc_id",
                filters={"id": chunk_id}
            )
            
            if chunk_result.data:
                retry_chunks.append(chunk_result.data[0])
        
        if not retry_chunks:
            logger.warning("âš ï¸ å†å‡¦ç†å¯¾è±¡ã®ãƒãƒ£ãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return failed_chunk_ids
        
        # å†å‡¦ç†å®Ÿè¡Œï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã—ã¦æ…é‡ã«å‡¦ç†ï¼‰
        still_failed = []
        retry_batch_size = 5  # å†å‡¦ç†æ™‚ã¯å°ã•ãªãƒãƒƒãƒã‚µã‚¤ã‚º
        
        for i in range(0, len(retry_chunks), retry_batch_size):
            batch = retry_chunks[i:i + retry_batch_size]
            logger.info(f"ğŸ”„ å†å‡¦ç†ãƒãƒƒãƒ {i//retry_batch_size + 1}: {len(batch)}ä»¶")
            
            # ã‚ˆã‚Šé•·ã„å¾…æ©Ÿæ™‚é–“
            await asyncio.sleep(self.api_delay * 2)
            
            successful, failed = await self._process_chunk_batch(batch)
            still_failed.extend(failed)
            
            # ãƒãƒƒãƒé–“ã§ã‚ˆã‚Šé•·ãå¾…æ©Ÿ
            if i + retry_batch_size < len(retry_chunks):
                await asyncio.sleep(self.api_delay * 3)
        
        logger.info(f"ğŸ”„ å†å‡¦ç†å®Œäº†: {len(failed_chunk_ids) - len(still_failed)}ä»¶æˆåŠŸ, {len(still_failed)}ä»¶å¤±æ•—")
        return still_failed
    
    def _print_progress(self, current_batch: int, total_batches: int, batch_stats: Tuple[List[str], List[str]]):
        """é€²æ—è¡¨ç¤º"""
        successful, failed = batch_stats
        elapsed_time = time.time() - self.stats["start_time"]
        
        logger.info(f"ğŸ“Š ãƒãƒƒãƒé€²æ—: {current_batch}/{total_batches} | "
                   f"æˆåŠŸ: {len(successful)} | å¤±æ•—: {len(failed)} | "
                   f"çµŒéæ™‚é–“: {elapsed_time:.1f}ç§’")
    
    def _print_final_stats(self):
        """æœ€çµ‚çµ±è¨ˆè¡¨ç¤º"""
        total_time = self.stats["end_time"] - self.stats["start_time"]
        success_rate = (self.stats["successful_embeddings"] / self.stats["total_chunks"] * 100) if self.stats["total_chunks"] > 0 else 0
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ ãƒãƒƒãƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆå®Œäº† - æœ€çµ‚çµ±è¨ˆ")
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {self.stats['total_chunks']}")
        logger.info(f"âœ… æˆåŠŸ: {self.stats['successful_embeddings']}")
        logger.info(f"âŒ å¤±æ•—: {self.stats['failed_embeddings']}")
        logger.info(f"ğŸ”„ ãƒªãƒˆãƒ©ã‚¤å›æ•°: {self.stats['retry_count']}")
        logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
        logger.info(f"â±ï¸ ç·å‡¦ç†æ™‚é–“: {total_time:.1f}ç§’")
        if self.stats['total_chunks'] > 0:
            logger.info(f"âš¡ å¹³å‡å‡¦ç†é€Ÿåº¦: {self.stats['total_chunks'] / total_time:.2f} ãƒãƒ£ãƒ³ã‚¯/ç§’")
        logger.info("=" * 60)
    
    async def generate_embeddings_for_document(self, doc_id: str, max_chunks: int = None) -> bool:
        """æŒ‡å®šã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒãƒ£ãƒ³ã‚¯ã«å¯¾ã—ã¦ãƒãƒƒãƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ"""
        if not self.auto_generate:
            logger.info("ğŸ”„ AUTO_GENERATE_EMBEDDINGS=false ã®ãŸã‚ã€è‡ªå‹•ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return True
        
        if not self._init_clients():
            return False
        
        try:
            self.stats["start_time"] = time.time()
            logger.info(f"ğŸ§  ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ {doc_id} ã®ãƒãƒƒãƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆé–‹å§‹")
            
            # è©²å½“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®embeddingæœªç”Ÿæˆãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
            pending_chunks = self._get_pending_chunks(doc_id, max_chunks)
            
            if not pending_chunks:
                logger.info("âœ… æ–°ã—ãå‡¦ç†ã™ã¹ããƒãƒ£ãƒ³ã‚¯ã¯ã‚ã‚Šã¾ã›ã‚“")
                return True
            
            self.stats["total_chunks"] = len(pending_chunks)
            logger.info(f"ğŸ“‹ {self.stats['total_chunks']}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒãƒƒãƒå‡¦ç†ã—ã¾ã™")
            
            all_failed_chunks = []
            
            # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
            total_batches = (len(pending_chunks) + self.batch_size - 1) // self.batch_size
            
            for i in range(0, len(pending_chunks), self.batch_size):
                batch = pending_chunks[i:i + self.batch_size]
                batch_num = (i // self.batch_size) + 1
                
                logger.info(f"ğŸ“¦ ãƒãƒƒãƒ {batch_num}/{total_batches} å‡¦ç†é–‹å§‹ ({len(batch)}ãƒãƒ£ãƒ³ã‚¯)")
                
                # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
                successful, failed = await self._process_chunk_batch(batch)
                all_failed_chunks.extend(failed)
                
                # é€²æ—è¡¨ç¤º
                self._print_progress(batch_num, total_batches, (successful, failed))
                
                # ãƒãƒƒãƒé–“ã®å¾…æ©Ÿï¼ˆAPIåˆ¶é™å¯¾ç­–ï¼‰
                if i + self.batch_size < len(pending_chunks):
                    await asyncio.sleep(self.api_delay)
            
            # å¤±æ•—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã®å†å‡¦ç†
            if all_failed_chunks:
                logger.info(f"ğŸ”„ å¤±æ•—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã®å†å‡¦ç†ã‚’é–‹å§‹: {len(all_failed_chunks)}ä»¶")
                final_failed = await self._retry_failed_chunks(all_failed_chunks)
                
                if final_failed:
                    logger.warning(f"âš ï¸ æœ€çµ‚çš„ã«å¤±æ•—ã—ãŸãƒãƒ£ãƒ³ã‚¯: {len(final_failed)}ä»¶")
                    for chunk_id in final_failed:
                        logger.warning(f"  - å¤±æ•—ãƒãƒ£ãƒ³ã‚¯ID: {chunk_id}")
            
            self.stats["end_time"] = time.time()
            self._print_final_stats()
            
            return self.stats["successful_embeddings"] > 0
            
        except Exception as e:
            logger.error(f"âŒ ãƒãƒƒãƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def generate_embeddings_for_all_pending(self, limit: int = None) -> bool:
        """å…¨ã¦ã®æœªå‡¦ç†ãƒãƒ£ãƒ³ã‚¯ã«å¯¾ã—ã¦ãƒãƒƒãƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ"""
        if not self.auto_generate:
            logger.info("ğŸ”„ AUTO_GENERATE_EMBEDDINGS=false ã®ãŸã‚ã€è‡ªå‹•ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return True
        
        if not self._init_clients():
            return False
        
        try:
            self.stats["start_time"] = time.time()
            logger.info("ğŸ§  å…¨æœªå‡¦ç†ãƒãƒ£ãƒ³ã‚¯ã®ãƒãƒƒãƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆé–‹å§‹")
            
            # å…¨ã¦ã®æœªå‡¦ç†ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
            pending_chunks = self._get_pending_chunks(limit=limit)
            
            if not pending_chunks:
                logger.info("âœ… å‡¦ç†ã™ã¹ããƒãƒ£ãƒ³ã‚¯ã¯ã‚ã‚Šã¾ã›ã‚“")
                return True
            
            self.stats["total_chunks"] = len(pending_chunks)
            logger.info(f"ğŸ“‹ {self.stats['total_chunks']}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒãƒƒãƒå‡¦ç†ã—ã¾ã™")
            
            all_failed_chunks = []
            
            # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
            total_batches = (len(pending_chunks) + self.batch_size - 1) // self.batch_size
            
            for i in range(0, len(pending_chunks), self.batch_size):
                batch = pending_chunks[i:i + self.batch_size]
                batch_num = (i // self.batch_size) + 1
                
                logger.info(f"ğŸ“¦ ãƒãƒƒãƒ {batch_num}/{total_batches} å‡¦ç†é–‹å§‹ ({len(batch)}ãƒãƒ£ãƒ³ã‚¯)")
                
                # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
                successful, failed = await self._process_chunk_batch(batch)
                all_failed_chunks.extend(failed)
                
                # é€²æ—è¡¨ç¤º
                self._print_progress(batch_num, total_batches, (successful, failed))
                
                # ãƒãƒƒãƒé–“ã®å¾…æ©Ÿï¼ˆAPIåˆ¶é™å¯¾ç­–ï¼‰
                if i + self.batch_size < len(pending_chunks):
                    await asyncio.sleep(self.api_delay)
            
            # å¤±æ•—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã®å†å‡¦ç†
            if all_failed_chunks:
                logger.info(f"ğŸ”„ å¤±æ•—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã®å†å‡¦ç†ã‚’é–‹å§‹: {len(all_failed_chunks)}ä»¶")
                final_failed = await self._retry_failed_chunks(all_failed_chunks)
                
                if final_failed:
                    logger.warning(f"âš ï¸ æœ€çµ‚çš„ã«å¤±æ•—ã—ãŸãƒãƒ£ãƒ³ã‚¯: {len(final_failed)}ä»¶")
            
            self.stats["end_time"] = time.time()
            self._print_final_stats()
            
            return self.stats["successful_embeddings"] > 0
            
        except Exception as e:
            logger.error(f"âŒ ãƒãƒƒãƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
batch_embedding_generator = BatchEmbeddingGenerator()

async def batch_generate_embeddings_for_document(doc_id: str, max_chunks: int = None) -> bool:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒãƒƒãƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆï¼ˆå¤–éƒ¨å‘¼ã³å‡ºã—ç”¨ï¼‰"""
    return await batch_embedding_generator.generate_embeddings_for_document(doc_id, max_chunks)

async def batch_generate_embeddings_for_all_pending(limit: int = None) -> bool:
    """å…¨æœªå‡¦ç†ãƒãƒ£ãƒ³ã‚¯ã®ãƒãƒƒãƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆï¼ˆå¤–éƒ¨å‘¼ã³å‡ºã—ç”¨ï¼‰"""
    return await batch_embedding_generator.generate_embeddings_for_all_pending(limit)
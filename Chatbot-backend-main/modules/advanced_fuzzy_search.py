"""
Advanced Fuzzy Search Implementation
é«˜åº¦ãªãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ï¼ˆnormalize_texté–¢æ•°ã¨æ–‡å­—æ•°å·®ã‚’è€ƒæ…®ã—ãŸã‚¹ã‚³ã‚¢è¨ˆç®—ï¼‰

ã”è³ªå•ã®ã‚ˆã†ãªé«˜åº¦ãªPostgreSQLã‚¯ã‚¨ãƒªã‚’å®Ÿè£…ï¼š
SELECT *,
  similarity(normalize_text(content), normalize_text(:query)) AS sim,
  abs(length(normalize_text(content)) - length(normalize_text(:query))) AS len_diff,
  (similarity(normalize_text(content), normalize_text(:query)) - 0.02 * len_diff) AS final_score
FROM chunks
WHERE similarity(normalize_text(content), normalize_text(:query)) > 0.45
ORDER BY final_score DESC
LIMIT 20;
"""

import logging
import os
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import psycopg2
from psycopg2.extras import RealDictCursor
from supabase_adapter import execute_query

logger = logging.getLogger(__name__)

@dataclass
class AdvancedFuzzyResult:
    """é«˜åº¦ãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢çµæœ"""
    chunk_id: str
    doc_id: str
    content: str
    document_name: str
    document_type: str
    similarity_score: float
    length_diff: int
    final_score: float
    normalized_content: str
    normalized_query: str
    chunk_index: int = 0
    company_id: str = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'chunk_id': self.chunk_id,
            'doc_id': self.doc_id,
            'content': self.content,
            'document_name': self.document_name,
            'document_type': self.document_type,
            'similarity_score': self.similarity_score,
            'length_diff': self.length_diff,
            'final_score': self.final_score,
            'normalized_content': self.normalized_content,
            'normalized_query': self.normalized_query,
            'chunk_index': self.chunk_index,
            'company_id': self.company_id,
            'search_method': 'advanced_fuzzy_search'
        }

class AdvancedFuzzySearchSystem:
    """é«˜åº¦ãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.db_url = self._get_db_url()
        self.initialized = False
        
    def _get_db_url(self) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã‚’æ§‹ç¯‰"""
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        if "supabase.co" in supabase_url:
            project_id = supabase_url.split("://")[1].split(".")[0]
            return f"postgresql://postgres.{project_id}:{os.getenv('DB_PASSWORD', '')}@aws-0-ap-northeast-1.pooler.supabase.com:6543/postgres"
        else:
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                raise ValueError("DATABASE_URL ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return db_url
    
    async def initialize(self) -> bool:
        """é«˜åº¦ãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–"""
        if self.initialized:
            return True
            
        try:
            logger.info("ğŸ”§ é«˜åº¦ãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–é–‹å§‹")
            
            # PostgreSQLé–¢æ•°ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
            await self._execute_initialization_sql()
            
            self.initialized = True
            logger.info("âœ… é«˜åº¦ãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            logger.error(f"âŒ é«˜åº¦ãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def _execute_initialization_sql(self):
        """åˆæœŸåŒ–SQLã‚’å®Ÿè¡Œ"""
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    # pg_trgmæ‹¡å¼µã‚’æœ‰åŠ¹åŒ–
                    cur.execute("CREATE EXTENSION IF NOT EXISTS pg_trgm;")
                    
                    # æ³¨æ„: normalize_texté–¢æ•°ã¨calculate_advanced_fuzzy_scoreé–¢æ•°ã¯
                    # Supabaseå´ã«æ—¢ã«å®šç¾©ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€é‡è¤‡å®šç¾©ã‚’å‰Šé™¤
                    
                    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
                    try:
                        cur.execute("""
                            CREATE INDEX IF NOT EXISTS idx_chunks_normalized_content_trgm 
                            ON chunks USING gin (normalize_text(content) gin_trgm_ops);
                        """)
                    except Exception as e:
                        logger.warning(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆè­¦å‘Š: {e}")
                    
                    try:
                        cur.execute("""
                            CREATE INDEX IF NOT EXISTS idx_chunks_content_length 
                            ON chunks (length(content));
                        """)
                    except Exception as e:
                        logger.warning(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆè­¦å‘Š: {e}")
                    
                    conn.commit()
                    logger.info("âœ… PostgreSQLæ‹¡å¼µã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆå®Œäº†")
                    
        except Exception as e:
            logger.error(f"âŒ åˆæœŸåŒ–SQLå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    async def advanced_fuzzy_search(self, 
                                  query: str, 
                                  company_id: str = None,
                                  threshold: float = 0.45,
                                  length_penalty: float = 0.012,
                                  limit: int = 50) -> List[AdvancedFuzzyResult]:
        """
        é«˜åº¦ãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢ã‚’å®Ÿè¡Œ
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            company_id: ä¼šç¤¾IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            threshold: æœ€çµ‚ã‚¹ã‚³ã‚¢ã®é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.45ï¼‰
            length_penalty: æ–‡å­—æ•°å·®ã®ãƒšãƒŠãƒ«ãƒ†ã‚£ä¿‚æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.012ï¼‰
            limit: çµæœæ•°åˆ¶é™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50ï¼‰
        
        Returns:
            é«˜åº¦ãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
        """
        if not self.initialized:
            await self.initialize()
        
        try:
            logger.info(f"ğŸ” é«˜åº¦ãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢é–‹å§‹: '{query}' (é–¾å€¤: {threshold}, ãƒšãƒŠãƒ«ãƒ†ã‚£: {length_penalty})")
            
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®ç²¾åº¦æ”¹è‰¯ç‰ˆã‚¯ã‚¨ãƒª
                    sql = """
                    WITH normalized AS (
                        SELECT
                            c.id as chunk_id,
                            c.doc_id,
                            c.content,
                            c.chunk_index,
                            c.company_id,
                            ds.name as document_name,
                            ds.type as document_type,
                            normalize_text(c.content) AS norm_content,
                            normalize_text(%s) AS norm_query
                        FROM chunks c
                        LEFT JOIN document_sources ds ON c.doc_id = ds.id
                        WHERE c.content IS NOT NULL
                          AND length(c.content) > 10
                    """
                    
                    params = [query]
                    
                    # ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’è¿½åŠ 
                    if company_id:
                        sql += " AND c.company_id = %s"
                        params.append(company_id)
                    
                    sql += """
                    )
                    SELECT *,
                        similarity(norm_content, norm_query) AS sim,
                        abs(length(norm_content) - length(norm_query)) AS len_diff,
                        (
                            similarity(norm_content, norm_query)
                            - %s * abs(length(norm_content) - length(norm_query))  -- æ¸›ç‚¹å¼±ã‚ã«ä¿®æ­£
                            + CASE
                                WHEN norm_content = norm_query THEN 0.4               -- å®Œå…¨ä¸€è‡´ãƒ–ãƒ¼ã‚¹ãƒˆæ§ãˆã‚ã«
                                WHEN norm_content LIKE norm_query || '%%' THEN 0.2     -- å‰æ–¹ä¸€è‡´ã‚‚èª¿æ•´
                                ELSE 0
                              END
                        ) AS final_score,
                        norm_content as normalized_content,
                        norm_query as normalized_query
                    FROM normalized
                    WHERE similarity(norm_content, norm_query) > %s
                    ORDER BY final_score DESC
                    LIMIT %s
                    """
                    
                    params.extend([length_penalty, threshold, limit])
                    
                    cur.execute(sql, params)
                    rows = cur.fetchall()
                    
                    results = []
                    for row in rows:
                        result = AdvancedFuzzyResult(
                            chunk_id=str(row['chunk_id']),
                            doc_id=str(row['doc_id']),
                            content=row['content'] or '',
                            document_name=row['document_name'] or 'Unknown',
                            document_type=row['document_type'] or 'unknown',
                            similarity_score=float(row['sim']),
                            length_diff=int(row['len_diff']),
                            final_score=float(row['final_score']),
                            normalized_content=row['normalized_content'] or '',
                            normalized_query=row['normalized_query'] or '',
                            chunk_index=int(row['chunk_index']) if row['chunk_index'] else 0,
                            company_id=row['company_id']
                        )
                        results.append(result)
                    
                    logger.info(f"âœ… é«˜åº¦ãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢å®Œäº†: {len(results)}ä»¶ã®çµæœ")
                    
                    # çµæœã®è©³ç´°ãƒ­ã‚°
                    if results:
                        logger.info("ğŸ“Š æ¤œç´¢çµæœè©³ç´°:")
                        for i, result in enumerate(results[:5]):  # ä¸Šä½5ä»¶
                            logger.info(f"  {i+1}. {result.document_name} - æœ€çµ‚ã‚¹ã‚³ã‚¢: {result.final_score:.4f}")
                            logger.info(f"      é¡ä¼¼åº¦: {result.similarity_score:.4f}, æ–‡å­—æ•°å·®: {result.length_diff}")
                            logger.info(f"      å†…å®¹: {result.content[:100]}...")
                    
                    return results
                    
        except Exception as e:
            logger.error(f"âŒ é«˜åº¦ãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    async def test_normalize_function(self, test_text: str) -> Dict[str, str]:
        """normalize_texté–¢æ•°ã®ãƒ†ã‚¹ãƒˆ"""
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    cur.execute("SELECT normalize_text(%s) as normalized", [test_text])
                    result = cur.fetchone()
                    
                    return {
                        'original': test_text,
                        'normalized': result['normalized']
                    }
                    
        except Exception as e:
            logger.error(f"normalize_texté–¢æ•°ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {'original': test_text, 'normalized': 'ã‚¨ãƒ©ãƒ¼'}
    
    async def get_similarity_distribution(self, query: str, company_id: str = None) -> Dict[str, Any]:
        """é¡ä¼¼åº¦åˆ†å¸ƒã®åˆ†æ"""
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    sql = """
                    SELECT 
                        COUNT(*) as total_chunks,
                        AVG(similarity(normalize_text(content), normalize_text(%s))) as avg_similarity,
                        MIN(similarity(normalize_text(content), normalize_text(%s))) as min_similarity,
                        MAX(similarity(normalize_text(content), normalize_text(%s))) as max_similarity,
                        STDDEV(similarity(normalize_text(content), normalize_text(%s))) as std_similarity
                    FROM chunks
                    WHERE content IS NOT NULL AND length(content) > 10
                    """
                    
                    params = [query, query, query, query]
                    
                    if company_id:
                        sql += " AND company_id = %s"
                        params.append(company_id)
                    
                    cur.execute(sql, params)
                    result = cur.fetchone()
                    
                    return {
                        'query': query,
                        'total_chunks': result['total_chunks'],
                        'avg_similarity': float(result['avg_similarity']) if result['avg_similarity'] else 0.0,
                        'min_similarity': float(result['min_similarity']) if result['min_similarity'] else 0.0,
                        'max_similarity': float(result['max_similarity']) if result['max_similarity'] else 0.0,
                        'std_similarity': float(result['std_similarity']) if result['std_similarity'] else 0.0
                    }
                    
        except Exception as e:
            logger.error(f"é¡ä¼¼åº¦åˆ†å¸ƒåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {}


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_advanced_fuzzy_search_system = None

def get_advanced_fuzzy_search_instance() -> Optional[AdvancedFuzzySearchSystem]:
    """é«˜åº¦ãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
    global _advanced_fuzzy_search_system
    
    if _advanced_fuzzy_search_system is None:
        try:
            _advanced_fuzzy_search_system = AdvancedFuzzySearchSystem()
            logger.info("âœ… é«˜åº¦ãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆå®Œäº†")
        except Exception as e:
            logger.error(f"âŒ é«˜åº¦ãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    return _advanced_fuzzy_search_system

async def advanced_fuzzy_search(query: str, 
                              company_id: str = None,
                              threshold: float = 0.45,
                              length_penalty: float = 0.012,
                              limit: int = 50) -> List[Dict[str, Any]]:
    """
    é«˜åº¦ãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
    
    Returns:
        æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆï¼ˆè¾æ›¸å½¢å¼ï¼‰
    """
    instance = get_advanced_fuzzy_search_instance()
    if not instance:
        logger.error("âŒ é«˜åº¦ãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return []
    
    results = await instance.advanced_fuzzy_search(query, company_id, threshold, length_penalty, limit)
    return [result.to_dict() for result in results]

def advanced_fuzzy_search_available() -> bool:
    """é«˜åº¦ãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        return bool(api_key and supabase_url and supabase_key)
    except Exception:
        return False 
"""
é«˜é€Ÿãƒãƒ£ãƒƒãƒˆå‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ä¸¦åˆ—åŒ–ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€é«˜é€ŸRAGã‚’æ´»ç”¨ã—ãŸãƒãƒ£ãƒƒãƒˆå‡¦ç†ã®æœ€é©åŒ–ç‰ˆ
"""
import asyncio
from typing import Dict, Any, Optional
from datetime import datetime

def safe_print(text):
    """å®‰å…¨ãªprinté–¢æ•°"""
    try:
        print(text)
    except UnicodeEncodeError:
        try:
            safe_text = str(text).encode('utf-8', errors='replace').decode('utf-8')
            print(safe_text)
        except:
            print("[å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: Unicodeæ–‡å­—ã‚’å«ã‚€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸]")

async def process_chat_fast(message, db, current_user: dict = None) -> Dict[str, Any]:
    """é«˜é€Ÿãƒãƒ£ãƒƒãƒˆå‡¦ç†ï¼ˆä¸¦åˆ—åŒ– + ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨ï¼‰"""
    
    start_time = datetime.now()
    safe_print(f"ğŸš€ é«˜é€Ÿãƒãƒ£ãƒƒãƒˆå‡¦ç†é–‹å§‹: {start_time}")
    
    try:
        # 1. åŸºæœ¬ãƒã‚§ãƒƒã‚¯ï¼ˆé«˜é€ŸåŒ–ï¼‰
        message_text = getattr(message, 'text', '') or getattr(message, 'message', '')
        if not message_text:
            raise ValueError("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ†ã‚­ã‚¹ãƒˆãŒç„¡åŠ¹ã§ã™")
        
        safe_print(f"ğŸ“ å‡¦ç†é–‹å§‹: '{message_text[:50]}...'")
        
        # 2. ä¸€èˆ¬ä¼šè©±åˆ¤å®šï¼ˆæ—©æœŸãƒªã‚¿ãƒ¼ãƒ³ï¼‰
        from .chat import is_casual_conversation, generate_casual_response
        if is_casual_conversation(message_text):
            safe_print(f"ğŸ’¬ ä¸€èˆ¬ä¼šè©±ã¨ã—ã¦åˆ¤å®š: é«˜é€Ÿå¿œç­”ãƒ¢ãƒ¼ãƒ‰")
            
            company_name = "WorkMate"
            casual_response = await generate_casual_response(message_text, company_name)
            
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ä¿å­˜ï¼ˆéåŒæœŸã§å®Ÿè¡Œã€å¿œç­”é€Ÿåº¦ã«å½±éŸ¿ã—ãªã„ï¼‰
            if message.user_id:
                asyncio.create_task(save_casual_chat_async(message, casual_response, db))
            
            elapsed = (datetime.now() - start_time).total_seconds()
            safe_print(f"âœ… ä¸€èˆ¬ä¼šè©±å‡¦ç†å®Œäº†: {elapsed:.2f}ç§’")
            
            return {
                "response": casual_response,
                "source": "",
                "remaining_questions": None,
                "limit_reached": False
            }
        
        # 3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ä¸¦åˆ—å–å¾—
        safe_print(f"ğŸ”„ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—å–å¾—é–‹å§‹")
        if message.user_id:
            from .chat_optimized import get_user_data_parallel
            company_id, limits_check, recent_messages = await get_user_data_parallel(message.user_id, db)
            
            # åˆ©ç”¨åˆ¶é™ãƒã‚§ãƒƒã‚¯
            if not limits_check.get("is_unlimited", False) and not limits_check.get("allowed", True):
                safe_print(f"âŒ åˆ©ç”¨åˆ¶é™åˆ°é”")
                return {
                    "response": f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ãƒ‡ãƒ¢ç‰ˆã®è³ªå•å›æ•°åˆ¶é™ï¼ˆ{limits_check.get('limit', 0)}å›ï¼‰ã«é”ã—ã¾ã—ãŸã€‚",
                    "remaining_questions": 0,
                    "limit_reached": True
                }
        else:
            company_id = None
            limits_check = {"is_unlimited": True, "allowed": True, "remaining": 0}
            recent_messages = []
        
        safe_print(f"âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: company_id={company_id}")
        
        # 4. ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã®ä¸¦åˆ—å–å¾—
        safe_print(f"ğŸ”„ ãƒªã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—å–å¾—é–‹å§‹")
        uploaded_by = current_user["id"] if current_user and current_user.get("role") == "admin" else None
        
        from .chat_optimized import get_resource_data_parallel, get_special_instructions_async
        active_sources, active_knowledge_text, active_resource_names = await get_resource_data_parallel(company_id, uploaded_by, db)
        
        if not active_sources:
            safe_print(f"âŒ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹ãªã—")
            return {
                "response": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ç¾åœ¨ã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚",
                "remaining_questions": limits_check.get("remaining", 0),
                "limit_reached": False
            }
        
        safe_print(f"âœ… ãƒªã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(active_sources)}ä»¶ã®ãƒªã‚½ãƒ¼ã‚¹")
        
        # SpecialæŒ‡ç¤ºã‚’ä¸¦åˆ—å–å¾—
        special_instructions_text = await get_special_instructions_async(active_sources, db)
        
        # 5. ğŸš€ ä¸¦åˆ—é«˜é€ŸRAGæ¤œç´¢ï¼ˆæœ€å„ªå…ˆï¼‰
        if active_knowledge_text and len(active_knowledge_text) > 50000:
            safe_print(f"ğŸ”„ ä¸¦åˆ—é«˜é€ŸRAGæ¤œç´¢é–‹å§‹: {len(active_knowledge_text):,}æ–‡å­—")
            
            try:
                # ã€æœ€å„ªå…ˆã€‘ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’è©¦è¡Œ
                from .chat import PARALLEL_VECTOR_SEARCH_AVAILABLE
                
                if PARALLEL_VECTOR_SEARCH_AVAILABLE:
                    safe_print(f"âš¡ ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ä½¿ç”¨è©¦è¡Œ")
                    
                    try:
                        from .parallel_vector_search import get_parallel_vector_search_instance_sync
                        
                        parallel_search_system = get_parallel_vector_search_instance_sync()
                        if parallel_search_system:
                            safe_print(f"âœ… ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ å–å¾—æˆåŠŸ")
                            parallel_result = parallel_search_system.parallel_comprehensive_search_sync(
                                message_text, company_id, max_results=50
                            )
                            
                            if parallel_result and len(parallel_result.strip()) > 0:
                                active_knowledge_text = parallel_result
                                safe_print(f"âœ… ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æˆåŠŸ: {len(active_knowledge_text):,}æ–‡å­—")
                            else:
                                safe_print(f"âš ï¸ ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœãŒç©º - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                                raise ValueError("ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœãŒç©º")
                        else:
                            safe_print(f"âŒ ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ å–å¾—å¤±æ•—")
                            raise ValueError("ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ å–å¾—å¤±æ•—")
                            
                    except Exception as parallel_error:
                        safe_print(f"âŒ ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å¤±æ•—: {parallel_error}")
                        safe_print(f"ğŸ”„ å¾“æ¥RAGã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                        raise parallel_error
                else:
                    safe_print(f"âš ï¸ ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢åˆ©ç”¨ä¸å¯ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                    
                # ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯1ã€‘é«˜é€ŸRAGæ¤œç´¢
                from .chat import SPEED_RAG_AVAILABLE
                
                if SPEED_RAG_AVAILABLE and len(active_knowledge_text) > 100000:
                    safe_print(f"âš¡ é«˜é€ŸRAGä½¿ç”¨è©¦è¡Œ")
                    
                    try:
                        from .rag_optimized import high_speed_rag
                        
                        # é«˜é€ŸRAGã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®å­˜åœ¨ç¢ºèª
                        if hasattr(high_speed_rag, 'lightning_search'):
                            safe_print(f"ğŸ”§ é«˜é€ŸRAG lightning_search ãƒ¡ã‚½ãƒƒãƒ‰ç¢ºèªæ¸ˆã¿")
                            active_knowledge_text = await high_speed_rag.lightning_search(
                                message_text, active_knowledge_text, max_results=50
                            )
                            safe_print(f"âœ… é«˜é€ŸRAGæ¤œç´¢æˆåŠŸ")
                        else:
                            safe_print(f"âŒ é«˜é€ŸRAGãƒ¡ã‚½ãƒƒãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                            raise AttributeError("lightning_search method not found")
                            
                    except (ImportError, AttributeError, TypeError) as rag_error:
                        safe_print(f"âŒ é«˜é€ŸRAGå‡¦ç†å¤±æ•—: {type(rag_error).__name__}: {rag_error}")
                        safe_print(f"ğŸ”„ å¾“æ¥RAGã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                        from .chat import simple_rag_search
                        active_knowledge_text = simple_rag_search(
                            active_knowledge_text, message_text, max_results=50, company_id=company_id
                        )
                else:
                    safe_print(f"ğŸ” å¾“æ¥RAGä½¿ç”¨ï¼ˆæ¡ä»¶: SPEED_RAG={SPEED_RAG_AVAILABLE}, ã‚µã‚¤ã‚º={len(active_knowledge_text):,}ï¼‰")
                    from .chat import simple_rag_search
                    active_knowledge_text = simple_rag_search(
                        active_knowledge_text, message_text, max_results=50, company_id=company_id
                    )
                
                safe_print(f"âœ… RAGæ¤œç´¢å®Œäº†: {len(active_knowledge_text):,}æ–‡å­—")
                
            except Exception as e:
                safe_print(f"âŒ RAGå‡¦ç†ã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")
                safe_print(f"ğŸ”„ æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¾“æ¥RAGä½¿ç”¨")
                try:
                    from .chat import simple_rag_search
                    active_knowledge_text = simple_rag_search(
                        active_knowledge_text, message_text, max_results=50, company_id=company_id
                    )
                    safe_print(f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯RAGæˆåŠŸ")
                except Exception as fallback_error:
                    safe_print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯RAGã‚‚å¤±æ•—: {fallback_error}")
                    # æœ€å¾Œã®æ‰‹æ®µï¼šçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ç¸®å°ã—ã¦è¿”ã™
                    active_knowledge_text = active_knowledge_text[:100000]
                    safe_print(f"âš ï¸ çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’{len(active_knowledge_text):,}æ–‡å­—ã«ç¸®å°")
        
        # 6. ä¼šè©±å±¥æ­´ã®æœ€é©åŒ–æ§‹ç¯‰
        from .prompt_cache import build_conversation_history_fast
        conversation_history = build_conversation_history_fast(recent_messages)
        
        # 7. æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        safe_print(f"ğŸ”„ æœ€é©åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆé–‹å§‹")
        from .prompt_cache import (
            build_context_cached_prompt, estimate_prompt_size, 
            truncate_knowledge_for_size_limit, gemini_context_cache,
            generate_content_with_cache
        )
        from .config import setup_gemini_with_cache
        
        # ã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯ï¼ˆé«˜é€Ÿæ¨å®šï¼‰
        MAX_PROMPT_SIZE = 400000
        estimated_size = estimate_prompt_size(
            company_name="WorkMate",
            active_resource_names=active_resource_names,
            active_knowledge_text=active_knowledge_text,
            conversation_history=conversation_history,
            message_text=message_text,
            special_instructions_text=special_instructions_text
        )
        
        # ã‚µã‚¤ã‚ºåˆ¶é™å¯¾å¿œ
        if estimated_size > MAX_PROMPT_SIZE:
            safe_print(f"âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µã‚¤ã‚ºè¶…é: {estimated_size:,} > {MAX_PROMPT_SIZE:,}")
            other_content_size = estimated_size - len(active_knowledge_text)
            active_knowledge_text = truncate_knowledge_for_size_limit(
                active_knowledge_text, MAX_PROMPT_SIZE, other_content_size
            )
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        prompt, cached_content_id = build_context_cached_prompt(
            company_name="WorkMate",
            active_resource_names=active_resource_names,
            active_knowledge_text=active_knowledge_text,
            conversation_history=conversation_history,
            message_text=message_text,
            special_instructions_text=special_instructions_text
        )
        
        safe_print(f"âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆå®Œäº†: {len(prompt):,}æ–‡å­—")
        
        # 8. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œGemini APIå‘¼ã³å‡ºã—
        safe_print(f"ğŸ¤– Gemini APIå‘¼ã³å‡ºã—é–‹å§‹")
        from .chat import model
        
        try:
            if cached_content_id:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆï¼šã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
                cache_model = setup_gemini_with_cache()
                safe_print(f"ğŸ¯ é«˜é€Ÿãƒãƒ£ãƒƒãƒˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ï¼‰: {cached_content_id}")
                
                response = generate_content_with_cache(cache_model, prompt, cached_content_id)
            else:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ï¼šé€šå¸¸ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
                safe_print(f"ğŸ¤– é«˜é€Ÿãƒãƒ£ãƒƒãƒˆï¼ˆæ–°è¦ï¼‰: {len(prompt):,}æ–‡å­—")
                response = model.generate_content(prompt)
                
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                if gemini_context_cache.should_cache_context(active_knowledge_text):
                    virtual_content_id = f"fast_cache_{hash(active_knowledge_text) % 100000}"
                    gemini_context_cache.store_context_cache(active_knowledge_text, virtual_content_id)
                    safe_print(f"ğŸ’¾ é«˜é€Ÿãƒãƒ£ãƒƒãƒˆç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜: {virtual_content_id}")
            
            response_text = response.text if response and hasattr(response, 'text') else "å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼"
            cache_status = "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨" if cached_content_id else "æ–°è¦ä½œæˆ"
            safe_print(f"âœ… Geminiå¿œç­”å—ä¿¡: {len(response_text)}æ–‡å­— ({cache_status})")
        except Exception as e:
            safe_print(f"âŒ Gemini API ã‚¨ãƒ©ãƒ¼: {str(e)}")
            response_text = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)[:100]}"
        
        # 9. åˆ©ç”¨åˆ¶é™æ›´æ–°ï¼ˆéåŒæœŸã§å®Ÿè¡Œï¼‰
        remaining_questions = None
        if message.user_id and not limits_check.get("is_unlimited", False):
            # éåŒæœŸã§æ›´æ–°ï¼ˆå¿œç­”é€Ÿåº¦ã«å½±éŸ¿ã—ãªã„ï¼‰
            asyncio.create_task(update_usage_and_save_chat_async(
                message, response_text, db, len(active_sources)
            ))
            remaining_questions = limits_check.get("remaining", 0) - 1
        
        # 10. ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ
        elapsed = (datetime.now() - start_time).total_seconds()
        safe_print(f"ğŸ‰ é«˜é€Ÿãƒãƒ£ãƒƒãƒˆå‡¦ç†å®Œäº†: {elapsed:.2f}ç§’")
        
        return {
            "response": response_text,
            "source": "",
            "remaining_questions": remaining_questions,
            "limit_reached": remaining_questions <= 0 if remaining_questions is not None else False
        }
        
    except Exception as e:
        elapsed = (datetime.now() - start_time).total_seconds()
        safe_print(f"âŒ é«˜é€Ÿãƒãƒ£ãƒƒãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e} ({elapsed:.2f}ç§’)")
        
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return {
            "response": f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)[:100]}",
            "source": "",
            "remaining_questions": 0,
            "limit_reached": False
        }

async def save_casual_chat_async(message, response_text: str, db):
    """ä¸€èˆ¬ä¼šè©±ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´éåŒæœŸä¿å­˜"""
    try:
        from modules.token_counter import TokenUsageTracker
        from supabase_adapter import select_data
        
        # ä¼šç¤¾IDå–å¾—
        company_id = None
        if message.user_id:
            user_result = select_data("users", columns="company_id", filters={"id": message.user_id})
            if user_result.data and len(user_result.data) > 0:
                company_id = user_result.data[0].get('company_id')
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ä¿å­˜
        tracker = TokenUsageTracker(db)
        chat_id = tracker.save_chat_with_prompts(
            user_message=getattr(message, 'text', '') or getattr(message, 'message', ''),
            bot_response=response_text,
            user_id=message.user_id,
            prompt_references=0,  # ãƒŠãƒ¬ãƒƒã‚¸å‚ç…§ãªã—
            company_id=company_id,
            employee_id=getattr(message, 'employee_id', None),
            employee_name=getattr(message, 'employee_name', None),
            category="ä¸€èˆ¬ä¼šè©±",
            sentiment="neutral",
            model="gemini-pro"
        )
        
        safe_print(f"âœ… ä¸€èˆ¬ä¼šè©±å±¥æ­´ä¿å­˜å®Œäº†: {chat_id}")
        
    except Exception as e:
        safe_print(f"ä¸€èˆ¬ä¼šè©±å±¥æ­´ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

async def update_usage_and_save_chat_async(message, response_text: str, db, prompt_references: int):
    """åˆ©ç”¨åˆ¶é™æ›´æ–°ã¨ãƒãƒ£ãƒƒãƒˆå±¥æ­´ä¿å­˜ã®éåŒæœŸå‡¦ç†"""
    try:
        from modules.token_counter import TokenUsageTracker
        from .chat_optimized import update_usage_async
        from supabase_adapter import select_data
        
        # ä¸¦åˆ—å®Ÿè¡Œ
        user_result_task = asyncio.create_task(get_user_company_async(message.user_id))
        usage_update_task = update_usage_async(message.user_id, db)
        
        # ä¼šç¤¾IDå–å¾—å®Œäº†ã‚’å¾…ã¤
        company_id = await user_result_task
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ä¿å­˜
        tracker = TokenUsageTracker(db)
        chat_id = tracker.save_chat_with_prompts(
            user_message=getattr(message, 'text', '') or getattr(message, 'message', ''),
            bot_response=response_text,
            user_id=message.user_id,
            prompt_references=prompt_references,
            company_id=company_id,
            employee_id=getattr(message, 'employee_id', None),
            employee_name=getattr(message, 'employee_name', None),
            category="çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ¤œç´¢",
            sentiment="neutral",
            model="gemini-pro"
        )
        
        # åˆ©ç”¨åˆ¶é™æ›´æ–°å®Œäº†ã‚’å¾…ã¤
        await usage_update_task
        
        safe_print(f"âœ… å±¥æ­´ä¿å­˜ãƒ»åˆ©ç”¨åˆ¶é™æ›´æ–°å®Œäº†: {chat_id}")
        
    except Exception as e:
        safe_print(f"å±¥æ­´ä¿å­˜ãƒ»åˆ©ç”¨åˆ¶é™æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

async def get_user_company_async(user_id: str) -> Optional[str]:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¼šç¤¾IDéåŒæœŸå–å¾—"""
    try:
        from supabase_adapter import select_data
        user_result = select_data("users", columns="company_id", filters={"id": user_id})
        if user_result.data and len(user_result.data) > 0:
            return user_result.data[0].get('company_id')
        return None
    except Exception as e:
        safe_print(f"ä¼šç¤¾IDéåŒæœŸå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

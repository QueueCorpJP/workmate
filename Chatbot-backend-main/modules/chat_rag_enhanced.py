"""
æ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ  (Enhanced RAG System)
PDFå¾ŒåŠã®é‡è¦æƒ…å ±ã‚‚ç¢ºå®Ÿã«å–å¾—ã™ã‚‹ãŸã‚ã®æ‹¡å¼µãƒãƒ£ãƒƒãƒˆç”¨RAGã‚·ã‚¹ãƒ†ãƒ 

å¾“æ¥ã®å•é¡Œç‚¹:
1. å›ºå®šLIMIT (10ä»¶) ã§PDFå¾ŒåŠã®ãƒãƒ£ãƒ³ã‚¯ãŒé™¤å¤–ã•ã‚Œã‚‹
2. åŒä¸€æ–‡æ›¸ã‹ã‚‰ã®æƒ…å ±ãŒé™å®šçš„
3. ä½ç½®ã«ã‚ˆã‚‹åå‘ãŒä¿®æ­£ã•ã‚Œãªã„

æ‹¡å¼µæ©Ÿèƒ½:
1. å‹•çš„LIMITèª¿æ•´ - ã‚¯ã‚¨ãƒªã®è¤‡é›‘ã•ã«å¿œã˜ã¦æ¤œç´¢æ•°ã‚’èª¿æ•´
2. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå…¨ä½“ã‚«ãƒãƒ¬ãƒƒã‚¸ - PDFå…¨ä½“ã‹ã‚‰é–¢é€£æƒ…å ±ã‚’åé›†
3. æ„å‘³çš„é–¢é€£æ€§å†è©•ä¾¡ - LLMã‚’ä½¿ã£ãŸé–¢é€£æ€§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
4. çµæœã®å¤šæ§˜æ€§ä¿è¨¼ - ç•°ãªã‚‹æ–‡æ›¸ãƒ»ä½ç½®ã‹ã‚‰ãƒãƒ©ãƒ³ã‚¹è‰¯ãå–å¾—
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional, Tuple
from .comprehensive_search_system import comprehensive_search
from .chat_rag import format_search_results

logger = logging.getLogger(__name__)

class EnhancedRAGSystem:
    """æ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.default_initial_limit = 60  # åˆæœŸæ¤œç´¢ã®ä¸Šé™ã‚’å¤§å¹…ã«å¢—åŠ 
        self.default_final_limit = 20    # æœ€çµ‚çµæœæ•°ã‚‚å¢—åŠ 
        self.min_document_diversity = 4  # æœ€ä½4ã¤ã®ç•°ãªã‚‹æ–‡æ›¸ã‹ã‚‰å–å¾—
        
    async def enhanced_rag_search(self,
                                query: str,
                                context: str = "",
                                company_id: str = None,
                                adaptive_limits: bool = True) -> List[Dict[str, Any]]:
        """
        æ‹¡å¼µRAGæ¤œç´¢ã®å®Ÿè¡Œ
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            context: ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            company_id: ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿
            adaptive_limits: é©å¿œçš„LIMITèª¿æ•´ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        """
        try:
            logger.info(f"ğŸ” æ‹¡å¼µRAGæ¤œç´¢é–‹å§‹: '{query}'")
            
            # 1. ã‚¯ã‚¨ãƒªè¤‡é›‘ã•åˆ†æã«ã‚ˆã‚‹å‹•çš„LIMITèª¿æ•´
            initial_limit, final_limit = self._calculate_adaptive_limits(
                query, context
            ) if adaptive_limits else (self.default_initial_limit, self.default_final_limit)
            
            logger.info(f"å‹•çš„LIMITè¨­å®š: åˆæœŸ={initial_limit}, æœ€çµ‚={final_limit}")
            
            # 2. åŒ…æ‹¬çš„æ¤œç´¢ã®å®Ÿè¡Œ
            search_results = await comprehensive_search(
                query=query,
                company_id=company_id,
                initial_limit=initial_limit,
                final_limit=final_limit
            )
            
            if not search_results:
                logger.warning("æ‹¡å¼µRAGæ¤œç´¢: çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return []
            
            # 3. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè€ƒæ…®ã®é–¢é€£æ€§å†è©•ä¾¡
            if context:
                search_results = self._rerank_with_context(search_results, query, context)
            
            # 4. æ–‡æ›¸ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã¨ãƒ­ã‚°å‡ºåŠ›
            coverage_info = self._analyze_document_coverage(search_results)
            self._log_enhanced_rag_summary(search_results, coverage_info, query)
            
            logger.info(f"âœ… æ‹¡å¼µRAGæ¤œç´¢å®Œäº†: {len(search_results)}ä»¶ã®é«˜å“è³ªçµæœ")
            return search_results
            
        except Exception as e:
            logger.error(f"âŒ æ‹¡å¼µRAGæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _calculate_adaptive_limits(self, query: str, context: str = "") -> Tuple[int, int]:
        """ã‚¯ã‚¨ãƒªã®è¤‡é›‘ã•ã«åŸºã¥ãå‹•çš„LIMITè¨ˆç®—"""
        
        # åŸºæœ¬è¨ˆç®—è¦ç´ 
        query_length = len(query)
        query_words = len(query.split())
        context_length = len(context) if context else 0
        
        # è¤‡é›‘ã•æŒ‡æ¨™ã®è¨ˆç®—
        complexity_score = 0
        
        # 1. ã‚¯ã‚¨ãƒªé•·ã«ã‚ˆã‚‹èª¿æ•´
        if query_length > 100:
            complexity_score += 3  # é•·ã„ã‚¯ã‚¨ãƒªã¯è©³ç´°ãªå›ç­”ãŒå¿…è¦
        elif query_length > 50:
            complexity_score += 2
        elif query_length > 20:
            complexity_score += 1
        
        # 2. å˜èªæ•°ã«ã‚ˆã‚‹èª¿æ•´
        if query_words > 15:
            complexity_score += 2  # å¤šãã®æ¦‚å¿µã‚’å«ã‚€è¤‡é›‘ãªã‚¯ã‚¨ãƒª
        elif query_words > 8:
            complexity_score += 1
        
        # 3. å°‚é–€ç”¨èªãƒ»æŠ€è¡“ç”¨èªã®æ¤œå‡º
        technical_keywords = [
            'æ‰‹é †', 'æ–¹æ³•', 'ä»•çµ„ã¿', 'åŸç†', 'è©³ç´°', 'æ¯”è¼ƒ', 'é•ã„', 'åŠ¹æœ',
            'API', 'ã‚·ã‚¹ãƒ†ãƒ ', 'å®Ÿè£…', 'è¨­è¨ˆ', 'é–‹ç™º', 'é‹ç”¨', 'ç®¡ç†',
            'åˆ†æ', 'è©•ä¾¡', 'æ”¹å–„', 'æœ€é©', 'æˆ¦ç•¥', 'è¨ˆç”»'
        ]
        
        technical_count = sum(1 for keyword in technical_keywords if keyword in query)
        complexity_score += min(technical_count, 3)  # æœ€å¤§3ç‚¹
        
        # 4. ç–‘å•è©ã«ã‚ˆã‚‹èª¿æ•´ï¼ˆè©³ç´°ãªèª¬æ˜ãŒå¿…è¦ï¼‰
        question_words = ['ã©ã®ã‚ˆã†ã«', 'ãªãœ', 'ã©ã†ã—ã¦', 'ã„ã¤', 'ã©ã“ã§', 'ã ã‚ŒãŒ', 'ä½•ã‚’']
        question_count = sum(1 for qword in question_words if qword in query)
        complexity_score += min(question_count, 2)  # æœ€å¤§2ç‚¹
        
        # 5. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•ã«ã‚ˆã‚‹èª¿æ•´
        if context_length > 500:
            complexity_score += 2  # é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¯ç¶™ç¶šçš„ãªè­°è«–
        elif context_length > 200:
            complexity_score += 1
        
        # è¤‡é›‘ã•ã‚¹ã‚³ã‚¢ã«åŸºã¥ãLIMITè¨ˆç®—
        base_initial = self.default_initial_limit
        base_final = self.default_final_limit
        
        # è¤‡é›‘ã•ã«å¿œã˜ãŸå€ç‡
        if complexity_score >= 8:
            multiplier = 2.0  # éå¸¸ã«è¤‡é›‘
        elif complexity_score >= 6:
            multiplier = 1.7  # è¤‡é›‘
        elif complexity_score >= 4:
            multiplier = 1.4  # ã‚„ã‚„è¤‡é›‘
        elif complexity_score >= 2:
            multiplier = 1.2  # æ¨™æº–ã‚ˆã‚Šå°‘ã—è¤‡é›‘
        else:
            multiplier = 1.0  # æ¨™æº–
        
        initial_limit = min(int(base_initial * multiplier), 100)  # æœ€å¤§100ä»¶
        final_limit = min(int(base_final * multiplier), 30)       # æœ€å¤§30ä»¶
        
        logger.info(f"è¤‡é›‘ã•åˆ†æ: ã‚¹ã‚³ã‚¢={complexity_score}, å€ç‡={multiplier:.1f}")
        
        return initial_limit, final_limit
    
    def _rerank_with_context(self, 
                           results: List[Dict[str, Any]], 
                           query: str, 
                           context: str) -> List[Dict[str, Any]]:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®ã—ãŸé–¢é€£æ€§å†è©•ä¾¡"""
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        context_keywords = self._extract_context_keywords(context)
        query_keywords = set(query.lower().split())
        
        # å„çµæœã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé–¢é€£æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        for result in results:
            content = result.get('content', '').lower()
            content_words = set(content.split())
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã®é–¢é€£æ€§
            context_relevance = 0
            for keyword in context_keywords:
                if keyword in content:
                    context_relevance += 0.1
            
            # ã‚¯ã‚¨ãƒªã¨ã®ç›´æ¥é–¢é€£æ€§
            query_relevance = len(query_keywords & content_words) / max(len(query_keywords), 1)
            
            # ç·åˆé–¢é€£æ€§ã‚¹ã‚³ã‚¢
            context_bonus = context_relevance + query_relevance * 0.2
            
            # æ—¢å­˜ã‚¹ã‚³ã‚¢ã«è¿½åŠ 
            original_score = result.get('score', 0)
            result['score'] = original_score + context_bonus
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è©³ç´°ã‚’è¨˜éŒ²
            if 'metadata' not in result:
                result['metadata'] = {}
            result['metadata']['context_bonus'] = context_bonus
            result['metadata']['context_relevance'] = context_relevance
            result['metadata']['query_relevance'] = query_relevance
        
        # æ›´æ–°ã•ã‚ŒãŸã‚¹ã‚³ã‚¢ã§å†ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x.get('score', 0), reverse=True)
        
        return results
    
    def _extract_context_keywords(self, context: str) -> List[str]:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        if not context:
            return []
        
        # åŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        import re
        
        keywords = []
        
        # åè©å¥ã®æŠ½å‡ºï¼ˆã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ãƒ»è‹±èªï¼‰
        kanji_katakana = re.findall(r'[ä¸€-é¾ ã€…ã€†ã€¤ã‚¡-ãƒ¶ãƒ¼]{2,}', context)
        keywords.extend(kanji_katakana)
        
        english_words = re.findall(r'[a-zA-Z]{3,}', context)
        keywords.extend(english_words)
        
        # é‡è¤‡é™¤å»ã¨é »åº¦é †ã‚½ãƒ¼ãƒˆ
        keyword_freq = {}
        for keyword in keywords:
            keyword_freq[keyword] = keyword_freq.get(keyword, 0) + 1
        
        # é »åº¦é †ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’è¿”ã™
        sorted_keywords = sorted(keyword_freq.keys(), 
                               key=lambda x: keyword_freq[x], 
                               reverse=True)
        
        return sorted_keywords[:10]  # ä¸Šä½10å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    
    def _analyze_document_coverage(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ–‡æ›¸ã‚«ãƒãƒ¬ãƒƒã‚¸ã®åˆ†æ"""
        if not results:
            return {}
        
        document_stats = {}
        position_distribution = {"å‰åŠ": 0, "ä¸­ç›¤": 0, "å¾ŒåŠ": 0}
        
        for result in results:
            doc_name = result.get('document_name', 'Unknown')
            doc_coverage = result.get('document_coverage', 0)
            
            # æ–‡æ›¸åˆ¥çµ±è¨ˆ
            if doc_name not in document_stats:
                document_stats[doc_name] = {
                    'count': 0,
                    'min_coverage': 1.0,
                    'max_coverage': 0.0,
                    'avg_score': 0.0,
                    'total_score': 0.0
                }
            
            stats = document_stats[doc_name]
            stats['count'] += 1
            stats['min_coverage'] = min(stats['min_coverage'], doc_coverage)
            stats['max_coverage'] = max(stats['max_coverage'], doc_coverage)
            stats['total_score'] += result.get('score', 0)
            stats['avg_score'] = stats['total_score'] / stats['count']
            
            # ä½ç½®åˆ†å¸ƒ
            if doc_coverage <= 0.33:
                position_distribution["å‰åŠ"] += 1
            elif doc_coverage <= 0.66:
                position_distribution["ä¸­ç›¤"] += 1
            else:
                position_distribution["å¾ŒåŠ"] += 1
        
        return {
            'document_stats': document_stats,
            'position_distribution': position_distribution,
            'total_documents': len(document_stats),
            'total_results': len(results)
        }
    
    def _log_enhanced_rag_summary(self, 
                                results: List[Dict[str, Any]], 
                                coverage_info: Dict[str, Any], 
                                query: str):
        """æ‹¡å¼µRAGæ¤œç´¢çµæœã®ã‚µãƒãƒªãƒ¼ãƒ­ã‚°"""
        
        logger.info("ğŸ“Š æ‹¡å¼µRAGæ¤œç´¢ã‚µãƒãƒªãƒ¼:")
        logger.info(f"  ã‚¯ã‚¨ãƒª: '{query[:50]}{'...' if len(query) > 50 else ''}'")
        logger.info(f"  ç·çµæœæ•°: {coverage_info.get('total_results', 0)}")
        logger.info(f"  å¯¾è±¡æ–‡æ›¸æ•°: {coverage_info.get('total_documents', 0)}")
        
        # ä½ç½®åˆ†å¸ƒ
        pos_dist = coverage_info.get('position_distribution', {})
        logger.info("  ä½ç½®åˆ†å¸ƒ:")
        for position, count in pos_dist.items():
            percentage = (count / coverage_info.get('total_results', 1)) * 100
            logger.info(f"    {position}: {count}ä»¶ ({percentage:.1f}%)")
        
        # æ–‡æ›¸åˆ¥çµ±è¨ˆï¼ˆä¸Šä½5æ–‡æ›¸ï¼‰
        doc_stats = coverage_info.get('document_stats', {})
        sorted_docs = sorted(doc_stats.items(), 
                           key=lambda x: x[1]['count'], 
                           reverse=True)
        
        logger.info("  ä¸»è¦æ–‡æ›¸ï¼ˆä¸Šä½5æ–‡æ›¸ï¼‰:")
        for doc_name, stats in sorted_docs[:5]:
            coverage_range = f"{stats['min_coverage']:.1%}-{stats['max_coverage']:.1%}"
            logger.info(f"    {doc_name}: {stats['count']}ä»¶, "
                       f"ã‚«ãƒãƒ¬ãƒƒã‚¸ç¯„å›²: {coverage_range}, "
                       f"å¹³å‡ã‚¹ã‚³ã‚¢: {stats['avg_score']:.3f}")
        
        # é«˜ã‚¹ã‚³ã‚¢çµæœã®è©³ç´°ï¼ˆä¸Šä½3ä»¶ï¼‰
        logger.info("  é«˜ã‚¹ã‚³ã‚¢çµæœï¼ˆä¸Šä½3ä»¶ï¼‰:")
        for i, result in enumerate(results[:3], 1):
            score = result.get('score', 0)
            doc_name = result.get('document_name', 'Unknown')
            coverage = result.get('document_coverage', 0)
            content_preview = result.get('content', '')[:100] + '...'
            
            logger.info(f"    {i}. {doc_name} (ã‚¹ã‚³ã‚¢: {score:.3f}, "
                       f"ä½ç½®: {coverage:.1%})")
            logger.info(f"       å†…å®¹: {content_preview}")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
enhanced_rag_system = EnhancedRAGSystem()

async def enhanced_rag_search(query: str, 
                            context: str = "", 
                            company_id: str = None,
                            adaptive_limits: bool = True) -> List[Dict[str, Any]]:
    """æ‹¡å¼µRAGæ¤œç´¢ã®å®Ÿè¡Œï¼ˆå¤–éƒ¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰"""
    return await enhanced_rag_system.enhanced_rag_search(
        query, context, company_id, adaptive_limits
    )

def enhanced_format_search_results(results: List[Dict[str, Any]], 
                                 max_length: int = 3000) -> str:
    """æ‹¡å¼µã•ã‚ŒãŸæ¤œç´¢çµæœãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆå¾“æ¥ã‚ˆã‚Šå¤šãã®æƒ…å ±ã‚’å«ã‚€ï¼‰"""
    if not results:
        return "é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    formatted_results = []
    current_length = 0
    
    for i, result in enumerate(results, 1):
        # æ–‡æ›¸æƒ…å ±ã®å–å¾—
        doc_name = result.get('document_name', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—')
        content = result.get('content', '')
        score = result.get('score', 0)
        chunk_index = result.get('chunk_index', 0)
        doc_coverage = result.get('document_coverage', 0)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        metadata = result.get('metadata', {})
        search_methods = result.get('search_methods', [])
        
        # çµæœãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆè©³ç´°ç‰ˆï¼‰
        formatted_result = f"ã€çµæœ {i}ã€‘\n"
        formatted_result += f"æ–‡æ›¸: {doc_name}\n"
        formatted_result += f"ãƒãƒ£ãƒ³ã‚¯: #{chunk_index} (æ–‡æ›¸å†…ä½ç½®: {doc_coverage:.1%})\n"
        
        # æ¤œç´¢æ–¹æ³•ã®è¡¨ç¤º
        if search_methods:
            methods_str = ', '.join(search_methods) if isinstance(search_methods, list) else str(search_methods)
            formatted_result += f"æ¤œç´¢æ–¹æ³•: {methods_str}\n"
        
        # å†…å®¹
        content_length = min(600, max_length - current_length - 200)  # ã‚ˆã‚Šå¤šãã®å†…å®¹ã‚’å«ã‚€
        if content_length > 0:
            formatted_result += f"å†…å®¹: {content[:content_length]}{'...' if len(content) > content_length else ''}\n"
        
        # é–¢é€£åº¦ã‚¹ã‚³ã‚¢
        if score > 0:
            formatted_result += f"é–¢é€£åº¦: {score:.3f}\n"
        
        formatted_result += "\n"
        
        # é•·ã•ãƒã‚§ãƒƒã‚¯
        if current_length + len(formatted_result) > max_length:
            break
        
        formatted_results.append(formatted_result)
        current_length += len(formatted_result)
    
    result_text = ''.join(formatted_results)
    
    # ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’è¿½åŠ 
    doc_count = len(set(r.get('document_name', 'Unknown') for r in results))
    summary = f"\nğŸ“Š æ¤œç´¢ã‚µãƒãƒªãƒ¼: {len(results)}ä»¶ã®çµæœã‚’{doc_count}å€‹ã®æ–‡æ›¸ã‹ã‚‰å–å¾—\n"
    
    return result_text + summary 
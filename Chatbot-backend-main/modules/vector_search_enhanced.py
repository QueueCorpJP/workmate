"""
å¼·åŒ–ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
- é©å¿œçš„é¡ä¼¼åº¦é–¾å€¤
- ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªçµæœãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
- æ”¹å–„ã•ã‚ŒãŸã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸãƒãƒ£ãƒ³ã‚¯çµ±åˆ
"""

import os
import logging
import asyncio
from typing import List, Dict, Tuple, Optional, Set
from dotenv import load_dotenv
import psycopg2
from psycopg2.extras import RealDictCursor
import numpy as np
from datetime import datetime
import re
from dataclasses import dataclass
from collections import defaultdict

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

logger = logging.getLogger(__name__)

@dataclass
class EnhancedSearchResult:
    """å¼·åŒ–ã•ã‚ŒãŸæ¤œç´¢çµæœ"""
    chunk_id: str
    document_id: str
    document_name: str
    content: str
    similarity_score: float
    relevance_score: float  # ç·åˆé–¢é€£åº¦ã‚¹ã‚³ã‚¢
    chunk_index: int
    document_type: str
    search_method: str
    context_bonus: float = 0.0
    quality_score: float = 0.0
    metadata: Dict = None

class EnhancedVectorSearchSystem:
    """å¼·åŒ–ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.use_vertex_ai = os.getenv("USE_VERTEX_AI", "true").lower() == "true"
        self.embedding_model = os.getenv("EMBEDDING_MODEL", "text-multilingual-embedding-002")
        self.expected_dimensions = 768 if "text-multilingual-embedding-002" in self.embedding_model else 3072
        
        self.db_url = self._get_db_url()
        self.pgvector_available = False
        
        # æ¤œç´¢å“è³ªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.min_similarity_threshold = 0.3  # æœ€å°é¡ä¼¼åº¦é–¾å€¤ã‚’ä¸Šã’ã‚‹
        self.adaptive_threshold_enabled = True
        self.context_window_size = 3  # å‰å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚’è€ƒæ…®
        self.quality_weight = 0.3
        self.similarity_weight = 0.4
        self.context_weight = 0.3
        
        # pgvectoræ‹¡å¼µæ©Ÿèƒ½ã®ç¢ºèª
        self._check_pgvector_availability()
        
        # Vertex AI Embeddingã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        if self.use_vertex_ai:
            from .vertex_ai_embedding import get_vertex_ai_embedding_client, vertex_ai_embedding_available
            if vertex_ai_embedding_available():
                self.vertex_client = get_vertex_ai_embedding_client()
                logger.info(f"âœ… å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–: {self.embedding_model} ({self.expected_dimensions}æ¬¡å…ƒ)")
            else:
                logger.error("âŒ Vertex AI EmbeddingãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                raise ValueError("Vertex AI Embeddingã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        else:
            self.vertex_client = None
    
    def _get_db_url(self) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã‚’æ§‹ç¯‰"""
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        if "supabase.co" in supabase_url:
            project_id = supabase_url.split("://")[1].split(".")[0]
            return f"postgresql://postgres.{project_id}:{os.getenv('DB_PASSWORD', '')}@aws-0-ap-northeast-1.pooler.supabase.com:6543/postgres"
        else:
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                raise ValueError("DATABASE_URL ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return db_url
    
    def _check_pgvector_availability(self):
        """pgvectoræ‹¡å¼µæ©Ÿèƒ½ã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        SELECT EXISTS(
                            SELECT 1 FROM pg_extension WHERE extname = 'vector'
                        ) as pgvector_installed
                    """)
                    result = cur.fetchone()
                    self.pgvector_available = result['pgvector_installed'] if result else False
                    
                    if self.pgvector_available:
                        logger.info("âœ… pgvectoræ‹¡å¼µæ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
                    else:
                        logger.warning("âš ï¸ pgvectoræ‹¡å¼µæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™")
                        
        except Exception as e:
            logger.error(f"âŒ pgvectorç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            self.pgvector_available = False
    
    def generate_query_embedding(self, query: str) -> List[float]:
        """ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
        try:
            logger.info(f"ğŸ§  ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆä¸­: {query[:50]}...")
            
            if self.vertex_client:
                embedding_vector = self.vertex_client.generate_embedding(query)
                
                if embedding_vector and len(embedding_vector) > 0:
                    if len(embedding_vector) != self.expected_dimensions:
                        logger.warning(f"äºˆæœŸã—ãªã„æ¬¡å…ƒæ•°: {len(embedding_vector)}æ¬¡å…ƒï¼ˆæœŸå¾…å€¤: {self.expected_dimensions}æ¬¡å…ƒï¼‰")
                    logger.info(f"âœ… åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº†: {len(embedding_vector)}æ¬¡å…ƒ")
                    return embedding_vector
                else:
                    logger.error("åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                    return []
            else:
                logger.error("Vertex AI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                return []
        
        except Exception as e:
            logger.error(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def calculate_adaptive_threshold(self, similarities: List[float]) -> float:
        """é©å¿œçš„é¡ä¼¼åº¦é–¾å€¤ã‚’è¨ˆç®—"""
        if not similarities or not self.adaptive_threshold_enabled:
            return self.min_similarity_threshold
        
        # çµ±è¨ˆçš„åˆ†æã«ã‚ˆã‚‹é–¾å€¤è¨ˆç®—
        similarities = sorted(similarities, reverse=True)
        
        if len(similarities) < 3:
            return self.min_similarity_threshold
        
        # ä¸Šä½25%ã®å¹³å‡ã‹ã‚‰å‹•çš„é–¾å€¤ã‚’è¨ˆç®—
        top_quarter = similarities[:max(1, len(similarities) // 4)]
        avg_top = sum(top_quarter) / len(top_quarter)
        
        # æœ€å°é–¾å€¤ã¨å‹•çš„é–¾å€¤ã®æœ€å¤§å€¤ã‚’ä½¿ç”¨
        adaptive_threshold = max(self.min_similarity_threshold, avg_top * 0.6)
        
        logger.info(f"ğŸ“Š é©å¿œçš„é–¾å€¤: {adaptive_threshold:.3f} (æœ€å°: {self.min_similarity_threshold}, ä¸Šä½å¹³å‡: {avg_top:.3f})")
        return adaptive_threshold
    
    def calculate_quality_score(self, content: str, query: str) -> float:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        if not content or not query:
            return 0.0
        
        quality_score = 0.0
        content_lower = content.lower()
        query_lower = query.lower()
        query_terms = set(query_lower.split())
        
        # 1. é•·ã•ã«ã‚ˆã‚‹å“è³ªè©•ä¾¡
        content_length = len(content)
        if 100 <= content_length <= 2000:
            quality_score += 0.3
        elif 50 <= content_length <= 3000:
            quality_score += 0.2
        elif content_length > 3000:
            quality_score += 0.1
        
        # 2. ã‚¯ã‚¨ãƒªç”¨èªã®å«æœ‰ç‡
        content_terms = set(content_lower.split())
        term_overlap = len(query_terms & content_terms)
        term_coverage = term_overlap / len(query_terms) if query_terms else 0
        quality_score += term_coverage * 0.4
        
        # 3. æ§‹é€ çš„è¦ç´ ã®å­˜åœ¨
        structural_patterns = [
            r'\d+\.',  # ç•ªå·ä»˜ããƒªã‚¹ãƒˆ
            r'ãƒ»',     # ç®‡æ¡æ›¸ã
            r'ã€.*?ã€‘', # ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦‹å‡ºã—
            r'â– .*?â– ', # å¼·èª¿è¦‹å‡ºã—
            r'æ‰‹é †|æ–¹æ³•|ã‚„ã‚Šæ–¹|ãƒ—ãƒ­ã‚»ã‚¹',  # æ‰‹é †é–¢é€£
            r'é€£çµ¡å…ˆ|é›»è©±|ãƒ¡ãƒ¼ãƒ«|å•ã„åˆã‚ã›',  # é€£çµ¡å…ˆæƒ…å ±
        ]
        
        for pattern in structural_patterns:
            if re.search(pattern, content):
                quality_score += 0.05
        
        # 4. æƒ…å ±å¯†åº¦ï¼ˆå¥èª­ç‚¹ã®é©åˆ‡ãªä½¿ç”¨ï¼‰
        sentences = content.split('ã€‚')
        if len(sentences) > 1:
            avg_sentence_length = sum(len(s) for s in sentences) / len(sentences)
            if 20 <= avg_sentence_length <= 100:
                quality_score += 0.1
        
        return min(quality_score, 1.0)
    
    def get_context_chunks(self, target_chunk_index: int, document_id: str, company_id: str = None) -> List[Dict]:
        """å‰å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—ã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰"""
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    # å‰å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
                    sql = """
                    SELECT 
                        c.id,
                        c.chunk_index,
                        c.content,
                        c.doc_id
                    FROM chunks c
                    WHERE c.doc_id = %s
                      AND c.chunk_index BETWEEN %s AND %s
                      AND c.content IS NOT NULL
                    """
                    
                    params = [
                        document_id,
                        max(0, target_chunk_index - self.context_window_size),
                        target_chunk_index + self.context_window_size
                    ]
                    
                    if company_id:
                        sql += " AND c.company_id = %s"
                        params.append(company_id)
                    
                    sql += " ORDER BY c.chunk_index"
                    
                    cur.execute(sql, params)
                    results = cur.fetchall()
                    
                    return [dict(row) for row in results]
        
        except Exception as e:
            logger.error(f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def calculate_context_bonus(self, target_chunk: Dict, context_chunks: List[Dict], query: str) -> float:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒ¼ãƒŠã‚¹ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        if not context_chunks or len(context_chunks) <= 1:
            return 0.0
        
        context_bonus = 0.0
        query_terms = set(query.lower().split())
        
        # å‰å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã§ã®ã‚¯ã‚¨ãƒªç”¨èªã®å‡ºç¾ã‚’ãƒã‚§ãƒƒã‚¯
        for chunk in context_chunks:
            if chunk['id'] != target_chunk.get('chunk_id', ''):
                chunk_terms = set(chunk['content'].lower().split())
                term_overlap = len(query_terms & chunk_terms)
                if term_overlap > 0:
                    context_bonus += 0.1 * (term_overlap / len(query_terms))
        
        # é€£ç¶šæ€§ãƒœãƒ¼ãƒŠã‚¹ï¼ˆå‰å¾Œã®ãƒãƒ£ãƒ³ã‚¯ãŒé€£ç¶šã—ã¦ã„ã‚‹å ´åˆï¼‰
        chunk_indices = sorted([chunk['chunk_index'] for chunk in context_chunks])
        consecutive_count = 0
        for i in range(1, len(chunk_indices)):
            if chunk_indices[i] - chunk_indices[i-1] == 1:
                consecutive_count += 1
        
        if consecutive_count > 0:
            context_bonus += 0.05 * consecutive_count
        
        return min(context_bonus, 0.3)  # æœ€å¤§30%ã®ãƒœãƒ¼ãƒŠã‚¹
    
    async def enhanced_vector_search(self, query: str, company_id: str = None, max_results: int = 15) -> List[EnhancedSearchResult]:
        """å¼·åŒ–ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œ"""
        try:
            logger.info(f"ğŸ” å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢é–‹å§‹: '{query[:50]}...'")
            
            # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            query_vector = self.generate_query_embedding(query)
            if not query_vector:
                logger.error("ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã«å¤±æ•—")
                return []
            
            # åˆæœŸæ¤œç´¢ï¼ˆã‚ˆã‚Šå¤šãã®å€™è£œã‚’å–å¾—ï¼‰
            initial_results = await self._execute_initial_search(query_vector, company_id, max_results * 3)
            
            if not initial_results:
                logger.warning("åˆæœŸæ¤œç´¢ã§çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return []
            
            # é©å¿œçš„é–¾å€¤ã®è¨ˆç®—
            similarities = [r['similarity_score'] for r in initial_results]
            adaptive_threshold = self.calculate_adaptive_threshold(similarities)
            
            # çµæœã®å¼·åŒ–å‡¦ç†
            enhanced_results = []
            for result in initial_results:
                if result['similarity_score'] >= adaptive_threshold:
                    enhanced_result = await self._enhance_search_result(result, query, company_id)
                    if enhanced_result:
                        enhanced_results.append(enhanced_result)
            
            # ç·åˆã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹ã‚½ãƒ¼ãƒˆ
            enhanced_results.sort(key=lambda x: x.relevance_score, reverse=True)
            
            # é‡è¤‡é™¤å»ã¨æœ€çµ‚é¸æŠ
            final_results = self._deduplicate_and_select(enhanced_results, max_results)
            
            logger.info(f"âœ… å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Œäº†: {len(final_results)}ä»¶ã®é«˜å“è³ªçµæœ")
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
            for i, result in enumerate(final_results[:5]):
                logger.info(f"  {i+1}. {result.document_name} [ãƒãƒ£ãƒ³ã‚¯{result.chunk_index}]")
                logger.info(f"     é¡ä¼¼åº¦: {result.similarity_score:.3f}, é–¢é€£åº¦: {result.relevance_score:.3f}")
                logger.info(f"     å“è³ª: {result.quality_score:.3f}, ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {result.context_bonus:.3f}")
            
            return final_results
        
        except Exception as e:
            logger.error(f"âŒ å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
            return []
    
    async def _execute_initial_search(self, query_vector: List[float], company_id: str = None, limit: int = 45) -> List[Dict]:
        """åˆæœŸãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œ"""
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    if self.pgvector_available:
                        # pgvectorã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿæ¤œç´¢
                        sql = """
                        SELECT
                            c.id as chunk_id,
                            c.doc_id as document_id,
                            c.chunk_index,
                            c.content as snippet,
                            ds.name,
                            ds.type,
                            ds.special,
                            1 - (c.embedding <=> %s::vector) as similarity_score
                        FROM chunks c
                        LEFT JOIN document_sources ds ON ds.id = c.doc_id
                        WHERE c.embedding IS NOT NULL
                        """
                        
                        params = [query_vector]
                        
                        if company_id:
                            sql += " AND c.company_id = %s"
                            params.append(company_id)
                        
                        sql += " ORDER BY c.embedding <=> %s::vector LIMIT %s"
                        params.extend([query_vector, limit])
                        
                    else:
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢
                        logger.warning("âš ï¸ pgvectorãŒç„¡åŠ¹ã®ãŸã‚ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’ä½¿ç”¨")
                        sql = """
                        SELECT
                            c.id as chunk_id,
                            c.doc_id as document_id,
                            c.chunk_index,
                            c.content as snippet,
                            ds.name,
                            ds.type,
                            ds.special,
                            0.5 as similarity_score
                        FROM chunks c
                        LEFT JOIN document_sources ds ON ds.id = c.doc_id
                        WHERE c.embedding IS NOT NULL
                        """
                        
                        params = []
                        
                        if company_id:
                            sql += " AND c.company_id = %s"
                            params.append(company_id)
                        
                        sql += " ORDER BY RANDOM() LIMIT %s"
                        params.append(limit)
                    
                    cur.execute(sql, params)
                    results = cur.fetchall()
                    
                    return [dict(row) for row in results]
        
        except Exception as e:
            logger.error(f"åˆæœŸæ¤œç´¢å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def _enhance_search_result(self, result: Dict, query: str, company_id: str = None) -> Optional[EnhancedSearchResult]:
        """æ¤œç´¢çµæœã‚’å¼·åŒ–"""
        try:
            # å“è³ªã‚¹ã‚³ã‚¢ã®è¨ˆç®—
            quality_score = self.calculate_quality_score(result['snippet'] or '', query)
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®å–å¾—
            context_chunks = self.get_context_chunks(
                result['chunk_index'], 
                result['document_id'], 
                company_id
            )
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒ¼ãƒŠã‚¹ã®è¨ˆç®—
            context_bonus = self.calculate_context_bonus(result, context_chunks, query)
            
            # ç·åˆé–¢é€£åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
            relevance_score = (
                result['similarity_score'] * self.similarity_weight +
                quality_score * self.quality_weight +
                context_bonus * self.context_weight
            )
            
            return EnhancedSearchResult(
                chunk_id=result['chunk_id'],
                document_id=result['document_id'],
                document_name=result['name'] or 'Unknown',
                content=result['snippet'] or '',
                similarity_score=result['similarity_score'],
                relevance_score=relevance_score,
                chunk_index=result['chunk_index'],
                document_type=result['type'] or 'document',
                search_method='enhanced_vector',
                context_bonus=context_bonus,
                quality_score=quality_score,
                metadata={
                    'special': result.get('special'),
                    'context_chunks_count': len(context_chunks)
                }
            )
        
        except Exception as e:
            logger.error(f"æ¤œç´¢çµæœå¼·åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _deduplicate_and_select(self, results: List[EnhancedSearchResult], max_results: int) -> List[EnhancedSearchResult]:
        """é‡è¤‡é™¤å»ã¨æœ€çµ‚é¸æŠ"""
        seen_content = set()
        seen_documents = defaultdict(int)
        final_results = []
        
        for result in results:
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆå…ˆé ­100æ–‡å­—ï¼‰
            content_key = result.content[:100].strip()
            if content_key in seen_content:
                continue
            
            # åŒä¸€æ–‡æ›¸ã‹ã‚‰ã®çµæœæ•°åˆ¶é™ï¼ˆæœ€å¤§3ä»¶ï¼‰
            if seen_documents[result.document_id] >= 3:
                continue
            
            # æœ€å°å“è³ªé–¾å€¤ãƒã‚§ãƒƒã‚¯
            if result.quality_score < 0.2:
                continue
            
            seen_content.add(content_key)
            seen_documents[result.document_id] += 1
            final_results.append(result)
            
            if len(final_results) >= max_results:
                break
        
        return final_results
    
    async def get_enhanced_document_content(self, query: str, company_id: str = None, max_results: int = 10) -> str:
        """å¼·åŒ–ã•ã‚ŒãŸæ–‡æ›¸å†…å®¹å–å¾—"""
        try:
            # å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œ
            search_results = await self.enhanced_vector_search(query, company_id, max_results)
            
            if not search_results:
                logger.warning("é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return ""
            
            # çµæœã‚’çµ„ã¿ç«‹ã¦
            relevant_content = []
            total_length = 0
            max_total_length = 50000
            
            logger.info(f"ğŸ“Š å¼·åŒ–æ¤œç´¢çµæœã‚’å‡¦ç†ä¸­: {len(search_results)}ä»¶")
            
            for i, result in enumerate(search_results):
                logger.info(f"  {i+1}. {result.document_name} [ãƒãƒ£ãƒ³ã‚¯{result.chunk_index}]")
                logger.info(f"     é–¢é€£åº¦: {result.relevance_score:.3f} (é¡ä¼¼åº¦: {result.similarity_score:.3f})")
                
                if result.content and len(result.content.strip()) > 0:
                    content_piece = f"\n=== {result.document_name} - ãƒãƒ£ãƒ³ã‚¯{result.chunk_index} (é–¢é€£åº¦: {result.relevance_score:.3f}) ===\n{result.content}\n"
                    
                    if total_length + len(content_piece) <= max_total_length:
                        relevant_content.append(content_piece)
                        total_length += len(content_piece)
                        logger.info(f"    - è¿½åŠ å®Œäº† ({len(content_piece)}æ–‡å­—)")
                    else:
                        logger.info(f"    - æ–‡å­—æ•°åˆ¶é™ã«ã‚ˆã‚Šé™¤å¤–")
                        break
                else:
                    logger.info(f"    - ç©ºã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            
            final_content = "\n".join(relevant_content)
            logger.info(f"âœ… æœ€çµ‚çš„ãªé–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„: {len(final_content)}æ–‡å­—")
            
            return final_content
        
        except Exception as e:
            logger.error(f"âŒ å¼·åŒ–æ–‡æ›¸å†…å®¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
            return ""

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_enhanced_vector_search_instance = None

def get_enhanced_vector_search_instance() -> Optional[EnhancedVectorSearchSystem]:
    """å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
    global _enhanced_vector_search_instance
    
    if _enhanced_vector_search_instance is None:
        try:
            _enhanced_vector_search_instance = EnhancedVectorSearchSystem()
            logger.info("âœ… å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            logger.error(f"âŒ å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    return _enhanced_vector_search_instance

def enhanced_vector_search_available() -> bool:
    """å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        use_vertex_ai = os.getenv("USE_VERTEX_AI", "true").lower() == "true"
        
        return bool(api_key and supabase_url and supabase_key and use_vertex_ai)
    except Exception:
        return False
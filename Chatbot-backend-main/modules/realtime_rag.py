"""
ğŸš€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ãƒ•ãƒ­ãƒ¼
è³ªå•å—ä»˜ã€œRAGå‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å›ç­”ï¼‰ã®å®Ÿè£…

ã‚¹ãƒ†ãƒƒãƒ—:
âœï¸ Step 1. è³ªå•å…¥åŠ› - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã«è³ªå•ã‚’å…¥åŠ›
ğŸ§  Step 2. embedding ç”Ÿæˆ - Vertex AI text-multilingual-embedding-002 ã‚’ä½¿ã£ã¦ã€è³ªå•æ–‡ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ï¼ˆ768æ¬¡å…ƒï¼‰
ğŸ” Step 3. é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢ï¼ˆTop-Kï¼‰ - Supabaseã® chunks ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã€ãƒ™ã‚¯ãƒˆãƒ«è·é›¢ãŒè¿‘ã„ãƒãƒ£ãƒ³ã‚¯ã‚’ pgvector ã‚’ç”¨ã„ã¦å–å¾—
ğŸ’¡ Step 4. LLMã¸é€ä¿¡ - Top-K ãƒãƒ£ãƒ³ã‚¯ã¨å…ƒã®è³ªå•ã‚’ Gemini Flash 2.5 ã«æ¸¡ã—ã¦ã€è¦ç´„ã›ãšã«ã€ŒåŸæ–‡ãƒ™ãƒ¼ã‚¹ã€ã§å›ç­”ã‚’ç”Ÿæˆ
âš¡ï¸ Step 5. å›ç­”è¡¨ç¤º
"""

import os
import logging
import asyncio
from typing import List, Dict, Optional, Tuple, Any
from dotenv import load_dotenv
import requests
import json
import psycopg2
from psycopg2.extras import RealDictCursor
from datetime import datetime
import urllib.parse  # è¿½åŠ 
import re # è¿½åŠ 

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

logger = logging.getLogger(__name__)

class RealtimeRAGProcessor:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆGeminiè³ªå•åˆ†æçµ±åˆç‰ˆï¼‰"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.use_vertex_ai = False  # Vertex AIã‚’ç„¡åŠ¹åŒ–
        self.embedding_model = "gemini-embedding-001"  # Geminiã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        self.expected_dimensions = 3072  # gemini-embedding-001ã¯3072æ¬¡å…ƒ
        
        # API ã‚­ãƒ¼ã®è¨­å®š
        self.api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        
        self.chat_model = "gemini-2.5-flash"  # æœ€æ–°ã®Gemini Flash 2.5
        self.db_url = self._get_db_url()
        
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY ã¾ãŸã¯ GEMINI_API_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # Gemini API ã®ç›´æ¥å‘¼ã³å‡ºã—ç”¨URL
        self.api_base_url = "https://generativelanguage.googleapis.com/v1beta"
        
        # Gemini Embeddingã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ï¼ˆåŸ‹ã‚è¾¼ã¿ç”¨ï¼‰
        try:
            from .multi_api_embedding import get_multi_api_embedding_client, multi_api_embedding_available
            if multi_api_embedding_available():
                self.embedding_client = get_multi_api_embedding_client()
                logger.info(f"âœ… Embedding ClientåˆæœŸåŒ–: {self.embedding_model} ({self.expected_dimensions}æ¬¡å…ƒ)")
            else:
                logger.error("âŒ Embedding ClientãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                raise ValueError("Embedding Clientã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        except ImportError:
            logger.error("âŒ multi_api_embedding ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            raise ValueError("Embedding Clientã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        # ğŸ§  Geminiè³ªå•åˆ†æã‚·ã‚¹ãƒ†ãƒ ã‚’ç„¡åŠ¹åŒ–ï¼ˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°æ¤œç´¢ã®ã¿ä½¿ç”¨ï¼‰
        self.gemini_analyzer = None
        logger.info("âœ… ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°æ¤œç´¢ã®ã¿ã‚’ä½¿ç”¨ï¼ˆGeminiè³ªå•åˆ†æã‚·ã‚¹ãƒ†ãƒ ã¯ç„¡åŠ¹åŒ–ï¼‰")
        
        logger.info(f"âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–å®Œäº†: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°={self.embedding_model} ({self.expected_dimensions}æ¬¡å…ƒ)")

    async def _keyword_search(self, query: str, company_id: Optional[str], limit: int = 40) -> List[Dict]:
        """
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢ï¼ˆILIKEã‚’ä½¿ç”¨ï¼‰
        """
        logger.info(f"ğŸ”‘ Step 3-Keyword: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢é–‹å§‹ (Top-{limit})")
        # ã‚¯ã‚¨ãƒªã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆä¾‹ï¼šWPD4100389ï¼‰
        keywords = re.findall(r'[A-Z]+\d+', query)
        if not keywords:
            logger.info("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return []
        
        search_term = keywords[0] # ç°¡å˜ã®ãŸã‚æœ€åˆã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
        
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    sql_keyword = """
                    SELECT
                        c.id, c.doc_id, c.chunk_index, c.content,
                        ds.name as document_name, ds.type as document_type,
                        0.9 as similarity_score, -- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã¯é«˜ã‚¹ã‚³ã‚¢
                        'keyword' as search_method
                    FROM chunks c
                    LEFT JOIN document_sources ds ON ds.id = c.doc_id
                    WHERE c.content ILIKE %s
                    """
                    params_keyword = [f"%{search_term}%"]

                    if company_id:
                        sql_keyword += " AND c.company_id = %s"
                        params_keyword.append(company_id)
                    
                    sql_keyword += " LIMIT %s"
                    params_keyword.append(limit)
                    
                    cur.execute(sql_keyword, params_keyword)
                    results = cur.fetchall()
                    logger.info(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{search_term}' ã§ {len(results)} ä»¶ãƒ’ãƒƒãƒˆã—ã¾ã—ãŸã€‚")
                    return [dict(row) for row in results]
        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _get_db_url(self) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã‚’æ§‹ç¯‰"""
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # Supabase URLã‹ã‚‰æ¥ç¶šæƒ…å ±ã‚’æŠ½å‡º
        if "supabase.co" in supabase_url:
            project_id = supabase_url.split("://")[1].split(".")[0]
            return f"postgresql://postgres.{project_id}:{os.getenv('DB_PASSWORD', '')}@aws-0-ap-northeast-1.pooler.supabase.com:6543/postgres"
        else:
            # ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã®å ´åˆ
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                raise ValueError("DATABASE_URL ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return db_url
    
    async def step1_receive_question(self, question: str, company_id: str = None) -> Dict:
        """
        âœï¸ Step 1. è³ªå•å…¥åŠ›
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã«è³ªå•ã‚’å…¥åŠ›
        """
        # ChatMessageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰æ–‡å­—åˆ—ã‚’å–å¾—
        if hasattr(question, 'text'):
            question_text = question.text
        else:
            question_text = str(question)
        
        logger.info(f"âœï¸ Step 1: è³ªå•å—ä»˜ - '{question_text[:50]}...'")
        
        if not question or not question.strip():
            raise ValueError("è³ªå•ãŒç©ºã§ã™")
        
        # è³ªå•ã®å‰å‡¦ç†
        processed_question = question.strip()
        
        return {
            "original_question": question,
            "processed_question": processed_question,
            "company_id": company_id,
            "timestamp": datetime.now().isoformat(),
            "step": 1
        }
    
    async def step2_generate_embedding(self, question: str) -> List[float]:
        """
        ğŸ§  Step 2. embedding ç”Ÿæˆ
        Gemini embedding-001 ã‚’ä½¿ã£ã¦ã€è³ªå•æ–‡ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ï¼ˆ3072æ¬¡å…ƒï¼‰
        """
        logger.info(f"ğŸ§  Step 2: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆä¸­...")
        
        try:
            # gemini-embedding-001ãƒ¢ãƒ‡ãƒ«ã§3072æ¬¡å…ƒã‚’ç”Ÿæˆ
            embedding_vector = await self.embedding_client.generate_embedding(
                question
            )
            
            if embedding_vector and len(embedding_vector) > 0:
                # æ¬¡å…ƒæ•°ãƒã‚§ãƒƒã‚¯
                if len(embedding_vector) != self.expected_dimensions:
                    logger.warning(f"äºˆæœŸã—ãªã„æ¬¡å…ƒæ•°: {len(embedding_vector)}æ¬¡å…ƒï¼ˆæœŸå¾…å€¤: {self.expected_dimensions}æ¬¡å…ƒï¼‰")
                
                logger.info(f"âœ… Step 2å®Œäº†: {len(embedding_vector)}æ¬¡å…ƒã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”ŸæˆæˆåŠŸ")
                return embedding_vector
            else:
                raise ValueError("ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            
        except Exception as e:
            logger.error(f"âŒ Step 2ã‚¨ãƒ©ãƒ¼: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆå¤±æ•— - {e}")
            raise
    
    async def step3_similarity_search(self, query_embedding: List[float], company_id: str = None, top_k: int = 100) -> List[Dict]:
        """
        ğŸ” Step 3. é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢ï¼ˆTop-Kï¼‰
        Supabaseã® chunks ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã€ãƒ™ã‚¯ãƒˆãƒ«è·é›¢ãŒè¿‘ã„ãƒãƒ£ãƒ³ã‚¯ã‚’ pgvector ã‚’ç”¨ã„ã¦å–å¾—
        PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å«ã‚€ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’å¹³ç­‰ã«æ¤œç´¢å¯¾è±¡ã¨ã™ã‚‹
        """
        logger.info(f"ğŸ” Step 3: é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢é–‹å§‹ (Top-{top_k})")
        
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    # ğŸ” ã¾ãšã€åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒåˆ©ç”¨å¯èƒ½ãªãƒãƒ£ãƒ³ã‚¯ã§ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢
                    
                    # Convert query vector to proper string format and cast to vector type
                    vector_str = '[' + ','.join(map(str, query_embedding)) + ']'
                    
                    sql_vector = f"""
                    SELECT
                        c.id,
                        c.doc_id,
                        c.chunk_index,
                        c.content,
                        ds.name as document_name,
                        ds.type as document_type,
                        1 - (c.embedding <=> '{vector_str}'::vector) as similarity_score,
                        'vector' as search_method
                    FROM chunks c
                    LEFT JOIN document_sources ds ON ds.id = c.doc_id
                    WHERE c.embedding IS NOT NULL
                      AND c.content IS NOT NULL
                      AND LENGTH(c.content) > 10
                    """
                    
                    params_vector = []
                    
                    # ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                    if company_id:
                        sql_vector += " AND c.company_id = %s"
                        params_vector.append(company_id)
                    
                    # ãƒ™ã‚¯ãƒˆãƒ«è·é›¢é †ã§ã‚½ãƒ¼ãƒˆ
                    sql_vector += f" ORDER BY c.embedding <=> '{vector_str}'::vector LIMIT %s"
                    params_vector.append(top_k)
                    
                    logger.info(f"å®Ÿè¡ŒSQL: ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢ (Top-{top_k})")
                    cur.execute(sql_vector, params_vector)
                    vector_results = cur.fetchall()
                    
                    # ğŸ” PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœã‚’ç¢ºèª
                    pdf_vector_count = len([r for r in vector_results if r['document_type'] == 'pdf'])
                    excel_vector_count = len([r for r in vector_results if r['document_type'] == 'excel'])
                    
                    logger.info(f"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœ: PDF={pdf_vector_count}ä»¶, Excel={excel_vector_count}ä»¶, ç·è¨ˆ={len(vector_results)}ä»¶")
                    
                    # çµæœã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                    similar_chunks = []
                    for row in vector_results:
                        similar_chunks.append({
                            'chunk_id': row['id'],
                            'doc_id': row['doc_id'],
                            'chunk_index': row['chunk_index'],
                            'content': row['content'],
                            'document_name': row['document_name'],
                            'document_type': row['document_type'],
                            'similarity_score': float(row['similarity_score']),
                            'search_method': row['search_method']
                        })
                    
                    # ğŸ” PDFãƒ•ã‚¡ã‚¤ãƒ«ã®çµæœãŒå°‘ãªã„å ´åˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’å®Ÿè¡Œ
                    if pdf_vector_count < 10:  # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®çµæœãŒ10ä»¶æœªæº€ã®å ´åˆ
                        logger.info("ğŸ“„ PDFãƒ•ã‚¡ã‚¤ãƒ«ã®çµæœãŒå°‘ãªã„ãŸã‚ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’å®Ÿè¡Œ")
                        
                        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒãªã„ãƒãƒ£ãƒ³ã‚¯ã«å¯¾ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã‚’å®Ÿè¡Œ
                        sql_text = """
                        SELECT
                            c.id,
                            c.doc_id,
                            c.chunk_index,
                            c.content,
                            ds.name as document_name,
                            ds.type as document_type,
                            0.5 as similarity_score,
                            'text_fallback' as search_method
                        FROM chunks c
                        LEFT JOIN document_sources ds ON ds.id = c.doc_id
                        WHERE c.embedding IS NULL
                          AND c.content IS NOT NULL
                          AND LENGTH(c.content) > 10
                          AND ds.type = 'pdf'
                        """
                        
                        params_text = []
                        
                        # ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                        if company_id:
                            sql_text += " AND c.company_id = %s"
                            params_text.append(company_id)
                        
                        # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒãªã„å ´åˆï¼‰
                        sql_text += " ORDER BY RANDOM() LIMIT %s"
                        params_text.append(min(10, top_k))
                        
                        logger.info("å®Ÿè¡ŒSQL: PDFãƒ•ã‚¡ã‚¤ãƒ«å‘ã‘ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢")
                        cur.execute(sql_text, params_text)
                        text_fallback_results = cur.fetchall()
                        
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœã‚’è¿½åŠ 
                        for row in text_fallback_results:
                            similar_chunks.append({
                                'chunk_id': row['id'],
                                'doc_id': row['doc_id'],
                                'chunk_index': row['chunk_index'],
                                'content': row['content'],
                                'document_name': row['document_name'],
                                'document_type': row['document_type'],
                                'similarity_score': float(row['similarity_score']),
                                'search_method': row['search_method']
                            })
                        
                        logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã§{len(text_fallback_results)}ä»¶ã®PDFãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ ")
                    
                    # ğŸ” ã•ã‚‰ã«ã€ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã®è¿½åŠ æ¤œç´¢
                    file_type_distribution = {}
                    for chunk in similar_chunks:
                        doc_type = chunk['document_type'] or 'unknown'
                        file_type_distribution[doc_type] = file_type_distribution.get(doc_type, 0) + 1
                    
                    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®çµæœãŒä¾ç„¶ã¨ã—ã¦å°‘ãªã„å ´åˆ
                    if file_type_distribution.get('pdf', 0) < 5:
                        logger.info("ğŸ“„ PDFãƒ•ã‚¡ã‚¤ãƒ«çµæœãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€è¿½åŠ æ¤œç´¢ã‚’å®Ÿè¡Œ")
                        
                        # ä¼šç¤¾å…¨ä½“ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä»£è¡¨çš„ãªãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
                        sql_pdf_supplement = """
                        SELECT
                            c.id,
                            c.doc_id,
                            c.chunk_index,
                            c.content,
                            ds.name as document_name,
                            ds.type as document_type,
                            0.4 as similarity_score,
                            'pdf_supplement' as search_method
                        FROM chunks c
                        LEFT JOIN document_sources ds ON ds.id = c.doc_id
                        WHERE c.content IS NOT NULL
                          AND LENGTH(c.content) > 10
                          AND ds.type = 'pdf'
                          AND ds.active = true
                        """
                        
                        params_pdf_supplement = []
                        
                        # ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                        if company_id:
                            sql_pdf_supplement += " AND c.company_id = %s"
                            params_pdf_supplement.append(company_id)
                        
                        # æœ€æ–°ã®ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰å„ªå…ˆçš„ã«å–å¾—
                        sql_pdf_supplement += " ORDER BY c.id DESC LIMIT %s"
                        params_pdf_supplement.append(5)
                        
                        logger.info("å®Ÿè¡ŒSQL: PDFè£œå®Œæ¤œç´¢")
                        cur.execute(sql_pdf_supplement, params_pdf_supplement)
                        pdf_supplement_results = cur.fetchall()
                        
                        # PDFè£œå®Œçµæœã‚’è¿½åŠ 
                        for row in pdf_supplement_results:
                            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                            if not any(chunk['chunk_id'] == row['id'] for chunk in similar_chunks):
                                similar_chunks.append({
                                    'chunk_id': row['id'],
                                    'doc_id': row['doc_id'],
                                    'chunk_index': row['chunk_index'],
                                    'content': row['content'],
                                    'document_name': row['document_name'],
                                    'document_type': row['document_type'],
                                    'similarity_score': float(row['similarity_score']),
                                    'search_method': row['search_method']
                                })
                        
                        logger.info(f"PDFè£œå®Œæ¤œç´¢ã§{len(pdf_supplement_results)}ä»¶ã®ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ ")
                    
                    # çµæœã‚’é¡ä¼¼åº¦é †ã§ã‚½ãƒ¼ãƒˆ
                    similar_chunks.sort(key=lambda x: x['similarity_score'], reverse=True)
                    
                    # æœ€å¤§ãƒãƒ£ãƒ³ã‚¯æ•°ã«åˆ¶é™
                    final_chunks = similar_chunks[:top_k]
                    
                    logger.info(f"âœ… Step 3å®Œäº†: {len(final_chunks)}å€‹ã®é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—")
                    
                    # ğŸ” è©³ç´°ãƒãƒ£ãƒ³ã‚¯é¸æŠãƒ­ã‚°ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGï¼‰
                    print("\n" + "="*80)
                    print(f"ğŸ” ã€Step 3: é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢çµæœã€‘")
                    print(f"ğŸ“Š å–å¾—ãƒãƒ£ãƒ³ã‚¯æ•°: {len(final_chunks)}ä»¶ (Top-{top_k})")
                    print(f"ğŸ¢ ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿: {'é©ç”¨ (' + company_id + ')' if company_id else 'æœªé©ç”¨ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ï¼‰'}")
                    print(f"ğŸ§  ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«: {self.embedding_model} ({self.expected_dimensions}æ¬¡å…ƒ)")
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆã‚’è¡¨ç¤º
                    final_file_type_distribution = {}
                    for chunk in final_chunks:
                        doc_type = chunk['document_type'] or 'unknown'
                        final_file_type_distribution[doc_type] = final_file_type_distribution.get(doc_type, 0) + 1
                    
                    print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥çµæœ: {final_file_type_distribution}")
                    print("="*80)
                    
                    for i, chunk in enumerate(final_chunks):
                        similarity = chunk['similarity_score']
                        doc_name = chunk['document_name'] or 'Unknown'
                        chunk_idx = chunk['chunk_index']
                        content_preview = (chunk['content'] or '')[:150].replace('\n', ' ')
                        search_method = chunk['search_method']
                        
                        # é¡ä¼¼åº¦ã«åŸºã¥ãè©•ä¾¡
                        if similarity > 0.8:
                            evaluation = "ğŸŸ¢ éå¸¸ã«é«˜ã„é–¢é€£æ€§"
                        elif similarity > 0.6:
                            evaluation = "ğŸŸ¡ é«˜ã„é–¢é€£æ€§"
                        elif similarity > 0.4:
                            evaluation = "ğŸŸ  ä¸­ç¨‹åº¦ã®é–¢é€£æ€§"
                        elif similarity > 0.2:
                            evaluation = "ğŸ”´ ä½ã„é–¢é€£æ€§"
                        else:
                            evaluation = "âš« æ¥µã‚ã¦ä½ã„é–¢é€£æ€§"
                        
                        print(f"  {i+1:2d}. ğŸ“„ {doc_name} ({chunk['document_type']})")
                        print(f"      ğŸ§© ãƒãƒ£ãƒ³ã‚¯#{chunk_idx} | ğŸ¯ é¡ä¼¼åº¦: {similarity:.4f} | {evaluation}")
                        print(f"      ğŸ” æ¤œç´¢æ–¹æ³•: {search_method}")
                        print(f"      ğŸ“ å†…å®¹: {content_preview}...")
                        print(f"      ğŸ”— ãƒãƒ£ãƒ³ã‚¯ID: {chunk['chunk_id']} | ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID: {chunk['doc_id']}")
                        print()
                    
                    print("="*80 + "\n")
                    
                    return final_chunks
        
        except Exception as e:
            logger.error(f"âŒ Step 3ã‚¨ãƒ©ãƒ¼: é¡ä¼¼æ¤œç´¢å¤±æ•— - {e}")
            raise
    
    async def step4_generate_answer(self, question: str, similar_chunks: List[Dict], company_name: str = "ãŠå®¢æ§˜ã®ä¼šç¤¾", company_id: str = None) -> Dict[str, Any]:
        """
        ğŸ’¡ Step 4. LLMã¸é€ä¿¡
        Top-K ãƒãƒ£ãƒ³ã‚¯ã¨å…ƒã®è³ªå•ã‚’ Gemini Flash 2.5 ã«æ¸¡ã—ã¦ã€è¦ç´„ã›ãšã«ã€ŒåŸæ–‡ãƒ™ãƒ¼ã‚¹ã€ã§å›ç­”ã‚’ç”Ÿæˆ
        """
        logger.info(f"ğŸ’¡ Step 4: LLMå›ç­”ç”Ÿæˆé–‹å§‹ ({len(similar_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ä½¿ç”¨)")
        
        if not similar_chunks:
            logger.warning("é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ä¸€èˆ¬çš„ãªå›ç­”ã‚’ç”Ÿæˆ")
            return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã”è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚ˆã‚Šå…·ä½“çš„ãªè³ªå•ã‚’ã—ã¦ã„ãŸã ã‘ã¾ã™ã§ã—ã‚‡ã†ã‹ã€‚"
        
        try:
            # ğŸ” Step 4: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ãƒ­ã‚°
            print("\n" + "="*80)
            print(f"ğŸ’¡ ã€Step 4: LLMå›ç­”ç”Ÿæˆ - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ã€‘")
            print(f"ğŸ“Š åˆ©ç”¨å¯èƒ½ãƒãƒ£ãƒ³ã‚¯æ•°: {len(similar_chunks)}å€‹")
            print(f"ğŸ“ æœ€å¤§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {200000:,}æ–‡å­—")  # ã•ã‚‰ã«å¤šãã®æƒ…å ±ã‚’å«ã‚ã‚‹
            print("="*80)
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ï¼ˆåŸæ–‡ãƒ™ãƒ¼ã‚¹ï¼‰
            context_parts = []
            total_length = 0
            max_context_length = 200000  # 20ä¸‡æ–‡å­—ã«åˆ¶é™ï¼ˆã•ã‚‰ã«å¤šãã®æƒ…å ±ã‚’å«ã‚ã‚‹ï¼‰
            used_chunks = []
            
            for i, chunk in enumerate(similar_chunks):
                chunk_content = f"ã€å‚è€ƒè³‡æ–™{i+1}: {chunk['document_name']} - ãƒãƒ£ãƒ³ã‚¯{chunk['chunk_index']}ã€‘\n{chunk['content']}\n"
                chunk_length = len(chunk_content)
                
                print(f"  {i+1:2d}. ğŸ“„ {chunk['document_name']} [ãƒãƒ£ãƒ³ã‚¯#{chunk['chunk_index']}]")
                print(f"      ğŸ¯ é¡ä¼¼åº¦: {chunk['similarity_score']:.4f}")
                print(f"      ğŸ“ æ–‡å­—æ•°: {chunk_length:,}æ–‡å­—")
                
                if total_length + chunk_length > max_context_length:
                    print(f"      âŒ é™¤å¤–: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·åˆ¶é™è¶…é (ç¾åœ¨: {total_length:,}æ–‡å­—)")
                    print(f"         ğŸ’¡ {i}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’æœ€çµ‚çš„ã«ä½¿ç”¨")
                    break
                
                context_parts.append(chunk_content)
                total_length += chunk_length
                used_chunks.append(chunk)
                print(f"      âœ… æ¡ç”¨: ç´¯è¨ˆ {total_length:,}æ–‡å­—")
                print(f"      ğŸ“ å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {(chunk['content'] or '')[:100].replace(chr(10), ' ')}...")
                print()
            
            context = "\n".join(context_parts)
            
            print(f"ğŸ“‹ æœ€çµ‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±:")
            print(f"   âœ… ä½¿ç”¨ãƒãƒ£ãƒ³ã‚¯æ•°: {len(used_chunks)}å€‹")
            print(f"   ï¿½ï¿½ ç·æ–‡å­—æ•°: {len(context):,}æ–‡å­—")
            print("="*80 + "\n")
            
            # ğŸ¯ ç‰¹åˆ¥æŒ‡ç¤ºã‚’å–å¾—ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸€ç•ªå‰ã«é…ç½®
            special_instructions_text = ""
            if company_id:
                try:
                    with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                        with conn.cursor() as cur:
                            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ã®ç‰¹åˆ¥æŒ‡ç¤ºã‚’å–å¾—
                            sql = """
                            SELECT DISTINCT ds.name, ds.special
                            FROM document_sources ds 
                            WHERE ds.company_id = %s 
                            AND ds.active = true 
                            AND ds.special IS NOT NULL 
                            AND ds.special != ''
                            ORDER BY ds.name
                            """
                            cur.execute(sql, [company_id])
                            special_results = cur.fetchall()
                            
                            if special_results:
                                special_instructions = []
                                print(f"ğŸ¯ ç‰¹åˆ¥æŒ‡ç¤ºã‚’å–å¾—ã—ã¾ã—ãŸ: {len(special_results)}ä»¶")
                                for i, row in enumerate(special_results, 1):
                                    resource_name = row['name']
                                    special_instruction = row['special']
                                    special_instructions.append(f"{i}. ã€{resource_name}ã€‘: {special_instruction}")
                                    print(f"   {i}. {resource_name}: {special_instruction}")
                                
                                special_instructions_text = "ç‰¹åˆ¥ãªå›ç­”æŒ‡ç¤ºï¼ˆä»¥ä¸‹ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’å‚ç…§ã™ã‚‹éš›ã¯ã€å„ãƒªã‚½ãƒ¼ã‚¹ã®æŒ‡ç¤ºã«å¾“ã£ã¦ãã ã•ã„ï¼‰ï¼š\n" + "\n".join(special_instructions) + "\n\n"
                                print(f"âœ… ç‰¹åˆ¥æŒ‡ç¤ºã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ å®Œäº†")
                            else:
                                print(f"â„¹ï¸ ç‰¹åˆ¥æŒ‡ç¤ºãŒè¨­å®šã•ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                                
                except Exception as e:
                    print(f"âš ï¸ ç‰¹åˆ¥æŒ‡ç¤ºå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                    logger.warning(f"ç‰¹åˆ¥æŒ‡ç¤ºå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            
            # æ”¹å–„ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ï¼ˆç‰¹åˆ¥æŒ‡ç¤ºã‚’ä¸€ç•ªå‰ã«é…ç½®ï¼‰- ã‚ˆã‚ŠæŸ”è»Ÿãªæ¤œç´¢å¯¾å¿œ
            prompt = f"""{special_instructions_text}ã‚ãªãŸã¯{company_name}ã®ç¤¾å†…å‘ã‘ä¸å¯§ã§è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ã€å›ç­”ã®éš›ã®é‡è¦ãªæŒ‡é‡ã€‘
â€¢ å›ç­”ã¯ä¸å¯§ãªæ•¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚
â€¢ **æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒé–¢é€£ã™ã‚‹ã¨åˆ¤æ–­ã—ãŸå‚è€ƒè³‡æ–™ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚ä»¥ä¸‹ã®åŸºæº–ã§ç©æ¥µçš„ã«æ´»ç”¨ã—ã¦ãã ã•ã„ï¼š**

**ã€æŸ”è»Ÿãªæƒ…å ±æä¾›åŸºæº–ã€‘**
â€¢ **ä¼šç¤¾åã«ã¤ã„ã¦ï¼šã€Œæ ªå¼ä¼šç¤¾ã‚ã„ã†ã€ã‚’æ¢ã—ã¦ã„ã‚‹å ´åˆã€ã€Œæ ªå¼ä¼šç¤¾ ã„ã†ã€ã€Œæ ªå¼ä¼šç¤¾ã€€ã‚ã„ã€ã€Œãˆ±ã‚ã„ã€ã€Œ(æ ª)ã‚ã„ã€ãªã©ã‚‚åŒã˜ä¼šç¤¾ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„**
â€¢ **éƒ¨åˆ†ä¸€è‡´ã§ã‚‚æœ‰åŠ¹ï¼šè³ªå•ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä¸€éƒ¨ã§ã‚‚å‚è€ƒè³‡æ–™ã«å«ã¾ã‚Œã¦ã„ã‚Œã°ã€é–¢é€£æƒ…å ±ã¨ã—ã¦æä¾›ã—ã¦ãã ã•ã„**
â€¢ **é¡ä¼¼æƒ…å ±ã®æä¾›ï¼šå®Œå…¨ä¸€è‡´ã§ãªãã¦ã‚‚ã€ä¼¼ãŸã‚ˆã†ãªä¼šç¤¾åã€é–¢é€£ã™ã‚‹æ¥­ç•Œæƒ…å ±ã€é¡ä¼¼ã®ã‚µãƒ¼ãƒ“ã‚¹ãªã©ãŒã‚ã‚Œã°ç©æ¥µçš„ã«ç´¹ä»‹ã—ã¦ãã ã•ã„**
â€¢ **æ¨æ¸¬ã¨èª¬æ˜ï¼šå‚è€ƒè³‡æ–™ã‹ã‚‰æ¨æ¸¬ã§ãã‚‹ã“ã¨ã‚„ã€é–¢é€£ã™ã‚‹å†…å®¹ãŒã‚ã‚Œã°ã€Œå‚è€ƒè³‡æ–™ã«ã¯â—‹â—‹ã¨ã„ã†æƒ…å ±ãŒã”ã–ã„ã¾ã™ã€ã¨ã—ã¦æä¾›ã—ã¦ãã ã•ã„**
â€¢ **æ–­ç‰‡æƒ…å ±ã‚‚æ´»ç”¨ï¼šè¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã‚„ä¸€éƒ¨ã®æƒ…å ±ã§ã‚ã£ã¦ã‚‚ã€è³ªå•ã«é–¢é€£ã™ã‚‹éƒ¨åˆ†ãŒã‚ã‚Œã°æ„å‘³ã®ã‚ã‚‹æƒ…å ±ã¨ã—ã¦è§£é‡ˆã—ã¦ãã ã•ã„**

**ã€å›ç­”ãƒ‘ã‚¿ãƒ¼ãƒ³ä¾‹ã€‘**
â€¢ ã€Œæ ªå¼ä¼šç¤¾ã§ã‚ã„ã†ã€ã«ã¤ã„ã¦è³ªå•ã•ã‚ŒãŸå ´åˆï¼š
  - å®Œå…¨ä¸€è‡´ãŒè¦‹ã¤ã‹ã‚‰ãªãã¦ã‚‚ã€ã€Œæ ªå¼ä¼šç¤¾ã€ã€Œã‚ã„ã€ã‚’å«ã‚€ä¼šç¤¾ãŒã‚ã‚Œã°ç´¹ä»‹ã™ã‚‹
  - é¡ä¼¼ã®åå‰ã®ä¼šç¤¾ãŒã‚ã‚Œã°ã€Œé¡ä¼¼ã®ä¼šç¤¾åã¨ã—ã¦â—‹â—‹ãŒã”ã–ã„ã¾ã™ã€ã¨æ¡ˆå†…
  - é–¢é€£ã™ã‚‹æ¥­ç•Œã®ä¼šç¤¾ãŒã‚ã‚Œã°å‚è€ƒæƒ…å ±ã¨ã—ã¦æä¾›

**ã€é¿ã‘ã‚‹ã¹ãå›ç­”ã€‘**
â€¢ âŒã€Œæƒ…å ±ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ã€ã€Œç¢ºèªã§ãã¾ã›ã‚“ã§ã—ãŸã€ï¼ˆå‚è€ƒè³‡æ–™ãŒã‚ã‚‹å ´åˆï¼‰
â€¢ âŒ å®Œå…¨ä¸€è‡´ã®ã¿ã‚’æ±‚ã‚ã‚‹å³æ ¼ãªåˆ¤æ–­
â€¢ âœ… ä»£ã‚ã‚Šã«ï¼šã€Œå‚è€ƒè³‡æ–™ã‚’ç¢ºèªã—ãŸã¨ã“ã‚ã€â—‹â—‹ã¨ã„ã†æƒ…å ±ãŒã”ã–ã„ã¾ã™ã€

**ã€ãã®ä»–ã®æŒ‡é‡ã€‘**
â€¢ æƒ…å ±ã®å‡ºå…¸ã¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«åã¯æ˜ç¤ºå¯èƒ½ã§ã™ãŒã€å†…éƒ¨æ§‹é€ æƒ…å ±ï¼ˆè¡Œç•ªå·ç­‰ï¼‰ã¯å‡ºåŠ›ã—ãªã„
â€¢ å°‚é–€çš„ãªå†…å®¹ã‚‚åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜
â€¢ æ–‡æœ«ã«ã¯ã€Œã”ä¸æ˜ãªç‚¹ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã€ãŠæ°—è»½ã«ãŠç”³ã—ä»˜ã‘ãã ã•ã„ã€‚ã€ã‚’è¿½åŠ 

**ã€ãã®ä»–ã®æŒ‡é‡ã€‘**
â€¢ æƒ…å ±ã®å‡ºå…¸ã¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«åã¯æ˜ç¤ºå¯èƒ½ã§ã™ãŒã€å†…éƒ¨æ§‹é€ æƒ…å ±ï¼ˆè¡Œç•ªå·ç­‰ï¼‰ã¯å‡ºåŠ›ã—ãªã„
â€¢ å°‚é–€çš„ãªå†…å®¹ã‚‚åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜
â€¢ å®Ÿéš›ã«å‚ç…§ã—ãŸè³‡æ–™ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å›ç­”æ–‡ä¸­ã§æ˜ç¢ºã«è¨€åŠã—ã¦ãã ã•ã„
â€¢ æ–‡æœ«ã«ã¯ã€Œã”ä¸æ˜ãªç‚¹ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã€ãŠæ°—è»½ã«ãŠç”³ã—ä»˜ã‘ãã ã•ã„ã€‚ã€ã‚’è¿½åŠ 

ã”è³ªå•ï¼š
{question}

å‚è€ƒã¨ãªã‚‹è³‡æ–™ï¼š
{context}

ä¸Šè¨˜ã®å‚è€ƒè³‡æ–™ã‚’åŸºã«ã€æŸ”è»Ÿã‹ã¤ç©æ¥µçš„ã«ã”è³ªå•ã«ãŠç­”ãˆã„ãŸã—ã¾ã™ï¼š"""

            logger.info("ğŸ¤– Gemini Flash 2.5ã«å›ç­”ç”Ÿæˆã‚’ä¾é ¼ä¸­...")
            logger.info(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt):,}æ–‡å­—")
            
            # Gemini API ã¸ã®ç›´æ¥ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            api_url = f"{self.api_base_url}/models/{self.chat_model}:generateContent"
            
            headers = {
                "Content-Type": "application/json",
                "x-goog-api-key": self.api_key
            }
            
            request_data = {
                "contents": [
                    {
                        "parts": [
                            {
                                "text": prompt
                            }
                        ]
                    }
                ],
                "generationConfig": {
                    "temperature": 0.2,
                    "maxOutputTokens": 8192,
                    "topP": 0.9,
                    "topK": 50
                }
            }
            
            try:
                response = requests.post(api_url, headers=headers, json=request_data, timeout=60)
                response.raise_for_status()
                
                logger.info("ğŸ“¥ Geminiã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å—ä¿¡å®Œäº†")
                
                response_data = response.json()
                answer = None
                
                if "candidates" in response_data and response_data["candidates"]:
                    logger.info(f"ğŸ“‹ å€™è£œæ•°: {len(response_data['candidates'])}")
                    
                    try:
                        candidate = response_data["candidates"][0]
                        if "content" in candidate and "parts" in candidate["content"]:
                            parts = candidate["content"]["parts"]
                            if parts and "text" in parts[0]:
                                answer = parts[0]["text"]
                                logger.info(f"âœ… å›ç­”å–å¾—æˆåŠŸ: {len(answer)}æ–‡å­—")
                                
                                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®é–¢é€£ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ã£ã¦æ­£ç¢ºãªã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—
                                logger.info("ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–¢é€£ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã‚½ãƒ¼ã‚¹æƒ…å ±å–å¾—é–‹å§‹...")
                                
                                # used_chunksã«å«ã¾ã‚Œã‚‹doc_idã‚’æŠ½å‡º
                                doc_ids = []
                                for chunk in used_chunks:
                                    doc_id = chunk.get('document_id') or chunk.get('doc_id')
                                    if doc_id and doc_id not in doc_ids:
                                        doc_ids.append(doc_id)
                                
                                logger.info(f"ğŸ“„ æ¤œç´¢å¯¾è±¡doc_id: {doc_ids}")
                                
                                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰document_sourcesãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¤œç´¢ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—
                                actual_source_names = []
                                if doc_ids:
                                    try:
                                        from .database import get_database_connection
                                        
                                        with get_database_connection() as conn:
                                            with conn.cursor() as cur:
                                                # document_sources ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ name ã‚’å–å¾—
                                                placeholders = ','.join(['%s'] * len(doc_ids))
                                                query = f"""
                                                SELECT id, name, type 
                                                FROM document_sources 
                                                WHERE id IN ({placeholders}) AND active = true
                                                """
                                                
                                                cur.execute(query, doc_ids)
                                                source_results = cur.fetchall()
                                                
                                                logger.info(f"ğŸ“„ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢çµæœ: {len(source_results)}ä»¶")
                                                
                                                for row in source_results:
                                                    source_id, source_name, source_type = row
                                                    if source_name and source_name not in actual_source_names:
                                                        actual_source_names.append(source_name)
                                                        logger.info(f"âœ… æœ‰åŠ¹ã‚½ãƒ¼ã‚¹: {source_name} (ID: {source_id}, Type: {source_type})")
                                                
                                    except Exception as db_error:
                                        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®ã‚½ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {db_error}")
                                        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯used_chunksã®æ—¢å­˜æƒ…å ±ã‚’ä½¿ç”¨
                                        actual_source_names = [chunk.get('document_name', 'Unknown') for chunk in used_chunks if chunk.get('document_name')]
                                
                                # used_chunksã‚’å®Ÿéš›ã«å–å¾—ã•ã‚ŒãŸã‚½ãƒ¼ã‚¹åã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                                filtered_used_chunks = []
                                for chunk in used_chunks:
                                    chunk_doc_name = chunk.get('document_name', '')
                                    if chunk_doc_name in actual_source_names:
                                        filtered_used_chunks.append(chunk)
                                
                                logger.info(f"ğŸ¯ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–¢é€£çµæœ: {len(filtered_used_chunks)}ä»¶ã®ãƒãƒ£ãƒ³ã‚¯ãŒå®Ÿéš›ã®ã‚½ãƒ¼ã‚¹ã¨ä¸€è‡´")
                                
                                # ã•ã‚‰ã«å›ç­”å†…å®¹ã¨ã®é–¢é€£æ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã®ã¿ã‚’ç‰¹å®š
                                final_used_chunks = []
                                actually_used_sources = []
                                
                                for chunk in filtered_used_chunks:
                                    chunk_content = chunk.get('content', '') or chunk.get('snippet', '')
                                    chunk_doc_name = chunk.get('document_name', '')
                                    
                                    if chunk_content and len(chunk_content) > 20:
                                        # ãƒãƒ£ãƒ³ã‚¯å†…å®¹ã®ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æŠ½å‡ºï¼ˆ3æ–‡å­—ä»¥ä¸Šã®å˜èªï¼‰
                                        import re
                                        key_phrases = re.findall(r'\b\w{3,}\b', chunk_content)
                                        
                                        # å›ç­”æ–‡ä¸­ã«ãƒãƒ£ãƒ³ã‚¯ã®ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                                        matched_phrases = 0
                                        for phrase in key_phrases[:15]:  # æœ€åˆã®15å€‹ã®ãƒ•ãƒ¬ãƒ¼ã‚ºã§åˆ¤å®šï¼ˆæ‹¡å¼µï¼‰
                                            # 1. å®Œå…¨ä¸€è‡´
                                            if phrase in answer:
                                                matched_phrases += 1
                                            # 2. éƒ¨åˆ†ä¸€è‡´ï¼ˆ3æ–‡å­—ä»¥ä¸Šã§ã€é•·ã„ãƒ•ãƒ¬ãƒ¼ã‚ºã®å ´åˆï¼‰
                                            elif len(phrase) >= 6:
                                                for answer_word in answer.split():
                                                    if phrase in answer_word or answer_word in phrase:
                                                        matched_phrases += 0.5  # éƒ¨åˆ†ãƒãƒƒãƒã¯0.5ç‚¹
                                                        break
                                        
                                        # ä¸€å®šä»¥ä¸Šã®ãƒ•ãƒ¬ãƒ¼ã‚ºãƒãƒƒãƒãŒã‚ã‚Œã°å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸã¨åˆ¤å®š
                                        # åˆ†æ¯ã‚’èª¿æ•´ï¼ˆéƒ¨åˆ†ãƒãƒƒãƒã‚‚è€ƒæ…®ï¼‰
                                        max_possible_matches = min(len(key_phrases), 15)
                                        relevance_score = matched_phrases / max_possible_matches if max_possible_matches > 0 else 0
                                        
                                        # é–¢é€£æ€§é–¾å€¤ã‚’ç·©å’Œã—ã€çŸ­ã„ãƒãƒ£ãƒ³ã‚¯ã«ã¯ç‰¹åˆ¥å‡¦ç†
                                        min_threshold = 0.05  # 5%ã«ç·©å’Œ
                                        
                                        # çŸ­ã„ãƒãƒ£ãƒ³ã‚¯ï¼ˆ100æ–‡å­—æœªæº€ï¼‰ã¯é–¾å€¤ã‚’æ›´ã«ç·©å’Œ
                                        if len(chunk_content) < 100:
                                            min_threshold = 0.02  # 2%ã«ç·©å’Œ
                                            logger.info(f"ğŸ“ çŸ­ã„ãƒãƒ£ãƒ³ã‚¯æ¤œå‡º: {chunk_doc_name} (é•·ã•: {len(chunk_content)}æ–‡å­—)")
                                        
                                        if relevance_score >= min_threshold:
                                            final_used_chunks.append(chunk)
                                            if chunk_doc_name not in actually_used_sources:
                                                actually_used_sources.append(chunk_doc_name)
                                            logger.info(f"âœ… ä½¿ç”¨ãƒãƒ£ãƒ³ã‚¯ç¢ºå®š: {chunk_doc_name} (é–¢é€£åº¦: {relevance_score:.2f}, é–¾å€¤: {min_threshold:.2f})")
                                        else:
                                            logger.info(f"âŒ ä½¿ç”¨ãƒãƒ£ãƒ³ã‚¯é™¤å¤–: {chunk_doc_name} (é–¢é€£åº¦: {relevance_score:.2f}, é–¾å€¤: {min_threshold:.2f})")
                                
                                # çµæœãŒä¸ååˆ†ãªå ´åˆã®åŒ…æ‹¬çš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                                if len(final_used_chunks) < 3 and filtered_used_chunks:
                                    logger.warning(f"âš ï¸ é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯çµæœãŒä¸ååˆ†ï¼ˆ{len(final_used_chunks)}ä»¶ï¼‰ã€‚é«˜é¡ä¼¼åº¦ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ ")
                                    
                                    # é¡ä¼¼åº¦é †ã§ã‚½ãƒ¼ãƒˆ
                                    sorted_chunks = sorted(filtered_used_chunks, key=lambda x: x.get('similarity_score', 0), reverse=True)
                                    
                                    # ä¸Šä½ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ ï¼ˆæœ€å¤§5ä»¶ã¾ã§ï¼‰
                                    for chunk in sorted_chunks[:5]:
                                        chunk_doc_name = chunk.get('document_name', '')
                                        if chunk not in final_used_chunks:
                                            final_used_chunks.append(chunk)
                                            if chunk_doc_name and chunk_doc_name not in actually_used_sources:
                                                actually_used_sources.append(chunk_doc_name)
                                                logger.info(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¿½åŠ : {chunk_doc_name} (é¡ä¼¼åº¦: {chunk.get('similarity_score', 0):.2f})")
                                        
                                        # æœ€ä½3ä»¶ç¢ºä¿ã—ãŸã‚‰çµ‚äº†
                                        if len(final_used_chunks) >= 3:
                                            break
                                
                                # æœ€çµ‚å®‰å…¨ãƒã‚§ãƒƒã‚¯ï¼šå…¨ã¦é™¤å¤–ã•ã‚ŒãŸå ´åˆã®ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                                if not final_used_chunks and used_chunks:
                                    logger.error("ğŸš¨ å…¨ãƒãƒ£ãƒ³ã‚¯ãŒé™¤å¤–ã•ã‚Œã¾ã—ãŸã€‚å…ƒã®used_chunksã‚’ä½¿ç”¨ï¼ˆå®‰å…¨è£…ç½®ï¼‰")
                                    final_used_chunks = used_chunks[:3]  # å…ƒã®æœ€å¤§3ä»¶
                                    actually_used_sources = list(set([chunk.get('document_name', 'Unknown') for chunk in final_used_chunks if chunk.get('document_name')]))
                                
                                logger.info(f"ğŸ“ æœ€çµ‚ç¢ºå®šã‚½ãƒ¼ã‚¹: {actually_used_sources}")
                                logger.info(f"ğŸ¯ æœ€çµ‚ä½¿ç”¨ãƒãƒ£ãƒ³ã‚¯æ•°: {len(final_used_chunks)}ä»¶")
                                
                                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœã‚’é©ç”¨
                                used_chunks = final_used_chunks
                                    
                            else:
                                logger.warning("âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ãƒ¼ãƒ„ãŒç©ºã§ã™")
                        else:
                            logger.warning("âš ï¸ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¾ãŸã¯ãƒ‘ãƒ¼ãƒ„ãŒç©ºã§ã™")
                            
                    except Exception as e:
                        logger.error(f"âŒ ãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æã‚¨ãƒ©ãƒ¼: {e}")
                        answer = None
                else:
                    logger.warning("âš ï¸ ç„¡åŠ¹ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¾ãŸã¯å€™è£œãªã—")
                    
            except requests.exceptions.RequestException as e:
                logger.error(f"âŒ API ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                answer = None
            except Exception as e:
                logger.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
                answer = None
            
            # å›ç­”ã®æ¤œè¨¼ã¨å‡¦ç†
            if answer and len(answer.strip()) > 0:
                logger.info(f"âœ… Step 4å®Œäº†: {len(answer)}æ–‡å­—ã®å›ç­”ã‚’ç”Ÿæˆ")
                logger.info(f"ğŸ“ å›ç­”ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {answer[:100]}...")
                
                # å›ç­”ãŒçŸ­ã™ãã‚‹å ´åˆã®å‡¦ç†
                if len(answer) < 50:
                    logger.warning(f"âš ï¸ å›ç­”ãŒçŸ­ã™ãã¾ã™ï¼ˆ{len(answer)}æ–‡å­—ï¼‰- è£œå¼·å‡¦ç†ã‚’å®Ÿè¡Œ")
                    fallback_answer = f"""å‚è€ƒè³‡æ–™ã‚’ç¢ºèªã„ãŸã—ã¾ã—ãŸã€‚

{answer}

ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ãŒå¿…è¦ã§ã—ãŸã‚‰ã€å…·ä½“çš„ãªé …ç›®ã‚’ãŠæ•™ãˆãã ã•ã„ã€‚å‚è€ƒè³‡æ–™ã‹ã‚‰æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã„ãŸã—ã¾ã™ã€‚"""
                    return {
                        "answer": fallback_answer,
                        "used_chunks": used_chunks
                    }
                
                return {
                    "answer": answer,
                    "used_chunks": used_chunks
                }
            else:
                logger.error("âŒ LLMã‹ã‚‰ã®å›ç­”ãŒç©ºã¾ãŸã¯å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                logger.error(f"   ãƒ¬ã‚¹ãƒãƒ³ã‚¹è©³ç´°: {response}")
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å›ç­”ã®ç”Ÿæˆï¼ˆå‚è€ƒè³‡æ–™ã®æƒ…å ±ã‚’ä½¿ç”¨ï¼‰
                fallback_parts = []
                fallback_parts.append("å‚è€ƒè³‡æ–™ã‚’ç¢ºèªã„ãŸã—ã¾ã—ãŸãŒã€ã‚·ã‚¹ãƒ†ãƒ çš„ãªå•é¡Œã«ã‚ˆã‚Šè©³ç´°ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                
                # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰éƒ¨åˆ†çš„ãªæƒ…å ±ã‚’æä¾›
                if used_chunks and used_chunks[0].get('content'):
                    first_chunk_content = used_chunks[0]['content'][:300]
                    fallback_parts.append(f"\nå‚è€ƒè³‡æ–™ã®ä¸€éƒ¨ã‚’ã”ç´¹ä»‹ã„ãŸã—ã¾ã™ï¼š\n{first_chunk_content}...")
                
                fallback_parts.append("\nã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ã«ã¤ã„ã¦ã¯ã€æ”¹ã‚ã¦ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚")
                
                return {
                    "answer": "\n".join(fallback_parts),
                    "used_chunks": used_chunks
                }
        
        except Exception as e:
            logger.error(f"âŒ Step 4ã‚¨ãƒ©ãƒ¼: LLMå›ç­”ç”Ÿæˆå¤±æ•— - {e}")
            import traceback
            logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã§ã‚‚å¯èƒ½ãªé™ã‚Šæƒ…å ±ã‚’æä¾›
            error_response_parts = ["ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"]
            
            if used_chunks and len(used_chunks) > 0:
                error_response_parts.append(f"\næ¤œç´¢ã§ã¯{len(used_chunks)}ä»¶ã®é–¢é€£è³‡æ–™ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
                if used_chunks[0].get('content'):
                    first_content = used_chunks[0]['content'][:200]
                    error_response_parts.append(f"é–¢é€£æƒ…å ±ã®ä¸€éƒ¨: {first_content}...")
            
            error_response_parts.append("\nã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
            
            return {
                "answer": "\n".join(error_response_parts),
                "used_chunks": []  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®ãƒªã‚¹ãƒˆ
            }
    
    async def step5_display_answer(self, answer: str, metadata: Dict = None, used_chunks: List = None) -> Dict:
        """
        âš¡ï¸ Step 5. å›ç­”è¡¨ç¤º
        æœ€çµ‚çš„ãªå›ç­”ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        """
        logger.info(f"âš¡ï¸ Step 5: å›ç­”è¡¨ç¤ºæº–å‚™å®Œäº†")
        
        result = {
            "answer": answer,
            "timestamp": datetime.now().isoformat(),
            "step": 5,
            "status": "completed"
        }
        
        if metadata:
            result.update(metadata)
        
        # ä½¿ç”¨ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã®è©³ç´°æƒ…å ±ã‚’è¿½åŠ  - main.pyãŒæœŸå¾…ã™ã‚‹å½¢å¼ã§è¿”ã™
        if used_chunks:
            source_documents = []
            seen_names = set()
            for chunk in used_chunks[:5]:  # æœ€å¤§5å€‹ã®ã‚½ãƒ¼ã‚¹æ–‡æ›¸
                doc_name = chunk.get('document_name', 'Unknown Document')
                # é‡è¤‡ã™ã‚‹åå‰ã¯é™¤å¤–ã—ã€ã‚·ã‚¹ãƒ†ãƒ å›ç­”ç­‰ã¯é™¤å¤–
                if doc_name not in seen_names and doc_name not in ['ã‚·ã‚¹ãƒ†ãƒ å›ç­”', 'unknown', 'Unknown']:
                    doc_info = {
                        "name": doc_name,  # main.pyãŒæœŸå¾…ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å
                        "filename": doc_name,  # å¾Œæ–¹äº’æ›æ€§
                        "document_name": doc_name,  # å¾Œæ–¹äº’æ›æ€§
                        "document_type": chunk.get('document_type', 'unknown'),
                        "similarity_score": chunk.get('similarity_score', 0.0)
                    }
                    source_documents.append(doc_info)
                    seen_names.add(doc_name)
            
            result["sources"] = source_documents  # main.pyãŒæœŸå¾…ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å
            result["source_documents"] = source_documents  # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™
            result["total_sources"] = len(used_chunks)
        
        logger.info(f"âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†å®Œäº†: {len(answer)}æ–‡å­—ã®å›ç­”")
        return result
    
    async def process_realtime_rag(self, question: str, company_id: str = None, company_name: str = "ãŠå®¢æ§˜ã®ä¼šç¤¾", top_k: int = 100) -> Dict:
        """
        ğŸš€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ãƒ•ãƒ­ãƒ¼å…¨ä½“ã®å®Ÿè¡Œï¼ˆGeminiè³ªå•åˆ†æçµ±åˆç‰ˆï¼‰
        æ–°ã—ã„3æ®µéšã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: Geminiåˆ†æ â†’ SQLæ¤œç´¢ â†’ Embeddingæ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        """
        # ChatMessageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰æ–‡å­—åˆ—ã‚’å–å¾—
        if hasattr(question, 'text'):
            question_text = question.text
        else:
            question_text = str(question)
        
        logger.info(f"ğŸš€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†é–‹å§‹: '{question_text[:50]}...'")
        
        try:
            # Step 1: è³ªå•å…¥åŠ›
            step1_result = await self.step1_receive_question(question, company_id)
            processed_question = step1_result["processed_question"]
            
            # Step 2: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
            query_embedding = await self.step2_generate_embedding(processed_question)

            # Step 3: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’ä¸¦åˆ—å®Ÿè¡Œ
            search_tasks = [
                self.step3_similarity_search(query_embedding, company_id, top_k),
                self._keyword_search(processed_question, company_id, 30) # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã¯30ä»¶ã¾ã§
            ]
            results_list = await asyncio.gather(*search_tasks, return_exceptions=True)

            vector_results = results_list[0] if not isinstance(results_list[0], Exception) else []
            keyword_results = results_list[1] if not isinstance(results_list[1], Exception) else []

            # çµæœã®çµ±åˆã¨é‡è¤‡é™¤å»
            all_chunks = {r['chunk_id']: r for r in vector_results}
            for r in keyword_results:
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢çµæœã‚’å„ªå…ˆçš„ã«ä¸Šæ›¸ã
                all_chunks[r['id']] = {
                    'chunk_id': r['id'],
                    'doc_id': r['doc_id'],
                    'chunk_index': r['chunk_index'],
                    'content': r['content'],
                    'document_name': r['document_name'],
                    'document_type': r['document_type'],
                    'similarity_score': float(r['similarity_score']),
                    'search_method': r['search_method']
                }

            # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
            sorted_chunks = sorted(all_chunks.values(), key=lambda x: x['similarity_score'], reverse=True)
            
            # æœ€çµ‚çš„ãªãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
            similar_chunks = sorted_chunks[:top_k]
            
            metadata = {
                "original_question": question,
                "processed_question": processed_question,
                "chunks_used": len(similar_chunks),
                "top_similarity": similar_chunks[0]["similarity_score"] if similar_chunks else 0.0,
                "company_id": company_id,
                "company_name": company_name,
                "search_method": "hybrid_search" # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã«å¤‰æ›´
            }
            
            # Step 4: LLMå›ç­”ç”Ÿæˆ
            generation_result = await self.step4_generate_answer(processed_question, similar_chunks, company_name, company_id)
            answer = generation_result["answer"]
            actually_used_chunks = generation_result["used_chunks"]
            
            # Step 5: å›ç­”è¡¨ç¤ºï¼ˆå®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚’å«ã‚ã‚‹ï¼‰
            result = await self.step5_display_answer(answer, metadata, actually_used_chunks)
            
            logger.info(f"ğŸ‰ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†æˆåŠŸå®Œäº†")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            error_result = {
                "answer": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "status": "error"
            }
            return error_result

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_realtime_rag_processor = None

def get_realtime_rag_processor() -> Optional[RealtimeRAGProcessor]:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
    global _realtime_rag_processor
    
    if _realtime_rag_processor is None:
        try:
            _realtime_rag_processor = RealtimeRAGProcessor()
            logger.info("âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            logger.error(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    return _realtime_rag_processor

async def process_question_realtime(question: str, company_id: str = None, company_name: str = "ãŠå®¢æ§˜ã®ä¼šç¤¾", top_k: int = 100) -> Dict:
    """
    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ã®å¤–éƒ¨å‘¼ã³å‡ºã—ç”¨é–¢æ•°
    
    Args:
        question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
        company_id: ä¼šç¤¾IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        company_name: ä¼šç¤¾åï¼ˆå›ç­”ç”Ÿæˆç”¨ï¼‰
        top_k: å–å¾—ã™ã‚‹é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æ•°
    
    Returns:
        Dict: å‡¦ç†çµæœï¼ˆå›ç­”ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç­‰ï¼‰
    """
    processor = get_realtime_rag_processor()
    if not processor:
        return {
            "answer": "ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚",
            "error": "RealtimeRAGProcessor initialization failed",
            "timestamp": datetime.now().isoformat(),
            "status": "error"
        }
    
    return await processor.process_realtime_rag(question, company_id, company_name, top_k)

def realtime_rag_available() -> bool:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        return bool(api_key and supabase_url and supabase_key)
    except Exception:
        return False
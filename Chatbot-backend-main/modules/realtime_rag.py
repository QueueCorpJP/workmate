"""
ğŸš€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ãƒ•ãƒ­ãƒ¼
è³ªå•å—ä»˜ã€œRAGå‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å›ç­”ï¼‰ã®å®Ÿè£…

ã‚¹ãƒ†ãƒƒãƒ—:
âœï¸ Step 1. è³ªå•å…¥åŠ› - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã«è³ªå•ã‚’å…¥åŠ›
ğŸ§  Step 2. embedding ç”Ÿæˆ - Vertex AI text-multilingual-embedding-002 ã‚’ä½¿ã£ã¦ã€è³ªå•æ–‡ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ï¼ˆ768æ¬¡å…ƒï¼‰
ğŸ” Step 3. é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢ï¼ˆTop-Kï¼‰ - Supabaseã® chunks ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã€ãƒ™ã‚¯ãƒˆãƒ«è·é›¢ãŒè¿‘ã„ãƒãƒ£ãƒ³ã‚¯ã‚’ pgvector ã‚’ç”¨ã„ã¦å–å¾—
ğŸ’¡ Step 4. LLMã¸é€ä¿¡ - Top-K ãƒãƒ£ãƒ³ã‚¯ã¨å…ƒã®è³ªå•ã‚’ Gemini Flash 2.5 ã«æ¸¡ã—ã¦ã€è¦ç´„ã›ãšã«ã€ŒåŸæ–‡ãƒ™ãƒ¼ã‚¹ã€ã§å›ç­”ã‚’ç”Ÿæˆ
âš¡ï¸ Step 5. å›ç­”è¡¨ç¤º
"""

import os
import logging
import asyncio
from typing import List, Dict, Optional, Tuple
from dotenv import load_dotenv
import google.generativeai as genai
import psycopg2
from psycopg2.extras import RealDictCursor
import json
from datetime import datetime
import urllib.parse  # è¿½åŠ 

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

logger = logging.getLogger(__name__)

class RealtimeRAGProcessor:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆGeminiè³ªå•åˆ†æçµ±åˆç‰ˆï¼‰"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.use_vertex_ai = os.getenv("USE_VERTEX_AI", "true").lower() == "true"
        self.embedding_model = os.getenv("EMBEDDING_MODEL", "text-multilingual-embedding-002")  # Vertex AI text-multilingual-embedding-002ã‚’ä½¿ç”¨ï¼ˆ768æ¬¡å…ƒï¼‰
        self.expected_dimensions = 768 if "text-multilingual-embedding-002" in self.embedding_model else 3072
        
        # API ã‚­ãƒ¼ã®è¨­å®š
        self.api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        
        self.chat_model = "gemini-2.5-flash"  # æœ€æ–°ã®Gemini Flash 2.5
        self.db_url = self._get_db_url()
        
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY ã¾ãŸã¯ GEMINI_API_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ï¼ˆãƒãƒ£ãƒƒãƒˆç”¨ï¼‰
        genai.configure(api_key=self.api_key)
        self.chat_client = genai.GenerativeModel(self.chat_model)
        
        # Vertex AI Embeddingã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ï¼ˆåŸ‹ã‚è¾¼ã¿ç”¨ï¼‰
        if self.use_vertex_ai:
            from .vertex_ai_embedding import get_vertex_ai_embedding_client, vertex_ai_embedding_available
            if vertex_ai_embedding_available():
                self.vertex_client = get_vertex_ai_embedding_client()
                logger.info(f"âœ… Vertex AI EmbeddingåˆæœŸåŒ–: {self.embedding_model} ({self.expected_dimensions}æ¬¡å…ƒ)")
            else:
                logger.error("âŒ Vertex AI EmbeddingãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                raise ValueError("Vertex AI Embeddingã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        else:
            self.vertex_client = None
        
        # ğŸ§  Geminiè³ªå•åˆ†æã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        self.gemini_analyzer = None
        try:
            from .gemini_question_analyzer import get_gemini_question_analyzer
            self.gemini_analyzer = get_gemini_question_analyzer()
            if self.gemini_analyzer:
                logger.info("âœ… Geminiè³ªå•åˆ†æã‚·ã‚¹ãƒ†ãƒ çµ±åˆå®Œäº†")
            else:
                logger.warning("âš ï¸ Geminiè³ªå•åˆ†æã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ï¼ˆå¾“æ¥æ–¹å¼ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
        except ImportError as e:
            logger.warning(f"âš ï¸ Geminiè³ªå•åˆ†æã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
        
        logger.info(f"âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–å®Œäº†: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°={self.embedding_model} ({self.expected_dimensions}æ¬¡å…ƒ)")
    
    def _get_db_url(self) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã‚’æ§‹ç¯‰"""
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # Supabase URLã‹ã‚‰æ¥ç¶šæƒ…å ±ã‚’æŠ½å‡º
        if "supabase.co" in supabase_url:
            project_id = supabase_url.split("://")[1].split(".")[0]
            return f"postgresql://postgres.{project_id}:{os.getenv('DB_PASSWORD', '')}@aws-0-ap-northeast-1.pooler.supabase.com:6543/postgres"
        else:
            # ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã®å ´åˆ
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                raise ValueError("DATABASE_URL ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return db_url
    
    async def step1_receive_question(self, question: str, company_id: str = None) -> Dict:
        """
        âœï¸ Step 1. è³ªå•å…¥åŠ›
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã«è³ªå•ã‚’å…¥åŠ›
        """
        logger.info(f"âœï¸ Step 1: è³ªå•å—ä»˜ - '{question[:50]}...'")
        
        if not question or not question.strip():
            raise ValueError("è³ªå•ãŒç©ºã§ã™")
        
        # è³ªå•ã®å‰å‡¦ç†
        processed_question = question.strip()
        
        return {
            "original_question": question,
            "processed_question": processed_question,
            "company_id": company_id,
            "timestamp": datetime.now().isoformat(),
            "step": 1
        }
    
    async def step2_generate_embedding(self, question: str) -> List[float]:
        """
        ğŸ§  Step 2. embedding ç”Ÿæˆ
        Vertex AI text-multilingual-embedding-002 ã‚’ä½¿ã£ã¦ã€è³ªå•æ–‡ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ï¼ˆ768æ¬¡å…ƒï¼‰
        """
        logger.info(f"ğŸ§  Step 2: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆä¸­...")
        
        try:
            if self.use_vertex_ai and self.vertex_client:
                # Vertex AIä½¿ç”¨
                embedding_vector = self.vertex_client.generate_embedding(question)
                
                if embedding_vector and len(embedding_vector) > 0:
                    # æ¬¡å…ƒæ•°ãƒã‚§ãƒƒã‚¯
                    if len(embedding_vector) != self.expected_dimensions:
                        logger.warning(f"äºˆæœŸã—ãªã„æ¬¡å…ƒæ•°: {len(embedding_vector)}æ¬¡å…ƒï¼ˆæœŸå¾…å€¤: {self.expected_dimensions}æ¬¡å…ƒï¼‰")
                    
                    logger.info(f"âœ… Step 2å®Œäº†: {len(embedding_vector)}æ¬¡å…ƒã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”ŸæˆæˆåŠŸ")
                    return embedding_vector
                else:
                    raise ValueError("Vertex AI ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Gemini APIä½¿ç”¨ï¼ˆéæ¨å¥¨ï¼‰
                logger.warning("âš ï¸ Vertex AIãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€Gemini APIã‚’ä½¿ç”¨")
                response = genai.embed_content(
                    model="models/text-embedding-004",  # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´
                    content=question
                )
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
                embedding_vector = None
                if isinstance(response, dict) and 'embedding' in response:
                    embedding_vector = response['embedding']
                elif hasattr(response, 'embedding') and response.embedding:
                    embedding_vector = response.embedding
                else:
                    logger.error(f"äºˆæœŸã—ãªã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼: {type(response)}")
                    raise ValueError("ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                
                if not embedding_vector:
                    raise ValueError("ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«ãŒç©ºã§ã™")
                
                logger.info(f"âœ… Step 2å®Œäº†: {len(embedding_vector)}æ¬¡å…ƒã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”ŸæˆæˆåŠŸï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
                return embedding_vector
            
        except Exception as e:
            logger.error(f"âŒ Step 2ã‚¨ãƒ©ãƒ¼: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆå¤±æ•— - {e}")
            raise
    
    async def step3_similarity_search(self, query_embedding: List[float], company_id: str = None, top_k: int = 20) -> List[Dict]:
        """
        ğŸ” Step 3. é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢ï¼ˆTop-Kï¼‰
        Supabaseã® chunks ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã€ãƒ™ã‚¯ãƒˆãƒ«è·é›¢ãŒè¿‘ã„ãƒãƒ£ãƒ³ã‚¯ã‚’ pgvector ã‚’ç”¨ã„ã¦å–å¾—
        PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å«ã‚€ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’å¹³ç­‰ã«æ¤œç´¢å¯¾è±¡ã¨ã™ã‚‹
        """
        logger.info(f"ğŸ” Step 3: é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢é–‹å§‹ (Top-{top_k})")
        
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    # ğŸ” ã¾ãšã€åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒåˆ©ç”¨å¯èƒ½ãªãƒãƒ£ãƒ³ã‚¯ã§ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢
                    vector_str = '[' + ','.join(map(str, query_embedding)) + ']'
                    
                    sql_vector = """
                    SELECT
                        c.id,
                        c.doc_id,
                        c.chunk_index,
                        c.content,
                        ds.name as document_name,
                        ds.type as document_type,
                        1 - (c.embedding <=> %s) as similarity_score,
                        'vector' as search_method
                    FROM chunks c
                    LEFT JOIN document_sources ds ON ds.id = c.doc_id
                    WHERE c.embedding IS NOT NULL
                      AND c.content IS NOT NULL
                      AND LENGTH(c.content) > 10
                    """
                    
                    params_vector = [vector_str]
                    
                    # ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                    if company_id:
                        sql_vector += " AND c.company_id = %s"
                        params_vector.append(company_id)
                    
                    # ãƒ™ã‚¯ãƒˆãƒ«è·é›¢é †ã§ã‚½ãƒ¼ãƒˆ
                    sql_vector += " ORDER BY c.embedding <=> %s LIMIT %s"
                    params_vector.extend([vector_str, top_k])
                    
                    logger.info(f"å®Ÿè¡ŒSQL: ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢ (Top-{top_k})")
                    cur.execute(sql_vector, params_vector)
                    vector_results = cur.fetchall()
                    
                    # ğŸ” PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœã‚’ç¢ºèª
                    pdf_vector_count = len([r for r in vector_results if r['document_type'] == 'pdf'])
                    excel_vector_count = len([r for r in vector_results if r['document_type'] == 'excel'])
                    
                    logger.info(f"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœ: PDF={pdf_vector_count}ä»¶, Excel={excel_vector_count}ä»¶, ç·è¨ˆ={len(vector_results)}ä»¶")
                    
                    # çµæœã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                    similar_chunks = []
                    for row in vector_results:
                        similar_chunks.append({
                            'chunk_id': row['id'],
                            'doc_id': row['doc_id'],
                            'chunk_index': row['chunk_index'],
                            'content': row['content'],
                            'document_name': row['document_name'],
                            'document_type': row['document_type'],
                            'similarity_score': float(row['similarity_score']),
                            'search_method': row['search_method']
                        })
                    
                    # ğŸ” PDFãƒ•ã‚¡ã‚¤ãƒ«ã®çµæœãŒå°‘ãªã„å ´åˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’å®Ÿè¡Œ
                    if pdf_vector_count < 3:  # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®çµæœãŒ3ä»¶æœªæº€ã®å ´åˆ
                        logger.info("ğŸ“„ PDFãƒ•ã‚¡ã‚¤ãƒ«ã®çµæœãŒå°‘ãªã„ãŸã‚ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’å®Ÿè¡Œ")
                        
                        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒãªã„ãƒãƒ£ãƒ³ã‚¯ã«å¯¾ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã‚’å®Ÿè¡Œ
                        sql_text = """
                        SELECT
                            c.id,
                            c.doc_id,
                            c.chunk_index,
                            c.content,
                            ds.name as document_name,
                            ds.type as document_type,
                            0.5 as similarity_score,
                            'text_fallback' as search_method
                        FROM chunks c
                        LEFT JOIN document_sources ds ON ds.id = c.doc_id
                        WHERE c.embedding IS NULL
                          AND c.content IS NOT NULL
                          AND LENGTH(c.content) > 10
                          AND ds.type = 'pdf'
                        """
                        
                        params_text = []
                        
                        # ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                        if company_id:
                            sql_text += " AND c.company_id = %s"
                            params_text.append(company_id)
                        
                        # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒãªã„å ´åˆï¼‰
                        sql_text += " ORDER BY RANDOM() LIMIT %s"
                        params_text.append(min(10, top_k))
                        
                        logger.info("å®Ÿè¡ŒSQL: PDFãƒ•ã‚¡ã‚¤ãƒ«å‘ã‘ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢")
                        cur.execute(sql_text, params_text)
                        text_fallback_results = cur.fetchall()
                        
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœã‚’è¿½åŠ 
                        for row in text_fallback_results:
                            similar_chunks.append({
                                'chunk_id': row['id'],
                                'doc_id': row['doc_id'],
                                'chunk_index': row['chunk_index'],
                                'content': row['content'],
                                'document_name': row['document_name'],
                                'document_type': row['document_type'],
                                'similarity_score': float(row['similarity_score']),
                                'search_method': row['search_method']
                            })
                        
                        logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã§{len(text_fallback_results)}ä»¶ã®PDFãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ ")
                    
                    # ğŸ” ã•ã‚‰ã«ã€ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã®è¿½åŠ æ¤œç´¢
                    file_type_distribution = {}
                    for chunk in similar_chunks:
                        doc_type = chunk['document_type'] or 'unknown'
                        file_type_distribution[doc_type] = file_type_distribution.get(doc_type, 0) + 1
                    
                    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®çµæœãŒä¾ç„¶ã¨ã—ã¦å°‘ãªã„å ´åˆ
                    if file_type_distribution.get('pdf', 0) < 2:
                        logger.info("ğŸ“„ PDFãƒ•ã‚¡ã‚¤ãƒ«çµæœãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€è¿½åŠ æ¤œç´¢ã‚’å®Ÿè¡Œ")
                        
                        # ä¼šç¤¾å…¨ä½“ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä»£è¡¨çš„ãªãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
                        sql_pdf_supplement = """
                        SELECT
                            c.id,
                            c.doc_id,
                            c.chunk_index,
                            c.content,
                            ds.name as document_name,
                            ds.type as document_type,
                            0.4 as similarity_score,
                            'pdf_supplement' as search_method
                        FROM chunks c
                        LEFT JOIN document_sources ds ON ds.id = c.doc_id
                        WHERE c.content IS NOT NULL
                          AND LENGTH(c.content) > 10
                          AND ds.type = 'pdf'
                          AND ds.active = true
                        """
                        
                        params_pdf_supplement = []
                        
                        # ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                        if company_id:
                            sql_pdf_supplement += " AND c.company_id = %s"
                            params_pdf_supplement.append(company_id)
                        
                        # æœ€æ–°ã®ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰å„ªå…ˆçš„ã«å–å¾—
                        sql_pdf_supplement += " ORDER BY c.id DESC LIMIT %s"
                        params_pdf_supplement.append(5)
                        
                        logger.info("å®Ÿè¡ŒSQL: PDFè£œå®Œæ¤œç´¢")
                        cur.execute(sql_pdf_supplement, params_pdf_supplement)
                        pdf_supplement_results = cur.fetchall()
                        
                        # PDFè£œå®Œçµæœã‚’è¿½åŠ 
                        for row in pdf_supplement_results:
                            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                            if not any(chunk['chunk_id'] == row['id'] for chunk in similar_chunks):
                                similar_chunks.append({
                                    'chunk_id': row['id'],
                                    'doc_id': row['doc_id'],
                                    'chunk_index': row['chunk_index'],
                                    'content': row['content'],
                                    'document_name': row['document_name'],
                                    'document_type': row['document_type'],
                                    'similarity_score': float(row['similarity_score']),
                                    'search_method': row['search_method']
                                })
                        
                        logger.info(f"PDFè£œå®Œæ¤œç´¢ã§{len(pdf_supplement_results)}ä»¶ã®ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ ")
                    
                    # çµæœã‚’é¡ä¼¼åº¦é †ã§ã‚½ãƒ¼ãƒˆ
                    similar_chunks.sort(key=lambda x: x['similarity_score'], reverse=True)
                    
                    # æœ€å¤§ãƒãƒ£ãƒ³ã‚¯æ•°ã«åˆ¶é™
                    final_chunks = similar_chunks[:top_k]
                    
                    logger.info(f"âœ… Step 3å®Œäº†: {len(final_chunks)}å€‹ã®é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—")
                    
                    # ğŸ” è©³ç´°ãƒãƒ£ãƒ³ã‚¯é¸æŠãƒ­ã‚°ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGï¼‰
                    print("\n" + "="*80)
                    print(f"ğŸ” ã€Step 3: é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢çµæœã€‘")
                    print(f"ğŸ“Š å–å¾—ãƒãƒ£ãƒ³ã‚¯æ•°: {len(final_chunks)}ä»¶ (Top-{top_k})")
                    print(f"ğŸ¢ ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿: {'é©ç”¨ (' + company_id + ')' if company_id else 'æœªé©ç”¨ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ï¼‰'}")
                    print(f"ğŸ§  ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«: {self.embedding_model} ({self.expected_dimensions}æ¬¡å…ƒ)")
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆã‚’è¡¨ç¤º
                    final_file_type_distribution = {}
                    for chunk in final_chunks:
                        doc_type = chunk['document_type'] or 'unknown'
                        final_file_type_distribution[doc_type] = final_file_type_distribution.get(doc_type, 0) + 1
                    
                    print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥çµæœ: {final_file_type_distribution}")
                    print("="*80)
                    
                    for i, chunk in enumerate(final_chunks):
                        similarity = chunk['similarity_score']
                        doc_name = chunk['document_name'] or 'Unknown'
                        chunk_idx = chunk['chunk_index']
                        content_preview = (chunk['content'] or '')[:150].replace('\n', ' ')
                        search_method = chunk['search_method']
                        
                        # é¡ä¼¼åº¦ã«åŸºã¥ãè©•ä¾¡
                        if similarity > 0.8:
                            evaluation = "ğŸŸ¢ éå¸¸ã«é«˜ã„é–¢é€£æ€§"
                        elif similarity > 0.6:
                            evaluation = "ğŸŸ¡ é«˜ã„é–¢é€£æ€§"
                        elif similarity > 0.4:
                            evaluation = "ğŸŸ  ä¸­ç¨‹åº¦ã®é–¢é€£æ€§"
                        elif similarity > 0.2:
                            evaluation = "ğŸ”´ ä½ã„é–¢é€£æ€§"
                        else:
                            evaluation = "âš« æ¥µã‚ã¦ä½ã„é–¢é€£æ€§"
                        
                        print(f"  {i+1:2d}. ğŸ“„ {doc_name} ({chunk['document_type']})")
                        print(f"      ğŸ§© ãƒãƒ£ãƒ³ã‚¯#{chunk_idx} | ğŸ¯ é¡ä¼¼åº¦: {similarity:.4f} | {evaluation}")
                        print(f"      ğŸ” æ¤œç´¢æ–¹æ³•: {search_method}")
                        print(f"      ğŸ“ å†…å®¹: {content_preview}...")
                        print(f"      ğŸ”— ãƒãƒ£ãƒ³ã‚¯ID: {chunk['chunk_id']} | ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID: {chunk['doc_id']}")
                        print()
                    
                    print("="*80 + "\n")
                    
                    return final_chunks
        
        except Exception as e:
            logger.error(f"âŒ Step 3ã‚¨ãƒ©ãƒ¼: é¡ä¼¼æ¤œç´¢å¤±æ•— - {e}")
            raise
    
    async def step4_generate_answer(self, question: str, similar_chunks: List[Dict], company_name: str = "ãŠå®¢æ§˜ã®ä¼šç¤¾", company_id: str = None) -> str:
        """
        ğŸ’¡ Step 4. LLMã¸é€ä¿¡
        Top-K ãƒãƒ£ãƒ³ã‚¯ã¨å…ƒã®è³ªå•ã‚’ Gemini Flash 2.5 ã«æ¸¡ã—ã¦ã€è¦ç´„ã›ãšã«ã€ŒåŸæ–‡ãƒ™ãƒ¼ã‚¹ã€ã§å›ç­”ã‚’ç”Ÿæˆ
        """
        logger.info(f"ğŸ’¡ Step 4: LLMå›ç­”ç”Ÿæˆé–‹å§‹ ({len(similar_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ä½¿ç”¨)")
        
        if not similar_chunks:
            logger.warning("é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ä¸€èˆ¬çš„ãªå›ç­”ã‚’ç”Ÿæˆ")
            return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã”è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚ˆã‚Šå…·ä½“çš„ãªè³ªå•ã‚’ã—ã¦ã„ãŸã ã‘ã¾ã™ã§ã—ã‚‡ã†ã‹ã€‚"
        
        try:
            # ğŸ” Step 4: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ãƒ­ã‚°
            print("\n" + "="*80)
            print(f"ğŸ’¡ ã€Step 4: LLMå›ç­”ç”Ÿæˆ - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ã€‘")
            print(f"ğŸ“Š åˆ©ç”¨å¯èƒ½ãƒãƒ£ãƒ³ã‚¯æ•°: {len(similar_chunks)}å€‹")
            print(f"ğŸ“ æœ€å¤§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {80000:,}æ–‡å­—")  # åˆ¶é™ã‚’å°‘ã—ä¸‹ã’ã‚‹
            print("="*80)
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ï¼ˆåŸæ–‡ãƒ™ãƒ¼ã‚¹ï¼‰
            context_parts = []
            total_length = 0
            max_context_length = 80000  # 8ä¸‡æ–‡å­—ã«åˆ¶é™ï¼ˆå®‰å…¨ã®ãŸã‚ï¼‰
            used_chunks = []
            
            for i, chunk in enumerate(similar_chunks):
                chunk_content = f"ã€å‚è€ƒè³‡æ–™{i+1}: {chunk['document_name']} - ãƒãƒ£ãƒ³ã‚¯{chunk['chunk_index']}ã€‘\n{chunk['content']}\n"
                chunk_length = len(chunk_content)
                
                print(f"  {i+1:2d}. ğŸ“„ {chunk['document_name']} [ãƒãƒ£ãƒ³ã‚¯#{chunk['chunk_index']}]")
                print(f"      ğŸ¯ é¡ä¼¼åº¦: {chunk['similarity_score']:.4f}")
                print(f"      ğŸ“ æ–‡å­—æ•°: {chunk_length:,}æ–‡å­—")
                
                if total_length + chunk_length > max_context_length:
                    print(f"      âŒ é™¤å¤–: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·åˆ¶é™è¶…é (ç¾åœ¨: {total_length:,}æ–‡å­—)")
                    print(f"         ğŸ’¡ {i}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’æœ€çµ‚çš„ã«ä½¿ç”¨")
                    break
                
                context_parts.append(chunk_content)
                total_length += chunk_length
                used_chunks.append(chunk)
                print(f"      âœ… æ¡ç”¨: ç´¯è¨ˆ {total_length:,}æ–‡å­—")
                print(f"      ğŸ“ å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {(chunk['content'] or '')[:100].replace(chr(10), ' ')}...")
                print()
            
            context = "\n".join(context_parts)
            
            print(f"ğŸ“‹ æœ€çµ‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±:")
            print(f"   âœ… ä½¿ç”¨ãƒãƒ£ãƒ³ã‚¯æ•°: {len(used_chunks)}å€‹")
            print(f"   ï¿½ï¿½ ç·æ–‡å­—æ•°: {len(context):,}æ–‡å­—")
            print("="*80 + "\n")
            
            # ğŸ¯ ç‰¹åˆ¥æŒ‡ç¤ºã‚’å–å¾—ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸€ç•ªå‰ã«é…ç½®
            special_instructions_text = ""
            if company_id:
                try:
                    with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                        with conn.cursor() as cur:
                            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ã®ç‰¹åˆ¥æŒ‡ç¤ºã‚’å–å¾—
                            sql = """
                            SELECT DISTINCT ds.name, ds.special
                            FROM document_sources ds 
                            WHERE ds.company_id = %s 
                            AND ds.active = true 
                            AND ds.special IS NOT NULL 
                            AND ds.special != ''
                            ORDER BY ds.name
                            """
                            cur.execute(sql, [company_id])
                            special_results = cur.fetchall()
                            
                            if special_results:
                                special_instructions = []
                                print(f"ğŸ¯ ç‰¹åˆ¥æŒ‡ç¤ºã‚’å–å¾—ã—ã¾ã—ãŸ: {len(special_results)}ä»¶")
                                for i, row in enumerate(special_results, 1):
                                    resource_name = row['name']
                                    special_instruction = row['special']
                                    special_instructions.append(f"{i}. ã€{resource_name}ã€‘: {special_instruction}")
                                    print(f"   {i}. {resource_name}: {special_instruction}")
                                
                                special_instructions_text = "ç‰¹åˆ¥ãªå›ç­”æŒ‡ç¤ºï¼ˆä»¥ä¸‹ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’å‚ç…§ã™ã‚‹éš›ã¯ã€å„ãƒªã‚½ãƒ¼ã‚¹ã®æŒ‡ç¤ºã«å¾“ã£ã¦ãã ã•ã„ï¼‰ï¼š\n" + "\n".join(special_instructions) + "\n\n"
                                print(f"âœ… ç‰¹åˆ¥æŒ‡ç¤ºã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ å®Œäº†")
                            else:
                                print(f"â„¹ï¸ ç‰¹åˆ¥æŒ‡ç¤ºãŒè¨­å®šã•ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                                
                except Exception as e:
                    print(f"âš ï¸ ç‰¹åˆ¥æŒ‡ç¤ºå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                    logger.warning(f"ç‰¹åˆ¥æŒ‡ç¤ºå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            
            # æ”¹å–„ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ï¼ˆç‰¹åˆ¥æŒ‡ç¤ºã‚’ä¸€ç•ªå‰ã«é…ç½®ï¼‰- ã‚ˆã‚ŠæŸ”è»Ÿãªæ¤œç´¢å¯¾å¿œ
            prompt = f"""{special_instructions_text}ã‚ãªãŸã¯{company_name}ã®ç¤¾å†…å‘ã‘ä¸å¯§ã§è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ã€å›ç­”ã®éš›ã®é‡è¦ãªæŒ‡é‡ã€‘
â€¢ å›ç­”ã¯ä¸å¯§ãªæ•¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚
â€¢ **æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒé–¢é€£ã™ã‚‹ã¨åˆ¤æ–­ã—ãŸå‚è€ƒè³‡æ–™ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚ä»¥ä¸‹ã®åŸºæº–ã§ç©æ¥µçš„ã«æ´»ç”¨ã—ã¦ãã ã•ã„ï¼š**

**ã€æŸ”è»Ÿãªæƒ…å ±æä¾›åŸºæº–ã€‘**
â€¢ **ä¼šç¤¾åã«ã¤ã„ã¦ï¼šã€Œæ ªå¼ä¼šç¤¾ã‚ã„ã†ã€ã‚’æ¢ã—ã¦ã„ã‚‹å ´åˆã€ã€Œæ ªå¼ä¼šç¤¾ ã„ã†ã€ã€Œæ ªå¼ä¼šç¤¾ã€€ã‚ã„ã€ã€Œãˆ±ã‚ã„ã€ã€Œ(æ ª)ã‚ã„ã€ãªã©ã‚‚åŒã˜ä¼šç¤¾ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„**
â€¢ **éƒ¨åˆ†ä¸€è‡´ã§ã‚‚æœ‰åŠ¹ï¼šè³ªå•ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä¸€éƒ¨ã§ã‚‚å‚è€ƒè³‡æ–™ã«å«ã¾ã‚Œã¦ã„ã‚Œã°ã€é–¢é€£æƒ…å ±ã¨ã—ã¦æä¾›ã—ã¦ãã ã•ã„**
â€¢ **é¡ä¼¼æƒ…å ±ã®æä¾›ï¼šå®Œå…¨ä¸€è‡´ã§ãªãã¦ã‚‚ã€ä¼¼ãŸã‚ˆã†ãªä¼šç¤¾åã€é–¢é€£ã™ã‚‹æ¥­ç•Œæƒ…å ±ã€é¡ä¼¼ã®ã‚µãƒ¼ãƒ“ã‚¹ãªã©ãŒã‚ã‚Œã°ç©æ¥µçš„ã«ç´¹ä»‹ã—ã¦ãã ã•ã„**
â€¢ **æ¨æ¸¬ã¨èª¬æ˜ï¼šå‚è€ƒè³‡æ–™ã‹ã‚‰æ¨æ¸¬ã§ãã‚‹ã“ã¨ã‚„ã€é–¢é€£ã™ã‚‹å†…å®¹ãŒã‚ã‚Œã°ã€Œå‚è€ƒè³‡æ–™ã«ã¯â—‹â—‹ã¨ã„ã†æƒ…å ±ãŒã”ã–ã„ã¾ã™ã€ã¨ã—ã¦æä¾›ã—ã¦ãã ã•ã„**
â€¢ **æ–­ç‰‡æƒ…å ±ã‚‚æ´»ç”¨ï¼šè¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã‚„ä¸€éƒ¨ã®æƒ…å ±ã§ã‚ã£ã¦ã‚‚ã€è³ªå•ã«é–¢é€£ã™ã‚‹éƒ¨åˆ†ãŒã‚ã‚Œã°æ„å‘³ã®ã‚ã‚‹æƒ…å ±ã¨ã—ã¦è§£é‡ˆã—ã¦ãã ã•ã„**

**ã€å›ç­”ãƒ‘ã‚¿ãƒ¼ãƒ³ä¾‹ã€‘**
â€¢ ã€Œæ ªå¼ä¼šç¤¾ã§ã‚ã„ã†ã€ã«ã¤ã„ã¦è³ªå•ã•ã‚ŒãŸå ´åˆï¼š
  - å®Œå…¨ä¸€è‡´ãŒè¦‹ã¤ã‹ã‚‰ãªãã¦ã‚‚ã€ã€Œæ ªå¼ä¼šç¤¾ã€ã€Œã‚ã„ã€ã‚’å«ã‚€ä¼šç¤¾ãŒã‚ã‚Œã°ç´¹ä»‹ã™ã‚‹
  - é¡ä¼¼ã®åå‰ã®ä¼šç¤¾ãŒã‚ã‚Œã°ã€Œé¡ä¼¼ã®ä¼šç¤¾åã¨ã—ã¦â—‹â—‹ãŒã”ã–ã„ã¾ã™ã€ã¨æ¡ˆå†…
  - é–¢é€£ã™ã‚‹æ¥­ç•Œã®ä¼šç¤¾ãŒã‚ã‚Œã°å‚è€ƒæƒ…å ±ã¨ã—ã¦æä¾›

**ã€é¿ã‘ã‚‹ã¹ãå›ç­”ã€‘**
â€¢ âŒã€Œæƒ…å ±ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ã€ã€Œç¢ºèªã§ãã¾ã›ã‚“ã§ã—ãŸã€ï¼ˆå‚è€ƒè³‡æ–™ãŒã‚ã‚‹å ´åˆï¼‰
â€¢ âŒ å®Œå…¨ä¸€è‡´ã®ã¿ã‚’æ±‚ã‚ã‚‹å³æ ¼ãªåˆ¤æ–­
â€¢ âœ… ä»£ã‚ã‚Šã«ï¼šã€Œå‚è€ƒè³‡æ–™ã‚’ç¢ºèªã—ãŸã¨ã“ã‚ã€â—‹â—‹ã¨ã„ã†æƒ…å ±ãŒã”ã–ã„ã¾ã™ã€

**ã€ãã®ä»–ã®æŒ‡é‡ã€‘**
â€¢ æƒ…å ±ã®å‡ºå…¸ã¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«åã¯æ˜ç¤ºå¯èƒ½ã§ã™ãŒã€å†…éƒ¨æ§‹é€ æƒ…å ±ï¼ˆè¡Œç•ªå·ç­‰ï¼‰ã¯å‡ºåŠ›ã—ãªã„
â€¢ å°‚é–€çš„ãªå†…å®¹ã‚‚åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜
â€¢ æ–‡æœ«ã«ã¯ã€Œã”ä¸æ˜ãªç‚¹ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã€ãŠæ°—è»½ã«ãŠç”³ã—ä»˜ã‘ãã ã•ã„ã€‚ã€ã‚’è¿½åŠ 

ã”è³ªå•ï¼š
{question}

å‚è€ƒã¨ãªã‚‹è³‡æ–™ï¼š
{context}

ä¸Šè¨˜ã®å‚è€ƒè³‡æ–™ã‚’åŸºã«ã€æŸ”è»Ÿã‹ã¤ç©æ¥µçš„ã«ã”è³ªå•ã«ãŠç­”ãˆã„ãŸã—ã¾ã™ï¼š"""

            logger.info("ğŸ¤– Gemini Flash 2.5ã«å›ç­”ç”Ÿæˆã‚’ä¾é ¼ä¸­...")
            logger.info(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt):,}æ–‡å­—")
            
            # Gemini Flash 2.5ã§å›ç­”ç”Ÿæˆï¼ˆè¨­å®šã‚’èª¿æ•´ï¼‰
            response = self.chat_client.generate_content(
                prompt,
                generation_config=genai.GenerationConfig(
                    temperature=0.2,  # å°‘ã—å‰µé€ æ€§ã‚’ä¸Šã’ã‚‹
                    max_output_tokens=4096,  # å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å¢—åŠ 
                    top_p=0.9,  # å¤šæ§˜æ€§ã‚’å°‘ã—ä¸Šã’ã‚‹
                    top_k=50    # ã‚ˆã‚Šå¤šãã®å€™è£œã‚’è€ƒæ…®
                )
            )
            
            logger.info("ğŸ“¥ Geminiã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å—ä¿¡å®Œäº†")
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç†ã®æ”¹å–„ï¼ˆè©³ç´°ãªãƒ‡ãƒãƒƒã‚°æƒ…å ±ä»˜ãï¼‰
            answer = None
            
            if response:
                logger.info(f"âœ… ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå­˜åœ¨: {type(response)}")
                
                # å€™è£œã®ç¢ºèª
                if hasattr(response, 'candidates') and response.candidates:
                    logger.info(f"ğŸ“‹ å€™è£œæ•°: {len(response.candidates)}")
                    
                    try:
                        # ã¾ãš response.text ã‚’è©¦ã™
                        answer = response.text
                        if answer:
                            answer = answer.strip()
                            logger.info("âœ… response.textã‹ã‚‰å›ç­”ã‚’å–å¾—")
                        else:
                            logger.warning("âš ï¸ response.textãŒç©º")
                    except (ValueError, AttributeError) as e:
                        logger.warning(f"âš ï¸ response.textä½¿ç”¨ä¸å¯: {e}")
                        
                        # partsã‹ã‚‰æ‰‹å‹•ã§æŠ½å‡º
                        try:
                            parts = []
                            for i, candidate in enumerate(response.candidates):
                                logger.info(f"   å€™è£œ{i+1}: {type(candidate)}")
                                
                                if hasattr(candidate, 'content') and candidate.content:
                                    if hasattr(candidate.content, 'parts'):
                                        for j, part in enumerate(candidate.content.parts):
                                            logger.info(f"     ãƒ‘ãƒ¼ãƒˆ{j+1}: {type(part)}")
                                            if hasattr(part, 'text') and part.text:
                                                parts.append(part.text)
                                                logger.info(f"     ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(part.text)}æ–‡å­—")
                            
                            if parts:
                                answer = ''.join(parts).strip()
                                logger.info("âœ… partsã‹ã‚‰å›ç­”ã‚’æŠ½å‡º")
                            else:
                                logger.error("âŒ partsã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
                        except Exception as parts_error:
                            logger.error(f"âŒ partsæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {parts_error}")
                else:
                    logger.error("âŒ å€™è£œãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            else:
                logger.error("âŒ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒç©ºã§ã™")
            
            # å›ç­”ã®æ¤œè¨¼ã¨å‡¦ç†
            if answer and len(answer.strip()) > 0:
                logger.info(f"âœ… Step 4å®Œäº†: {len(answer)}æ–‡å­—ã®å›ç­”ã‚’ç”Ÿæˆ")
                logger.info(f"ğŸ“ å›ç­”ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {answer[:100]}...")
                
                # å›ç­”ãŒçŸ­ã™ãã‚‹å ´åˆã®å‡¦ç†
                if len(answer) < 50:
                    logger.warning(f"âš ï¸ å›ç­”ãŒçŸ­ã™ãã¾ã™ï¼ˆ{len(answer)}æ–‡å­—ï¼‰- è£œå¼·å‡¦ç†ã‚’å®Ÿè¡Œ")
                    fallback_answer = f"""å‚è€ƒè³‡æ–™ã‚’ç¢ºèªã„ãŸã—ã¾ã—ãŸã€‚

{answer}

ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ãŒå¿…è¦ã§ã—ãŸã‚‰ã€å…·ä½“çš„ãªé …ç›®ã‚’ãŠæ•™ãˆãã ã•ã„ã€‚å‚è€ƒè³‡æ–™ã‹ã‚‰æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã„ãŸã—ã¾ã™ã€‚"""
                    return fallback_answer
                
                return answer
            else:
                logger.error("âŒ LLMã‹ã‚‰ã®å›ç­”ãŒç©ºã¾ãŸã¯å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                logger.error(f"   ãƒ¬ã‚¹ãƒãƒ³ã‚¹è©³ç´°: {response}")
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å›ç­”ã®ç”Ÿæˆï¼ˆå‚è€ƒè³‡æ–™ã®æƒ…å ±ã‚’ä½¿ç”¨ï¼‰
                fallback_parts = []
                fallback_parts.append("å‚è€ƒè³‡æ–™ã‚’ç¢ºèªã„ãŸã—ã¾ã—ãŸãŒã€ã‚·ã‚¹ãƒ†ãƒ çš„ãªå•é¡Œã«ã‚ˆã‚Šè©³ç´°ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                
                # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰éƒ¨åˆ†çš„ãªæƒ…å ±ã‚’æä¾›
                if used_chunks and used_chunks[0].get('content'):
                    first_chunk_content = used_chunks[0]['content'][:300]
                    fallback_parts.append(f"\nå‚è€ƒè³‡æ–™ã®ä¸€éƒ¨ã‚’ã”ç´¹ä»‹ã„ãŸã—ã¾ã™ï¼š\n{first_chunk_content}...")
                
                fallback_parts.append("\nã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ã«ã¤ã„ã¦ã¯ã€æ”¹ã‚ã¦ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚")
                
                return "\n".join(fallback_parts)
        
        except Exception as e:
            logger.error(f"âŒ Step 4ã‚¨ãƒ©ãƒ¼: LLMå›ç­”ç”Ÿæˆå¤±æ•— - {e}")
            import traceback
            logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã§ã‚‚å¯èƒ½ãªé™ã‚Šæƒ…å ±ã‚’æä¾›
            error_response_parts = ["ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"]
            
            if similar_chunks and len(similar_chunks) > 0:
                error_response_parts.append(f"\næ¤œç´¢ã§ã¯{len(similar_chunks)}ä»¶ã®é–¢é€£è³‡æ–™ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
                if similar_chunks[0].get('content'):
                    first_content = similar_chunks[0]['content'][:200]
                    error_response_parts.append(f"é–¢é€£æƒ…å ±ã®ä¸€éƒ¨: {first_content}...")
            
            error_response_parts.append("\nã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
            
            return "\n".join(error_response_parts)
    
    async def step5_display_answer(self, answer: str, metadata: Dict = None, used_chunks: List = None) -> Dict:
        """
        âš¡ï¸ Step 5. å›ç­”è¡¨ç¤º
        æœ€çµ‚çš„ãªå›ç­”ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        """
        logger.info(f"âš¡ï¸ Step 5: å›ç­”è¡¨ç¤ºæº–å‚™å®Œäº†")
        
        result = {
            "answer": answer,
            "timestamp": datetime.now().isoformat(),
            "step": 5,
            "status": "completed"
        }
        
        if metadata:
            result.update(metadata)
        
        # ä½¿ç”¨ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã®è©³ç´°æƒ…å ±ã‚’è¿½åŠ 
        if used_chunks:
            source_documents = []
            for chunk in used_chunks[:5]:  # æœ€å¤§5å€‹ã®ã‚½ãƒ¼ã‚¹æ–‡æ›¸
                doc_info = {
                    "document_name": chunk.get('document_name', 'Unknown Document'),
                    "document_type": chunk.get('document_type', 'unknown'),
                    "chunk_id": chunk.get('chunk_id', ''),
                    "similarity_score": chunk.get('similarity_score', 0.0),
                    "content_preview": (chunk.get('content', '') or '')[:100] + "..." if chunk.get('content') else ""
                }
                source_documents.append(doc_info)
            
            result["source_documents"] = source_documents
            result["total_sources"] = len(used_chunks)
        
        logger.info(f"âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†å®Œäº†: {len(answer)}æ–‡å­—ã®å›ç­”")
        return result
    
    async def process_realtime_rag(self, question: str, company_id: str = None, company_name: str = "ãŠå®¢æ§˜ã®ä¼šç¤¾", top_k: int = 20) -> Dict:
        """
        ğŸš€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ãƒ•ãƒ­ãƒ¼å…¨ä½“ã®å®Ÿè¡Œï¼ˆGeminiè³ªå•åˆ†æçµ±åˆç‰ˆï¼‰
        æ–°ã—ã„3æ®µéšã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: Geminiåˆ†æ â†’ SQLæ¤œç´¢ â†’ Embeddingæ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        """
        logger.info(f"ğŸš€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†é–‹å§‹: '{question[:50]}...'")
        
        try:
            # Step 1: è³ªå•å…¥åŠ›
            step1_result = await self.step1_receive_question(question, company_id)
            processed_question = step1_result["processed_question"]
            
            # ğŸ§  æ–°ã—ã„3æ®µéšæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
            if self.gemini_analyzer:
                logger.info("ğŸ§  Geminiè³ªå•åˆ†æã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ãŸ3æ®µéšæ¤œç´¢ã‚’å®Ÿè¡Œ")
                
                # Geminiè³ªå•åˆ†æ â†’ SQLæ¤œç´¢ â†’ Embeddingæ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                search_results, analysis_result = await self.gemini_analyzer.intelligent_search(
                    question=processed_question,
                    company_id=company_id,
                    limit=top_k
                )
                
                # SearchResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
                similar_chunks = []
                for result in search_results:
                    similar_chunks.append({
                        'chunk_id': result.chunk_id,
                        'doc_id': result.document_id,
                        'chunk_index': 0,  # SearchResultã«ã¯chunk_indexãŒãªã„ãŸã‚0ã‚’è¨­å®š
                        'content': result.content,
                        'document_name': result.document_name,
                        'document_type': 'unknown',  # SearchResultã«ã¯document_typeãŒãªã„ãŸã‚'unknown'ã‚’è¨­å®š
                        'similarity_score': result.score
                    })
                
                search_method = search_results[0].search_method if search_results else "no_results"
                
                logger.info(f"âœ… 3æ®µéšæ¤œç´¢å®Œäº†: {search_method}ã§{len(similar_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—")
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«æ¤œç´¢æ–¹æ³•ã‚’è¿½åŠ 
                metadata = {
                    "original_question": question,
                    "processed_question": processed_question,
                    "chunks_used": len(similar_chunks),
                    "top_similarity": similar_chunks[0]["similarity_score"] if similar_chunks else 0.0,
                    "company_id": company_id,
                    "company_name": company_name,
                    "search_method": search_method,
                    "gemini_analysis": {
                        "intent": analysis_result.intent.value if analysis_result else "unknown",
                        "confidence": analysis_result.confidence if analysis_result else 0.0,
                        "target_entity": analysis_result.target_entity if analysis_result else "",
                        "keywords": analysis_result.keywords if analysis_result else [],
                        "reasoning": analysis_result.reasoning if analysis_result else ""
                    },
                    "keywords": analysis_result.keywords if analysis_result else []
                }
                
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®Embeddingæ¤œç´¢ã®ã¿
                logger.warning("âš ï¸ Geminiè³ªå•åˆ†æã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€å¾“æ¥ã®Embeddingæ¤œç´¢ã‚’ä½¿ç”¨")
                
                # Step 2: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
                query_embedding = await self.step2_generate_embedding(processed_question)
                
                # Step 3: é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢
                similar_chunks = await self.step3_similarity_search(query_embedding, company_id, top_k)
                
                metadata = {
                    "original_question": question,
                    "processed_question": processed_question,
                    "chunks_used": len(similar_chunks),
                    "top_similarity": similar_chunks[0]["similarity_score"] if similar_chunks else 0.0,
                    "company_id": company_id,
                    "company_name": company_name,
                    "search_method": "embedding_fallback"
                }
            
            # Step 4: LLMå›ç­”ç”Ÿæˆ
            answer = await self.step4_generate_answer(processed_question, similar_chunks, company_name, company_id)
            
            # Step 5: å›ç­”è¡¨ç¤ºï¼ˆä½¿ç”¨ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚’å«ã‚ã‚‹ï¼‰
            result = await self.step5_display_answer(answer, metadata, similar_chunks)
            
            logger.info(f"ğŸ‰ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†æˆåŠŸå®Œäº†")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            error_result = {
                "answer": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "status": "error"
            }
            return error_result

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_realtime_rag_processor = None

def get_realtime_rag_processor() -> Optional[RealtimeRAGProcessor]:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
    global _realtime_rag_processor
    
    if _realtime_rag_processor is None:
        try:
            _realtime_rag_processor = RealtimeRAGProcessor()
            logger.info("âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            logger.error(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    return _realtime_rag_processor

async def process_question_realtime(question: str, company_id: str = None, company_name: str = "ãŠå®¢æ§˜ã®ä¼šç¤¾", top_k: int = 20) -> Dict:
    """
    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ã®å¤–éƒ¨å‘¼ã³å‡ºã—ç”¨é–¢æ•°
    
    Args:
        question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
        company_id: ä¼šç¤¾IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        company_name: ä¼šç¤¾åï¼ˆå›ç­”ç”Ÿæˆç”¨ï¼‰
        top_k: å–å¾—ã™ã‚‹é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æ•°
    
    Returns:
        Dict: å‡¦ç†çµæœï¼ˆå›ç­”ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç­‰ï¼‰
    """
    processor = get_realtime_rag_processor()
    if not processor:
        return {
            "answer": "ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚",
            "error": "RealtimeRAGProcessor initialization failed",
            "timestamp": datetime.now().isoformat(),
            "status": "error"
        }
    
    return await processor.process_realtime_rag(question, company_id, company_name, top_k)

def realtime_rag_available() -> bool:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        return bool(api_key and supabase_url and supabase_key)
    except Exception:
        return False
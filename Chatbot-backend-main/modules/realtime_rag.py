"""
ğŸš€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ãƒ•ãƒ­ãƒ¼
è³ªå•å—ä»˜ã€œRAGå‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å›ç­”ï¼‰ã®å®Ÿè£…

ã‚¹ãƒ†ãƒƒãƒ—:
âœï¸ Step 1. è³ªå•å…¥åŠ› - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã«è³ªå•ã‚’å…¥åŠ›
ğŸ§  Step 2. embedding ç”Ÿæˆ - Vertex AI text-multilingual-embedding-002 ã‚’ä½¿ã£ã¦ã€è³ªå•æ–‡ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ï¼ˆ768æ¬¡å…ƒï¼‰
ğŸ” Step 3. é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢ï¼ˆTop-Kï¼‰ - Supabaseã® chunks ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã€ãƒ™ã‚¯ãƒˆãƒ«è·é›¢ãŒè¿‘ã„ãƒãƒ£ãƒ³ã‚¯ã‚’ pgvector ã‚’ç”¨ã„ã¦å–å¾—
ğŸ’¡ Step 4. LLMã¸é€ä¿¡ - Top-K ãƒãƒ£ãƒ³ã‚¯ã¨å…ƒã®è³ªå•ã‚’ Gemini Flash 2.5 ã«æ¸¡ã—ã¦ã€è¦ç´„ã›ãšã«ã€ŒåŸæ–‡ãƒ™ãƒ¼ã‚¹ã€ã§å›ç­”ã‚’ç”Ÿæˆ
âš¡ï¸ Step 5. å›ç­”è¡¨ç¤º
"""

import asyncio
import time
import logging
from typing import List, Dict, Optional, Tuple, Any
from dotenv import load_dotenv
import os
from datetime import datetime
import requests
import psycopg2
from psycopg2.extras import RealDictCursor
from supabase import create_client, Client
from modules.database import SupabaseConnection
from modules.token_counter import TokenCounter
from modules.models import ChatResponse, ChatMessage
import urllib.parse  # è¿½åŠ 
import re # è¿½åŠ 
from modules.config import setup_gemini
from modules.multi_gemini_client import get_multi_gemini_client, multi_gemini_available
import json

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# Geminiãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆå¾“æ¥ç‰ˆï¼‰
try:
    model = setup_gemini()
except Exception as e:
    logging.error(f"Geminiãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
    model = None

# Multi Gemini Clientã®åˆæœŸåŒ–ï¼ˆé…å»¶åˆæœŸåŒ–ï¼‰
multi_gemini_client = None

def get_or_init_multi_gemini_client():
    """Multi Gemini Clientã®å–å¾—ã¾ãŸã¯åˆæœŸåŒ–"""
    global multi_gemini_client
    if multi_gemini_client is None:
        try:
            multi_gemini_client = get_multi_gemini_client()
            logger.info("âœ… Multi Gemini ClientåˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            logger.error(f"Multi Gemini ClientåˆæœŸåŒ–ã«å¤±æ•—: {e}")
            multi_gemini_client = False  # åˆæœŸåŒ–å¤±æ•—ã‚’ãƒãƒ¼ã‚¯
    return multi_gemini_client if multi_gemini_client is not False else None

try:
    from modules.question_splitter import question_splitter
except ImportError:
    logging.warning("question_splitterã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚è³ªå•åˆ†å‰²æ©Ÿèƒ½ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¾ã™ã€‚")
    question_splitter = None

logger = logging.getLogger(__name__)

# safe_printé–¢æ•°ã®å®šç¾©ï¼ˆWindowsç’°å¢ƒã§ã®Unicodeå¯¾å¿œï¼‰
def safe_print(text):
    """Windowsç’°å¢ƒã§ã®Unicodeæ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œã‚’å›é¿ã™ã‚‹å®‰å…¨ãªprinté–¢æ•°"""
    try:
        print(text)
    except UnicodeEncodeError:
        print(text.encode('utf-8', errors='replace').decode('utf-8'))
    except Exception as e:
        print(f"Print error: {e}")

class RealtimeRAGProcessor:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆGeminiè³ªå•åˆ†æçµ±åˆç‰ˆï¼‰"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.use_vertex_ai = False  # Vertex AIã‚’ç„¡åŠ¹åŒ–
        self.embedding_model = "gemini-embedding-001"  # Geminiã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        self.expected_dimensions = 3072  # gemini-embedding-001ã¯3072æ¬¡å…ƒ
        
        # API ã‚­ãƒ¼ã®è¨­å®š
        self.api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        
        self.chat_model = "gemini-2.5-flash"  # æœ€æ–°ã®Gemini Flash 2.5
        self.db_url = self._get_db_url()
        
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY ã¾ãŸã¯ GEMINI_API_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # Gemini API ã®ç›´æ¥å‘¼ã³å‡ºã—ç”¨URL
        self.api_base_url = "https://generativelanguage.googleapis.com/v1beta"
        
        # Gemini Embeddingã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ï¼ˆåŸ‹ã‚è¾¼ã¿ç”¨ï¼‰
        try:
            from .multi_api_embedding import get_multi_api_embedding_client, multi_api_embedding_available
            if multi_api_embedding_available():
                self.embedding_client = get_multi_api_embedding_client()
                logger.info(f"âœ… Embedding ClientåˆæœŸåŒ–: {self.embedding_model} ({self.expected_dimensions}æ¬¡å…ƒ)")
            else:
                logger.error("âŒ Embedding ClientãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                raise ValueError("Embedding Clientã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        except ImportError:
            logger.error("âŒ multi_api_embedding ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            raise ValueError("Embedding Clientã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        # ğŸ§  Geminiè³ªå•åˆ†æã‚·ã‚¹ãƒ†ãƒ ã‚’ç„¡åŠ¹åŒ–ï¼ˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°æ¤œç´¢ã®ã¿ä½¿ç”¨ï¼‰
        self.gemini_analyzer = None
        logger.info("âœ… ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°æ¤œç´¢ã®ã¿ã‚’ä½¿ç”¨ï¼ˆGeminiè³ªå•åˆ†æã‚·ã‚¹ãƒ†ãƒ ã¯ç„¡åŠ¹åŒ–ï¼‰")
        
        logger.info(f"âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–å®Œäº†: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°={self.embedding_model} ({self.expected_dimensions}æ¬¡å…ƒ)")

    async def _keyword_search(self, query: str, company_id: Optional[str], limit: int = 50) -> List[Dict]:  # ğŸš€ğŸš€ 30â†’50ã«å¢—åŠ ï¼ˆæƒ…å ±å®Œå…¨æ€§é‡è¦–ï¼‰
        """
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢ï¼ˆILIKEã‚’ä½¿ç”¨ï¼‰
        """
        logger.info(f"ğŸ”‘ Step 3-Keyword: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢é–‹å§‹ (Top-{limit})")
        # ã‚¯ã‚¨ãƒªã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆä¾‹ï¼šWPD4100389ï¼‰
        keywords = re.findall(r'[A-Z]+\d+', query)
        if not keywords:
            logger.info("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return []
        
        search_term = keywords[0] # ç°¡å˜ã®ãŸã‚æœ€åˆã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
        
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    sql_keyword = """
                    SELECT
                        c.id, c.doc_id, c.chunk_index, c.content,
                        ds.name as document_name, ds.type as document_type,
                        0.9 as similarity_score, -- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã¯é«˜ã‚¹ã‚³ã‚¢
                        'keyword' as search_method
                    FROM chunks c
                    LEFT JOIN document_sources ds ON ds.id = c.doc_id
                    WHERE c.content ILIKE %s
                    AND ds.active = true
                    """
                    params_keyword = [f"%{search_term}%"]

                    if company_id:
                        sql_keyword += " AND c.company_id = %s"
                        params_keyword.append(company_id)
                    
                    sql_keyword += " LIMIT %s"
                    params_keyword.append(limit)
                    
                    cur.execute(sql_keyword, params_keyword)
                    results = cur.fetchall()
                    logger.info(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{search_term}' ã§ {len(results)} ä»¶ãƒ’ãƒƒãƒˆã—ã¾ã—ãŸã€‚")
                    return [dict(row) for row in results]
        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _get_db_url(self) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã‚’æ§‹ç¯‰"""
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # Supabase URLã‹ã‚‰æ¥ç¶šæƒ…å ±ã‚’æŠ½å‡º
        if "supabase.co" in supabase_url:
            project_id = supabase_url.split("://")[1].split(".")[0]
            return f"postgresql://postgres.{project_id}:{os.getenv('DB_PASSWORD', '')}@aws-0-ap-northeast-1.pooler.supabase.com:6543/postgres"
        else:
            # ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã®å ´åˆ
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                raise ValueError("DATABASE_URL ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return db_url
    
    async def step1_receive_question(self, question: str, company_id: str = None) -> Dict:
        """
        âœï¸ Step 1. è³ªå•å…¥åŠ›
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã«è³ªå•ã‚’å…¥åŠ›
        """
        # ChatMessageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰æ–‡å­—åˆ—ã‚’å–å¾—
        if hasattr(question, 'text'):
            question_text = question.text
        else:
            question_text = str(question)
        
        logger.info(f"âœï¸ Step 1: è³ªå•å—ä»˜ - '{question_text[:50]}...'")
        
        if not question or not question.strip():
            raise ValueError("è³ªå•ãŒç©ºã§ã™")
        
        # è³ªå•ã®å‰å‡¦ç†
        processed_question = question.strip()
        
        return {
            "original_question": question,
            "processed_question": processed_question,
            "company_id": company_id,
            "timestamp": datetime.now().isoformat(),
            "step": 1
        }
    
    async def step2_generate_embedding(self, question: str) -> List[float]:
        """
        ğŸ§  Step 2. embedding ç”Ÿæˆ
        Gemini embedding-001 ã‚’ä½¿ã£ã¦ã€è³ªå•æ–‡ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ï¼ˆ3072æ¬¡å…ƒï¼‰
        """
        logger.info(f"ğŸ§  Step 2: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆä¸­...")
        
        try:
            # gemini-embedding-001ãƒ¢ãƒ‡ãƒ«ã§3072æ¬¡å…ƒã‚’ç”Ÿæˆ
            embedding_vector = await self.embedding_client.generate_embedding(
                question
            )
            
            if embedding_vector and len(embedding_vector) > 0:
                # æ¬¡å…ƒæ•°ãƒã‚§ãƒƒã‚¯
                if len(embedding_vector) != self.expected_dimensions:
                    logger.warning(f"äºˆæœŸã—ãªã„æ¬¡å…ƒæ•°: {len(embedding_vector)}æ¬¡å…ƒï¼ˆæœŸå¾…å€¤: {self.expected_dimensions}æ¬¡å…ƒï¼‰")
                
                logger.info(f"âœ… Step 2å®Œäº†: {len(embedding_vector)}æ¬¡å…ƒã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”ŸæˆæˆåŠŸ")
                return embedding_vector
            else:
                raise ValueError("ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            
        except Exception as e:
            logger.error(f"âŒ Step 2ã‚¨ãƒ©ãƒ¼: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆå¤±æ•— - {e}")
            raise
    
    async def step3_similarity_search(self, query_embedding: List[float], company_id: str = None, top_k: int = 35) -> List[Dict]:  # ğŸ¯ğŸ¯ 40â†’35ã«æœ€é©åŒ–ï¼ˆãƒ™ã‚¹ãƒˆãƒãƒ©ãƒ³ã‚¹ï¼‰
        """
        ğŸ” Step 3. é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢ï¼ˆTop-Kï¼‰
        Supabaseã® chunks ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã€ãƒ™ã‚¯ãƒˆãƒ«è·é›¢ãŒè¿‘ã„ãƒãƒ£ãƒ³ã‚¯ã‚’ pgvector ã‚’ç”¨ã„ã¦å–å¾—
        PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å«ã‚€ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’å¹³ç­‰ã«æ¤œç´¢å¯¾è±¡ã¨ã™ã‚‹
        """
        logger.info(f"ğŸ” Step 3: é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢é–‹å§‹ (Top-{top_k})")
        
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    # ğŸ” ã¾ãšã€åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒåˆ©ç”¨å¯èƒ½ãªãƒãƒ£ãƒ³ã‚¯ã§ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢
                    
                    # Convert query vector to proper string format and cast to vector type
                    vector_str = '[' + ','.join(map(str, query_embedding)) + ']'
                    
                    sql_vector = f"""
                    SELECT
                        c.id,
                        c.doc_id,
                        c.chunk_index,
                        c.content,
                        ds.name as document_name,
                        ds.type as document_type,
                        1 - (c.embedding <=> '{vector_str}'::vector) as similarity_score,
                        'vector' as search_method
                    FROM chunks c
                    LEFT JOIN document_sources ds ON ds.id = c.doc_id
                    WHERE c.embedding IS NOT NULL
                      AND c.content IS NOT NULL
                      AND LENGTH(c.content) > 10
                      AND ds.active = true
                    """
                    
                    params_vector = []
                    
                    # ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                    if company_id:
                        sql_vector += " AND c.company_id = %s"
                        params_vector.append(company_id)
                    
                    # ãƒ™ã‚¯ãƒˆãƒ«è·é›¢é †ã§ã‚½ãƒ¼ãƒˆ
                    sql_vector += f" ORDER BY c.embedding <=> '{vector_str}'::vector LIMIT %s"
                    params_vector.append(top_k)
                    
                    logger.info(f"å®Ÿè¡ŒSQL: ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢ (Top-{top_k})")
                    cur.execute(sql_vector, params_vector)
                    vector_results = cur.fetchall()
                    
                    # ğŸ” PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœã‚’ç¢ºèª
                    pdf_vector_count = len([r for r in vector_results if r['document_type'] == 'pdf'])
                    excel_vector_count = len([r for r in vector_results if r['document_type'] == 'excel'])
                    
                    logger.info(f"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœ: PDF={pdf_vector_count}ä»¶, Excel={excel_vector_count}ä»¶, ç·è¨ˆ={len(vector_results)}ä»¶")
                    
                    # çµæœã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                    similar_chunks = []
                    for row in vector_results:
                        similar_chunks.append({
                            'chunk_id': row['id'],
                            'doc_id': row['doc_id'],
                            'chunk_index': row['chunk_index'],
                            'content': row['content'],
                            'document_name': row['document_name'],
                            'document_type': row['document_type'],
                            'similarity_score': float(row['similarity_score']),
                            'search_method': row['search_method']
                        })
                    
                    # ğŸ” PDFãƒ•ã‚¡ã‚¤ãƒ«ã®çµæœãŒå°‘ãªã„å ´åˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’å®Ÿè¡Œ
                    if pdf_vector_count < 10:  # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®çµæœãŒ10ä»¶æœªæº€ã®å ´åˆ
                        logger.info("ğŸ“„ PDFãƒ•ã‚¡ã‚¤ãƒ«ã®çµæœãŒå°‘ãªã„ãŸã‚ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’å®Ÿè¡Œ")
                        
                        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒãªã„ãƒãƒ£ãƒ³ã‚¯ã«å¯¾ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã‚’å®Ÿè¡Œ
                        sql_text = """
                        SELECT
                            c.id,
                            c.doc_id,
                            c.chunk_index,
                            c.content,
                            ds.name as document_name,
                            ds.type as document_type,
                            0.5 as similarity_score,
                            'text_fallback' as search_method
                        FROM chunks c
                        LEFT JOIN document_sources ds ON ds.id = c.doc_id
                        WHERE c.embedding IS NULL
                          AND c.content IS NOT NULL
                          AND LENGTH(c.content) > 10
                          AND ds.type = 'pdf'
                        """
                        
                        params_text = []
                        
                        # ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                        if company_id:
                            sql_text += " AND c.company_id = %s"
                            params_text.append(company_id)
                        
                        # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒãªã„å ´åˆï¼‰
                        sql_text += " ORDER BY RANDOM() LIMIT %s"
                        params_text.append(min(10, top_k))
                        
                        logger.info("å®Ÿè¡ŒSQL: PDFãƒ•ã‚¡ã‚¤ãƒ«å‘ã‘ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢")
                        cur.execute(sql_text, params_text)
                        text_fallback_results = cur.fetchall()
                        
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœã‚’è¿½åŠ 
                        for row in text_fallback_results:
                            similar_chunks.append({
                                'chunk_id': row['id'],
                                'doc_id': row['doc_id'],
                                'chunk_index': row['chunk_index'],
                                'content': row['content'],
                                'document_name': row['document_name'],
                                'document_type': row['document_type'],
                                'similarity_score': float(row['similarity_score']),
                                'search_method': row['search_method']
                            })
                        
                        logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã§{len(text_fallback_results)}ä»¶ã®PDFãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ ")
                    
                    # ğŸ” ã•ã‚‰ã«ã€ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã®è¿½åŠ æ¤œç´¢
                    file_type_distribution = {}
                    for chunk in similar_chunks:
                        doc_type = chunk['document_type'] or 'unknown'
                        file_type_distribution[doc_type] = file_type_distribution.get(doc_type, 0) + 1
                    
                    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®çµæœãŒä¾ç„¶ã¨ã—ã¦å°‘ãªã„å ´åˆ
                    if file_type_distribution.get('pdf', 0) < 5:
                        logger.info("ğŸ“„ PDFãƒ•ã‚¡ã‚¤ãƒ«çµæœãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€è¿½åŠ æ¤œç´¢ã‚’å®Ÿè¡Œ")
                        
                        # ä¼šç¤¾å…¨ä½“ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä»£è¡¨çš„ãªãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
                        sql_pdf_supplement = """
                        SELECT
                            c.id,
                            c.doc_id,
                            c.chunk_index,
                            c.content,
                            ds.name as document_name,
                            ds.type as document_type,
                            0.4 as similarity_score,
                            'pdf_supplement' as search_method
                        FROM chunks c
                        LEFT JOIN document_sources ds ON ds.id = c.doc_id
                        WHERE c.content IS NOT NULL
                          AND LENGTH(c.content) > 10
                          AND ds.type = 'pdf'
                          AND ds.active = true
                        """
                        
                        params_pdf_supplement = []
                        
                        # ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                        if company_id:
                            sql_pdf_supplement += " AND c.company_id = %s"
                            params_pdf_supplement.append(company_id)
                        
                        # æœ€æ–°ã®ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰å„ªå…ˆçš„ã«å–å¾—
                        sql_pdf_supplement += " ORDER BY c.id DESC LIMIT %s"
                        params_pdf_supplement.append(5)
                        
                        logger.info("å®Ÿè¡ŒSQL: PDFè£œå®Œæ¤œç´¢")
                        cur.execute(sql_pdf_supplement, params_pdf_supplement)
                        pdf_supplement_results = cur.fetchall()
                        
                        # PDFè£œå®Œçµæœã‚’è¿½åŠ 
                        for row in pdf_supplement_results:
                            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                            if not any(chunk['chunk_id'] == row['id'] for chunk in similar_chunks):
                                similar_chunks.append({
                                    'chunk_id': row['id'],
                                    'doc_id': row['doc_id'],
                                    'chunk_index': row['chunk_index'],
                                    'content': row['content'],
                                    'document_name': row['document_name'],
                                    'document_type': row['document_type'],
                                    'similarity_score': float(row['similarity_score']),
                                    'search_method': row['search_method']
                                })
                        
                        logger.info(f"PDFè£œå®Œæ¤œç´¢ã§{len(pdf_supplement_results)}ä»¶ã®ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ ")
                    
                    # çµæœã‚’é¡ä¼¼åº¦é †ã§ã‚½ãƒ¼ãƒˆ
                    similar_chunks.sort(key=lambda x: x['similarity_score'], reverse=True)
                    
                    # æœ€å¤§ãƒãƒ£ãƒ³ã‚¯æ•°ã«åˆ¶é™
                    final_chunks = similar_chunks[:top_k]
                    
                    logger.info(f"âœ… Step 3å®Œäº†: {len(final_chunks)}å€‹ã®é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—")
                    
                    # ğŸ” è©³ç´°ãƒãƒ£ãƒ³ã‚¯é¸æŠãƒ­ã‚°ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGï¼‰
                    print("\n" + "="*80)
                    print(f"ğŸ” ã€Step 3: é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢çµæœã€‘")
                    print(f"ğŸ“Š å–å¾—ãƒãƒ£ãƒ³ã‚¯æ•°: {len(final_chunks)}ä»¶ (Top-{top_k})")
                    print(f"ğŸ¢ ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿: {'é©ç”¨ (' + company_id + ')' if company_id else 'æœªé©ç”¨ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ï¼‰'}")
                    print(f"ğŸ§  ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«: {self.embedding_model} ({self.expected_dimensions}æ¬¡å…ƒ)")
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆã‚’è¡¨ç¤º
                    final_file_type_distribution = {}
                    for chunk in final_chunks:
                        doc_type = chunk['document_type'] or 'unknown'
                        final_file_type_distribution[doc_type] = final_file_type_distribution.get(doc_type, 0) + 1
                    
                    print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥çµæœ: {final_file_type_distribution}")
                    print("="*80)
                    
                    for i, chunk in enumerate(final_chunks):
                        similarity = chunk['similarity_score']
                        doc_name = chunk['document_name'] or 'Unknown'
                        chunk_idx = chunk['chunk_index']
                        content_preview = (chunk['content'] or '')[:150].replace('\n', ' ')
                        search_method = chunk['search_method']
                        
                        # é¡ä¼¼åº¦ã«åŸºã¥ãè©•ä¾¡
                        if similarity > 0.8:
                            evaluation = "ğŸŸ¢ éå¸¸ã«é«˜ã„é–¢é€£æ€§"
                        elif similarity > 0.6:
                            evaluation = "ğŸŸ¡ é«˜ã„é–¢é€£æ€§"
                        elif similarity > 0.4:
                            evaluation = "ğŸŸ  ä¸­ç¨‹åº¦ã®é–¢é€£æ€§"
                        elif similarity > 0.2:
                            evaluation = "ğŸ”´ ä½ã„é–¢é€£æ€§"
                        else:
                            evaluation = "âš« æ¥µã‚ã¦ä½ã„é–¢é€£æ€§"
                        
                        print(f"  {i+1:2d}. ğŸ“„ {doc_name} ({chunk['document_type']})")
                        print(f"      ğŸ§© ãƒãƒ£ãƒ³ã‚¯#{chunk_idx} | ğŸ¯ é¡ä¼¼åº¦: {similarity:.4f} | {evaluation}")
                        print(f"      ğŸ” æ¤œç´¢æ–¹æ³•: {search_method}")
                        print(f"      ğŸ“ å†…å®¹: {content_preview}...")
                        print(f"      ğŸ”— ãƒãƒ£ãƒ³ã‚¯ID: {chunk['chunk_id']} | ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID: {chunk['doc_id']}")
                        print()
                    
                    print("="*80 + "\n")
                    
                    return final_chunks
        
        except Exception as e:
            logger.error(f"âŒ Step 3ã‚¨ãƒ©ãƒ¼: é¡ä¼¼æ¤œç´¢å¤±æ•— - {e}")
            raise
    
    async def step4_generate_answer(self, question: str, similar_chunks: List[Dict], company_name: str = "ãŠå®¢æ§˜ã®ä¼šç¤¾", company_id: str = None, user_id: str = None) -> Dict[str, Any]:
        """
        ğŸ’¡ Step 4. LLMã¸é€ä¿¡
        Top-K ãƒãƒ£ãƒ³ã‚¯ã¨å…ƒã®è³ªå•ã‚’ Gemini Flash 2.5 ã«æ¸¡ã—ã¦ã€è¦ç´„ã›ãšã«ã€ŒåŸæ–‡ãƒ™ãƒ¼ã‚¹ã€ã§å›ç­”ã‚’ç”Ÿæˆ
        """
        logger.info(f"ğŸ’¡ Step 4: LLMå›ç­”ç”Ÿæˆé–‹å§‹ ({len(similar_chunks) if similar_chunks else 0}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ä½¿ç”¨)")
        
        if not similar_chunks or len(similar_chunks) == 0:
            logger.warning("é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ä¸€èˆ¬çš„ãªå›ç­”ã‚’ç”Ÿæˆ")
            return {
                "answer": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã”è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚ˆã‚Šå…·ä½“çš„ãªè³ªå•ã‚’ã—ã¦ã„ãŸã ã‘ã¾ã™ã§ã—ã‚‡ã†ã‹ã€‚",
                "used_chunks": []
            }
        
        try:
            # ğŸ” Step 4: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ãƒ­ã‚°
            print("\n" + "="*80)
            print(f"ğŸ’¡ ã€Step 4: LLMå›ç­”ç”Ÿæˆ - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ã€‘")
            print(f"ğŸ“Š åˆ©ç”¨å¯èƒ½ãƒãƒ£ãƒ³ã‚¯æ•°: {len(similar_chunks)}å€‹")
            
            # ğŸš€ğŸš€ğŸš€ ç„¡é™ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼šæƒ…å ±å®Œå…¨æ€§çµ¶å¯¾å„ªå…ˆ
            question_length = len(question)
            base_limit = 500000  # ğŸš€ğŸš€ğŸš€ 50ä¸‡æ–‡å­—ï¼ˆç„¡é™ãƒ¢ãƒ¼ãƒ‰ãƒ»æƒ…å ±å®Œå…¨æ€§çµ¶å¯¾å„ªå…ˆï¼‰
            
            # ğŸ† ç„¡é™ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼šä¸Šé™åˆ¶é™ã‚’å¤§å¹…ç·©å’Œ
            if question_length > 10000:
                # è¶…é•·è³ªå•ã«ã¯ç„¡é™ã«è¿‘ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æä¾›
                max_context_length = base_limit + (question_length * 2.0)  # ğŸ† ä¸Šé™ãªã—ï¼
                print(f"ğŸ† ç„¡é™ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {max_context_length:,}æ–‡å­— (æƒ…å ±å®Œå…¨æ€§çµ¶å¯¾å„ªå…ˆ)")
            elif question_length > 5000:
                # é•·ã„è³ªå•ã«ã¯å¤§é‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æä¾›
                max_context_length = base_limit + (question_length * 1.5)  # ğŸ† åˆ¶é™ç·©å’Œï¼
                print(f"ğŸ† å¤§é‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {max_context_length:,}æ–‡å­— (æƒ…å ±å®Œå…¨æ€§çµ¶å¯¾å„ªå…ˆ)")
            elif question_length > 2000:
                max_context_length = base_limit + (question_length * 1.0)  # ğŸ† åˆ¶é™ç·©å’Œï¼
                print(f"ğŸ† å¤§é‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {max_context_length:,}æ–‡å­— (æƒ…å ±å®Œå…¨æ€§çµ¶å¯¾å„ªå…ˆ)")
            else:
                max_context_length = base_limit  # ğŸ† 50ä¸‡æ–‡å­—ï¼ˆç„¡é™ãƒ¢ãƒ¼ãƒ‰åŸºæº–ï¼‰
                print(f"ğŸ† ç„¡é™æ¨™æº–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {max_context_length:,}æ–‡å­— (æƒ…å ±å®Œå…¨æ€§çµ¶å¯¾å„ªå…ˆ)")
            
            print("="*80)
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ï¼ˆåŸæ–‡ãƒ™ãƒ¼ã‚¹ï¼‰
            context_parts = []
            total_length = 0
            used_chunks = []
            
            for i, chunk in enumerate(similar_chunks):
                chunk_content = f"ã€{chunk['document_name']}ã€‘\n{chunk['content']}\n"
                chunk_length = len(chunk_content)
                
                print(f"  {i+1:2d}. ğŸ“„ {chunk['document_name']} [ãƒãƒ£ãƒ³ã‚¯#{chunk['chunk_index']}]")
                print(f"      ğŸ¯ é¡ä¼¼åº¦: {chunk['similarity_score']:.4f}")
                print(f"      ğŸ“ æ–‡å­—æ•°: {chunk_length:,}æ–‡å­—")
                
                if total_length + chunk_length > max_context_length:
                    print(f"      âŒ é™¤å¤–: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·åˆ¶é™è¶…é (ç¾åœ¨: {total_length:,}æ–‡å­—)")
                    print(f"         ğŸ’¡ {i}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’æœ€çµ‚çš„ã«ä½¿ç”¨")
                    break
                
                context_parts.append(chunk_content)
                total_length += chunk_length
                used_chunks.append(chunk)
                print(f"      âœ… æ¡ç”¨: ç´¯è¨ˆ {total_length:,}æ–‡å­—")
                print(f"      ğŸ“ å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {(chunk['content'] or '')[:100].replace(chr(10), ' ')}...")
                print()
            
            context = "\n".join(context_parts)
            
            # ğŸ† ç„¡é™ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼šæƒ…å ±æŠœã‘çµ¶å¯¾é˜²æ­¢
            # åˆ¶é™ã‚’å¤§å¹…ç·©å’Œã—ã¦ã€ã™ã¹ã¦ã®æƒ…å ±ã‚’æ¼ã‚‰ã•ãšåé›†
            
            print(f"ğŸ“‹ æœ€çµ‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±:")
            print(f"   âœ… ä½¿ç”¨ãƒãƒ£ãƒ³ã‚¯æ•°: {len(used_chunks)}å€‹")
            print(f"   ğŸ“ ç·æ–‡å­—æ•°: {len(context):,}æ–‡å­—")
            print("="*80 + "\n")
            
            # ğŸ¯ ç‰¹åˆ¥æŒ‡ç¤ºã‚’å–å¾—ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸€ç•ªå‰ã«é…ç½®
            special_instructions_text = ""
            if company_id:
                try:
                    with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                        with conn.cursor() as cur:
                            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ã®ç‰¹åˆ¥æŒ‡ç¤ºã‚’å–å¾—
                            sql = """
                            SELECT DISTINCT ds.name, ds.special
                            FROM document_sources ds 
                            WHERE ds.company_id = %s 
                            AND ds.active = true 
                            AND ds.special IS NOT NULL 
                            AND ds.special != ''
                            ORDER BY ds.name
                            """
                            cur.execute(sql, [company_id])
                            special_results = cur.fetchall()
                            
                            if special_results:
                                special_instructions = []
                                print(f"ğŸ¯ ç‰¹åˆ¥æŒ‡ç¤ºã‚’å–å¾—ã—ã¾ã—ãŸ: {len(special_results)}ä»¶")
                                for i, row in enumerate(special_results, 1):
                                    resource_name = row['name']
                                    special_instruction = row['special']
                                    special_instructions.append(f"{i}. ã€{resource_name}ã€‘: {special_instruction}")
                                    print(f"   {i}. {resource_name}: {special_instruction}")
                                
                                special_instructions_text = "ç‰¹åˆ¥ãªå›ç­”æŒ‡ç¤ºï¼ˆä»¥ä¸‹ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’å‚ç…§ã™ã‚‹éš›ã¯ã€å„ãƒªã‚½ãƒ¼ã‚¹ã®æŒ‡ç¤ºã«å¾“ã£ã¦ãã ã•ã„ï¼‰ï¼š\n" + "\n".join(special_instructions) + "\n\n"
                                print(f"âœ… ç‰¹åˆ¥æŒ‡ç¤ºã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ å®Œäº†")
                            else:
                                print(f"â„¹ï¸ ç‰¹åˆ¥æŒ‡ç¤ºãŒè¨­å®šã•ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                                
                except Exception as e:
                    print(f"âš ï¸ ç‰¹åˆ¥æŒ‡ç¤ºå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                    logger.warning(f"ç‰¹åˆ¥æŒ‡ç¤ºå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ğŸ¯ è¤‡é›‘ãªè³ªå•ã®æ¤œå‡ºï¼ˆè¡¨å½¢å¼ã€è¤‡æ•°æ¡ä»¶ãªã©ï¼‰
            complex_indicators = [
                'è¡¨å½¢å¼', 'è¡¨ã§', 'ãƒ†ãƒ¼ãƒ–ãƒ«', 'ä¸€è¦§è¡¨', 'è©³ç´°ã‚’è¡¨', 
                'Ã—', 'âœ•', 'ç‰©ä»¶ç•ªå·ã”ã¨',
                'æ¡ä»¶:', 'æŒ‡ç¤º:', 'æ³¨æ„äº‹é …', 'è¡¨ç¤ºæ¡ä»¶'
            ]
            
            is_complex_query = any(indicator in question for indicator in complex_indicators)
            is_table_query = any(table in question for table in ['è¡¨å½¢å¼', 'è¡¨ã§', 'ãƒ†ãƒ¼ãƒ–ãƒ«', 'ç‰©ä»¶ç•ªå·ã”ã¨'])
            
            logger.info(f"ğŸ” è³ªå•åˆ†æ: è¤‡é›‘={is_complex_query}, è¡¨å½¢å¼={is_table_query}")
            
            # è¤‡é›‘ãªè³ªå•ãƒ»è¡¨å½¢å¼å›ç­”ç”¨ã®å®Œç’§ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            if is_complex_query or is_table_query:
                logger.info("ğŸ“Š è¤‡é›‘ãªè³ªå•æ¤œå‡º - å°‚ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨")
                prompt = f"""{special_instructions_text}ã‚ãªãŸã¯{company_name}ã®å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ğŸ¯ **å®Œç’§ãªå›ç­”ã®è¦ä»¶**ï¼š
â€¢ **å®Œå…¨æ€§**: æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†…ã®ã™ã¹ã¦ã®è©²å½“æƒ…å ±ã‚’æ¼ã‚Œãªãå«ã‚ã‚‹
â€¢ **æ­£ç¢ºæ€§**: å„é …ç›®ã®è©³ç´°æƒ…å ±ã‚’çœç•¥ã›ãšã«æ­£ç¢ºã«è¨˜è¼‰ã™ã‚‹  
â€¢ **é©åˆ‡æ€§**: è³ªå•ã«æœ€ã‚‚é©ã—ãŸå½¢å¼ï¼ˆè¡¨å½¢å¼ãƒ»ç®‡æ¡æ›¸ããƒ»æ–‡ç« ç­‰ï¼‰ã§å›ç­”ã™ã‚‹
â€¢ **æ˜ç¢ºæ€§**: èª­ã¿ã‚„ã™ãæ•´ç†ã•ã‚ŒãŸæ§‹é€ ã§æƒ…å ±ã‚’æç¤ºã™ã‚‹
â€¢ **å®Œçµæ€§**: å›ç­”ã®æœ€å¾Œã«è©²å½“é …ç›®ã®ç·ä»¶æ•°ã‚’æ˜è¨˜ã™ã‚‹

ğŸš¨ **çµ¶å¯¾éµå®ˆäº‹é …**ï¼š
â€¢ **æ–‡å­—æ•°åˆ¶é™**: å›ç­”ã¯å¿…ãš5000æ–‡å­—ä»¥å†…ã§å®Œçµã•ã›ã¦ãã ã•ã„
â€¢ **çœç•¥ç¦æ­¢**: ã€Œçœç•¥ã•ã‚Œã¾ã—ãŸã€ç­‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯çµ¶å¯¾ã«ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„
â€¢ **è¦ç´„é‡è¦–**: æƒ…å ±ã‚’è¦ç´„ãƒ»é›†ç´„ã—ã¦ç°¡æ½”ã«è¡¨ç¾ã—ã¦ãã ã•ã„

ã€è³ªå•ã€‘
{question}

ã€å‚è€ƒè³‡æ–™ã€‘
{context}

ä¸Šè¨˜ã®å‚è€ƒè³‡æ–™ã«åŸºã¥ã„ã¦ã€5000æ–‡å­—ä»¥å†…ã§å®Œå…¨ã§æ­£ç¢ºãªå›ç­”ã‚’æä¾›ã„ãŸã—ã¾ã™ï¼š"""
            
            else:
                # é€šå¸¸ã®è³ªå•ç”¨ã®å®Œç’§ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                prompt = f"""{special_instructions_text}ã‚ãªãŸã¯{company_name}ã®å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ğŸ¯ **å„ªç§€ãªå›ç­”ã®è¦ä»¶**ï¼š
â€¢ **å®Œå…¨æ€§**: æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†…ã®ã™ã¹ã¦ã®é–¢é€£æƒ…å ±ã‚’æ´»ç”¨ã™ã‚‹
â€¢ **æ­£ç¢ºæ€§**: å‚è€ƒè³‡æ–™ã®å†…å®¹ã‚’æ­£ç¢ºã«åæ˜ ã—ã€æ¨æ¸¬ã¨äº‹å®Ÿã‚’æ˜ç¢ºã«åŒºåˆ¥ã™ã‚‹
â€¢ **ä¸å¯§æ€§**: æ•¬èªã‚’ä½¿ç”¨ã—ã€è¦ªåˆ‡ã§åˆ†ã‹ã‚Šã‚„ã™ã„èª¬æ˜ã‚’å¿ƒãŒã‘ã‚‹  
â€¢ **é©åˆ‡æ€§**: è³ªå•ã®æ„å›³ã‚’æ­£ã—ãç†è§£ã—ã€æœ€é©ãªå½¢å¼ã§å›ç­”ã™ã‚‹
â€¢ **å®Œçµæ€§**: å¿…è¦ã«å¿œã˜ã¦æƒ…å ±ã®ä»¶æ•°ã‚„å‡ºå…¸ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ˜è¨˜ã™ã‚‹

ğŸš¨ **çµ¶å¯¾éµå®ˆäº‹é …**ï¼š
â€¢ **æ–‡å­—æ•°åˆ¶é™**: å›ç­”ã¯å¿…ãš5000æ–‡å­—ä»¥å†…ã§å®Œçµã•ã›ã¦ãã ã•ã„
â€¢ **çœç•¥ç¦æ­¢**: ã€Œçœç•¥ã•ã‚Œã¾ã—ãŸã€ç­‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯çµ¶å¯¾ã«ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„
â€¢ **è¦ç´„é‡è¦–**: æƒ…å ±ã‚’è¦ç´„ãƒ»é›†ç´„ã—ã¦ç°¡æ½”ã«è¡¨ç¾ã—ã¦ãã ã•ã„

ã€è³ªå•ã€‘
{question}

ã€å‚è€ƒè³‡æ–™ã€‘
{context}

ä¸Šè¨˜ã®å‚è€ƒè³‡æ–™ã«åŸºã¥ã„ã¦ã€5000æ–‡å­—ä»¥å†…ã§æ­£ç¢ºã§å®Œå…¨ãªå›ç­”ã‚’æä¾›ã„ãŸã—ã¾ã™ï¼š"""

            logger.info("ğŸ¤– Gemini Flash 2.5ã«å›ç­”ç”Ÿæˆã‚’ä¾é ¼ä¸­...")
            logger.info(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt):,}æ–‡å­—")
            
            # ğŸ†ğŸ†ğŸ† ç„¡é™ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ï¼šåˆ¶é™ã‚’å®Ÿè³ªçš„ã«ç„¡åŠ¹åŒ–
            max_safe_prompt_length = 2000000  # ğŸ†ğŸ†ğŸ† 200ä¸‡æ–‡å­—ï¼ˆç„¡é™ãƒ¢ãƒ¼ãƒ‰ãƒ»æƒ…å ±å®Œå…¨æ€§çµ¶å¯¾å„ªå…ˆï¼‰
            if len(prompt) > max_safe_prompt_length:
                logger.warning(f"âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ãŒåˆ¶é™è¶…é: {len(prompt):,} > {max_safe_prompt_length:,}æ–‡å­—")
                
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã®ã¿ã‚’ç·Šæ€¥å‰Šæ¸›
                context_start = prompt.find("ã€å‚è€ƒè³‡æ–™ã€‘")
                context_end = prompt.find("ä¸Šè¨˜ã®å‚è€ƒè³‡æ–™ã«åŸºã¥ã„ã¦")
                
                if context_start != -1 and context_end != -1:
                    # ä»–ã®éƒ¨åˆ†ï¼ˆè³ªå•ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç­‰ï¼‰ã®é•·ã•ã‚’è¨ˆç®—
                    other_parts_length = len(prompt) - (context_end - context_start)
                    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”¨ã«ä½¿ãˆã‚‹æœ€å¤§é•·ã‚’è¨ˆç®—
                    max_context_allowed = max_safe_prompt_length - other_parts_length - 100  # 100æ–‡å­—ã¯ãƒãƒ¼ã‚¸ãƒ³
                    
                    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å®‰å…¨ãªé•·ã•ã«å‰Šæ¸›
                    current_context = prompt[context_start:context_end]
                    if len(current_context) > max_context_allowed:
                        truncated_context = current_context[:max_context_allowed] + "\n... (ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç·Šæ€¥å‰Šæ¸›)"
                        prompt = prompt[:context_start] + truncated_context + prompt[context_end:]
                        logger.warning(f"ğŸ”§ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç·Šæ€¥å‰Šæ¸›: {len(current_context):,} â†’ {len(truncated_context):,}æ–‡å­—")
                        logger.info(f"ğŸ“ å‰Šæ¸›å¾Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt):,}æ–‡å­—")
            
            # Multi Gemini Client ã‚’ä½¿ç”¨ã—ãŸ API å‘¼ã³å‡ºã—ï¼ˆMAX_TOKENSå®Œå…¨è§£æ±ºã®ãŸã‚å¤§å¹…å¼•ãä¸Šã’ï¼‰
            generation_config = {
                "temperature": 0.05 if (is_complex_query or is_table_query) else 0.1,  # è¤‡é›‘ãªè³ªå•ã¯æ›´ã«ç¢ºå®šçš„ã«
                "maxOutputTokens": 32768,  # ğŸš€ğŸš€ MAX_TOKENSå®Œå…¨è§£æ±ºã®ãŸã‚å¤§å¹…å¼•ãä¸Šã’ï¼ˆç´„50000æ–‡å­—å¯¾å¿œï¼‰
                "topP": 0.7 if (is_complex_query or is_table_query) else 0.8,  # ã‚ˆã‚Šé›†ä¸­çš„ãªå¿œç­”
                "topK": 20 if (is_complex_query or is_table_query) else 40  # é¸æŠè‚¢ã‚’çµã‚‹
            }
            
            try:
                # Enhanced Multi Gemini Client ã‚’ä½¿ç”¨ï¼ˆã‚­ãƒ¥ãƒ¼ç®¡ç†ãƒ»è¤‡æ•°è³ªå•å¯¾å¿œï¼‰
                response_data = None
                try:
                    from .enhanced_multi_client import get_enhanced_multi_gemini_client, enhanced_multi_gemini_available
                    enhanced_client = get_enhanced_multi_gemini_client()
                    if enhanced_multi_gemini_available():
                        logger.info("ğŸš€ Enhanced Multi Gemini Clientä½¿ç”¨ã§APIå‘¼ã³å‡ºã—é–‹å§‹")
                        response_data = await enhanced_client.generate_content_async(
                            prompt, 
                            generation_config,
                            user_id=user_id,
                            company_id=company_id
                        )
                        logger.info("ğŸ“¥ Enhanced Multi Gemini Clientã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å—ä¿¡å®Œäº†")
                    else:
                        raise ImportError("Enhanced Multi Gemini Clientåˆ©ç”¨ä¸å¯")
                except (ImportError, Exception) as enhanced_error:
                    logger.warning(f"âš ï¸ Enhanced Multi Gemini Clientå¤±æ•—: {enhanced_error}")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®Multi Gemini Clientï¼ˆ10å›ãƒªãƒˆãƒ©ã‚¤ï¼‰
                    client = get_or_init_multi_gemini_client()
                    if client and multi_gemini_available():
                        logger.info("ğŸ”„ Multi Gemini Clientä½¿ç”¨ã§APIå‘¼ã³å‡ºã—é–‹å§‹ï¼ˆ10å›ãƒªãƒˆãƒ©ã‚¤ï¼‰")
                        response_data_str = client.generate_content(prompt, generation_config)
                        import json
                        try:
                            response_data = json.loads(response_data_str)
                        except:
                            # ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆã¯è¾æ›¸å½¢å¼ã«å¤‰æ›
                            response_data = {
                                "candidates": [
                                    {
                                        "content": {
                                            "parts": [
                                                {"text": response_data_str}
                                            ]
                                        }
                                    }
                                ]
                            }
                        logger.info("ğŸ“¥ Multi Gemini Clientã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å—ä¿¡å®Œäº†ï¼ˆ10å›ãƒªãƒˆãƒ©ã‚¤å¾Œï¼‰")
                    else:
                        logger.error("âŒ Multi Gemini Clientåˆ©ç”¨ä¸å¯ã€å…¨APIã‚­ãƒ¼å¤±æ•—")
                        raise Exception("å…¨APIã‚­ãƒ¼ã§ãƒªãƒˆãƒ©ã‚¤å¤±æ•—")
                
                if not response_data:
                    raise Exception("ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                logger.info(f"ğŸ” Geminiãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ : {list(response_data.keys())}")
                
                answer = None
                
                if "candidates" in response_data and response_data["candidates"]:
                    logger.info(f"ğŸ“‹ å€™è£œæ•°: {len(response_data['candidates'])}")
                    
                    try:
                        candidate = response_data["candidates"][0]
                        logger.info(f"ğŸ” candidateæ§‹é€ : {list(candidate.keys()) if isinstance(candidate, dict) else type(candidate)}")
                        
                        if "finishReason" in candidate:
                            finish_reason = candidate['finishReason']
                            logger.info(f"ğŸ” finishReason: {finish_reason}")
                            
                            # MAX_TOKENSã‚¨ãƒ©ãƒ¼ã¯32768ãƒˆãƒ¼ã‚¯ãƒ³ã§è§£æ±ºæ¸ˆã¿
                            if finish_reason == "MAX_TOKENS":
                                logger.warning("âš ï¸ MAX_TOKENSã«åˆ°é”ã—ã¾ã—ãŸãŒã€å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™ï¼ˆ32768ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å†…ï¼‰")
                        
                        # Enhanced Multi Gemini Clientç”¨ã®è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œ
                        text_content = None
                        
                        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: æ¨™æº–çš„ãªGemini APIå½¢å¼
                        if "content" in candidate and "parts" in candidate["content"]:
                            parts = candidate["content"]["parts"]
                            if parts and "text" in parts[0]:
                                text_content = parts[0]["text"]
                                logger.info("âœ… æ¨™æº–å½¢å¼ã§ãƒ†ã‚­ã‚¹ãƒˆå–å¾—")
                        
                        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: Enhanced Multi Gemini Clientã®ç›´æ¥å½¢å¼
                        elif "text" in candidate:
                            text_content = candidate["text"]
                            logger.info("âœ… ç›´æ¥textå½¢å¼ã§ãƒ†ã‚­ã‚¹ãƒˆå–å¾—")
                        
                        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: messageå½¢å¼
                        elif "message" in candidate and "content" in candidate["message"]:
                            text_content = candidate["message"]["content"]
                            logger.info("âœ… message.contentå½¢å¼ã§ãƒ†ã‚­ã‚¹ãƒˆå–å¾—")
                        
                        if text_content:
                            answer = text_content
                            logger.info(f"âœ… å›ç­”å–å¾—æˆåŠŸ: {len(answer)}æ–‡å­—")
                            
                            # ğŸ¯ å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸã‚½ãƒ¼ã‚¹ã®ç‰¹å®šï¼ˆå›ç­”å†…å®¹ã¨ã®ç…§åˆï¼‰
                            actually_used_sources = []
                            actually_used_chunks = []
                            
                            if answer and len(answer.strip()) > 0:
                                logger.info("ğŸ” å›ç­”å†…å®¹ã¨ç…§åˆã—ã¦å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸã‚½ãƒ¼ã‚¹ã‚’ç‰¹å®šä¸­...")
                                
                                for chunk in used_chunks:
                                    chunk_content = chunk.get('content', '')
                                    chunk_doc_name = chunk.get('document_name', '')
                                    
                                    if not chunk_content or not chunk_doc_name or chunk_doc_name == 'None':
                                        continue
                                    
                                    # ãƒãƒ£ãƒ³ã‚¯å†…å®¹ãŒå®Ÿéš›ã«å›ç­”ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ãƒã‚§ãƒƒã‚¯
                                    is_used = self._is_chunk_actually_used(answer, chunk_content, chunk)
                                    
                                    if is_used:
                                        actually_used_chunks.append(chunk)
                                        if chunk_doc_name not in actually_used_sources:
                                            actually_used_sources.append(chunk_doc_name)
                                            logger.info(f"âœ… å®Ÿä½¿ç”¨ã‚½ãƒ¼ã‚¹ç¢ºå®š: {chunk_doc_name}")
                                    else:
                                        logger.info(f"âŒ æœªä½¿ç”¨ã‚½ãƒ¼ã‚¹é™¤å¤–: {chunk_doc_name}")
                                
                                # å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ãŒãªã„å ´åˆã®å®‰å…¨è£…ç½®ï¼ˆã‚ˆã‚Šåˆ¶é™çš„ã«ï¼‰
                                if not actually_used_sources and used_chunks:
                                    logger.warning("âš ï¸ å®Ÿä½¿ç”¨ã‚½ãƒ¼ã‚¹ãŒç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸ - ä¸Šä½2ã¤ã®ãƒãƒ£ãƒ³ã‚¯ã®ã¿ä½¿ç”¨")
                                    for chunk in used_chunks[:2]:
                                        chunk_doc_name = chunk.get('document_name', '')
                                        if chunk_doc_name and chunk_doc_name.strip() and chunk_doc_name != 'None':
                                            if chunk_doc_name not in actually_used_sources:
                                                actually_used_sources.append(chunk_doc_name)
                                                actually_used_chunks.append(chunk)
                            else:
                                # å›ç­”ãŒç©ºã®å ´åˆã‚‚åˆ¶é™çš„ã«ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                                logger.warning("âš ï¸ å›ç­”ãŒç©ºã®ãŸã‚ã€ä¸Šä½5ã¤ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚½ãƒ¼ã‚¹ã¨ã—ã¦ä½¿ç”¨")
                                for chunk in used_chunks[:5]:
                                    chunk_doc_name = chunk.get('document_name', '')
                                    if chunk_doc_name and chunk_doc_name.strip() and chunk_doc_name != 'None':
                                        if chunk_doc_name not in actually_used_sources:
                                            actually_used_sources.append(chunk_doc_name)
                                            actually_used_chunks.append(chunk)
                            
                            # ä½¿ç”¨ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã‚’æ›´æ–°
                            used_chunks = actually_used_chunks
                            
                            logger.info(f"ğŸ“ å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸã‚½ãƒ¼ã‚¹: {actually_used_sources}")
                            logger.info(f"ğŸ¯ å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯æ•°: {len(actually_used_chunks)}ä»¶")
                        
                        else:
                            logger.warning("âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    
                    except Exception as parse_error:
                        logger.error(f"âŒ ãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æã‚¨ãƒ©ãƒ¼: {parse_error}")
                        answer = None
                
                else:
                    logger.warning("âš ï¸ ç„¡åŠ¹ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¾ãŸã¯å€™è£œãªã—")
                    
            except Exception as e:
                error_msg = str(e)
                logger.error(f"âŒ LLMå›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {error_msg}")
                
                # ğŸš€ MAX_TOKENSã‚¨ãƒ©ãƒ¼ã¯32768ãƒˆãƒ¼ã‚¯ãƒ³ã§è§£æ±ºæ¸ˆã¿ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ä¸è¦
                
                # Multi Gemini Clientã®çŠ¶æ…‹æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
                client = get_or_init_multi_gemini_client()
                if client:
                    try:
                        status_info = client.get_status_info()
                        logger.info("ğŸ“Š Multi Gemini ClientçŠ¶æ…‹:")
                        for client_name, info in status_info.items():
                            logger.info(f"   {client_name}: {info['status']} (ãƒªãƒˆãƒ©ã‚¤: {info['retry_count']}/{client.max_retries})")
                    except Exception as status_error:
                        logger.error(f"çŠ¶æ…‹æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {status_error}")
                
                # HTTPExceptionã¨ã—ã¦å†ç™ºç”Ÿï¼ˆFastAPIãŒé©åˆ‡ã«å‡¦ç†ï¼‰
                from fastapi import HTTPException
                if "429" in error_msg or "rate limit" in error_msg.lower() or "quota exceeded" in error_msg.lower():
                    raise HTTPException(status_code=429, detail="APIåˆ¶é™ã®ãŸã‚ã€ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„")
                elif "MAX_TOKENS_ERROR" not in error_msg:  # MAX_TOKENSã‚¨ãƒ©ãƒ¼ã¯é€šå¸¸ã®å‡¦ç†ã‚’ç¶šè¡Œ
                    raise HTTPException(status_code=500, detail=f"LLMå›ç­”ç”Ÿæˆå¤±æ•—: {error_msg}")
            
            # å›ç­”ã®æ¤œè¨¼ã¨å‡¦ç†
            if answer and len(answer.strip()) > 0:
                logger.info(f"âœ… Step 4å®Œäº†: {len(answer)}æ–‡å­—ã®å›ç­”ã‚’ç”Ÿæˆ")
                logger.info(f"ğŸ“ å›ç­”ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {answer[:100]}...")
                
                # ğŸ¯ 5000æ–‡å­—åˆ¶é™ï¼ˆæƒ…å ±å®Œå…¨æ€§ã¨ãƒãƒ©ãƒ³ã‚¹ï¼‰
                max_answer_length = 5000  # ğŸ¯ 5000æ–‡å­—åˆ¶é™ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã«ã‚ˆã‚Šç¶­æŒï¼‰
                if len(answer) > max_answer_length:
                    logger.warning(f"âš ï¸ å›ç­”ãŒé•·ã™ãã¾ã™ ({len(answer):,}æ–‡å­—) - {max_answer_length:,}æ–‡å­—ã«çŸ­ç¸®ï¼ˆçœç•¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãªã—ï¼‰")
                    # ğŸ¯ è‡ªç„¶ãªæ–‡ç« å¢ƒç•Œã§åˆ‡ã‚Šæ¨ã¦ï¼ˆçœç•¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯è¿½åŠ ã—ãªã„ï¼‰
                    truncated = answer[:max_answer_length]
                    last_period = truncated.rfind('ã€‚')
                    last_newline = truncated.rfind('\n')
                    cut_point = max(last_period, last_newline)
                    
                    if cut_point > max_answer_length - 800:  # åˆ‡ã‚Œç›®ãŒè¿‘ã„å ´åˆ
                        answer = answer[:cut_point + 1].strip()
                    else:
                        # å¼·åˆ¶çš„ã«max_answer_lengthã§åˆ‡ã‚Šæ¨ã¦
                        answer = answer[:max_answer_length].strip()
                    
                    logger.info(f"âœ… 5000æ–‡å­—ä»¥å†…ã«çŸ­ç¸®å®Œäº†: {len(answer):,}æ–‡å­—")
                
                # å›ç­”ãŒçŸ­ã™ãã‚‹å ´åˆã®å‡¦ç†
                if len(answer) < 50:
                    logger.warning(f"âš ï¸ å›ç­”ãŒçŸ­ã™ãã¾ã™ï¼ˆ{len(answer)}æ–‡å­—ï¼‰- è£œå¼·å‡¦ç†ã‚’å®Ÿè¡Œ")
                    fallback_answer = f"""å‚è€ƒè³‡æ–™ã‚’ç¢ºèªã„ãŸã—ã¾ã—ãŸã€‚

{answer}

ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ãŒå¿…è¦ã§ã—ãŸã‚‰ã€å…·ä½“çš„ãªé …ç›®ã‚’ãŠæ•™ãˆãã ã•ã„ã€‚å‚è€ƒè³‡æ–™ã‹ã‚‰æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã„ãŸã—ã¾ã™ã€‚"""
                    return {
                        "answer": fallback_answer,
                        "used_chunks": used_chunks
                    }
                
                return {
                    "answer": answer,
                    "used_chunks": used_chunks
                }
            else:
                logger.error("âŒ LLMã‹ã‚‰ã®å›ç­”ãŒç©ºã¾ãŸã¯å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                # response_dataå¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if 'response_data' in locals() and response_data is not None:
                    logger.error(f"   ãƒ¬ã‚¹ãƒãƒ³ã‚¹è©³ç´°: {response_data}")
                else:
                    logger.error("   ãƒ¬ã‚¹ãƒãƒ³ã‚¹è©³ç´°: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¾ãŸã¯æ¥ç¶šã‚¨ãƒ©ãƒ¼ï¼‰")
                
                # ğŸ› ï¸ æ§‹é€ åŒ–ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ - è¤‡é›‘ãªè³ªå•ã«å¯¾ã™ã‚‹ä»£æ›¿å‡¦ç†
                fallback_parts = []
                
                # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°åˆ†æ
                question_length = len(question)
                context_length = len(context) if 'context' in locals() else 0
                
                # è¤‡é›‘ãªè³ªå•ã®å ´åˆã®ç‰¹åˆ¥ãªæ§‹é€ åŒ–ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if is_complex_query or is_table_query:
                    logger.info("ğŸ› ï¸ è¤‡é›‘ãªè³ªå•ç”¨ã®æ§‹é€ åŒ–ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é–‹å§‹")
                    
                    # é¡§å®¢æƒ…å ±ã‚’æŠ½å‡º
                    customer_info = self._extract_customer_info(question, used_chunks)
                    if customer_info:
                        fallback_parts.append("ğŸ“‹ **æ¤œç´¢çµæœã®æ¦‚è¦**")
                        fallback_parts.append(f"é¡§å®¢ã‚³ãƒ¼ãƒ‰: {customer_info.get('code', 'ä¸æ˜')}")
                        fallback_parts.append(f"ä¼šç¤¾å: {customer_info.get('name', 'ä¸æ˜')}")
                        fallback_parts.append("")
                    
                    # é–¢é€£ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹é€ åŒ–ã—ã¦è¡¨ç¤º
                    extracted_data = self._extract_structured_data(used_chunks, question)
                    if extracted_data:
                        fallback_parts.append("ğŸ“Š **é–¢é€£æƒ…å ±**")
                        for data_item in extracted_data[:5]:  # æœ€å¤§5ä»¶
                            fallback_parts.append(f"â€¢ {data_item}")
                        fallback_parts.append("")
                    
                    fallback_parts.append("âš ï¸ **å‡¦ç†çŠ¶æ³**")
                    fallback_parts.append("å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ä»¥ä¸‹ã®ä»£æ›¿æ¡ˆã‚’ã”ææ¡ˆã„ãŸã—ã¾ã™ï¼š")
                    fallback_parts.append("")
                    fallback_parts.append("ğŸ“ **æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**")
                    fallback_parts.append("1. é¡§å®¢ã‚³ãƒ¼ãƒ‰ã¨ä¼šç¤¾åã§ã®åŸºæœ¬æ¤œç´¢ã‚’å…ˆã«å®Ÿè¡Œ")
                    fallback_parts.append("2. å¥‘ç´„æƒ…å ±ã®ç¢ºèªã‚’æ®µéšçš„ã«å®Ÿæ–½")
                    
                else:
                    # é€šå¸¸ã®è³ªå•ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    if question_length > 8000:
                        fallback_parts.append("è³ªå•ãŒéå¸¸ã«é•·ã„ãŸã‚ã€å‡¦ç†ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
                        fallback_parts.append("ä»¥ä¸‹ã®æ–¹æ³•ã‚’ãŠè©¦ã—ãã ã•ã„ï¼š")
                        fallback_parts.append("1. è³ªå•ã‚’è¤‡æ•°ã®å°ã•ãªè³ªå•ã«åˆ†å‰²ã—ã¦ãã ã•ã„")
                        fallback_parts.append("2. æœ€ã‚‚é‡è¦ãªéƒ¨åˆ†ã‹ã‚‰é †ç•ªã«ãŠèããã ã•ã„")
                        fallback_parts.append("3. å…·ä½“çš„ãªé …ç›®åã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚ã¦ãã ã•ã„")
                    elif context_length > 300000:  # 30ä¸‡æ–‡å­—ä»¥ä¸Š
                        fallback_parts.append("å‚è€ƒè³‡æ–™ãŒå¤§é‡ã«ã‚ã‚‹ãŸã‚ã€å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã¾ã™ã€‚")
                        fallback_parts.append("ã‚ˆã‚Šå…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§çµã‚Šè¾¼ã‚“ã è³ªå•ã‚’ã—ã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ")
                    else:
                        fallback_parts.append("ã‚·ã‚¹ãƒ†ãƒ çš„ãªå•é¡Œã«ã‚ˆã‚Šè©³ç´°ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                
                # æ¤œç´¢ã§ããŸå‚è€ƒè³‡æ–™ã®æƒ…å ±ã‚’æä¾›
                if used_chunks:
                    fallback_parts.append(f"\næ¤œç´¢ã§ã¯{len(used_chunks)}ä»¶ã®é–¢é€£è³‡æ–™ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼š")
                    
                    # è¤‡æ•°ã®ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
                    for i, chunk in enumerate(used_chunks[:3]):  # æœ€å¤§3ã¤ã®ãƒãƒ£ãƒ³ã‚¯
                        if chunk.get('content'):
                            content_preview = chunk['content'][:200].replace('\n', ' ')
                            doc_name = chunk.get('document_name', f'è³‡æ–™{i+1}')
                            fallback_parts.append(f"\nã€{doc_name}ã€‘")
                            fallback_parts.append(f"{content_preview}...")
                    
                    if len(used_chunks) > 3:
                        fallback_parts.append(f"\nä»–ã«ã‚‚{len(used_chunks) - 3}ä»¶ã®é–¢é€£è³‡æ–™ãŒã‚ã‚Šã¾ã™ã€‚")
                
                fallback_parts.append("\nğŸ’¡ æ”¹å–„ææ¡ˆï¼š")
                fallback_parts.append("â€¢ ã‚ˆã‚Šå…·ä½“çš„ãªè³ªå•ã«ã—ã¦ãã ã•ã„")
                fallback_parts.append("â€¢ çŸ¥ã‚ŠãŸã„é …ç›®ã‚’æ˜ç¢ºã«ã—ã¦ãã ã•ã„")  
                fallback_parts.append("â€¢ è³ªå•ã‚’åˆ†å‰²ã—ã¦æ®µéšçš„ã«ãŠèããã ã•ã„")
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã«ã‚‚å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’æ§‹ç¯‰
                fallback_sources = []
                seen_names = set()
                for chunk in used_chunks[:3]:  # æœ€å¤§3ã¤ã®ã‚½ãƒ¼ã‚¹
                    doc_name = chunk.get('document_name', 'Unknown Document')
                    if doc_name not in seen_names and doc_name not in ['ã‚·ã‚¹ãƒ†ãƒ å›ç­”', 'unknown', 'Unknown']:
                        fallback_sources.append({
                            "name": doc_name,
                            "document_name": doc_name,
                            "document_type": chunk.get('document_type', 'unknown'),
                            "similarity_score": chunk.get('similarity_score', 0.0)
                        })
                        seen_names.add(doc_name)
                
                return {
                    "answer": "\n".join(fallback_parts),
                    "used_chunks": used_chunks,
                    "sources": fallback_sources,
                    "source_documents": fallback_sources
                }
        
        except Exception as e:
            logger.error(f"âŒ Step 4ã‚¨ãƒ©ãƒ¼: LLMå›ç­”ç”Ÿæˆå¤±æ•— - {e}")
            import traceback
            logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã§ã‚‚å¯èƒ½ãªé™ã‚Šæƒ…å ±ã‚’æä¾›
            error_response_parts = ["ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"]
            
            if used_chunks and len(used_chunks) > 0:
                error_response_parts.append(f"\næ¤œç´¢ã§ã¯{len(used_chunks)}ä»¶ã®é–¢é€£è³‡æ–™ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
                if used_chunks[0].get('content'):
                    first_content = used_chunks[0]['content'][:200]
                    error_response_parts.append(f"é–¢é€£æƒ…å ±ã®ä¸€éƒ¨: {first_content}...")
            
            error_response_parts.append("\nã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã«ã‚‚å¯èƒ½ãªé™ã‚Šã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’æ§‹ç¯‰
            error_sources = []
            seen_names = set()
            for chunk in used_chunks[:2]:  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯æœ€å¤§2ã¤ã®ã‚½ãƒ¼ã‚¹
                doc_name = chunk.get('document_name', 'Unknown Document')
                if doc_name not in seen_names and doc_name not in ['ã‚·ã‚¹ãƒ†ãƒ å›ç­”', 'unknown', 'Unknown']:
                    error_sources.append({
                        "name": doc_name,
                        "document_name": doc_name,
                        "document_type": chunk.get('document_type', 'unknown'),
                        "similarity_score": chunk.get('similarity_score', 0.0)
                    })
                    seen_names.add(doc_name)
            
            return {
                "answer": "\n".join(error_response_parts),
                "used_chunks": [],  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®ãƒªã‚¹ãƒˆ
                "sources": error_sources,
                "source_documents": error_sources
            }
    
    def _extract_customer_info(self, question: str, chunks: List[Dict]) -> Dict[str, str]:
        """è³ªå•ã¨ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰é¡§å®¢æƒ…å ±ã‚’æŠ½å‡º"""
        customer_info = {}
        
        # è³ªå•ã‹ã‚‰é¡§å®¢ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        import re
        code_match = re.search(r'SS\d{7}', question)
        if code_match:
            customer_info['code'] = code_match.group()
        
        # è³ªå•ã‹ã‚‰ä¼šç¤¾åã‚’æŠ½å‡º
        company_patterns = [
            r'ä¼šç¤¾åï¼šã€Œ([^ã€]+)ã€',
            r'ä¼šç¤¾å:ã€Œ([^ã€]+)ã€',
            r'æ ªå¼ä¼šç¤¾[^\sã€]+',
            r'ãˆ±[^\sã€]+',
            r'\(æ ª\)[^\sã€]+'
        ]
        
        for pattern in company_patterns:
            match = re.search(pattern, question)
            if match:
                customer_info['name'] = match.group(1) if match.groups() else match.group()
                break
        
        # ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ã‚‚æƒ…å ±ã‚’è£œå®Œ
        if not customer_info.get('name'):
            for chunk in chunks[:3]:  # æœ€åˆã®3ã¤ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒã‚§ãƒƒã‚¯
                content = chunk.get('content', '')
                if 'æ ªå¼ä¼šç¤¾' in content:
                    # æ ªå¼ä¼šç¤¾ã‚’å«ã‚€éƒ¨åˆ†ã‚’æŠ½å‡º
                    company_match = re.search(r'æ ªå¼ä¼šç¤¾[^\s,ã€|]+', content)
                    if company_match:
                        customer_info['name'] = company_match.group()
                        break
        
        return customer_info
    
    def _extract_structured_data(self, chunks: List[Dict], question: str) -> List[str]:
        """ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        extracted_data = []
        
        # é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        important_keywords = ['è¨­ç½®', 'å®Œäº†', 'å¥‘ç´„', 'æœˆé¡', 'æœŸé–“', 'WPC', 'CB', 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹']
        
        for chunk in chunks[:5]:  # æœ€å¤§5ã¤ã®ãƒãƒ£ãƒ³ã‚¯
            content = chunk.get('content', '')
            doc_name = chunk.get('document_name', 'Unknown')
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
            for keyword in important_keywords:
                if keyword in content and keyword.lower() in question.lower():
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‘¨è¾ºã®æ–‡è„ˆã‚’æŠ½å‡º
                    lines = content.split('\n')
                    for line in lines:
                        if keyword in line and len(line.strip()) > 10:
                            # ãƒ‡ãƒ¼ã‚¿ã£ã½ã„è¡Œã‚’æŠ½å‡º
                            if any(char in line for char in [':', 'ï¼š', '|', ',']):
                                extracted_data.append(f"{doc_name}: {line.strip()}")
                                break
                    break
            
            if len(extracted_data) >= 5:  # æœ€å¤§5ä»¶
                break
        
        return list(set(extracted_data))  # é‡è¤‡é™¤å»
    
    def _is_chunk_actually_used(self, answer: str, chunk_content: str, chunk: Dict) -> bool:
        """å›ç­”ã®å†…å®¹ã¨ãƒãƒ£ãƒ³ã‚¯ã®å†…å®¹ã‚’ç…§åˆã—ã¦ã€å®Ÿéš›ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’åˆ¤å®š"""
        if not answer or not chunk_content:
            return False
        
        # 1. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ï¼ˆé‡è¦ãªå˜èªã‚„ãƒ•ãƒ¬ãƒ¼ã‚ºã®ç…§åˆï¼‰
        import re
        
        # ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆ3æ–‡å­—ä»¥ä¸Šã®å˜èªï¼‰
        chunk_keywords = re.findall(r'\b\w{3,}\b', chunk_content)
        # æ•°å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ—¥ä»˜ã€é‡‘é¡ã€ã‚³ãƒ¼ãƒ‰ãªã©ï¼‰ã‚‚æŠ½å‡º
        chunk_numbers = re.findall(r'\b\d+\b', chunk_content)
        # ç‰¹æ®Šãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¼šç¤¾åã€ã‚³ãƒ¼ãƒ‰ãªã©ï¼‰
        chunk_patterns = re.findall(r'[A-Z]{2}\d{7}|æ ªå¼ä¼šç¤¾[^\s,ã€]+|ãˆ±[^\s,ã€]+', chunk_content)
        
        all_chunk_elements = chunk_keywords + chunk_numbers + chunk_patterns
        
        if not all_chunk_elements:
            return False
        
        # 2. å›ç­”å†…ã§ã®ä¸€è‡´ç‡ã‚’è¨ˆç®—
        matched_elements = 0
        total_elements = len(all_chunk_elements[:20])  # æœ€å¤§20è¦ç´ ã§åˆ¤å®š
        
        for element in all_chunk_elements[:20]:
            if len(element) >= 3:  # 3æ–‡å­—ä»¥ä¸Šã®è¦ç´ ã®ã¿
                # å®Œå…¨ä¸€è‡´
                if element in answer:
                    matched_elements += 1
                # éƒ¨åˆ†ä¸€è‡´ï¼ˆ6æ–‡å­—ä»¥ä¸Šã®è¦ç´ ã®å ´åˆï¼‰
                elif len(element) >= 6:
                    if any(element in word or word in element for word in answer.split()):
                        matched_elements += 0.5
        
        match_ratio = matched_elements / total_elements if total_elements > 0 else 0
        
        # 3. ç‰¹åˆ¥ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã®é‡ã¿ä»˜ã‘
        special_bonus = 0
        
        # ä¼šç¤¾åã‚„é¡§å®¢ã‚³ãƒ¼ãƒ‰ãªã©ã®é‡è¦æƒ…å ±ãŒä¸€è‡´ã™ã‚‹å ´åˆ
        for pattern in chunk_patterns:
            if pattern in answer:
                special_bonus += 0.3
        
        # æ—¥ä»˜ã‚„é‡‘é¡ãªã©ã®å…·ä½“çš„ãªæ•°å€¤ãŒä¸€è‡´ã™ã‚‹å ´åˆ
        important_numbers = [num for num in chunk_numbers if len(num) >= 4]  # 4æ¡ä»¥ä¸Šã®æ•°å­—
        for num in important_numbers:
            if num in answer:
                special_bonus += 0.2
        
        final_score = match_ratio + special_bonus
        
        # 4. ğŸ›¡ï¸ è¶…å®‰å…¨å‹ï¼šæœ¬ç•ªç’°å¢ƒæœ€é©åŒ–ï¼ˆç²¾åº¦é‡è¦– + å®‰å®šæ€§ï¼‰
        threshold = 0.2  # ğŸ›¡ï¸ 0.15â†’0.2ã«èª¿æ•´ï¼ˆè¶…å®‰å…¨å‹ï¼‰
        
        is_used = final_score >= threshold
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
        chunk_doc_name = chunk.get('document_name', 'Unknown')
        if is_used:
            logger.info(f"   âœ… ãƒãƒ£ãƒ³ã‚¯ä½¿ç”¨ç¢ºèª: {chunk_doc_name} (ã‚¹ã‚³ã‚¢: {final_score:.2f}, é–¾å€¤: {threshold})")
        else:
            logger.info(f"   âŒ ãƒãƒ£ãƒ³ã‚¯æœªä½¿ç”¨: {chunk_doc_name} (ã‚¹ã‚³ã‚¢: {final_score:.2f}, é–¾å€¤: {threshold})")
        
        return is_used
    
    async def step5_display_answer(self, answer: str, metadata: Dict = None, used_chunks: List = None) -> Dict:
        """
        âš¡ï¸ Step 5. å›ç­”è¡¨ç¤º
        æœ€çµ‚çš„ãªå›ç­”ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        """
        logger.info(f"âš¡ï¸ Step 5: å›ç­”è¡¨ç¤ºæº–å‚™å®Œäº†")
        
        result = {
            "answer": answer,
            "timestamp": datetime.now().isoformat(),
            "step": 5,
            "status": "completed"
        }
        
        if metadata:
            result.update(metadata)
        
        # ä½¿ç”¨ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã®è©³ç´°æƒ…å ±ã‚’è¿½åŠ  - main.pyãŒæœŸå¾…ã™ã‚‹å½¢å¼ã§è¿”ã™
        if used_chunks:
            source_documents = []
            seen_names = set()
            for chunk in used_chunks[:5]:  # æœ€å¤§5å€‹ã®ã‚½ãƒ¼ã‚¹æ–‡æ›¸
                doc_name = chunk.get('document_name', 'Unknown Document')
                # é‡è¤‡ã™ã‚‹åå‰ã¯é™¤å¤–ã—ã€ã‚·ã‚¹ãƒ†ãƒ å›ç­”ç­‰ã¯é™¤å¤–
                if doc_name not in seen_names and doc_name not in ['ã‚·ã‚¹ãƒ†ãƒ å›ç­”', 'unknown', 'Unknown']:
                    doc_info = {
                        "name": doc_name,  # main.pyãŒæœŸå¾…ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å
                        "filename": doc_name,  # å¾Œæ–¹äº’æ›æ€§
                        "document_name": doc_name,  # å¾Œæ–¹äº’æ›æ€§
                        "document_type": chunk.get('document_type', 'unknown'),
                        "similarity_score": chunk.get('similarity_score', 0.0)
                    }
                    source_documents.append(doc_info)
                    seen_names.add(doc_name)
            
            result["sources"] = source_documents  # main.pyãŒæœŸå¾…ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å
            result["source_documents"] = source_documents  # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™
            result["total_sources"] = len(used_chunks)
        
        logger.info(f"âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†å®Œäº†: {len(answer)}æ–‡å­—ã®å›ç­”")
        return result
    
    async def process_realtime_rag(self, question: str, company_id: str = None, company_name: str = "ãŠå®¢æ§˜ã®ä¼šç¤¾", top_k: int = 35, user_id: str = None) -> Dict:  # ğŸ¯ğŸ¯ 40â†’35ã«æœ€é©åŒ–ï¼ˆãƒ™ã‚¹ãƒˆãƒãƒ©ãƒ³ã‚¹ï¼‰
        """
        ğŸš€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ãƒ•ãƒ­ãƒ¼å…¨ä½“ã®å®Ÿè¡Œï¼ˆGeminiè³ªå•åˆ†æçµ±åˆç‰ˆï¼‰
        æ–°ã—ã„3æ®µéšã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: Geminiåˆ†æ â†’ SQLæ¤œç´¢ â†’ Embeddingæ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        """
        # ChatMessageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰æ–‡å­—åˆ—ã‚’å–å¾—
        if hasattr(question, 'text'):
            question_text = question.text
        else:
            question_text = str(question)
        
        logger.info(f"ğŸš€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†é–‹å§‹: '{question_text[:50]}...'")
        logger.info(f"ğŸ”§ ãƒ‡ãƒãƒƒã‚°: åˆ†å‰²ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œãƒã‚§ãƒƒã‚¯é–‹å§‹")
        
        # Geminiã«ã‚ˆã‚‹è‡ªå‹•è³ªå•åˆ†å‰²å‡¦ç†
        import re
        
        # è¤‡æ•°ã‚¿ã‚¹ã‚¯ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        multi_task_indicators = [
            r'WP[DN]\d{7}.*WP[DN]\d{7}',  # è¤‡æ•°ã®ç‰©ä»¶ç•ªå·
            r'(ã¨|ã€).*ã«ã¤ã„ã¦',  # è¤‡æ•°é …ç›®ã«ã¤ã„ã¦
            r'ã¾ãŸ.*ã‚‚',  # è¿½åŠ è¦æ±‚
            r'ã•ã‚‰ã«.*ã‚‚',  # ã•ã‚‰ãªã‚‹è¦æ±‚
            r'ã‚ã¨.*ã‚‚',  # è¿½åŠ é …ç›®
            r'[ï¼Ÿ?].*[ï¼Ÿ?]',  # è¤‡æ•°ã®ç–‘å•ç¬¦
            r'(1\.|2\.|3\.|â‘ |â‘¡|â‘¢)',  # ç•ªå·ä»˜ããƒªã‚¹ãƒˆ
        ]
        
        has_multi_tasks = any(re.search(pattern, question_text) for pattern in multi_task_indicators)
        is_long = len(question_text) > 30
        
        logger.info(f"ğŸ”§ ãƒ‡ãƒãƒƒã‚°: ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯å®Œäº†")
        logger.info(f"ğŸ” è³ªå•åˆ†æ: æ–‡å­—æ•°={len(question_text)}, è¤‡æ•°ã‚¿ã‚¹ã‚¯æ¤œå‡º={has_multi_tasks}")
        logger.info(f"ğŸ”§ ãƒ‡ãƒãƒƒã‚°: is_long={is_long}, åˆ†å‰²åˆ¤å®š={has_multi_tasks or is_long}")
        
        if has_multi_tasks or is_long:  # è¤‡æ•°ã‚¿ã‚¹ã‚¯ã¾ãŸã¯30æ–‡å­—ä»¥ä¸Šã§åˆ†å‰²æ¤œè¨
            reason = "è¤‡æ•°ã‚¿ã‚¹ã‚¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º" if has_multi_tasks else f"é•·æ–‡({len(question_text)}æ–‡å­—)"
            logger.info(f"ğŸ“ è¤‡æ•°ã‚¿ã‚¹ã‚¯åˆ†å‰²ã‚’æ¤œè¨: {reason}")
            
            try:
                # Geminiã«è³ªå•åˆ†å‰²ã‚’ä¾é ¼
                from .gemini_question_splitter import request_question_split
                split_result = await request_question_split(question_text)
                
                if split_result and len(split_result) > 1:
                    logger.info(f"âœ‚ï¸ GeminiãŒè³ªå•ã‚’{len(split_result)}å€‹ã«åˆ†å‰²")
                    
                    # å„åˆ†å‰²è³ªå•ã‚’ä¸¦åˆ—å‡¦ç†
                    segment_responses = []
                    for i, sub_question in enumerate(split_result):
                        logger.info(f"ğŸ” è³ªå•{i+1}å‡¦ç†: {sub_question[:50]}...")
                        try:
                            # åˆ†å‰²ã•ã‚ŒãŸè³ªå•ã‚’å€‹åˆ¥å‡¦ç†ï¼ˆå†åˆ†å‰²ã‚’é˜²ããŸã‚_process_single_segment_no_splitã‚’ä½¿ç”¨ï¼‰
                            response = await self._process_single_segment_no_split(sub_question, company_id, company_name, top_k, user_id)
                            segment_responses.append(response)
                        except Exception as e:
                            logger.error(f"âŒ è³ªå•{i+1}å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                    
                    # å›ç­”ã‚’ãƒãƒ¼ã‚¸
                    if segment_responses:
                        from .gemini_question_splitter import merge_multiple_responses
                        merged_result = await merge_multiple_responses(segment_responses, question_text)
                        logger.info(f"âœ… è¤‡æ•°è³ªå•å‡¦ç†å®Œäº†: {len(merged_result['answer'])}æ–‡å­—ã®çµ±åˆå›ç­”")
                        return merged_result
                    else:
                        logger.warning("âš ï¸ å…¨è³ªå•å‡¦ç†å¤±æ•— - é€šå¸¸å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                else:
                    logger.info("ğŸ” Geminiåˆ¤å®š: åˆ†å‰²ä¸è¦ã¾ãŸã¯å˜ä¸€è³ªå•")
            
            except Exception as e:
                logger.error(f"âŒ Geminiåˆ†å‰²å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e} - é€šå¸¸å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        else:
            logger.info(f"â© å˜ä¸€ã‚¿ã‚¹ã‚¯ã¨åˆ¤å®šã€åˆ†å‰²ã‚¹ã‚­ãƒƒãƒ—: æ–‡å­—æ•°={len(question_text)}, è¤‡æ•°ã‚¿ã‚¹ã‚¯={has_multi_tasks}")
        
        # é€šå¸¸ã®å‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼ˆåˆ†å‰²ã—ãªã„å ´åˆã¾ãŸã¯åˆ†å‰²å¤±æ•—æ™‚ï¼‰
        return await self._process_single_segment_no_split(question_text, company_id, company_name, top_k, user_id)
    
    async def _process_single_segment_no_split(self, question_text: str, company_id: str = None, company_name: str = "ãŠå®¢æ§˜ã®ä¼šç¤¾", top_k: int = 35, user_id: str = None) -> Dict:  # ğŸ¯ğŸ¯ 40â†’35ã«æœ€é©åŒ–ï¼ˆãƒ™ã‚¹ãƒˆãƒãƒ©ãƒ³ã‚¹ï¼‰
        """å˜ä¸€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å‡¦ç†ï¼ˆå¾“æ¥ã®process_realtime_ragã®å†…å®¹ï¼‰"""
        try:
            # Step 1: è³ªå•å…¥åŠ›
            step1_result = await self.step1_receive_question(question_text, company_id)
            processed_question = step1_result["processed_question"]
            
            # Step 2: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
            query_embedding = await self.step2_generate_embedding(processed_question)

            # Step 3: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’ä¸¦åˆ—å®Ÿè¡Œ
            search_tasks = [
                self.step3_similarity_search(query_embedding, company_id, top_k),
                self._keyword_search(processed_question, company_id, 50) # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã¯50ä»¶ã¾ã§
            ]
            results_list = await asyncio.gather(*search_tasks, return_exceptions=True)

            vector_results = results_list[0] if not isinstance(results_list[0], Exception) else []
            keyword_results = results_list[1] if not isinstance(results_list[1], Exception) else []

            # çµæœã®çµ±åˆã¨é‡è¤‡é™¤å»
            all_chunks = {r['chunk_id']: r for r in vector_results}
            for r in keyword_results:
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢çµæœã‚’å„ªå…ˆçš„ã«ä¸Šæ›¸ã
                all_chunks[r['id']] = {
                    'chunk_id': r['id'],
                    'doc_id': r['doc_id'],
                    'chunk_index': r['chunk_index'],
                    'content': r['content'],
                    'document_name': r['document_name'],
                    'document_type': r['document_type'],
                    'similarity_score': float(r['similarity_score']),
                    'search_method': r['search_method']
                }

            # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
            sorted_chunks = sorted(all_chunks.values(), key=lambda x: x['similarity_score'], reverse=True)
            
            # æœ€çµ‚çš„ãªãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
            similar_chunks = sorted_chunks[:top_k]
            
            metadata = {
                "original_question": question_text, # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®è³ªå•ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«å«ã‚ã‚‹
                "processed_question": processed_question,
                "chunks_used": len(similar_chunks),
                "top_similarity": similar_chunks[0]["similarity_score"] if similar_chunks else 0.0,
                "company_id": company_id,
                "company_name": company_name,
                "search_method": "hybrid_search" # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã«å¤‰æ›´
            }
            
            # Step 4: LLMå›ç­”ç”Ÿæˆ
            generation_result = await self.step4_generate_answer(processed_question, similar_chunks, company_name, company_id, user_id)
            answer = generation_result["answer"]
            actually_used_chunks = generation_result["used_chunks"]
            
            # Step 5: å›ç­”è¡¨ç¤ºï¼ˆå®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚’å«ã‚ã‚‹ï¼‰
            result = await self.step5_display_answer(answer, metadata, actually_used_chunks)
            
            logger.info(f"ğŸ‰ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†æˆåŠŸå®Œäº†")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã§ã‚‚å¯èƒ½ãªé™ã‚Šã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’æä¾›
            error_sources = []
            if 'similar_chunks' in locals() and similar_chunks:
                seen_names = set()
                for chunk in similar_chunks[:2]:  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯æœ€å¤§2ã¤ã®ã‚½ãƒ¼ã‚¹
                    doc_name = chunk.get('document_name', 'Unknown Document')
                    if doc_name not in seen_names and doc_name not in ['ã‚·ã‚¹ãƒ†ãƒ å›ç­”', 'unknown', 'Unknown']:
                        error_sources.append({
                            "name": doc_name,
                            "document_name": doc_name,
                            "document_type": chunk.get('document_type', 'unknown'),
                            "similarity_score": chunk.get('similarity_score', 0.0)
                        })
                        seen_names.add(doc_name)
            
            error_result = {
                "answer": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "status": "error",
                "sources": error_sources,
                "source_documents": error_sources
            }
            return error_result

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_realtime_rag_processor = None

def get_realtime_rag_processor() -> Optional[RealtimeRAGProcessor]:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
    global _realtime_rag_processor
    
    if _realtime_rag_processor is None:
        try:
            _realtime_rag_processor = RealtimeRAGProcessor()
            logger.info("âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            logger.error(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    return _realtime_rag_processor

async def process_question_realtime(question: str, company_id: str = None, company_name: str = "ãŠå®¢æ§˜ã®ä¼šç¤¾", top_k: int = 35, user_id: str = None) -> Dict:  # ğŸ¯ğŸ¯ 40â†’35ã«æœ€é©åŒ–ï¼ˆãƒ™ã‚¹ãƒˆãƒãƒ©ãƒ³ã‚¹ï¼‰
    """
    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ã®å¤–éƒ¨å‘¼ã³å‡ºã—ç”¨é–¢æ•°
    
    Args:
        question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
        company_id: ä¼šç¤¾IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        company_name: ä¼šç¤¾åï¼ˆå›ç­”ç”Ÿæˆç”¨ï¼‰
        top_k: å–å¾—ã™ã‚‹é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æ•°
    
    Returns:
        Dict: å‡¦ç†çµæœï¼ˆå›ç­”ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç­‰ï¼‰
    """
    processor = get_realtime_rag_processor()
    if not processor:
        return {
            "answer": "ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚",
            "error": "RealtimeRAGProcessor initialization failed",
            "timestamp": datetime.now().isoformat(),
            "status": "error"
        }
    
    return await processor.process_realtime_rag(question, company_id, company_name, top_k, user_id)

def realtime_rag_available() -> bool:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        return bool(api_key and supabase_url and supabase_key)
    except Exception:
        return False
"""
ğŸ“Š Excel ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»æ§‹é€ åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ä¹±é›‘ãªExcelãƒ‡ãƒ¼ã‚¿ã‚’æ§‹é€ åŒ–ã—ã€è³ªå•å¿œç­”å¯èƒ½ãªå½¢å¼ã«å¤‰æ›
"""

import pandas as pd
import numpy as np
import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from io import BytesIO
import unicodedata

logger = logging.getLogger(__name__)

class ExcelDataCleaner:
    """Excelãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ§‹é€ åŒ–ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.min_meaningful_length = 3  # æ„å‘³ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã®æœ€å°æ–‡å­—æ•°
        self.max_cell_length = 1000     # ã‚»ãƒ«å†…å®¹ã®æœ€å¤§æ–‡å­—æ•°
        
    def clean_excel_data(self, content: bytes) -> str:
        """
        ä¹±é›‘ãªExcelãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
        """
        try:
            excel_file = pd.ExcelFile(BytesIO(content))
            cleaned_parts = []
            
            for sheet_name in excel_file.sheet_names:
                try:
                    logger.info(f"ğŸ“Š ã‚·ãƒ¼ãƒˆå‡¦ç†é–‹å§‹: {sheet_name}")
                    
                    # è¤‡æ•°ã®èª­ã¿è¾¼ã¿æ–¹æ³•ã‚’è©¦è¡Œ
                    df = self._read_excel_sheet_robust(excel_file, sheet_name)
                    
                    if df is None or df.empty:
                        logger.warning(f"âš ï¸ ã‚·ãƒ¼ãƒˆ {sheet_name} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—")
                        continue
                    
                    # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                    cleaned_df = self._clean_dataframe(df)
                    
                    if cleaned_df.empty:
                        logger.warning(f"âš ï¸ ã‚·ãƒ¼ãƒˆ {sheet_name} ã«ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                        continue
                    
                    # æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
                    structured_text = self._convert_to_structured_text(cleaned_df, sheet_name)
                    
                    if structured_text.strip():
                        cleaned_parts.append(structured_text)
                        logger.info(f"âœ… ã‚·ãƒ¼ãƒˆ {sheet_name} å‡¦ç†å®Œäº†: {len(structured_text)} æ–‡å­—")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ ã‚·ãƒ¼ãƒˆ {sheet_name} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            if not cleaned_parts:
                logger.warning("âš ï¸ å‡¦ç†å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return "ã“ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯å‡¦ç†å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
            
            result = "\n\n".join(cleaned_parts)
            logger.info(f"ğŸ‰ Excelå‡¦ç†å®Œäº†: {len(cleaned_parts)}ã‚·ãƒ¼ãƒˆ, ç·æ–‡å­—æ•°: {len(result)}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Excelå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _read_excel_sheet_robust(self, excel_file, sheet_name: str) -> Optional[pd.DataFrame]:
        """
        Excelã‚·ãƒ¼ãƒˆã‚’å …ç‰¢ã«èª­ã¿è¾¼ã‚€ï¼ˆè¤‡æ•°ã®æ–¹æ³•ã‚’è©¦è¡Œï¼‰
        """
        read_methods = [
            # æ–¹æ³•1: ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã§èª­ã¿è¾¼ã¿
            lambda: pd.read_excel(excel_file, sheet_name=sheet_name, header=None),
            # æ–¹æ³•2: æœ€åˆã®è¡Œã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦èª­ã¿è¾¼ã¿
            lambda: pd.read_excel(excel_file, sheet_name=sheet_name, header=0),
            # æ–¹æ³•3: è¤‡æ•°è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦èª­ã¿è¾¼ã¿
            lambda: pd.read_excel(excel_file, sheet_name=sheet_name, header=None, skiprows=1),
            # æ–¹æ³•4: æ–‡å­—åˆ—ã¨ã—ã¦å…¨ã¦èª­ã¿è¾¼ã¿
            lambda: pd.read_excel(excel_file, sheet_name=sheet_name, header=None, dtype=str)
        ]
        
        for i, method in enumerate(read_methods):
            try:
                df = method()
                if df is not None and not df.empty:
                    logger.info(f"ğŸ“– ã‚·ãƒ¼ãƒˆ {sheet_name} èª­ã¿è¾¼ã¿æˆåŠŸï¼ˆæ–¹æ³•{i+1}ï¼‰")
                    return df
            except Exception as e:
                logger.debug(f"èª­ã¿è¾¼ã¿æ–¹æ³•{i+1}å¤±æ•—: {e}")
                continue
        
        logger.warning(f"âš ï¸ ã‚·ãƒ¼ãƒˆ {sheet_name} ã®å…¨ã¦ã®èª­ã¿è¾¼ã¿æ–¹æ³•ãŒå¤±æ•—")
        return None
    
    def _clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        DataFrameã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        """
        # 1. ç©ºã®è¡Œãƒ»åˆ—ã‚’å‰Šé™¤
        df = df.dropna(how='all').dropna(axis=1, how='all')
        
        if df.empty:
            return df
        
        # 2. ã‚»ãƒ«å†…å®¹ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        for col in df.columns:
            df[col] = df[col].apply(self._clean_cell_content)
        
        # 3. æ„å‘³ã®ãªã„è¡Œã‚’å‰Šé™¤
        df = df[df.apply(self._is_meaningful_row, axis=1)]
        
        # 4. é‡è¤‡è¡Œã‚’å‰Šé™¤
        df = df.drop_duplicates()
        
        # 5. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ
        df = df.reset_index(drop=True)
        
        return df
    
    def _clean_cell_content(self, cell_value) -> str:
        """
        ã‚»ãƒ«å†…å®¹ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        """
        if pd.isna(cell_value):
            return ""
        
        # æ–‡å­—åˆ—ã«å¤‰æ›
        text = str(cell_value).strip()
        
        # é•·ã™ãã‚‹ã‚»ãƒ«ã¯åˆ‡ã‚Šè©°ã‚
        if len(text) > self.max_cell_length:
            text = text[:self.max_cell_length] + "..."
        
        # Unicodeæ­£è¦åŒ–
        text = unicodedata.normalize('NFKC', text)
        
        # ä¸è¦ãªç©ºç™½ãƒ»æ”¹è¡Œã‚’æ•´ç†
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n+', '\n', text)
        
        # ç‰¹æ®Šæ–‡å­—ã®å‡¦ç†
        text = text.replace('\x00', '')  # NULLæ–‡å­—å‰Šé™¤
        text = text.replace('\ufeff', '')  # BOMå‰Šé™¤
        
        return text.strip()
    
    def _is_meaningful_row(self, row: pd.Series) -> bool:
        """
        è¡ŒãŒæ„å‘³ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚“ã§ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        """
        meaningful_cells = 0
        total_content_length = 0
        
        for cell in row:
            if pd.notna(cell) and str(cell).strip():
                cell_text = str(cell).strip()
                # ã‚·ãƒ¼ãƒˆåã‚„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ç¹°ã‚Šè¿”ã—ã‚’é™¤å¤–
                if (len(cell_text) >= self.min_meaningful_length and
                    not self._is_metadata_text(cell_text)):
                    meaningful_cells += 1
                    total_content_length += len(cell_text)
        
        # æ„å‘³ã®ã‚ã‚‹ã‚»ãƒ«ãŒ1å€‹ä»¥ä¸Šã€ã¾ãŸã¯ç·æ–‡å­—æ•°ãŒ5æ–‡å­—ä»¥ä¸Šï¼ˆåŸºæº–ã‚’ç·©å’Œï¼‰
        return meaningful_cells >= 1 or total_content_length >= 5
    
    def _is_metadata_text(self, text: str) -> bool:
        """
        ãƒ†ã‚­ã‚¹ãƒˆãŒãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚·ãƒ¼ãƒˆåãªã©ï¼‰ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
        """
        text_lower = text.lower()
        metadata_patterns = [
            'sheet', 'ã‚·ãƒ¼ãƒˆ', 'ä¸€è¦§è¡¨', 'æ¡ˆä»¶ä¸€è¦§', 'ãƒ—ãƒ­ãƒã‚¤ãƒ€',
            'no.', 'noã€‚', 'unnamed'
        ]
        
        # çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚„ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å¤–
        if len(text.strip()) <= 2:
            return True
            
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        for pattern in metadata_patterns:
            if pattern in text_lower:
                return True
                
        return False
    
    def _convert_to_structured_text(self, df: pd.DataFrame, sheet_name: str) -> str:
        """
        ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸDataFrameã‚’æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
        """
        if df.empty:
            return ""
        
        text_parts = [f"=== ã‚·ãƒ¼ãƒˆ: {sheet_name} ==="]
        
        # ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã‚’åˆ†æ
        structure_info = self._analyze_data_structure(df)
        
        if structure_info["has_headers"]:
            # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒã‚ã‚‹å ´åˆã®å‡¦ç†
            text_parts.append(self._format_with_headers(df, structure_info))
        else:
            # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒãªã„å ´åˆã®å‡¦ç†
            text_parts.append(self._format_without_headers(df))
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ 
        stats = self._generate_data_statistics(df)
        if stats:
            text_parts.append(f"\nã€ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã€‘\n{stats}")
        
        return "\n".join(text_parts)
    
    def _analyze_data_structure(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã‚’åˆ†æ
        """
        structure = {
            "has_headers": False,
            "header_row": None,
            "data_types": {},
            "patterns": []
        }
        
        # æœ€åˆã®æ•°è¡Œã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ¢ã™
        for i in range(min(5, len(df))):
            row = df.iloc[i]
            if self._looks_like_header(row):
                structure["has_headers"] = True
                structure["header_row"] = i
                break
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’åˆ†æ
        for col in df.columns:
            structure["data_types"][col] = self._analyze_column_type(df[col])
        
        return structure
    
    def _looks_like_header(self, row: pd.Series) -> bool:
        """
        è¡ŒãŒãƒ˜ãƒƒãƒ€ãƒ¼ã‚‰ã—ã„ã‹ãƒã‚§ãƒƒã‚¯
        """
        non_null_count = row.notna().sum()
        if non_null_count < 2:
            return False
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚‰ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
        header_keywords = [
            'åå‰', 'name', 'ä¼šç¤¾', 'company', 'ä½æ‰€', 'address', 
            'é›»è©±', 'phone', 'tel', 'æ—¥ä»˜', 'date', 'id', 'no',
            'ç•ªå·', 'ç¨®é¡', 'type', 'çŠ¶æ…‹', 'status', 'é‡‘é¡', 'amount'
        ]
        
        text_content = ' '.join([str(cell).lower() for cell in row if pd.notna(cell)])
        
        for keyword in header_keywords:
            if keyword in text_content:
                return True
        
        return False
    
    def _analyze_column_type(self, column: pd.Series) -> str:
        """
        åˆ—ã®ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’åˆ†æ
        """
        non_null_values = column.dropna()
        if len(non_null_values) == 0:
            return "empty"
        
        # æ•°å€¤ãƒã‚§ãƒƒã‚¯
        numeric_count = 0
        date_count = 0
        text_count = 0
        
        for value in non_null_values:
            str_value = str(value).strip()
            
            # æ•°å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³
            if re.match(r'^-?\d+\.?\d*$', str_value):
                numeric_count += 1
            # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³
            elif re.match(r'\d{4}[-/]\d{1,2}[-/]\d{1,2}', str_value):
                date_count += 1
            else:
                text_count += 1
        
        total = len(non_null_values)
        if numeric_count / total > 0.7:
            return "numeric"
        elif date_count / total > 0.5:
            return "date"
        else:
            return "text"
    
    def _format_with_headers(self, df: pd.DataFrame, structure_info: Dict) -> str:
        """
        ãƒ˜ãƒƒãƒ€ãƒ¼ã‚ã‚Šã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        """
        header_row = structure_info["header_row"]
        headers = df.iloc[header_row].tolist()
        data_rows = df.iloc[header_row + 1:]
        
        formatted_parts = []
        formatted_parts.append("ã€ãƒ‡ãƒ¼ã‚¿é …ç›®ã€‘")
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±
        valid_headers = []
        for i, header in enumerate(headers):
            if pd.notna(header) and str(header).strip():
                clean_header = str(header).strip()
                valid_headers.append((i, clean_header))
                formatted_parts.append(f"- {clean_header}")
        
        formatted_parts.append("\nã€ãƒ‡ãƒ¼ã‚¿å†…å®¹ã€‘")
        
        # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’å‡¦ç†
        for idx, row in data_rows.iterrows():
            row_data = []
            for col_idx, header_name in valid_headers:
                if col_idx < len(row):
                    cell_value = row.iloc[col_idx]
                    if pd.notna(cell_value) and str(cell_value).strip():
                        clean_value = str(cell_value).strip()
                        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚„ã‚·ãƒ¼ãƒˆåã®ç¹°ã‚Šè¿”ã—ã‚’é™¤å¤–
                        if not self._is_metadata_text(clean_value):
                            row_data.append(f"{header_name}: {clean_value}")
            
            if row_data:
                formatted_parts.append(f"â€¢ {' | '.join(row_data)}")
        
        return "\n".join(formatted_parts)
    
    def _format_without_headers(self, df: pd.DataFrame) -> str:
        """
        ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        """
        formatted_parts = []
        formatted_parts.append("ã€ãƒ‡ãƒ¼ã‚¿å†…å®¹ã€‘")
        
        for idx, row in df.iterrows():
            row_data = []
            for col_idx, cell_value in enumerate(row):
                if pd.notna(cell_value) and str(cell_value).strip():
                    clean_value = str(cell_value).strip()
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚„ã‚·ãƒ¼ãƒˆåã®ç¹°ã‚Šè¿”ã—ã‚’é™¤å¤–
                    if not self._is_metadata_text(clean_value):
                        row_data.append(clean_value)
            
            if row_data:
                formatted_parts.append(f"è¡Œ{idx + 1}: {' | '.join(row_data)}")
        
        return "\n".join(formatted_parts)
    
    def _generate_data_statistics(self, df: pd.DataFrame) -> str:
        """
        ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆæƒ…å ±ã‚’ç”Ÿæˆ
        """
        stats_parts = []
        
        # åŸºæœ¬çµ±è¨ˆ
        stats_parts.append(f"ç·è¡Œæ•°: {len(df)}")
        stats_parts.append(f"ç·åˆ—æ•°: {len(df.columns)}")
        
        # éç©ºã‚»ãƒ«æ•°
        non_empty_cells = df.notna().sum().sum()
        total_cells = len(df) * len(df.columns)
        fill_rate = (non_empty_cells / total_cells * 100) if total_cells > 0 else 0
        stats_parts.append(f"ãƒ‡ãƒ¼ã‚¿å……å¡«ç‡: {fill_rate:.1f}%")
        
        return " | ".join(stats_parts)
"""
ğŸ§  Gemini 2.5 Flashè³ªå•åˆ†è§£ãƒ»åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ 
è³ªå•ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åˆ†è§£ãƒ»åˆ†é¡ã—ã¦SQLæ¤œç´¢ã¨Embeddingæ¤œç´¢ã‚’æœ€é©åŒ–

å®Ÿè£…å†…å®¹:
1. Gemini 2.5 Flashã‚’ä½¿ã£ã¦è³ªå•ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åˆ†è§£ãƒ»åˆ†é¡
2. SQLã«å¤‰æ›ã—ã¦æ§‹é€ çš„ã«æ¢ç´¢ï¼ˆé«˜é€Ÿãƒ»ç²¾å¯†ï¼‰
3. çµæœãŒã‚¼ãƒ­ä»¶ãªã‚‰Embeddingæ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
"""

import os
import re
import json
import logging
import asyncio
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
from dotenv import load_dotenv
import psycopg2
from psycopg2.extras import RealDictCursor

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

logger = logging.getLogger(__name__)

class QueryIntent(Enum):
    """è³ªå•ã®æ„å›³åˆ†é¡"""
    SPECIFIC_INFO = "specific_info"      # å…·ä½“çš„æƒ…å ±ã‚’æ±‚ã‚ã‚‹ï¼ˆä»£è¡¨è€…åã€é€£çµ¡å…ˆãªã©ï¼‰
    GENERAL_INFO = "general_info"        # ä¸€èˆ¬çš„æƒ…å ±ã‚’æ±‚ã‚ã‚‹ï¼ˆã‚µãƒ¼ãƒ“ã‚¹æ¦‚è¦ãªã©ï¼‰
    COMPARISON = "comparison"            # æ¯”è¼ƒãƒ»é•ã„ã‚’æ±‚ã‚ã‚‹
    EXPLANATION = "explanation"          # èª¬æ˜ãƒ»ç†ç”±ã‚’æ±‚ã‚ã‚‹
    PROCEDURE = "procedure"              # æ‰‹é †ãƒ»æ–¹æ³•ã‚’æ±‚ã‚ã‚‹
    UNKNOWN = "unknown"                  # ä¸æ˜

@dataclass
class QueryAnalysisResult:
    """è³ªå•åˆ†æçµæœ"""
    intent: QueryIntent
    confidence: float
    target_entity: str              # å¯¾è±¡ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ï¼ˆä¼šç¤¾åã€äººåãªã©ï¼‰
    keywords: List[str]             # æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    sql_patterns: List[str]         # SQLæ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³
    embedding_fallback: bool        # Embeddingæ¤œç´¢ãŒå¿…è¦ã‹
    reasoning: str                  # åˆ¤å®šç†ç”±

@dataclass
class SearchResult:
    """æ¤œç´¢çµæœ"""
    chunk_id: str
    document_id: str
    document_name: str
    content: str
    score: float
    search_method: str
    metadata: Dict = None

class GeminiQuestionAnalyzer:
    """Gemini 2.5 Flashè³ªå•åˆ†è§£ãƒ»åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.db_url = self._get_db_url()
        self.gemini_model = self._setup_gemini()
        
        # SQLæ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.sql_templates = {
            "exact_match": "content ILIKE '%{keyword}%'",
            "company_representative": "content ~* '{company}.*ä»£è¡¨è€…|ä»£è¡¨è€….*{company}'",
            "contact_info": "content ~* '{entity}.*(é€£çµ¡å…ˆ|é›»è©±|ãƒ¡ãƒ¼ãƒ«|ä½æ‰€)'",
            "multiple_keywords": " AND ".join(["content ILIKE '%{keyword}%'" for keyword in ["{keyword1}", "{keyword2}"]]),
            "regex_pattern": "content ~* '{pattern}'"
        }
        
        logger.info("âœ… Geminiè³ªå•åˆ†è§£ãƒ»åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def _get_db_url(self) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã‚’æ§‹ç¯‰"""
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        if "supabase.co" in supabase_url:
            project_id = supabase_url.split("://")[1].split(".")[0]
            return f"postgresql://postgres.{project_id}:{os.getenv('DB_PASSWORD', '')}@aws-0-ap-northeast-1.pooler.supabase.com:6543/postgres"
        else:
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                raise ValueError("DATABASE_URL ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return db_url
    
    def _setup_gemini(self):
        """Gemini 2.5 Flashãƒ¢ãƒ‡ãƒ«ã®è¨­å®š"""
        try:
            import google.generativeai as genai
            
            api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
            if not api_key:
                raise ValueError("GEMINI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-2.5-flash')
            return model
        except Exception as e:
            logger.error(f"âŒ Geminiè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    async def analyze_question(self, question: str) -> QueryAnalysisResult:
        """
        ğŸ§  Gemini 2.5 Flashã‚’ä½¿ã£ã¦è³ªå•ã‚’åˆ†è§£ãƒ»åˆ†é¡
        
        Args:
            question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            
        Returns:
            QueryAnalysisResult: åˆ†æçµæœ
        """
        logger.info(f"ğŸ§  Geminiè³ªå•åˆ†æé–‹å§‹: '{question}'")
        
        if not self.gemini_model:
            logger.warning("âš ï¸ Geminiãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æã‚’å®Ÿè¡Œ")
            fallback_result = self._fallback_analysis(question)
            return await self._append_variants(question, fallback_result)
        
        try:
            # æ”¹å–„ã•ã‚ŒãŸGemini 2.5 Flashãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            prompt = f"""
è³ªå•ã‚’åˆ†æã—ã¦ã€æ¤œç´¢ã«æœ€é©åŒ–ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

è³ªå•: ã€Œ{question}ã€

åˆ†æé …ç›®:
1. æ„å›³ (intent): specific_info, general_info, comparison, explanation, procedure, unknown ã®ã„ãšã‚Œã‹
2. ä¿¡é ¼åº¦ (confidence): 0.0-1.0ã®æ•°å€¤
3. å¯¾è±¡ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ (target_entity): ä¼šç¤¾åã€äººåã€ã‚µãƒ¼ãƒ“ã‚¹åãªã©ï¼ˆè³ªå•ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹ï¼‰
4. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (keywords): æ¤œç´¢ã«æœ€é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆï¼ˆä»¥ä¸‹ã®æŒ‡é‡ã«å¾“ã†ï¼‰

ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã®æŒ‡é‡ã€‘
- é›»è©±ç•ªå·ãŒå«ã¾ã‚Œã‚‹å ´åˆ: é›»è©±ç•ªå·ãã®ã‚‚ã®ã‚’æœ€é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã—ã¦å«ã‚ã‚‹
- ä¼æ¥­åã‚’æ±‚ã‚ã‚‹å ´åˆ: ã€Œç¤¾åã€ã€Œä¼šç¤¾åã€ã€Œä¼æ¥­åã€ã€Œæ ªå¼ä¼šç¤¾ã€ã€Œæœ‰é™ä¼šç¤¾ã€ã€ŒåˆåŒä¼šç¤¾ã€ã‚’å«ã‚ã‚‹
- é€£çµ¡å…ˆã‚’æ±‚ã‚ã‚‹å ´åˆ: ã€Œé›»è©±ã€ã€ŒTELã€ã€Œé€£çµ¡å…ˆã€ã€Œä½æ‰€ã€ã€Œãƒ¡ãƒ¼ãƒ«ã€ã‚’å«ã‚ã‚‹
- ä»£è¡¨è€…ã‚’æ±‚ã‚ã‚‹å ´åˆ: ã€Œä»£è¡¨è€…ã€ã€Œç¤¾é•·ã€ã€Œä»£è¡¨å–ç· å½¹ã€ã€Œè²¬ä»»è€…ã€ã€Œãƒˆãƒƒãƒ—ã€ã€ŒCEOã€ã€Œãƒªãƒ¼ãƒ€ãƒ¼ã€ã€ŒçµŒå–¶è€…ã€ã€Œã‚ªãƒ¼ãƒŠãƒ¼ã€ã‚’å«ã‚ã‚‹
- å…·ä½“çš„ãªå›ºæœ‰åè©ï¼ˆé›»è©±ç•ªå·ã€ä¼šç¤¾åã€äººåãªã©ï¼‰ã¯å¿…ãšå«ã‚ã‚‹
- ä¸€èˆ¬çš„ãªåŠ©è©ï¼ˆã®ã€ã‚’ã€ã¯ã€ãŒã€ã«ã€ã§ã€ã¨ã€ã‹ã‚‰ã€ã¾ã§ï¼‰ã¯é™¤å¤–ã™ã‚‹

5. åˆ¤å®šç†ç”± (reasoning): åˆ¤å®šã®æ ¹æ‹ 

å›ç­”ä¾‹1ï¼ˆé›»è©±ç•ªå·ã‹ã‚‰ä¼æ¥­åã‚’æ¢ã™å ´åˆï¼‰:
{{
  "intent": "specific_info",
  "confidence": 0.95,
  "target_entity": "ä¼æ¥­å",
  "keywords": ["053-442-6707", "é›»è©±ç•ªå·", "ç¤¾å", "ä¼šç¤¾å", "ä¼æ¥­å", "æ ªå¼ä¼šç¤¾"],
  "reasoning": "ç‰¹å®šã®é›»è©±ç•ªå·ã‹ã‚‰ä¼æ¥­åã‚’ç‰¹å®šã™ã‚‹å…·ä½“çš„æƒ…å ±ã®è³ªå•"
}}

å›ç­”ä¾‹2ï¼ˆä»£è¡¨è€…åã‚’æ¢ã™å ´åˆï¼‰:
{{
  "intent": "specific_info",
  "confidence": 0.90,
  "target_entity": "ä»£è¡¨è€…å",
  "keywords": ["ABCæ ªå¼ä¼šç¤¾", "ä»£è¡¨è€…", "ç¤¾é•·", "ä»£è¡¨å–ç· å½¹", "è²¬ä»»è€…", "ãƒˆãƒƒãƒ—", "CEO", "ãƒªãƒ¼ãƒ€ãƒ¼"],
  "reasoning": "ç‰¹å®šä¼æ¥­ã®ä»£è¡¨è€…åã‚’æ±‚ã‚ã‚‹å…·ä½“çš„æƒ…å ±ã®è³ªå•"
}}

JSONå½¢å¼ã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
"""
            
            # Gemini 2.5 Flashã§åˆ†æå®Ÿè¡Œ
            import google.generativeai as genai
            response = self.gemini_model.generate_content(
                prompt,
                generation_config=genai.GenerationConfig(
                    temperature=0.1,  # ä¸€è²«æ€§é‡è¦–
                    max_output_tokens=1048576,  # 1Mãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå®Ÿè³ªç„¡åˆ¶é™ï¼‰
                    top_p=0.8,
                    top_k=50
                )
            )
            
            if not response or not response.candidates:
                logger.warning("âš ï¸ Geminiã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã™")
                fallback_result = self._fallback_analysis(question)
                return await self._append_variants(question, fallback_result)
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            try:
                extracted_text = ""
                for part in response.candidates[0].content.parts:
                    if hasattr(part, 'text'):
                        extracted_text += part.text
                
                if not extracted_text:
                    logger.warning("âš ï¸ Geminiå¿œç­”ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    fallback_result = self._fallback_analysis(question)
                    return await self._append_variants(question, fallback_result)
                
                logger.info("âœ… Geminiå¿œç­”ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
                analysis_data = json.loads(extracted_text.strip())
                logger.info("âœ… JSONã‚’æ­£å¸¸ã«è§£æã—ã¾ã—ãŸã€‚")
            except json.JSONDecodeError:
                # JSONã§ãªã„å ´åˆã¯ã€Markdownã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã®JSONã‚’æŠ½å‡º
                json_match = re.search(r'```json\n(.*?)```', extracted_text, re.DOTALL)
                if json_match:
                    try:
                        analysis_data = json.loads(json_match.group(1).strip())
                        logger.info("âœ… Markdownã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã®JSONã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
                        logger.info("âœ… JSONã‚’æ­£å¸¸ã«è§£æã—ã¾ã—ãŸã€‚")
                    except json.JSONDecodeError as e:
                        logger.warning(f"âš ï¸ æŠ½å‡ºã•ã‚ŒãŸJSONã®è§£æã«å¤±æ•—: {e}")
                        fallback_result = self._fallback_analysis(question)
                        return await self._append_variants(question, fallback_result)
                else:
                    logger.warning("âš ï¸ Geminiå¿œç­”ã‹ã‚‰JSONã‚’æŠ½å‡ºã§ãã¾ã›ã‚“")
                    fallback_result = self._fallback_analysis(question)
                    return await self._append_variants(question, fallback_result)
            
            # çµæœã®æ§‹ç¯‰
            intent = QueryIntent(analysis_data.get("intent", "unknown"))
            confidence = float(analysis_data.get("confidence", 0.5))
            target_entity = analysis_data.get("target_entity", "")
            keywords = analysis_data.get("keywords", [])
            reasoning = analysis_data.get("reasoning", "Geminiåˆ†æçµæœ")
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å¾Œå‡¦ç†ï¼ˆé‡è¤‡é™¤å»ã€ç©ºæ–‡å­—é™¤å»ã€åŒç¾©èªæ­£è¦åŒ–ï¼‰
            keywords = [k.strip() for k in keywords if k and k.strip()]
            keywords = list(dict.fromkeys(keywords))  # é †åºã‚’ä¿ã¡ãªãŒã‚‰é‡è¤‡é™¤å»
            
            # ğŸ”¥ åŒç¾©èªæ­£è¦åŒ–ã‚’ç„¡åŠ¹åŒ–ï¼ˆORæ¤œç´¢ã§å¯¾å¿œï¼‰
            # keywords = self._normalize_synonyms(keywords)
            
            # SQLæ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç”Ÿæˆ
            sql_patterns = self._generate_sql_patterns(intent, target_entity, keywords)
            
            # Embeddingæ¤œç´¢ãŒå¿…è¦ã‹ã®åˆ¤å®š
            embedding_fallback = intent in [QueryIntent.GENERAL_INFO, QueryIntent.EXPLANATION, QueryIntent.COMPARISON]
            
            result = QueryAnalysisResult(
                intent=intent,
                confidence=confidence,
                target_entity=target_entity,
                keywords=keywords,
                sql_patterns=sql_patterns,
                embedding_fallback=embedding_fallback,
                reasoning=reasoning
            )
            
            # ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
            result = await self._append_variants(question, result)
            
            logger.info(f"âœ… Geminiåˆ†æå®Œäº†: {intent.value} (ä¿¡é ¼åº¦: {confidence:.2f}) | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°: {len(result.keywords)}")
            logger.info(f"ğŸ¯ å¯¾è±¡ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£: {target_entity}")
            logger.info(f"ğŸ·ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {result.keywords}")
            logger.info(f"ğŸ’­ åˆ¤å®šç†ç”±: {reasoning}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Geminiåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            fallback_result = self._fallback_analysis(question)
            return await self._append_variants(question, fallback_result)
    
    def _fallback_analysis(self, question: str) -> QueryAnalysisResult:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æï¼ˆGeminiãŒåˆ©ç”¨ã§ããªã„å ´åˆï¼‰"""
        logger.info("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æå®Ÿè¡Œä¸­...")
        
        # ç°¡å˜ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        question_lower = question.lower()
        
        # æ„å›³ã®åˆ¤å®š
        if any(word in question_lower for word in ['ä»£è¡¨è€…', 'ç¤¾é•·', 'ãƒˆãƒƒãƒ—', 'ceo', 'é€£çµ¡å…ˆ', 'é›»è©±', 'ä½æ‰€', 'æ–™é‡‘', 'ä¾¡æ ¼']):
            intent = QueryIntent.SPECIFIC_INFO
            confidence = 0.8
        elif any(word in question_lower for word in ['ãªãœ', 'ã©ã†', 'ç†ç”±', 'èƒŒæ™¯']):
            intent = QueryIntent.EXPLANATION
            confidence = 0.6
        elif any(word in question_lower for word in ['æ‰‹é †', 'æ–¹æ³•', 'ã‚„ã‚Šæ–¹']):
            intent = QueryIntent.PROCEDURE
            confidence = 0.6
        else:
            intent = QueryIntent.GENERAL_INFO
            confidence = 0.5
        
        # æ”¹å–„ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        keywords = []
        
        # é›»è©±ç•ªå·ã®æ¤œå‡ºã¨è¿½åŠ 
        phone_patterns = [
            r'\d{2,4}-\d{2,4}-\d{4}',  # 03-1234-5678
            r'\d{3}-\d{3}-\d{4}',      # 090-123-4567
            r'\(\d{2,4}\)\s*\d{2,4}-\d{4}',  # (03) 1234-5678
            r'\d{10,11}'               # 01234567890
        ]
        
        for pattern in phone_patterns:
            phone_matches = re.findall(pattern, question)
            for phone in phone_matches:
                keywords.append(phone)
        
        # ä¼šç¤¾åã®æ¤œå‡º
        company_patterns = [
            r'([^ã€‚ã€\s]+(?:æ ªå¼ä¼šç¤¾|åˆåŒä¼šç¤¾|æœ‰é™ä¼šç¤¾))', 
            r'([^ã€‚ã€\s]+ä¼šç¤¾)',
            r'([^ã€‚ã€\s]+(?:Corporation|Corp|Inc|LLC))'
        ]
        target_entity = ""
        for pattern in company_patterns:
            match = re.search(pattern, question)
            if match:
                target_entity = match.group(1)
                keywords.append(target_entity)
                break
        
        # åŸºæœ¬çš„ãªå˜èªåˆ†å‰²ã«ã‚ˆã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        exclude_words = ['ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã§', 'ã¨', 'ã‹ã‚‰', 'ã¾ã§', 'ã§ã™', 'ã¾ã™', 'ã ', 'ã§ã‚ã‚‹', 'ã“ã‚Œ', 'ãã‚Œ', 'ã‚ã®', 'ãã®', 'ã“ã®']
        for word in question.split():
            clean_word = re.sub(r'[ã€‚ã€ï¼ï¼Ÿ]', '', word)
            if len(clean_word) > 1 and clean_word not in exclude_words:
                keywords.append(clean_word)
        
        # è³ªå•ã®æ„å›³ã«åŸºã¥ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¿½åŠ 
        if 'ä¼æ¥­å' in question or 'ä¼šç¤¾å' in question or 'ç¤¾å' in question:
            keywords.extend(['ä¼æ¥­å', 'ä¼šç¤¾å', 'ç¤¾å', 'æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾'])
            target_entity = "ä¼æ¥­å"
            intent = QueryIntent.SPECIFIC_INFO
            confidence = 0.8
        
        if any(word in question for word in ['ä»£è¡¨è€…', 'ç¤¾é•·', 'ãƒˆãƒƒãƒ—', 'CEO', 'ãƒªãƒ¼ãƒ€ãƒ¼', 'çµŒå–¶è€…', 'ã‚ªãƒ¼ãƒŠãƒ¼']):
            keywords.extend(['ä»£è¡¨è€…', 'ç¤¾é•·', 'ä»£è¡¨å–ç· å½¹', 'è²¬ä»»è€…', 'ãƒˆãƒƒãƒ—', 'CEO', 'ãƒªãƒ¼ãƒ€ãƒ¼', 'çµŒå–¶è€…', 'ã‚ªãƒ¼ãƒŠãƒ¼'])
            target_entity = "ä»£è¡¨è€…"
            intent = QueryIntent.SPECIFIC_INFO
            confidence = 0.8
        
        if 'é›»è©±' in question or 'TEL' in question or 'é€£çµ¡å…ˆ' in question:
            keywords.extend(['é›»è©±', 'TEL', 'é€£çµ¡å…ˆ', 'é›»è©±ç•ªå·'])
            intent = QueryIntent.SPECIFIC_INFO
            confidence = 0.8
        
        # é‡è¤‡é™¤å»ã¨ç©ºæ–‡å­—é™¤å»
        keywords = [k.strip() for k in keywords if k and k.strip()]
        keywords = list(dict.fromkeys(keywords))
        
        # ğŸ”¥ åŒç¾©èªæ­£è¦åŒ–ã‚’ç„¡åŠ¹åŒ–ï¼ˆORæ¤œç´¢ã§å¯¾å¿œï¼‰
        # keywords = self._normalize_synonyms(keywords)
        
        # å¯¾è±¡ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®æ¨æ¸¬ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ï¼‰
        if not target_entity:
            if any(k in keywords for k in ['ä¼æ¥­å', 'ä¼šç¤¾å', 'ç¤¾å']):
                target_entity = "ä¼æ¥­å"
            elif any(k in keywords for k in ['ä»£è¡¨è€…', 'ç¤¾é•·']):
                target_entity = "ä»£è¡¨è€…"
            elif any(k in keywords for k in ['é›»è©±', 'TEL', 'é€£çµ¡å…ˆ']):
                target_entity = "é€£çµ¡å…ˆ"
        
        sql_patterns = self._generate_sql_patterns(intent, target_entity, keywords)
        embedding_fallback = intent in [QueryIntent.GENERAL_INFO, QueryIntent.EXPLANATION]
        
        result = QueryAnalysisResult(
            intent=intent,
            confidence=confidence,
            target_entity=target_entity,
            keywords=keywords,
            sql_patterns=sql_patterns,
            embedding_fallback=embedding_fallback,
            reasoning="ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æã«ã‚ˆã‚‹åˆ¤å®šï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼‰"
        )
        
        logger.info(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æå®Œäº†: {len(keywords)}å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º")
        logger.info(f"ğŸ·ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords}")
        
        return result
    
    def _normalize_synonyms(self, keywords: List[str]) -> List[str]:
        """
        åŒç¾©èªæ­£è¦åŒ–: æ¤œç´¢ç²¾åº¦ã‚’ä¸Šã’ã‚‹ãŸã‚ã€åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ã®ä¸­ã§æœ€ã‚‚æ¤œç´¢ã—ã‚„ã™ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã‚’æ®‹ã™
        """
        # åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ã®å®šç¾©ï¼ˆæœ€åˆã®è¦ç´ ãŒå„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰
        synonym_groups = [
            # å½¹è·é–¢é€£
            ['ä»£è¡¨è€…', 'ãƒˆãƒƒãƒ—', 'CEO', 'ceo', 'ãƒªãƒ¼ãƒ€ãƒ¼', 'çµŒå–¶è€…', 'ã‚ªãƒ¼ãƒŠãƒ¼', 'ç¤¾é•·', 'ä»£è¡¨å–ç· å½¹', 'è²¬ä»»è€…'],
            # ä¼šç¤¾é–¢é€£
            ['ä¼šç¤¾', 'ä¼æ¥­', 'æ³•äºº', 'äº‹æ¥­è€…', 'çµ„ç¹”'],
            # é€£çµ¡å…ˆé–¢é€£
            ['é›»è©±ç•ªå·', 'TEL', 'Tel', 'tel', 'ï¼´ï¼¥ï¼¬', 'é›»è©±'],
            # ä½æ‰€é–¢é€£
            ['ä½æ‰€', 'æ‰€åœ¨åœ°', 'å ´æ‰€', 'ä½ç½®', 'ã‚¢ãƒ‰ãƒ¬ã‚¹'],
            # è³ªå•é–¢é€£
            ['æ•™ãˆã¦', 'çŸ¥ã‚ŠãŸã„', 'èããŸã„', 'åˆ†ã‹ã‚‰ãªã„'],
        ]
        
        normalized_keywords = keywords.copy()
        
        for group in synonym_groups:
            priority_keyword = group[0]  # ã‚°ãƒ«ãƒ¼ãƒ—ã®æœ€åˆãŒå„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            found_keywords = [k for k in normalized_keywords if k in group]
            
            if len(found_keywords) > 1:
                # è¤‡æ•°ã®åŒç¾©èªãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€å„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä»¥å¤–ã‚’é™¤å»
                for keyword in found_keywords:
                    if keyword != priority_keyword:
                        try:
                            normalized_keywords.remove(keyword)
                            logger.info(f"ğŸ”„ åŒç¾©èªæ­£è¦åŒ–: '{keyword}' â†’ '{priority_keyword}'")
                        except ValueError:
                            pass  # æ—¢ã«é™¤å»æ¸ˆã¿
        
        logger.info(f"âœ… åŒç¾©èªæ­£è¦åŒ–å®Œäº†: {len(keywords)}å€‹ â†’ {len(normalized_keywords)}å€‹")
        return normalized_keywords
    
    def _classify_keywords(self, keywords: List[str]) -> Tuple[List[str], Dict[str, List[str]]]:
        """
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å›ºæœ‰åè©ï¼ˆå¿…é ˆï¼‰ã¨åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆé¸æŠï¼‰ã«åˆ†é¡
        
        Returns:
            Tuple[å¿…é ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰, åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—è¾æ›¸]
        """
        # åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ã®å®šç¾©
        synonym_groups_def = {
            'position': ['ä»£è¡¨è€…', 'ãƒˆãƒƒãƒ—', 'CEO', 'ceo', 'ãƒªãƒ¼ãƒ€ãƒ¼', 'çµŒå–¶è€…', 'ã‚ªãƒ¼ãƒŠãƒ¼', 'ç¤¾é•·', 'ä»£è¡¨å–ç· å½¹', 'è²¬ä»»è€…'],
            'company_type': ['ä¼šç¤¾', 'ä¼æ¥­', 'æ³•äºº', 'äº‹æ¥­è€…', 'çµ„ç¹”', 'æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾', 'åˆåŒä¼šç¤¾'],
            'contact': ['é›»è©±ç•ªå·', 'TEL', 'Tel', 'tel', 'ï¼´ï¼¥ï¼¬', 'é›»è©±', 'é€£çµ¡å…ˆ'],
            'location': ['ä½æ‰€', 'æ‰€åœ¨åœ°', 'å ´æ‰€', 'ä½ç½®', 'ã‚¢ãƒ‰ãƒ¬ã‚¹'],
            'question_words': ['æ•™ãˆã¦', 'çŸ¥ã‚ŠãŸã„', 'èããŸã„', 'åˆ†ã‹ã‚‰ãªã„'],
        }
        
        required_keywords = []
        found_synonym_groups = {}
        
        for keyword in keywords:
            is_synonym = False
            
            # åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ã«å±ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            for group_name, group_words in synonym_groups_def.items():
                if keyword in group_words:
                    if group_name not in found_synonym_groups:
                        found_synonym_groups[group_name] = []
                    found_synonym_groups[group_name].append(keyword)
                    is_synonym = True
                    break
            
            # åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ã«å±ã•ãªã„å ´åˆã¯å›ºæœ‰åè©ã¨ã—ã¦æ‰±ã†
            if not is_synonym:
                # é›»è©±ç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚§ãƒƒã‚¯
                phone_patterns = [
                    r'\d{2,4}-\d{2,4}-\d{4}',
                    r'\d{3}-\d{3}-\d{4}',
                    r'\(\d{2,4}\)\s*\d{2,4}-\d{4}',
                    r'\d{10,11}'
                ]
                
                is_phone = any(re.match(pattern, keyword) for pattern in phone_patterns)
                
                # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚§ãƒƒã‚¯
                is_email = re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', keyword)
                
                # å›ºæœ‰åè©ã¨ã—ã¦åˆ†é¡ï¼ˆä¼šç¤¾åã€äººåã€é›»è©±ç•ªå·ã€ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãªã©ï¼‰
                if (len(keyword) >= 2 and not keyword in ['ã§ã™', 'ã¾ã™', 'ã¦ã„ã‚‹', 'ã ', 'ã§ã‚ã‚‹']) or is_phone or is_email:
                    required_keywords.append(keyword)
        
        logger.info(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†é¡çµæœ:")
        logger.info(f"   å›ºæœ‰åè©ï¼ˆå¿…é ˆï¼‰: {required_keywords}")
        logger.info(f"   åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆé¸æŠï¼‰: {found_synonym_groups}")
        
        return required_keywords, found_synonym_groups
    
    def _generate_sql_patterns(self, intent: QueryIntent, target_entity: str, keywords: List[str]) -> List[str]:
        """SQLæ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç”Ÿæˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰"""
        # æ–°ã—ã„å®Ÿè£…ã§ã¯ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’ãã®ã¾ã¾è¿”ã™
        # å®Ÿéš›ã®SQLæ§‹ç¯‰ã¯ execute_sql_search ã§è¡Œã†
        return keywords
    
    async def execute_sql_search(self, analysis: QueryAnalysisResult, company_id: str = None, limit: int = 20) -> List[SearchResult]:
        """
        ğŸ” SQLãƒ™ãƒ¼ã‚¹ã®æ§‹é€ çš„æ¤œç´¢ï¼ˆã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°å¼·åŒ–ç‰ˆï¼‰
        Geminiåˆ†æçµæœã«åŸºã¥ã„ã¦æœ€é©ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã—ã€ã‚ˆã‚Šæ­£ç¢ºãªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚’é©ç”¨
        """
        logger.info(f"ğŸ” SQLæ§‹é€ çš„æ¤œç´¢é–‹å§‹: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰={analysis.keywords}")
        
        try:
            all_results = []
            
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    # ğŸ¯ ãƒ‘ã‚¿ãƒ¼ãƒ³0: æ•…éšœå—ä»˜å°‚ç”¨ã®ç‰¹åˆ¥æ¤œç´¢
                    if 'æ•…éšœå—ä»˜' in analysis.keywords and ('ã‚·ãƒ¼ãƒˆ' in analysis.keywords or 'åç§°' in analysis.keywords):
                        logger.info("ğŸ¯ æ•…éšœå—ä»˜ã‚·ãƒ¼ãƒˆå°‚ç”¨æ¤œç´¢ã‚’å®Ÿè¡Œ")
                        
                        # æ•…éšœå—ä»˜ã‚·ãƒ¼ãƒˆã‚’ç›´æ¥æ¤œç´¢
                        direct_sql = """
                        SELECT DISTINCT
                            c.id as chunk_id,
                            c.doc_id as document_id,
                            c.chunk_index,
                            c.content as snippet,
                            ds.name as document_name,
                            ds.type as document_type,
                            3.0 as score  -- é«˜å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢
                        FROM chunks c
                        LEFT JOIN document_sources ds ON ds.id = c.doc_id
                        WHERE c.content IS NOT NULL
                          AND LENGTH(c.content) > 10
                          AND c.content LIKE '%æ•…éšœå—ä»˜ã‚·ãƒ¼ãƒˆ%'
                          AND ds.active = true
                        ORDER BY score DESC
                        LIMIT 5
                        """
                        
                        cur.execute(direct_sql)
                        direct_results = cur.fetchall()
                        
                        if direct_results:
                            logger.info(f"âœ… æ•…éšœå—ä»˜ã‚·ãƒ¼ãƒˆå°‚ç”¨æ¤œç´¢ã§{len(direct_results)}ä»¶ç™ºè¦‹")
                            
                            for row in direct_results:
                                enhanced_score = self._calculate_enhanced_score(
                                    content=row['snippet'] or '',
                                    keywords=analysis.keywords,
                                    required_keywords=['æ•…éšœå—ä»˜', 'ã‚·ãƒ¼ãƒˆ'],
                                    base_score=row['score']
                                )
                                
                                all_results.append(SearchResult(
                                    chunk_id=row['chunk_id'],
                                    document_id=row['document_id'],
                                    document_name=row['document_name'] or 'Unknown',
                                    content=row['snippet'] or '',
                                    score=enhanced_score,
                                    search_method='failure_sheet_direct_search',
                                    metadata={
                                        'special_search': True,
                                        'pattern': 'failure_sheet',
                                        'original_score': row['score'],
                                        'enhanced_score': enhanced_score
                                    }
                                ))
                    
                    # ğŸ¯ ãƒ‘ã‚¿ãƒ¼ãƒ³1: ã‚¹ãƒãƒ¼ãƒˆæ¤œç´¢ï¼ˆå›ºæœ‰åè©AND + åŒç¾©èªORï¼‰
                    required_keywords, synonym_groups = self._classify_keywords(analysis.keywords)
                    
                    if required_keywords or synonym_groups:
                        logger.info(f"ğŸ” ã‚¹ãƒãƒ¼ãƒˆæ¤œç´¢ï¼ˆå›ºæœ‰åè©AND + åŒç¾©èªORï¼‰: {analysis.keywords}")
                        logger.info(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†é¡çµæœ:")
                        logger.info(f"   å›ºæœ‰åè©ï¼ˆå¿…é ˆï¼‰: {required_keywords}")
                        logger.info(f"   åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆé¸æŠï¼‰: {synonym_groups}")
                        
                        results = await self._execute_smart_search(cur, required_keywords, synonym_groups, limit * 3)  # ã‚ˆã‚Šå¤šãã®çµæœã‚’å–å¾—
                        
                        if results:
                            logger.info(f"âœ… ã‚¹ãƒãƒ¼ãƒˆæ¤œç´¢ã§{len(results)}ä»¶ã®çµæœ")
                            logger.info(f"   å¿…é ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {required_keywords}")
                            logger.info(f"   åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—: {list(synonym_groups.keys())}")
                            
                            # ğŸ¯ ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°å¼·åŒ–ï¼šé–¢é€£åº¦è¨ˆç®—
                            for row in results:
                                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                                if not any(r.chunk_id == row['chunk_id'] for r in all_results):
                                    # ğŸ¯ å¼·åŒ–ã•ã‚ŒãŸã‚¹ã‚³ã‚¢è¨ˆç®—
                                    enhanced_score = self._calculate_enhanced_score(
                                        content=row['snippet'] or '',
                                        keywords=analysis.keywords,
                                        required_keywords=required_keywords,
                                        base_score=row['score']
                                    )
                                    
                                    all_results.append(SearchResult(
                                        chunk_id=row['chunk_id'],
                                        document_id=row['document_id'],
                                        document_name=row['document_name'] or 'Unknown',
                                        content=row['snippet'] or '',
                                        score=enhanced_score,  # ğŸ¯ å¼·åŒ–ã•ã‚ŒãŸã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨
                                        search_method='sql_smart_search',
                                        metadata={
                                            'required_keywords': required_keywords,
                                            'synonym_groups': synonym_groups,
                                            'document_type': row.get('document_type', 'unknown'),
                                            'original_score': row['score'],
                                            'enhanced_score': enhanced_score
                                        }
                                    ))
                        else:
                            logger.info("âŒ ã‚¹ãƒãƒ¼ãƒˆæ¤œç´¢ã§çµæœãªã—")
                    
                    # ğŸ¯ ãƒ‘ã‚¿ãƒ¼ãƒ³2: éƒ¨åˆ†ãƒãƒƒãƒæ¤œç´¢ï¼ˆçµæœãŒå°‘ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                    if len(all_results) < 5:
                        logger.info("ğŸ”„ éƒ¨åˆ†ãƒãƒƒãƒæ¤œç´¢ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦å®Ÿè¡Œ")
                        partial_results = await self._execute_partial_match_search(cur, analysis.keywords, limit * 2)  # ã‚ˆã‚Šå¤šãã®çµæœã‚’å–å¾—
                        
                        for row in partial_results:
                            if not any(r.chunk_id == row['chunk_id'] for r in all_results):
                                enhanced_score = self._calculate_enhanced_score(
                                    content=row['snippet'] or '',
                                    keywords=analysis.keywords,
                                    required_keywords=analysis.keywords,  # éƒ¨åˆ†ãƒãƒƒãƒã§ã¯å…¨ã¦ã‚’å¿…é ˆæ‰±ã„
                                    base_score=float(row['score']) * 0.8  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãªã®ã§åŸºæœ¬ã‚¹ã‚³ã‚¢ã‚’0.8å€
                                )
                                
                                all_results.append(SearchResult(
                                    chunk_id=row['chunk_id'],
                                    document_id=row['document_id'],
                                    document_name=row['document_name'] or 'Unknown',
                                    content=row['snippet'] or '',
                                    score=enhanced_score,
                                    search_method='sql_partial_search',
                                    metadata={
                                        'keywords': analysis.keywords,
                                        'document_type': row.get('document_type', 'unknown'),
                                        'original_score': row['score'],
                                        'enhanced_score': enhanced_score
                                    }
                                ))
                    
                    # ğŸ¯ ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆï¼ˆé«˜ã„é †ï¼‰
                    all_results.sort(key=lambda x: x.score, reverse=True)
                    
                    # ä¸Šä½çµæœã®ã¿ã‚’è¿”ã™
                    final_results = all_results[:limit]
                    
                    logger.info(f"âœ… SQLæ§‹é€ çš„æ¤œç´¢å®Œäº†: {len(final_results)}ä»¶ã®çµæœ")
                    return final_results
                    
        except Exception as e:
            logger.error(f"âŒ SQLæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
    
    def _calculate_enhanced_score(self, content: str, keywords: List[str], required_keywords: List[str], base_score: float) -> float:
        """
        ğŸ¯ å¼·åŒ–ã•ã‚ŒãŸã‚¹ã‚³ã‚¢è¨ˆç®—
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾é »åº¦ã€è¿‘æ¥åº¦ã€å®Œå…¨ä¸€è‡´ãªã©ã‚’è€ƒæ…®ã—ãŸè©³ç´°ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        """
        if not content:
            return float(base_score)
        
        content_lower = content.lower()
        enhanced_score = float(base_score)  # ğŸ”§ decimal.Decimal â†’ floatå¤‰æ›
        
        # ğŸ¯ 1. å®Œå…¨ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹ï¼ˆæœ€é‡è¦ï¼‰
        exact_matches = 0
        for keyword in required_keywords:
            if keyword.lower() in content_lower:
                exact_matches += 1
                # è¤‡æ•°æ–‡å­—ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å®Œå…¨ä¸€è‡´ã¯é«˜å¾—ç‚¹
                if len(keyword) > 2:
                    enhanced_score += 0.3
                else:
                    enhanced_score += 0.1
        
        # ğŸ¯ 2. è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¿‘æ¥åº¦ãƒœãƒ¼ãƒŠã‚¹
        if len(required_keywords) >= 2:
            keyword_positions = []
            for keyword in required_keywords:
                pos = content_lower.find(keyword.lower())
                if pos >= 0:
                    keyword_positions.append(pos)
            
            if len(keyword_positions) >= 2:
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ã®è·é›¢ã‚’è¨ˆç®—
                keyword_positions.sort()
                max_distance = keyword_positions[-1] - keyword_positions[0]
                
                # è¿‘ã„è·é›¢ã«ã‚ã‚‹å ´åˆã¯ãƒœãƒ¼ãƒŠã‚¹
                if max_distance < 100:  # 100æ–‡å­—ä»¥å†…
                    proximity_bonus = 0.4 - (max_distance / 250)  # è·é›¢ã«å¿œã˜ã¦æ¸›ç‚¹
                    enhanced_score += max(proximity_bonus, 0)
        
        # ğŸ¯ 3. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯†åº¦ãƒœãƒ¼ãƒŠã‚¹
        total_keyword_count = 0
        for keyword in keywords:
            total_keyword_count += content_lower.count(keyword.lower())
        
        if len(content) > 0:
            density = total_keyword_count / len(content) * 1000  # 1000æ–‡å­—ã‚ãŸã‚Šã®å‡ºç¾å›æ•°
            density_bonus = min(density * 0.1, 0.3)  # æœ€å¤§0.3ã®ãƒœãƒ¼ãƒŠã‚¹
            enhanced_score += density_bonus
        
        # ğŸ¯ 4. ç‰¹å®šãƒ‘ã‚¿ãƒ¼ãƒ³ãƒœãƒ¼ãƒŠã‚¹
        # è³ªå•ã«ç›´æ¥ç­”ãˆã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        answer_patterns = [
            r'â‘ .*ã‚·ãƒ¼ãƒˆ.*\(.*\)',  # â‘ ã‚·ãƒ¼ãƒˆå (å½¢å¼) ãƒ‘ã‚¿ãƒ¼ãƒ³
            r'åç§°.*[ï¼š:].*',       # åç§°: ... ãƒ‘ã‚¿ãƒ¼ãƒ³  
            r'.*ã‚·ãƒ¼ãƒˆ.*è¨˜å…¥.*',    # ã‚·ãƒ¼ãƒˆè¨˜å…¥ãƒ‘ã‚¿ãƒ¼ãƒ³
            r'.*ãƒ•ãƒ­ãƒ¼.*â‘ .*',      # ãƒ•ãƒ­ãƒ¼â‘ ãƒ‘ã‚¿ãƒ¼ãƒ³
        ]
        
        for pattern in answer_patterns:
            if re.search(pattern, content):
                enhanced_score += 0.2
        
        # ğŸ¯ 5. è¶…é‡è¦ï¼šæ•…éšœå—ä»˜ã‚·ãƒ¼ãƒˆå®Œå…¨ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹
        if 'æ•…éšœå—ä»˜ã‚·ãƒ¼ãƒˆ' in content:
            enhanced_score += 1.0  # æœ€å„ªå…ˆã«ã™ã‚‹ãŸã‚å¤§å¹…ãƒœãƒ¼ãƒŠã‚¹
            if 'EXCEL' in content:
                enhanced_score += 0.5  # ã•ã‚‰ã«å½¢å¼ã‚‚ä¸€è‡´ã™ã‚Œã°è¿½åŠ ãƒœãƒ¼ãƒŠã‚¹
        
        # ğŸ¯ 6. æ–‡æ›¸ã‚¿ã‚¤ãƒ—ãƒœãƒ¼ãƒŠã‚¹ï¼ˆPDFãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚’å„ªé‡ï¼‰
        if 'ãƒãƒ‹ãƒ¥ã‚¢ãƒ«' in content or 'manual' in content_lower:
            enhanced_score += 0.1
        
        # ã‚¹ã‚³ã‚¢ã®ä¸Šé™ã‚’è¨­å®šï¼ˆæ•…éšœå—ä»˜ã‚·ãƒ¼ãƒˆå°‚ç”¨æ¤œç´¢ã«å¯¾å¿œï¼‰
        return min(enhanced_score, 6.0)  # å°‚ç”¨æ¤œç´¢ãƒœãƒ¼ãƒŠã‚¹ã‚’åæ˜ ã§ãã‚‹ä¸Šé™
    
    async def _execute_smart_search(self, cursor, required_keywords: List[str], synonym_groups: Dict[str, List[str]], limit: int) -> List[dict]:
        """
        ğŸ¯ ã‚¹ãƒãƒ¼ãƒˆæ¤œç´¢ã®å®Ÿè¡Œ
        å›ºæœ‰åè©ï¼ˆå¿…é ˆï¼‰+ åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆé¸æŠï¼‰ã®ãƒ­ã‚¸ãƒƒã‚¯
        """
        if not required_keywords and not synonym_groups:
            return []
        
        # WHEREå¥ã®æ§‹ç¯‰
        where_conditions = []
        params = []
        
        # 1. å›ºæœ‰åè©ã¯å¿…é ˆï¼ˆORï¼‰- ã„ãšã‚Œã‹ã®è¡¨è¨˜ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°è‰¯ã„
        if required_keywords:
            required_conditions = []
            for keyword in required_keywords:
                if any(char in keyword for char in ['-', '(', ')', '.']):
                    required_conditions.append("c.content ~* %s")
                    params.append(re.escape(keyword))
                else:
                    required_conditions.append("c.content ILIKE %s")
                    params.append(f"%{keyword}%")
            
            if required_conditions:
                where_conditions.append(f"({' OR '.join(required_conditions)})")
        
        # 2. åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ã¯è¿½åŠ æ¡ä»¶ã¨ã—ã¦ AND ã§çµåˆ
        for group_name, synonyms in synonym_groups.items():
            if synonyms:
                or_conditions = []
                for synonym in synonyms:
                    if any(char in synonym for char in ['-', '(', ')', '.']):
                        or_conditions.append("c.content ~* %s")
                        params.append(re.escape(synonym))
                    else:
                        or_conditions.append("c.content ILIKE %s")
                        params.append(f"%{synonym}%")
                
                if or_conditions:
                    where_conditions.append(f"({' OR '.join(or_conditions)})")
        
        if not where_conditions:
            return []
        
        # æœ€çµ‚çš„ãªWHEREå¥
        final_where = ' AND '.join(where_conditions)
        
        sql = f"""
        SELECT DISTINCT
            c.id as chunk_id,
            c.doc_id as document_id,
            c.chunk_index,
            c.content as snippet,
            ds.name as document_name,
            ds.type as document_type,
            1.0 as score
        FROM chunks c
        LEFT JOIN document_sources ds ON ds.id = c.doc_id
        WHERE c.content IS NOT NULL
          AND LENGTH(c.content) > 10
          AND ds.active = true
          AND {final_where}
        ORDER BY score DESC LIMIT %s
        """
        
        params.append(limit)
        
        try:
            cursor.execute(sql, params)
            results = cursor.fetchall()
            return results
        except Exception as e:
            logger.warning(f"âš ï¸ ã‚¹ãƒãƒ¼ãƒˆæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def _execute_partial_match_search(self, cursor, keywords: List[str], limit: int) -> List[dict]:
        """
        ğŸ”„ éƒ¨åˆ†ãƒãƒƒãƒæ¤œç´¢ã®å®Ÿè¡Œ
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ç·©ã„æ¤œç´¢æ¡ä»¶
        """
        if not keywords:
            return []
        
        # ã„ãšã‚Œã‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°è‰¯ã„ï¼ˆORæ¤œç´¢ï¼‰
        or_conditions = []
        params = []
        
        for keyword in keywords:
            if any(char in keyword for char in ['-', '(', ')', '.']):
                or_conditions.append("c.content ~* %s")
                params.append(re.escape(keyword))
            else:
                or_conditions.append("c.content ILIKE %s")
                params.append(f"%{keyword}%")
        
        if not or_conditions:
            return []
        
        final_where = ' OR '.join(or_conditions)
        
        sql = f"""
        SELECT DISTINCT
            c.id as chunk_id,
            c.doc_id as document_id,
            c.chunk_index,
            c.content as snippet,
            ds.name as document_name,
            ds.type as document_type,
            0.8 as score
        FROM chunks c
        LEFT JOIN document_sources ds ON ds.id = c.doc_id
        WHERE c.content IS NOT NULL
          AND LENGTH(c.content) > 10
          AND ds.active = true
          AND ({final_where})
        ORDER BY score DESC LIMIT %s
        """
        
        params.append(limit)
        
        try:
            cursor.execute(sql, params)
            results = cursor.fetchall()
            return results
        except Exception as e:
            logger.warning(f"âš ï¸ éƒ¨åˆ†ãƒãƒƒãƒæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def execute_embedding_search(self, question: str, company_id: str = None, limit: int = 10) -> List[SearchResult]:
        """
        ğŸ“˜ Embeddingæ¤œç´¢ã®å®Ÿè¡Œï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        
        Args:
            question: æ¤œç´¢ã‚¯ã‚¨ãƒª
            company_id: ä¼šç¤¾IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            limit: çµæœæ•°åˆ¶é™
            
        Returns:
            List[SearchResult]: æ¤œç´¢çµæœ
        """
        logger.info(f"ğŸ“˜ Embeddingæ¤œç´¢é–‹å§‹ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰: '{question}'")
        
        try:
            # Vertex AI Embeddingã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å–å¾—
            from .vertex_ai_embedding import get_vertex_ai_embedding_client, vertex_ai_embedding_available
            
            if not vertex_ai_embedding_available():
                logger.warning("âš ï¸ Vertex AI EmbeddingãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                return []
            
            vertex_client = get_vertex_ai_embedding_client()
            if not vertex_client:
                logger.warning("âš ï¸ Vertex AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒå–å¾—ã§ãã¾ã›ã‚“")
                return []
            
            # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            query_embedding = vertex_client.generate_embedding(question)
            if not query_embedding:
                logger.warning("âš ï¸ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã«å¤±æ•—")
                return []
            
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    # pgvectorã‚’ä½¿ç”¨ã—ãŸãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
                    # Convert Python list to PostgreSQL vector format
                    vector_str = '[' + ','.join(map(str, query_embedding)) + ']'
                    
                    sql = f"""
                    SELECT DISTINCT
                        c.id as chunk_id,
                        c.doc_id as document_id,
                        c.chunk_index,
                        c.content as snippet,
                        ds.name as document_name,
                        ds.type as document_type,
                        (1 - (c.embedding <=> '{vector_str}'::vector)) as score
                    FROM chunks c
                    LEFT JOIN document_sources ds ON ds.id = c.doc_id
                    WHERE c.content IS NOT NULL
                      AND c.embedding IS NOT NULL
                      AND LENGTH(c.content) > 10
                      AND ds.active = true
                    """
                    
                    params = []
                    
                    if company_id:
                        sql += " AND c.company_id = %s"
                        params.append(company_id)
                    
                    sql += " ORDER BY score DESC LIMIT %s"
                    params.append(limit)
                    
                    cur.execute(sql, params)
                    results = cur.fetchall()
                    
                    search_results = []
                    for row in results:
                        search_results.append(SearchResult(
                            chunk_id=row['chunk_id'],
                            document_id=row['document_id'],
                            document_name=row['document_name'] or 'Unknown',
                            content=row['snippet'] or '',
                            score=row['score'],
                            search_method='embedding_fallback',
                            metadata={'similarity': row['score']}
                        ))
                    
                    logger.info(f"âœ… Embeddingæ¤œç´¢å®Œäº†: {len(search_results)}ä»¶ã®çµæœ")
                    return search_results
                    
        except Exception as e:
            logger.error(f"âŒ Embeddingæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def intelligent_search(self, question: str, company_id: str = None, limit: int = 20) -> Tuple[List[SearchResult], QueryAnalysisResult]:
        """
        ğŸš€ ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæ¤œç´¢ã®å®Ÿè¡Œ
        1. Gemini 2.5 Flashã§è³ªå•åˆ†è§£ãƒ»åˆ†é¡
        2. SQLæ§‹é€ çš„æ¤œç´¢
        3. çµæœãŒã‚¼ãƒ­ä»¶ãªã‚‰Embeddingæ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        Args:
            question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            company_id: ä¼šç¤¾IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            limit: çµæœæ•°åˆ¶é™
            
        Returns:
            Tuple[æ¤œç´¢çµæœãƒªã‚¹ãƒˆ, è³ªå•åˆ†æçµæœ]
        """
        logger.info(f"ğŸš€ ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæ¤œç´¢é–‹å§‹: '{question}'")
        
        # 1. Gemini 2.5 Flashã§è³ªå•åˆ†è§£ãƒ»åˆ†æ
        analysis = await self.analyze_question(question)
        
        # 2. SQLæ§‹é€ çš„æ¤œç´¢ã®å®Ÿè¡Œ
        sql_results = await self.execute_sql_search(analysis, company_id, limit)
        
        # 3. çµæœãŒã‚¼ãƒ­ä»¶ãªã‚‰Embeddingæ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not sql_results and analysis.embedding_fallback:
            logger.info("ğŸ”„ SQLæ¤œç´¢çµæœãŒã‚¼ãƒ­ä»¶ã®ãŸã‚ã€Embeddingæ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            embedding_results = await self.execute_embedding_search(question, company_id, limit)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœã‚’ãƒãƒ¼ã‚¯
            for result in embedding_results:
                result.search_method = "embedding_fallback"
            
            final_results = embedding_results
        else:
            final_results = sql_results
        
        # çµæœã®ãƒ­ã‚°å‡ºåŠ›
        self._log_search_results(question, analysis, final_results)
        
        logger.info(f"âœ… ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæ¤œç´¢å®Œäº†: {len(final_results)}ä»¶ã®çµæœ")
        return final_results, analysis
    
    def _log_search_results(self, question: str, analysis: QueryAnalysisResult, results: List[SearchResult]):
        """æ¤œç´¢çµæœã®è©³ç´°ãƒ­ã‚°å‡ºåŠ›"""
        logger.info("="*100)
        logger.info(f"ğŸ” ã€ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæ¤œç´¢çµæœã€‘{analysis.intent.value} - ã‚¯ã‚¨ãƒª: '{question}'")
        logger.info("="*100)
        logger.info(f"ğŸ§  è³ªå•æ„å›³: {analysis.intent.value}")
        logger.info(f"ğŸ“Š ä¿¡é ¼åº¦: {analysis.confidence:.3f}")
        logger.info(f"ğŸ¯ å¯¾è±¡ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£: {analysis.target_entity}")
        logger.info(f"ğŸ·ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {analysis.keywords}")
        logger.info(f"ğŸ’­ åˆ¤å®šç†ç”±: {analysis.reasoning}")
        logger.info(f"ğŸ”§ SQLæ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(analysis.sql_patterns)}")
        logger.info(f"ğŸ“ˆ é¸å®šãƒãƒ£ãƒ³ã‚¯æ•°: {len(results)}ä»¶")
        logger.info("-"*100)
        
        for i, result in enumerate(results[:5], 1):  # ä¸Šä½5ä»¶ã®ã¿è¡¨ç¤º
            logger.info(f"ğŸ“„ {i}. {result.document_name} [ãƒãƒ£ãƒ³ã‚¯#{result.chunk_id}]")
            logger.info(f"    ğŸ¯ ã‚¹ã‚³ã‚¢: {result.score:.4f}")
            logger.info(f"    ğŸ” æ¤œç´¢æ–¹æ³•: {result.search_method}")
            logger.info(f"    ğŸ“ å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {result.content[:100]}...")

    async def _append_variants(self, question: str, result: QueryAnalysisResult) -> QueryAnalysisResult:
        """QuestionVariantsGenerator ã§å¾—ãŸãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ keywords ã«è¿½åŠ ã™ã‚‹ï¼ˆé‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼‰"""
        try:
            from modules.question_variants_generator import generate_question_variants, variants_generator_available  # é…å»¶ import ã§å¾ªç’°å›é¿
        except Exception:
            return result  # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ãŒèª­ã¿è¾¼ã‚ãªã„å ´åˆã¯ä½•ã‚‚ã—ãªã„

        if not variants_generator_available():
            return result

        try:
            variants = await generate_question_variants(question)
            additional = variants.all_variants if variants and variants.all_variants else []
            if additional:
                # ğŸ¯ ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
                important_keywords = []
                
                # å…ƒã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä¿æŒ
                important_keywords.extend(result.keywords)
                
                # å„ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
                for variant in additional:
                    if len(variant) <= 10:  # 10æ–‡å­—ä»¥ä¸‹ã®çŸ­ã„èªå¥ã¯ãã®ã¾ã¾ä½¿ç”¨
                        important_keywords.append(variant)
                    else:
                        # é•·ã„æ–‡ç« ã‹ã‚‰é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
                        extracted_keywords = self._extract_important_keywords_from_text(variant)
                        important_keywords.extend(extracted_keywords)
                
                # æ³•äººæ ¼ãƒ™ãƒ¼ã‚¹ã§åŠè§’ã‚¹ãƒšãƒ¼ã‚¹æ­£è¦åŒ–
                extra = []
                legal_entities = [
                    'æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾', 'åˆåŒä¼šç¤¾', 'åˆè³‡ä¼šç¤¾', 'åˆåä¼šç¤¾',
                    'ä¸€èˆ¬ç¤¾å›£æ³•äºº', 'å…¬ç›Šç¤¾å›£æ³•äºº', 'ä¸€èˆ¬è²¡å›£æ³•äºº', 'å…¬ç›Šè²¡å›£æ³•äºº',
                    'ç¤¾ä¼šç¦ç¥‰æ³•äºº', 'å­¦æ ¡æ³•äºº', 'åŒ»ç™‚æ³•äºº',
                    'ãˆ±', 'ãˆ²', '(æ ª)', 'ï¼ˆæ ªï¼‰', '(æœ‰)', 'ï¼ˆæœ‰ï¼‰'
                ]
                patterns = [re.compile(fr'({re.escape(le)})[\sã€€]*([^\sã€€])') for le in legal_entities]
                for kw in important_keywords:
                    for pattern in patterns:
                        if pattern.search(kw):
                            spaced = pattern.sub(r"\1 \2", kw)
                            if spaced and spaced not in important_keywords and spaced not in extra:
                                extra.append(spaced)
                            break
                important_keywords.extend(extra)
                
                # é‡è¤‡é™¤å»ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                merged = list(dict.fromkeys(important_keywords))
                
                # æœ€å¤§10å€‹ã«åˆ¶é™ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è€ƒæ…®ï¼‰
                result.keywords = merged[:10]
                
                logger.info(f"ğŸ”„ ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³è¿½åŠ : +{len(additional)} â†’ ç·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ {len(result.keywords)} å€‹ (åŠè§’ã‚¹ãƒšãƒ¼ã‚¹æ­£è¦åŒ–å«ã‚€)")
        except Exception as e:
            logger.error(f"âŒ ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return result

    def _extract_important_keywords_from_text(self, text: str) -> List[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆé‡è¤‡ã‚ã‚Šï¼‰
        """
        keywords = []
        
        # åè©çš„ãªå˜èªã‚’æŠ½å‡ºï¼ˆæ—¥æœ¬èªã®å ´åˆï¼‰
        # ã‚«ã‚¿ã‚«ãƒŠèªï¼ˆ3æ–‡å­—ä»¥ä¸Šï¼‰
        katakana_words = re.findall(r'[ã‚¡-ãƒ¶ãƒ¼]{3,}', text)
        keywords.extend(katakana_words)
        
        # æ¼¢å­—ã‚’å«ã‚€å˜èªï¼ˆ2æ–‡å­—ä»¥ä¸Šï¼‰
        kanji_words = re.findall(r'[ä¸€-é¾ ]{2,}', text)
        keywords.extend(kanji_words)
        
        # ã²ã‚‰ãŒãªï¼ˆç‰¹å®šã®é‡è¦èªï¼‰
        important_hiragana = ['ã‚„ã™ã„', 'ãŸã‹ã„', 'ãŠãŠãã„', 'ã¡ã„ã•ã„', 'ã‚ãŸã‚‰ã—ã„', 'ãµã‚‹ã„']
        for word in important_hiragana:
            if word in text:
                keywords.append(word)
        
        # ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆï¼ˆ2æ–‡å­—ä»¥ä¸Šï¼‰
        alphabet_words = re.findall(r'[a-zA-Z]{2,}', text)
        keywords.extend(alphabet_words)
        
        # æ•°å­—ã‚’å«ã‚€èª
        number_words = re.findall(r'[0-9]+[å††ä¸‡åƒç™¾åå„„å…†å°å€‹ä»¶åäºº]', text)
        keywords.extend(number_words)
        
        # ç‰¹åˆ¥ãªèªå½™
        special_words = ['å®‰ã„', 'ãƒ‘ã‚½ã‚³ãƒ³', 'PC', 'ä¾¡æ ¼', 'å€¤æ®µ', 'æ–™é‡‘', 'è²»ç”¨', 'ã‚³ã‚¹ãƒˆ', 'ä½ä¾¡æ ¼', 'æ ¼å®‰', 'å®‰ä¾¡']
        for word in special_words:
            if word in text:
                keywords.append(word)
        
        # é‡è¤‡ã‚’é™¤å»ã—ã€ç©ºæ–‡å­—åˆ—ã‚’é™¤å¤–
        keywords = list(set([k for k in keywords if k.strip()]))
        
        return keywords[:5]  # æœ€å¤§5å€‹

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_gemini_analyzer_instance = None

def get_gemini_question_analyzer() -> Optional[GeminiQuestionAnalyzer]:
    """Geminiè³ªå•åˆ†è§£ãƒ»åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _gemini_analyzer_instance
    
    if _gemini_analyzer_instance is None:
        try:
            _gemini_analyzer_instance = GeminiQuestionAnalyzer()
            logger.info("âœ… Geminiè³ªå•åˆ†è§£ãƒ»åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            logger.error(f"âŒ Geminiè³ªå•åˆ†è§£ãƒ»åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    return _gemini_analyzer_instance

async def gemini_intelligent_search(question: str, company_id: str = None, limit: int = 20) -> Tuple[List[SearchResult], QueryAnalysisResult]:
    """Geminiã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæ¤œç´¢ã®å¤–éƒ¨å‘¼ã³å‡ºã—ç”¨é–¢æ•°"""
    analyzer = get_gemini_question_analyzer()
    if not analyzer:
        return [], None
    
    return await analyzer.intelligent_search(question, company_id, limit)

def gemini_analyzer_available() -> bool:
    """Geminiè³ªå•åˆ†è§£ãƒ»åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        return bool(api_key and supabase_url and supabase_key)
    except Exception:
        return False
"""
ğŸ§  Gemini 2.5 Flashè³ªå•åˆ†è§£ãƒ»åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ 
è³ªå•ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åˆ†è§£ãƒ»åˆ†é¡ã—ã¦SQLæ¤œç´¢ã¨Embeddingæ¤œç´¢ã‚’æœ€é©åŒ–

å®Ÿè£…å†…å®¹:
1. Gemini 2.5 Flashã‚’ä½¿ã£ã¦è³ªå•ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åˆ†è§£ãƒ»åˆ†é¡
2. SQLã«å¤‰æ›ã—ã¦æ§‹é€ çš„ã«æ¢ç´¢ï¼ˆé«˜é€Ÿãƒ»ç²¾å¯†ï¼‰
3. çµæœãŒã‚¼ãƒ­ä»¶ãªã‚‰Embeddingæ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
"""

import os
import re
import json
import logging
import asyncio
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
from dotenv import load_dotenv
import psycopg2
from psycopg2.extras import RealDictCursor

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

logger = logging.getLogger(__name__)

class QueryIntent(Enum):
    """è³ªå•ã®æ„å›³åˆ†é¡"""
    SPECIFIC_INFO = "specific_info"      # å…·ä½“çš„æƒ…å ±ã‚’æ±‚ã‚ã‚‹ï¼ˆä»£è¡¨è€…åã€é€£çµ¡å…ˆãªã©ï¼‰
    GENERAL_INFO = "general_info"        # ä¸€èˆ¬çš„æƒ…å ±ã‚’æ±‚ã‚ã‚‹ï¼ˆã‚µãƒ¼ãƒ“ã‚¹æ¦‚è¦ãªã©ï¼‰
    COMPARISON = "comparison"            # æ¯”è¼ƒãƒ»é•ã„ã‚’æ±‚ã‚ã‚‹
    EXPLANATION = "explanation"          # èª¬æ˜ãƒ»ç†ç”±ã‚’æ±‚ã‚ã‚‹
    PROCEDURE = "procedure"              # æ‰‹é †ãƒ»æ–¹æ³•ã‚’æ±‚ã‚ã‚‹
    UNKNOWN = "unknown"                  # ä¸æ˜

@dataclass
class QueryAnalysisResult:
    """è³ªå•åˆ†æçµæœ"""
    intent: QueryIntent
    confidence: float
    target_entity: str              # å¯¾è±¡ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ï¼ˆä¼šç¤¾åã€äººåãªã©ï¼‰
    keywords: List[str]             # æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    sql_patterns: List[str]         # SQLæ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³
    embedding_fallback: bool        # Embeddingæ¤œç´¢ãŒå¿…è¦ã‹
    reasoning: str                  # åˆ¤å®šç†ç”±

@dataclass
class SearchResult:
    """æ¤œç´¢çµæœ"""
    chunk_id: str
    document_id: str
    document_name: str
    content: str
    score: float
    search_method: str
    metadata: Dict = None

class GeminiQuestionAnalyzer:
    """Gemini 2.5 Flashè³ªå•åˆ†è§£ãƒ»åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.db_url = self._get_db_url()
        self.gemini_model = self._setup_gemini()
        
        # SQLæ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.sql_templates = {
            "exact_match": "content ILIKE '%{keyword}%'",
            "company_representative": "content ~* '{company}.*ä»£è¡¨è€…|ä»£è¡¨è€….*{company}'",
            "contact_info": "content ~* '{entity}.*(é€£çµ¡å…ˆ|é›»è©±|ãƒ¡ãƒ¼ãƒ«|ä½æ‰€)'",
            "multiple_keywords": " AND ".join(["content ILIKE '%{keyword}%'" for keyword in ["{keyword1}", "{keyword2}"]]),
            "regex_pattern": "content ~* '{pattern}'"
        }
        
        logger.info("âœ… Geminiè³ªå•åˆ†è§£ãƒ»åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def _get_db_url(self) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã‚’æ§‹ç¯‰"""
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        if "supabase.co" in supabase_url:
            project_id = supabase_url.split("://")[1].split(".")[0]
            return f"postgresql://postgres.{project_id}:{os.getenv('DB_PASSWORD', '')}@aws-0-ap-northeast-1.pooler.supabase.com:6543/postgres"
        else:
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                raise ValueError("DATABASE_URL ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return db_url
    
    def _setup_gemini(self):
        """Gemini 2.5 Flashãƒ¢ãƒ‡ãƒ«ã®è¨­å®š"""
        try:
            import google.generativeai as genai
            
            api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
            if not api_key:
                raise ValueError("GEMINI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-2.5-flash')
            return model
        except Exception as e:
            logger.error(f"âŒ Geminiè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    async def analyze_question(self, question: str) -> QueryAnalysisResult:
        """
        ğŸ§  Gemini 2.5 Flashã‚’ä½¿ã£ã¦è³ªå•ã‚’åˆ†è§£ãƒ»åˆ†é¡
        
        Args:
            question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            
        Returns:
            QueryAnalysisResult: åˆ†æçµæœ
        """
        logger.info(f"ğŸ§  Geminiè³ªå•åˆ†æé–‹å§‹: '{question}'")
        
        if not self.gemini_model:
            logger.warning("âš ï¸ Geminiãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æã‚’å®Ÿè¡Œ")
            fallback_result = self._fallback_analysis(question)
            return await self._append_variants(question, fallback_result)
        
        try:
            # æ”¹å–„ã•ã‚ŒãŸGemini 2.5 Flashãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            prompt = f"""
è³ªå•ã‚’åˆ†æã—ã¦ã€æ¤œç´¢ã«æœ€é©åŒ–ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

è³ªå•: ã€Œ{question}ã€

åˆ†æé …ç›®:
1. æ„å›³ (intent): specific_info, general_info, comparison, explanation, procedure, unknown ã®ã„ãšã‚Œã‹
2. ä¿¡é ¼åº¦ (confidence): 0.0-1.0ã®æ•°å€¤
3. å¯¾è±¡ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ (target_entity): ä¼šç¤¾åã€äººåã€ã‚µãƒ¼ãƒ“ã‚¹åãªã©ï¼ˆè³ªå•ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹ï¼‰
4. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (keywords): æ¤œç´¢ã«æœ€é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆï¼ˆä»¥ä¸‹ã®æŒ‡é‡ã«å¾“ã†ï¼‰

ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã®æŒ‡é‡ã€‘
- é›»è©±ç•ªå·ãŒå«ã¾ã‚Œã‚‹å ´åˆ: é›»è©±ç•ªå·ãã®ã‚‚ã®ã‚’æœ€é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã—ã¦å«ã‚ã‚‹
- ä¼æ¥­åã‚’æ±‚ã‚ã‚‹å ´åˆ: ã€Œç¤¾åã€ã€Œä¼šç¤¾åã€ã€Œä¼æ¥­åã€ã€Œæ ªå¼ä¼šç¤¾ã€ã€Œæœ‰é™ä¼šç¤¾ã€ã€ŒåˆåŒä¼šç¤¾ã€ã‚’å«ã‚ã‚‹
- é€£çµ¡å…ˆã‚’æ±‚ã‚ã‚‹å ´åˆ: ã€Œé›»è©±ã€ã€ŒTELã€ã€Œé€£çµ¡å…ˆã€ã€Œä½æ‰€ã€ã€Œãƒ¡ãƒ¼ãƒ«ã€ã‚’å«ã‚ã‚‹
- ä»£è¡¨è€…ã‚’æ±‚ã‚ã‚‹å ´åˆ: ã€Œä»£è¡¨è€…ã€ã€Œç¤¾é•·ã€ã€Œä»£è¡¨å–ç· å½¹ã€ã€Œè²¬ä»»è€…ã€ã€Œãƒˆãƒƒãƒ—ã€ã€ŒCEOã€ã€Œãƒªãƒ¼ãƒ€ãƒ¼ã€ã€ŒçµŒå–¶è€…ã€ã€Œã‚ªãƒ¼ãƒŠãƒ¼ã€ã‚’å«ã‚ã‚‹
- å…·ä½“çš„ãªå›ºæœ‰åè©ï¼ˆé›»è©±ç•ªå·ã€ä¼šç¤¾åã€äººåãªã©ï¼‰ã¯å¿…ãšå«ã‚ã‚‹
- ä¸€èˆ¬çš„ãªåŠ©è©ï¼ˆã®ã€ã‚’ã€ã¯ã€ãŒã€ã«ã€ã§ã€ã¨ã€ã‹ã‚‰ã€ã¾ã§ï¼‰ã¯é™¤å¤–ã™ã‚‹

5. åˆ¤å®šç†ç”± (reasoning): åˆ¤å®šã®æ ¹æ‹ 

å›ç­”ä¾‹1ï¼ˆé›»è©±ç•ªå·ã‹ã‚‰ä¼æ¥­åã‚’æ¢ã™å ´åˆï¼‰:
{{
  "intent": "specific_info",
  "confidence": 0.95,
  "target_entity": "ä¼æ¥­å",
  "keywords": ["053-442-6707", "é›»è©±ç•ªå·", "ç¤¾å", "ä¼šç¤¾å", "ä¼æ¥­å", "æ ªå¼ä¼šç¤¾"],
  "reasoning": "ç‰¹å®šã®é›»è©±ç•ªå·ã‹ã‚‰ä¼æ¥­åã‚’ç‰¹å®šã™ã‚‹å…·ä½“çš„æƒ…å ±ã®è³ªå•"
}}

å›ç­”ä¾‹2ï¼ˆä»£è¡¨è€…åã‚’æ¢ã™å ´åˆï¼‰:
{{
  "intent": "specific_info",
  "confidence": 0.90,
  "target_entity": "ä»£è¡¨è€…å",
  "keywords": ["ABCæ ªå¼ä¼šç¤¾", "ä»£è¡¨è€…", "ç¤¾é•·", "ä»£è¡¨å–ç· å½¹", "è²¬ä»»è€…", "ãƒˆãƒƒãƒ—", "CEO", "ãƒªãƒ¼ãƒ€ãƒ¼"],
  "reasoning": "ç‰¹å®šä¼æ¥­ã®ä»£è¡¨è€…åã‚’æ±‚ã‚ã‚‹å…·ä½“çš„æƒ…å ±ã®è³ªå•"
}}

JSONå½¢å¼ã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
"""
            
            # Gemini 2.5 Flashã§åˆ†æå®Ÿè¡Œ
            response = self.gemini_model.generate_content(
                prompt,
                generation_config={
                    'temperature': 0.1,  # ä¸€è²«æ€§é‡è¦–
                    'max_output_tokens': 1024,
                    'top_p': 0.8,
                    'top_k': 40
                }
            )
            
            if not response or not response.text:
                logger.warning("âš ï¸ Geminiã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã™")
                fallback_result = self._fallback_analysis(question)
                return await self._append_variants(question, fallback_result)
            
            # JSONè§£æ
            try:
                analysis_data = json.loads(response.text.strip())
            except json.JSONDecodeError:
                # JSONã§ãªã„å ´åˆã¯ã€JSONéƒ¨åˆ†ã‚’æŠ½å‡º
                json_match = re.search(r'\{.*\}', response.text, re.DOTALL)
                if json_match:
                    analysis_data = json.loads(json_match.group())
                else:
                    logger.warning("âš ï¸ Geminiå¿œç­”ã‹ã‚‰JSONã‚’æŠ½å‡ºã§ãã¾ã›ã‚“")
                    fallback_result = self._fallback_analysis(question)
                    return await self._append_variants(question, fallback_result)
            
            # çµæœã®æ§‹ç¯‰
            intent = QueryIntent(analysis_data.get("intent", "unknown"))
            confidence = float(analysis_data.get("confidence", 0.5))
            target_entity = analysis_data.get("target_entity", "")
            keywords = analysis_data.get("keywords", [])
            reasoning = analysis_data.get("reasoning", "Geminiåˆ†æçµæœ")
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å¾Œå‡¦ç†ï¼ˆé‡è¤‡é™¤å»ã€ç©ºæ–‡å­—é™¤å»ã€åŒç¾©èªæ­£è¦åŒ–ï¼‰
            keywords = [k.strip() for k in keywords if k and k.strip()]
            keywords = list(dict.fromkeys(keywords))  # é †åºã‚’ä¿ã¡ãªãŒã‚‰é‡è¤‡é™¤å»
            
            # ğŸ”¥ åŒç¾©èªæ­£è¦åŒ–ã‚’ç„¡åŠ¹åŒ–ï¼ˆORæ¤œç´¢ã§å¯¾å¿œï¼‰
            # keywords = self._normalize_synonyms(keywords)
            
            # SQLæ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç”Ÿæˆ
            sql_patterns = self._generate_sql_patterns(intent, target_entity, keywords)
            
            # Embeddingæ¤œç´¢ãŒå¿…è¦ã‹ã®åˆ¤å®š
            embedding_fallback = intent in [QueryIntent.GENERAL_INFO, QueryIntent.EXPLANATION, QueryIntent.COMPARISON]
            
            result = QueryAnalysisResult(
                intent=intent,
                confidence=confidence,
                target_entity=target_entity,
                keywords=keywords,
                sql_patterns=sql_patterns,
                embedding_fallback=embedding_fallback,
                reasoning=reasoning
            )
            
            # ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
            result = await self._append_variants(question, result)
            
            logger.info(f"âœ… Geminiåˆ†æå®Œäº†: {intent.value} (ä¿¡é ¼åº¦: {confidence:.2f}) | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°: {len(result.keywords)}")
            logger.info(f"ğŸ¯ å¯¾è±¡ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£: {target_entity}")
            logger.info(f"ğŸ·ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {result.keywords}")
            logger.info(f"ğŸ’­ åˆ¤å®šç†ç”±: {reasoning}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Geminiåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            fallback_result = self._fallback_analysis(question)
            return await self._append_variants(question, fallback_result)
    
    def _fallback_analysis(self, question: str) -> QueryAnalysisResult:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æï¼ˆGeminiãŒåˆ©ç”¨ã§ããªã„å ´åˆï¼‰"""
        logger.info("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æå®Ÿè¡Œä¸­...")
        
        # ç°¡å˜ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        question_lower = question.lower()
        
        # æ„å›³ã®åˆ¤å®š
        if any(word in question_lower for word in ['ä»£è¡¨è€…', 'ç¤¾é•·', 'ãƒˆãƒƒãƒ—', 'ceo', 'é€£çµ¡å…ˆ', 'é›»è©±', 'ä½æ‰€', 'æ–™é‡‘', 'ä¾¡æ ¼']):
            intent = QueryIntent.SPECIFIC_INFO
            confidence = 0.8
        elif any(word in question_lower for word in ['ãªãœ', 'ã©ã†', 'ç†ç”±', 'èƒŒæ™¯']):
            intent = QueryIntent.EXPLANATION
            confidence = 0.6
        elif any(word in question_lower for word in ['æ‰‹é †', 'æ–¹æ³•', 'ã‚„ã‚Šæ–¹']):
            intent = QueryIntent.PROCEDURE
            confidence = 0.6
        else:
            intent = QueryIntent.GENERAL_INFO
            confidence = 0.5
        
        # æ”¹å–„ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        keywords = []
        
        # é›»è©±ç•ªå·ã®æ¤œå‡ºã¨è¿½åŠ 
        phone_patterns = [
            r'\d{2,4}-\d{2,4}-\d{4}',  # 03-1234-5678
            r'\d{3}-\d{3}-\d{4}',      # 090-123-4567
            r'\(\d{2,4}\)\s*\d{2,4}-\d{4}',  # (03) 1234-5678
            r'\d{10,11}'               # 01234567890
        ]
        
        for pattern in phone_patterns:
            phone_matches = re.findall(pattern, question)
            for phone in phone_matches:
                keywords.append(phone)
        
        # ä¼šç¤¾åã®æ¤œå‡º
        company_patterns = [
            r'([^ã€‚ã€\s]+(?:æ ªå¼ä¼šç¤¾|åˆåŒä¼šç¤¾|æœ‰é™ä¼šç¤¾))', 
            r'([^ã€‚ã€\s]+ä¼šç¤¾)',
            r'([^ã€‚ã€\s]+(?:Corporation|Corp|Inc|LLC))'
        ]
        target_entity = ""
        for pattern in company_patterns:
            match = re.search(pattern, question)
            if match:
                target_entity = match.group(1)
                keywords.append(target_entity)
                break
        
        # åŸºæœ¬çš„ãªå˜èªåˆ†å‰²ã«ã‚ˆã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        exclude_words = ['ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã§', 'ã¨', 'ã‹ã‚‰', 'ã¾ã§', 'ã§ã™', 'ã¾ã™', 'ã ', 'ã§ã‚ã‚‹', 'ã“ã‚Œ', 'ãã‚Œ', 'ã‚ã®', 'ãã®', 'ã“ã®']
        for word in question.split():
            clean_word = re.sub(r'[ã€‚ã€ï¼ï¼Ÿ]', '', word)
            if len(clean_word) > 1 and clean_word not in exclude_words:
                keywords.append(clean_word)
        
        # è³ªå•ã®æ„å›³ã«åŸºã¥ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¿½åŠ 
        if 'ä¼æ¥­å' in question or 'ä¼šç¤¾å' in question or 'ç¤¾å' in question:
            keywords.extend(['ä¼æ¥­å', 'ä¼šç¤¾å', 'ç¤¾å', 'æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾'])
            target_entity = "ä¼æ¥­å"
            intent = QueryIntent.SPECIFIC_INFO
            confidence = 0.8
        
        if any(word in question for word in ['ä»£è¡¨è€…', 'ç¤¾é•·', 'ãƒˆãƒƒãƒ—', 'CEO', 'ãƒªãƒ¼ãƒ€ãƒ¼', 'çµŒå–¶è€…', 'ã‚ªãƒ¼ãƒŠãƒ¼']):
            keywords.extend(['ä»£è¡¨è€…', 'ç¤¾é•·', 'ä»£è¡¨å–ç· å½¹', 'è²¬ä»»è€…', 'ãƒˆãƒƒãƒ—', 'CEO', 'ãƒªãƒ¼ãƒ€ãƒ¼', 'çµŒå–¶è€…', 'ã‚ªãƒ¼ãƒŠãƒ¼'])
            target_entity = "ä»£è¡¨è€…"
            intent = QueryIntent.SPECIFIC_INFO
            confidence = 0.8
        
        if 'é›»è©±' in question or 'TEL' in question or 'é€£çµ¡å…ˆ' in question:
            keywords.extend(['é›»è©±', 'TEL', 'é€£çµ¡å…ˆ', 'é›»è©±ç•ªå·'])
            intent = QueryIntent.SPECIFIC_INFO
            confidence = 0.8
        
        # é‡è¤‡é™¤å»ã¨ç©ºæ–‡å­—é™¤å»
        keywords = [k.strip() for k in keywords if k and k.strip()]
        keywords = list(dict.fromkeys(keywords))
        
        # ğŸ”¥ åŒç¾©èªæ­£è¦åŒ–ã‚’ç„¡åŠ¹åŒ–ï¼ˆORæ¤œç´¢ã§å¯¾å¿œï¼‰
        # keywords = self._normalize_synonyms(keywords)
        
        # å¯¾è±¡ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®æ¨æ¸¬ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ï¼‰
        if not target_entity:
            if any(k in keywords for k in ['ä¼æ¥­å', 'ä¼šç¤¾å', 'ç¤¾å']):
                target_entity = "ä¼æ¥­å"
            elif any(k in keywords for k in ['ä»£è¡¨è€…', 'ç¤¾é•·']):
                target_entity = "ä»£è¡¨è€…"
            elif any(k in keywords for k in ['é›»è©±', 'TEL', 'é€£çµ¡å…ˆ']):
                target_entity = "é€£çµ¡å…ˆ"
        
        sql_patterns = self._generate_sql_patterns(intent, target_entity, keywords)
        embedding_fallback = intent in [QueryIntent.GENERAL_INFO, QueryIntent.EXPLANATION]
        
        result = QueryAnalysisResult(
            intent=intent,
            confidence=confidence,
            target_entity=target_entity,
            keywords=keywords,
            sql_patterns=sql_patterns,
            embedding_fallback=embedding_fallback,
            reasoning="ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æã«ã‚ˆã‚‹åˆ¤å®šï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼‰"
        )
        
        logger.info(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æå®Œäº†: {len(keywords)}å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º")
        logger.info(f"ğŸ·ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords}")
        
        return result
    
    def _normalize_synonyms(self, keywords: List[str]) -> List[str]:
        """
        åŒç¾©èªæ­£è¦åŒ–: æ¤œç´¢ç²¾åº¦ã‚’ä¸Šã’ã‚‹ãŸã‚ã€åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ã®ä¸­ã§æœ€ã‚‚æ¤œç´¢ã—ã‚„ã™ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã‚’æ®‹ã™
        """
        # åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ã®å®šç¾©ï¼ˆæœ€åˆã®è¦ç´ ãŒå„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰
        synonym_groups = [
            # å½¹è·é–¢é€£
            ['ä»£è¡¨è€…', 'ãƒˆãƒƒãƒ—', 'CEO', 'ceo', 'ãƒªãƒ¼ãƒ€ãƒ¼', 'çµŒå–¶è€…', 'ã‚ªãƒ¼ãƒŠãƒ¼', 'ç¤¾é•·', 'ä»£è¡¨å–ç· å½¹', 'è²¬ä»»è€…'],
            # ä¼šç¤¾é–¢é€£
            ['ä¼šç¤¾', 'ä¼æ¥­', 'æ³•äºº', 'äº‹æ¥­è€…', 'çµ„ç¹”'],
            # é€£çµ¡å…ˆé–¢é€£
            ['é›»è©±ç•ªå·', 'TEL', 'Tel', 'tel', 'ï¼´ï¼¥ï¼¬', 'é›»è©±'],
            # ä½æ‰€é–¢é€£
            ['ä½æ‰€', 'æ‰€åœ¨åœ°', 'å ´æ‰€', 'ä½ç½®', 'ã‚¢ãƒ‰ãƒ¬ã‚¹'],
            # è³ªå•é–¢é€£
            ['æ•™ãˆã¦', 'çŸ¥ã‚ŠãŸã„', 'èããŸã„', 'åˆ†ã‹ã‚‰ãªã„'],
        ]
        
        normalized_keywords = keywords.copy()
        
        for group in synonym_groups:
            priority_keyword = group[0]  # ã‚°ãƒ«ãƒ¼ãƒ—ã®æœ€åˆãŒå„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            found_keywords = [k for k in normalized_keywords if k in group]
            
            if len(found_keywords) > 1:
                # è¤‡æ•°ã®åŒç¾©èªãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€å„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä»¥å¤–ã‚’é™¤å»
                for keyword in found_keywords:
                    if keyword != priority_keyword:
                        try:
                            normalized_keywords.remove(keyword)
                            logger.info(f"ğŸ”„ åŒç¾©èªæ­£è¦åŒ–: '{keyword}' â†’ '{priority_keyword}'")
                        except ValueError:
                            pass  # æ—¢ã«é™¤å»æ¸ˆã¿
        
        logger.info(f"âœ… åŒç¾©èªæ­£è¦åŒ–å®Œäº†: {len(keywords)}å€‹ â†’ {len(normalized_keywords)}å€‹")
        return normalized_keywords
    
    def _classify_keywords(self, keywords: List[str]) -> Tuple[List[str], Dict[str, List[str]]]:
        """
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å›ºæœ‰åè©ï¼ˆå¿…é ˆï¼‰ã¨åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆé¸æŠï¼‰ã«åˆ†é¡
        
        Returns:
            Tuple[å¿…é ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰, åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—è¾æ›¸]
        """
        # åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ã®å®šç¾©
        synonym_groups_def = {
            'position': ['ä»£è¡¨è€…', 'ãƒˆãƒƒãƒ—', 'CEO', 'ceo', 'ãƒªãƒ¼ãƒ€ãƒ¼', 'çµŒå–¶è€…', 'ã‚ªãƒ¼ãƒŠãƒ¼', 'ç¤¾é•·', 'ä»£è¡¨å–ç· å½¹', 'è²¬ä»»è€…'],
            'company_type': ['ä¼šç¤¾', 'ä¼æ¥­', 'æ³•äºº', 'äº‹æ¥­è€…', 'çµ„ç¹”', 'æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾', 'åˆåŒä¼šç¤¾'],
            'contact': ['é›»è©±ç•ªå·', 'TEL', 'Tel', 'tel', 'ï¼´ï¼¥ï¼¬', 'é›»è©±', 'é€£çµ¡å…ˆ'],
            'location': ['ä½æ‰€', 'æ‰€åœ¨åœ°', 'å ´æ‰€', 'ä½ç½®', 'ã‚¢ãƒ‰ãƒ¬ã‚¹'],
            'question_words': ['æ•™ãˆã¦', 'çŸ¥ã‚ŠãŸã„', 'èããŸã„', 'åˆ†ã‹ã‚‰ãªã„'],
        }
        
        required_keywords = []
        found_synonym_groups = {}
        
        for keyword in keywords:
            is_synonym = False
            
            # åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ã«å±ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            for group_name, group_words in synonym_groups_def.items():
                if keyword in group_words:
                    if group_name not in found_synonym_groups:
                        found_synonym_groups[group_name] = []
                    found_synonym_groups[group_name].append(keyword)
                    is_synonym = True
                    break
            
            # åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ã«å±ã•ãªã„å ´åˆã¯å›ºæœ‰åè©ã¨ã—ã¦æ‰±ã†
            if not is_synonym:
                # é›»è©±ç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚§ãƒƒã‚¯
                phone_patterns = [
                    r'\d{2,4}-\d{2,4}-\d{4}',
                    r'\d{3}-\d{3}-\d{4}',
                    r'\(\d{2,4}\)\s*\d{2,4}-\d{4}',
                    r'\d{10,11}'
                ]
                
                is_phone = any(re.match(pattern, keyword) for pattern in phone_patterns)
                
                # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚§ãƒƒã‚¯
                is_email = re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', keyword)
                
                # å›ºæœ‰åè©ã¨ã—ã¦åˆ†é¡ï¼ˆä¼šç¤¾åã€äººåã€é›»è©±ç•ªå·ã€ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãªã©ï¼‰
                if (len(keyword) >= 2 and not keyword in ['ã§ã™', 'ã¾ã™', 'ã¦ã„ã‚‹', 'ã ', 'ã§ã‚ã‚‹']) or is_phone or is_email:
                    required_keywords.append(keyword)
        
        logger.info(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†é¡çµæœ:")
        logger.info(f"   å›ºæœ‰åè©ï¼ˆå¿…é ˆï¼‰: {required_keywords}")
        logger.info(f"   åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆé¸æŠï¼‰: {found_synonym_groups}")
        
        return required_keywords, found_synonym_groups
    
    def _generate_sql_patterns(self, intent: QueryIntent, target_entity: str, keywords: List[str]) -> List[str]:
        """SQLæ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç”Ÿæˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰"""
        # æ–°ã—ã„å®Ÿè£…ã§ã¯ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’ãã®ã¾ã¾è¿”ã™
        # å®Ÿéš›ã®SQLæ§‹ç¯‰ã¯ execute_sql_search ã§è¡Œã†
        return keywords
    
    async def execute_sql_search(self, analysis: QueryAnalysisResult, company_id: str = None, limit: int = 10) -> List[SearchResult]:
        """
        ğŸ” SQLæ§‹é€ çš„æ¤œç´¢ã®å®Ÿè¡Œ
        
        Args:
            analysis: è³ªå•åˆ†æçµæœ
            company_id: ä¼šç¤¾IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            limit: çµæœæ•°åˆ¶é™
            
        Returns:
            List[SearchResult]: æ¤œç´¢çµæœ
        """
        logger.info(f"ğŸ” SQLæ§‹é€ çš„æ¤œç´¢é–‹å§‹: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰={analysis.keywords}")
        
        all_results = []
        
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    
                    # ç‰¹æ®Šãªãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢ï¼ˆé›»è©±ç•ªå·ã€ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ç­‰ï¼‰
                    special_patterns = self._detect_special_patterns(analysis.keywords)
                    
                    if special_patterns:
                        logger.info(f"ğŸ¯ ç‰¹æ®Šãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢: {special_patterns}")
                        for pattern_type, pattern_value in special_patterns.items():
                            if pattern_type == "phone_number":
                                # é›»è©±ç•ªå·å°‚ç”¨æ¤œç´¢
                                phone_results = await self._search_by_phone_number(cur, pattern_value, company_id, limit)
                                all_results.extend(phone_results)
                            elif pattern_type == "email":
                                # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹å°‚ç”¨æ¤œç´¢
                                email_results = await self._search_by_email(cur, pattern_value, company_id, limit)
                                all_results.extend(email_results)
                    
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢ã‚’å®Ÿè¡Œ
                    if analysis.keywords:
                        # ğŸ”¥ ã‚¹ãƒãƒ¼ãƒˆæ¤œç´¢: å›ºæœ‰åè©ï¼ˆANDï¼‰+ åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆORï¼‰
                        if len(analysis.keywords) >= 2:
                            logger.info(f"ğŸ” ã‚¹ãƒãƒ¼ãƒˆæ¤œç´¢ï¼ˆå›ºæœ‰åè©AND + åŒç¾©èªORï¼‰: {analysis.keywords}")
                            
                            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å›ºæœ‰åè©ã¨åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†é¡
                            required_keywords, synonym_groups = self._classify_keywords(analysis.keywords)
                            
                            # WHEREå¥ã®æ§‹ç¯‰
                            where_conditions = []
                            params = []
                            
                            # 1. å›ºæœ‰åè©ã¯å¿…é ˆï¼ˆANDï¼‰
                            for keyword in required_keywords:
                                if any(char in keyword for char in ['-', '(', ')', '.']):
                                    where_conditions.append("c.content ~* %s")
                                    params.append(re.escape(keyword))
                                else:
                                    where_conditions.append("c.content ILIKE %s")
                                    params.append(f"%{keyword}%")
                            
                            # 2. åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ã¯é¸æŠï¼ˆORï¼‰
                            for group_name, synonyms in synonym_groups.items():
                                if synonyms:
                                    or_conditions = []
                                    for synonym in synonyms:
                                        if any(char in synonym for char in ['-', '(', ')', '.']):
                                            or_conditions.append("c.content ~* %s")
                                            params.append(re.escape(synonym))
                                        else:
                                            or_conditions.append("c.content ILIKE %s")
                                            params.append(f"%{synonym}%")
                                    
                                    if or_conditions:
                                        where_conditions.append(f"({' OR '.join(or_conditions)})")
                            
                            if where_conditions:
                                sql = f"""
                                SELECT DISTINCT
                                    c.id as chunk_id,
                                    c.doc_id as document_id,
                                    c.chunk_index,
                                    c.content as snippet,
                                    ds.name as document_name,
                                    ds.type as document_type,
                                    1.0 as score
                                FROM chunks c
                                LEFT JOIN document_sources ds ON ds.id = c.doc_id
                                WHERE c.content IS NOT NULL
                                  AND LENGTH(c.content) > 10
                                  AND ({' OR '.join(where_conditions)})
                                """
                                
                                # ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿
                                if company_id:
                                    sql += " AND c.company_id = %s"
                                    params.append(company_id)
                                
                                sql += " ORDER BY score DESC LIMIT %s"
                                params.append(limit)
                                
                                try:
                                    cur.execute(sql, params)
                                    results = cur.fetchall()
                                    
                                    for row in results:
                                        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                                        if not any(r.chunk_id == row['chunk_id'] for r in all_results):
                                            all_results.append(SearchResult(
                                                chunk_id=row['chunk_id'],
                                                document_id=row['document_id'],
                                                document_name=row['document_name'] or 'Unknown',
                                                content=row['snippet'] or '',
                                                score=row['score'],
                                                search_method='sql_smart_search',
                                                metadata={
                                                    'required_keywords': required_keywords,
                                                    'synonym_groups': synonym_groups
                                                }
                                            ))
                                    
                                    logger.info(f"âœ… ã‚¹ãƒãƒ¼ãƒˆæ¤œç´¢ã§{len(results)}ä»¶ã®çµæœ")
                                    logger.info(f"   å¿…é ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {required_keywords}")
                                    logger.info(f"   åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—: {list(synonym_groups.keys())}")
                                    
                                except Exception as e:
                                    logger.warning(f"âš ï¸ ã‚¹ãƒãƒ¼ãƒˆæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
                        
                        # 2. å€‹åˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆçµæœãŒå°‘ãªã„å ´åˆï¼‰
                        if len(all_results) < limit:
                            for i, keyword in enumerate(analysis.keywords):
                                logger.info(f"ğŸ” å€‹åˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ {i+1}/{len(analysis.keywords)}: {keyword}")
                                
                                # ç‰¹æ®Šæ–‡å­—ã‚’å«ã‚€å ´åˆã¯æ­£è¦è¡¨ç¾æ¤œç´¢
                                if any(char in keyword for char in ['-', '(', ')', '.']):
                                    condition = "c.content ~* %s"
                                    keyword_param = re.escape(keyword)
                                else:
                                    condition = "c.content ILIKE %s"
                                    keyword_param = f"%{keyword}%"
                                
                                sql = f"""
                                SELECT DISTINCT
                                    c.id as chunk_id,
                                    c.doc_id as document_id,
                                    c.chunk_index,
                                    c.content as snippet,
                                    ds.name as document_name,
                                    ds.type as document_type,
                                    1.0 as score
                                FROM chunks c
                                LEFT JOIN document_sources ds ON ds.id = c.doc_id
                                WHERE c.content IS NOT NULL
                                  AND LENGTH(c.content) > 10
                                  AND {condition}
                                """
                                
                                params = [keyword_param]
                                
                                # ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿
                                if company_id:
                                    sql += " AND c.company_id = %s"
                                    params.append(company_id)
                                
                                sql += " ORDER BY score DESC LIMIT %s"
                                params.append(limit)
                                
                                try:
                                    cur.execute(sql, params)
                                    results = cur.fetchall()
                                    
                                    for row in results:
                                        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                                        if not any(r.chunk_id == row['chunk_id'] for r in all_results):
                                            all_results.append(SearchResult(
                                                chunk_id=row['chunk_id'],
                                                document_id=row['document_id'],
                                                document_name=row['document_name'] or 'Unknown',
                                                content=row['snippet'] or '',
                                                score=row['score'],
                                                search_method=f'sql_keyword_{i+1}',
                                                metadata={'keyword': keyword}
                                            ))
                                    
                                    logger.info(f"âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã§{len(results)}ä»¶ã®çµæœ")
                                    
                                except Exception as e:
                                    logger.warning(f"âš ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
                                    continue
            
            # çµæœã‚’ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
            all_results.sort(key=lambda x: x.score, reverse=True)
            
            logger.info(f"âœ… SQLæ§‹é€ çš„æ¤œç´¢å®Œäº†: {len(all_results)}ä»¶ã®çµæœ")
            return all_results[:limit]
            
        except Exception as e:
            logger.error(f"âŒ SQLæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _detect_special_patterns(self, keywords: List[str]) -> Dict[str, str]:
        """ç‰¹æ®Šãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆé›»è©±ç•ªå·ã€ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ç­‰ï¼‰ã®æ¤œå‡º"""
        patterns = {}
        
        for keyword in keywords:
            # é›»è©±ç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³
            phone_patterns = [
                r'\d{2,4}-\d{2,4}-\d{4}',  # 03-1234-5678
                r'\d{3}-\d{3}-\d{4}',      # 090-123-4567
                r'\(\d{2,4}\)\s*\d{2,4}-\d{4}',  # (03) 1234-5678
                r'\d{10,11}'               # 01234567890
            ]
            
            for pattern in phone_patterns:
                if re.match(pattern, keyword):
                    patterns['phone_number'] = keyword
                    break
            
            # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
            if re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', keyword):
                patterns['email'] = keyword
        
        return patterns
    
    async def _search_by_phone_number(self, cursor, phone_number: str, company_id: str = None, limit: int = 10) -> List[SearchResult]:
        """é›»è©±ç•ªå·ã«ã‚ˆã‚‹å°‚ç”¨æ¤œç´¢"""
        logger.info(f"ğŸ“ é›»è©±ç•ªå·æ¤œç´¢: {phone_number}")
        
        results = []
        
        # è¤‡æ•°ã®é›»è©±ç•ªå·ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§æ¤œç´¢
        phone_variants = self._generate_phone_variants(phone_number)
        
        for variant in phone_variants:
            sql = """
            SELECT DISTINCT
                c.id as chunk_id,
                c.doc_id as document_id,
                c.chunk_index,
                c.content as snippet,
                ds.name as document_name,
                ds.type as document_type,
                2.0 as score
            FROM chunks c
            LEFT JOIN document_sources ds ON ds.id = c.doc_id
            WHERE c.content IS NOT NULL
              AND LENGTH(c.content) > 10
              AND c.content ~* %s
            """
            
            params = [re.escape(variant)]
            
            if company_id:
                sql += " AND c.company_id = %s"
                params.append(company_id)
            
            sql += " ORDER BY score DESC LIMIT %s"
            params.append(limit)
            
            try:
                cursor.execute(sql, params)
                rows = cursor.fetchall()
                
                for row in rows:
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if not any(r.chunk_id == row['chunk_id'] for r in results):
                        results.append(SearchResult(
                            chunk_id=row['chunk_id'],
                            document_id=row['document_id'],
                            document_name=row['document_name'] or 'Unknown',
                            content=row['snippet'] or '',
                            score=row['score'],
                            search_method='phone_search',
                            metadata={'phone_number': phone_number, 'variant': variant}
                        ))
                
                logger.info(f"ğŸ“ é›»è©±ç•ªå·ãƒãƒªã‚¢ãƒ³ãƒˆã€Œ{variant}ã€ã§{len(rows)}ä»¶")
                
            except Exception as e:
                logger.warning(f"âš ï¸ é›»è©±ç•ªå·æ¤œç´¢ã‚¨ãƒ©ãƒ¼ï¼ˆ{variant}ï¼‰: {e}")
                continue
        
        return results
    
    def _generate_phone_variants(self, phone_number: str) -> List[str]:
        """é›»è©±ç•ªå·ã®ç•°ãªã‚‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒãƒªã‚¢ãƒ³ãƒˆã‚’ç”Ÿæˆ"""
        # æ•°å­—ã®ã¿ã‚’æŠ½å‡º
        digits_only = re.sub(r'[^\d]', '', phone_number)
        
        variants = [phone_number]  # å…ƒã®å½¢å¼
        
        if len(digits_only) >= 10:
            # æ§˜ã€…ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç”Ÿæˆ
            if len(digits_only) == 10:
                # 03-1234-5678
                variants.append(f"{digits_only[:2]}-{digits_only[2:6]}-{digits_only[6:]}")
                # 03(1234)5678
                variants.append(f"{digits_only[:2]}({digits_only[2:6]}){digits_only[6:]}")
            elif len(digits_only) == 11:
                # 090-123-4567
                variants.append(f"{digits_only[:3]}-{digits_only[3:6]}-{digits_only[6:]}")
                # 090(123)4567
                variants.append(f"{digits_only[:3]}({digits_only[3:6]}){digits_only[6:]}")
        
        # æ•°å­—ã®ã¿
        variants.append(digits_only)
        
        return list(set(variants))  # é‡è¤‡ã‚’é™¤å»
    
    async def _search_by_email(self, cursor, email: str, company_id: str = None, limit: int = 10) -> List[SearchResult]:
        """ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã«ã‚ˆã‚‹å°‚ç”¨æ¤œç´¢"""
        logger.info(f"ğŸ“§ ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æ¤œç´¢: {email}")
        
        sql = """
        SELECT DISTINCT
            c.id as chunk_id,
            c.doc_id as document_id,
            c.chunk_index,
            c.content as snippet,
            ds.name as document_name,
            ds.type as document_type,
            2.0 as score
        FROM chunks c
        LEFT JOIN document_sources ds ON ds.id = c.doc_id
        WHERE c.content IS NOT NULL
          AND LENGTH(c.content) > 10
          AND c.content ILIKE %s
        """
        
        params = [f"%{email}%"]
        
        if company_id:
            sql += " AND c.company_id = %s"
            params.append(company_id)
        
        sql += " ORDER BY score DESC LIMIT %s"
        params.append(limit)
        
        try:
            cursor.execute(sql, params)
            rows = cursor.fetchall()
            
            results = []
            for row in rows:
                results.append(SearchResult(
                    chunk_id=row['chunk_id'],
                    document_id=row['document_id'],
                    document_name=row['document_name'] or 'Unknown',
                    content=row['snippet'] or '',
                    score=row['score'],
                    search_method='email_search',
                    metadata={'email': email}
                ))
            
            logger.info(f"ğŸ“§ ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã€Œ{email}ã€ã§{len(results)}ä»¶")
            return results
            
        except Exception as e:
            logger.warning(f"âš ï¸ ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def execute_embedding_search(self, question: str, company_id: str = None, limit: int = 10) -> List[SearchResult]:
        """
        ğŸ“˜ Embeddingæ¤œç´¢ã®å®Ÿè¡Œï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        
        Args:
            question: æ¤œç´¢ã‚¯ã‚¨ãƒª
            company_id: ä¼šç¤¾IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            limit: çµæœæ•°åˆ¶é™
            
        Returns:
            List[SearchResult]: æ¤œç´¢çµæœ
        """
        logger.info(f"ğŸ“˜ Embeddingæ¤œç´¢é–‹å§‹ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰: '{question}'")
        
        try:
            # Vertex AI Embeddingã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å–å¾—
            from .vertex_ai_embedding import get_vertex_ai_embedding_client, vertex_ai_embedding_available
            
            if not vertex_ai_embedding_available():
                logger.warning("âš ï¸ Vertex AI EmbeddingãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                return []
            
            vertex_client = get_vertex_ai_embedding_client()
            if not vertex_client:
                logger.warning("âš ï¸ Vertex AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒå–å¾—ã§ãã¾ã›ã‚“")
                return []
            
            # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            query_embedding = vertex_client.generate_embedding(question)
            if not query_embedding:
                logger.warning("âš ï¸ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã«å¤±æ•—")
                return []
            
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    # pgvectorã‚’ä½¿ç”¨ã—ãŸãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
                    sql = """
                    SELECT DISTINCT
                        c.id as chunk_id,
                        c.doc_id as document_id,
                        c.chunk_index,
                        c.content as snippet,
                        ds.name as document_name,
                        ds.type as document_type,
                        (1 - (c.embedding <=> %s::vector)) as score
                    FROM chunks c
                    LEFT JOIN document_sources ds ON ds.id = c.doc_id
                    WHERE c.content IS NOT NULL
                      AND c.embedding IS NOT NULL
                      AND LENGTH(c.content) > 10
                    """
                    
                    params = [str(query_embedding)]
                    
                    if company_id:
                        sql += " AND c.company_id = %s"
                        params.append(company_id)
                    
                    sql += " ORDER BY score DESC LIMIT %s"
                    params.append(limit)
                    
                    cur.execute(sql, params)
                    results = cur.fetchall()
                    
                    search_results = []
                    for row in results:
                        search_results.append(SearchResult(
                            chunk_id=row['chunk_id'],
                            document_id=row['document_id'],
                            document_name=row['document_name'] or 'Unknown',
                            content=row['snippet'] or '',
                            score=row['score'],
                            search_method='embedding_fallback',
                            metadata={'similarity': row['score']}
                        ))
                    
                    logger.info(f"âœ… Embeddingæ¤œç´¢å®Œäº†: {len(search_results)}ä»¶ã®çµæœ")
                    return search_results
                    
        except Exception as e:
            logger.error(f"âŒ Embeddingæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def intelligent_search(self, question: str, company_id: str = None, limit: int = 20) -> Tuple[List[SearchResult], QueryAnalysisResult]:
        """
        ğŸš€ ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæ¤œç´¢ã®å®Ÿè¡Œ
        1. Gemini 2.5 Flashã§è³ªå•åˆ†è§£ãƒ»åˆ†é¡
        2. SQLæ§‹é€ çš„æ¤œç´¢
        3. çµæœãŒã‚¼ãƒ­ä»¶ãªã‚‰Embeddingæ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        Args:
            question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            company_id: ä¼šç¤¾IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            limit: çµæœæ•°åˆ¶é™
            
        Returns:
            Tuple[æ¤œç´¢çµæœãƒªã‚¹ãƒˆ, è³ªå•åˆ†æçµæœ]
        """
        logger.info(f"ğŸš€ ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæ¤œç´¢é–‹å§‹: '{question}'")
        
        # 1. Gemini 2.5 Flashã§è³ªå•åˆ†è§£ãƒ»åˆ†æ
        analysis = await self.analyze_question(question)
        
        # 2. SQLæ§‹é€ çš„æ¤œç´¢ã®å®Ÿè¡Œ
        sql_results = await self.execute_sql_search(analysis, company_id, limit)
        
        # 3. çµæœãŒã‚¼ãƒ­ä»¶ãªã‚‰Embeddingæ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not sql_results and analysis.embedding_fallback:
            logger.info("ğŸ”„ SQLæ¤œç´¢çµæœãŒã‚¼ãƒ­ä»¶ã®ãŸã‚ã€Embeddingæ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            embedding_results = await self.execute_embedding_search(question, company_id, limit)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœã‚’ãƒãƒ¼ã‚¯
            for result in embedding_results:
                result.search_method = "embedding_fallback"
            
            final_results = embedding_results
        else:
            final_results = sql_results
        
        # çµæœã®ãƒ­ã‚°å‡ºåŠ›
        self._log_search_results(question, analysis, final_results)
        
        logger.info(f"âœ… ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæ¤œç´¢å®Œäº†: {len(final_results)}ä»¶ã®çµæœ")
        return final_results, analysis
    
    def _log_search_results(self, question: str, analysis: QueryAnalysisResult, results: List[SearchResult]):
        """æ¤œç´¢çµæœã®è©³ç´°ãƒ­ã‚°å‡ºåŠ›"""
        logger.info("="*100)
        logger.info(f"ğŸ” ã€ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæ¤œç´¢çµæœã€‘{analysis.intent.value} - ã‚¯ã‚¨ãƒª: '{question}'")
        logger.info("="*100)
        logger.info(f"ğŸ§  è³ªå•æ„å›³: {analysis.intent.value}")
        logger.info(f"ğŸ“Š ä¿¡é ¼åº¦: {analysis.confidence:.3f}")
        logger.info(f"ğŸ¯ å¯¾è±¡ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£: {analysis.target_entity}")
        logger.info(f"ğŸ·ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {analysis.keywords}")
        logger.info(f"ğŸ’­ åˆ¤å®šç†ç”±: {analysis.reasoning}")
        logger.info(f"ğŸ”§ SQLæ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(analysis.sql_patterns)}")
        logger.info(f"ğŸ“ˆ é¸å®šãƒãƒ£ãƒ³ã‚¯æ•°: {len(results)}ä»¶")
        logger.info("-"*100)
        
        for i, result in enumerate(results[:5], 1):  # ä¸Šä½5ä»¶ã®ã¿è¡¨ç¤º
            logger.info(f"ğŸ“„ {i}. {result.document_name} [ãƒãƒ£ãƒ³ã‚¯#{result.chunk_id}]")
            logger.info(f"    ğŸ¯ ã‚¹ã‚³ã‚¢: {result.score:.4f}")
            logger.info(f"    ğŸ” æ¤œç´¢æ–¹æ³•: {result.search_method}")
            logger.info(f"    ğŸ“ å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {result.content[:100]}...")

    async def _append_variants(self, question: str, result: QueryAnalysisResult) -> QueryAnalysisResult:
        """QuestionVariantsGenerator ã§å¾—ãŸãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ keywords ã«è¿½åŠ ã™ã‚‹"""
        try:
            from modules.question_variants_generator import generate_question_variants, variants_generator_available  # é…å»¶ import ã§å¾ªç’°å›é¿
        except Exception:
            return result  # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ãŒèª­ã¿è¾¼ã‚ãªã„å ´åˆã¯ä½•ã‚‚ã—ãªã„

        if not variants_generator_available():
            return result

        try:
            variants = await generate_question_variants(question)
            additional = variants.all_variants if variants and variants.all_variants else []
            if additional:
                merged = list(dict.fromkeys(result.keywords + additional))
                # æ³•äººæ ¼ãƒ™ãƒ¼ã‚¹ã§åŠè§’ã‚¹ãƒšãƒ¼ã‚¹æ­£è¦åŒ–
                extra = []
                legal_entities = [
                    'æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾', 'åˆåŒä¼šç¤¾', 'åˆè³‡ä¼šç¤¾', 'åˆåä¼šç¤¾',
                    'ä¸€èˆ¬ç¤¾å›£æ³•äºº', 'å…¬ç›Šç¤¾å›£æ³•äºº', 'ä¸€èˆ¬è²¡å›£æ³•äºº', 'å…¬ç›Šè²¡å›£æ³•äºº',
                    'ç¤¾ä¼šç¦ç¥‰æ³•äºº', 'å­¦æ ¡æ³•äºº', 'åŒ»ç™‚æ³•äºº',
                    'ãˆ±', 'ãˆ²', '(æ ª)', 'ï¼ˆæ ªï¼‰', '(æœ‰)', 'ï¼ˆæœ‰ï¼‰'
                ]
                patterns = [re.compile(fr'({re.escape(le)})[\sã€€]*([^\sã€€])') for le in legal_entities]
                for kw in merged:
                    for pattern in patterns:
                        if pattern.search(kw):
                            spaced = pattern.sub(r"\1 \2", kw)
                            if spaced and spaced not in merged and spaced not in extra:
                                extra.append(spaced)
                            break
                merged.extend(extra)
                result.keywords = merged
                logger.info(f"ğŸ”„ ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³è¿½åŠ : +{len(additional)+len(extra)} â†’ ç·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ {len(merged)} å€‹ (åŠè§’ã‚¹ãƒšãƒ¼ã‚¹æ­£è¦åŒ–å«ã‚€)")
        except Exception as e:
            logger.error(f"âŒ ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return result

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_gemini_analyzer_instance = None

def get_gemini_question_analyzer() -> Optional[GeminiQuestionAnalyzer]:
    """Geminiè³ªå•åˆ†è§£ãƒ»åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _gemini_analyzer_instance
    
    if _gemini_analyzer_instance is None:
        try:
            _gemini_analyzer_instance = GeminiQuestionAnalyzer()
            logger.info("âœ… Geminiè³ªå•åˆ†è§£ãƒ»åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            logger.error(f"âŒ Geminiè³ªå•åˆ†è§£ãƒ»åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    return _gemini_analyzer_instance

async def gemini_intelligent_search(question: str, company_id: str = None, limit: int = 20) -> Tuple[List[SearchResult], QueryAnalysisResult]:
    """Geminiã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæ¤œç´¢ã®å¤–éƒ¨å‘¼ã³å‡ºã—ç”¨é–¢æ•°"""
    analyzer = get_gemini_question_analyzer()
    if not analyzer:
        return [], None
    
    return await analyzer.intelligent_search(question, company_id, limit)

def gemini_analyzer_available() -> bool:
    """Geminiè³ªå•åˆ†è§£ãƒ»åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        return bool(api_key and supabase_url and supabase_key)
    except Exception:
        return False
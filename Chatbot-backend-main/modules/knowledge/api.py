"""
çŸ¥è­˜ãƒ™ãƒ¼ã‚¹APIé–¢æ•°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ãƒ•ã‚¡ã‚¤ãƒ«ã¨URLã®å‡¦ç†ã€ãƒªã‚½ãƒ¼ã‚¹ã®ç®¡ç†ã‚’è¡Œã†APIé–¢æ•°ã‚’æä¾›ã—ã¾ã™
"""
import uuid
import logging
import pandas as pd
import asyncio
from datetime import datetime
from fastapi import HTTPException, UploadFile, File, Depends, Request
from io import BytesIO
import PyPDF2
import traceback
from typing import Dict, List, Optional, Tuple, Any, Union
from psycopg2.extensions import connection as Connection
from ..database import get_db, update_usage_count, ensure_string
from ..auth import check_usage_limits
from .base import knowledge_base, _update_knowledge_base, _update_knowledge_base_from_list, get_active_resources
from .excel import process_excel_file
from .excel_sheets_processor import process_excel_file_with_sheets_api, is_excel_file
from .pdf import process_pdf_file
from .text import process_txt_file
from .image import process_image_file, is_image_file
from .csv_processor import process_csv_file, process_csv_with_gemini_ocr, is_csv_file
from .word_processor import process_word_file, is_word_file
from .file_detector import detect_file_type
from .url import extract_text_from_url, process_url_content
from ..company import DEFAULT_COMPANY_NAME
from ..utils import _process_video_file
import os
from .unnamed_column_handler import UnnamedColumnHandler
import tempfile
from fastapi.responses import JSONResponse

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logger = logging.getLogger(__name__)

def _is_date_like_pandas(value: str) -> bool:
    """pandaså‡¦ç†ç”¨ã®æ—¥ä»˜åˆ¤å®šé–¢æ•°"""
    import re
    from datetime import datetime
    
    if not value or not isinstance(value, str):
        return False
    
    value = value.strip()
    
    # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒªã‚¹ãƒˆ
    date_patterns = [
        r'^\d{4}[-/]\d{1,2}[-/]\d{1,2}$',  # 2024-01-01, 2024/1/1
        r'^\d{1,2}[-/]\d{1,2}[-/]\d{4}$',  # 01-01-2024, 1/1/2024
        r'^\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥$',    # 2024å¹´1æœˆ1æ—¥
        r'^\d{1,2}æœˆ\d{1,2}æ—¥$',           # 1æœˆ1æ—¥
        r'^\d{4}\d{2}\d{2}$',              # 20240101
    ]
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
    for pattern in date_patterns:
        if re.match(pattern, value):
            return True
    
    # Excelæ—¥ä»˜ã‚·ãƒªã‚¢ãƒ«å€¤ï¼ˆ30000-50000ç¨‹åº¦ï¼‰
    try:
        num_value = float(value)
        if 30000 <= num_value <= 50000:
            return True
    except (ValueError, TypeError):
        pass
    
    # å®Ÿéš›ã«æ—¥ä»˜ã¨ã—ã¦è§£æã§ãã‚‹ã‹è©¦è¡Œ
    try:
        # ä¸€èˆ¬çš„ãªæ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è§£æã‚’è©¦è¡Œ
        for fmt in ['%Y-%m-%d', '%Y/%m/%d', '%m/%d/%Y', '%d/%m/%Y', '%Y%m%d']:
            try:
                datetime.strptime(value, fmt)
                return True
            except ValueError:
                continue
    except:
        pass
    
    return False

# å…±é€šã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
EMPLOYEE_UPLOAD_ERROR = "ç¤¾å“¡ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã€‚ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚"
LIMIT_REACHED_ERROR = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ãƒ‡ãƒ¢ç‰ˆã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰åˆ¶é™ï¼ˆ{limit}å›ï¼‰ã«é”ã—ã¾ã—ãŸã€‚"
INVALID_FILE_ERROR = "ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚Excelã€PDFã€Wordã€CSVã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.xlsxã€.xlsã€.pdfã€.docã€.docxã€.csvã€.txtã€.jpgã€.pngç­‰ï¼‰ã®ã¿å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚"
PDF_SIZE_ERROR = "PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã¾ã™ ({size:.2f} MB)ã€‚10MBä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚"
VIDEO_SIZE_ERROR = "ãƒ“ãƒ‡ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã¾ã™ ({size:.2f} MB)ã€‚500MBä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚"
TIMEOUT_ERROR = "å‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã‚‹ã‹ã€è¤‡é›‘ã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²ã™ã‚‹ã‹ã€ã‚ˆã‚Šå°ã•ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"

# å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ç”¨ã®è¨­å®š
MAX_PROCESSING_TIME = 300  # 5åˆ†ã®å‡¦ç†æ™‚é–“åˆ¶é™
BATCH_SIZE_FOR_DB = 100    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜æ™‚ã®ãƒãƒƒãƒã‚µã‚¤ã‚º

def _get_user_info(user_id: str, db: Connection) -> Tuple[Optional[str], bool]:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—ã—ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¨©é™ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹"""
    if not user_id:
        return None, True
    
    try:
        from supabase_adapter import select_data
        user_result = select_data("users", columns="company_id,role", filters={"id": user_id})
        
        if not user_result.data or len(user_result.data) == 0:
            return None, True
        
        user = user_result.data[0]
        
        # ç¤¾å“¡ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸å¯
        if user.get('role') == 'employee':
            raise HTTPException(status_code=403, detail=EMPLOYEE_UPLOAD_ERROR)
    except HTTPException:
        # HTTPExceptionã¯å†åº¦ç™ºç”Ÿã•ã›ã‚‹
        raise
    except Exception as e:
        logger.error(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None, True
        
    return user['company_id'], True

def _check_upload_limits(user_id: str, db: Connection) -> Dict[str, Any]:
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰åˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹"""
    if not user_id:
        return {"allowed": True, "remaining": None, "limit_reached": False}
    
    try:
        limits_check = check_usage_limits(user_id, "document_upload", db)
        
        if not limits_check["is_unlimited"] and not limits_check["allowed"]:
            raise HTTPException(
                status_code=403,
                detail=LIMIT_REACHED_ERROR.format(limit=limits_check['limit'])
            )
        
        return {
            "allowed": True,
            "remaining": limits_check.get("remaining") if not limits_check["is_unlimited"] else None,
            "limit_reached": False
        }
    except HTTPException as e:
        if e.status_code == 404:
            # åˆ©ç”¨åˆ¶é™æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯è‡ªå‹•ä½œæˆ
            logger.warning(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user_id} ã®åˆ©ç”¨åˆ¶é™æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è‡ªå‹•ä½œæˆã‚’è©¦è¡Œã—ã¾ã™ã€‚")
            try:
                # åˆ©ç”¨åˆ¶é™ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ç›´æ¥ä½œæˆ
                from supabase_adapter import insert_data
                
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—ã—ã¦æ­£ã—ã„user_idã‚’ä½¿ç”¨
                from supabase_adapter import select_data
                user_result = select_data("users", columns="id, email, role", filters={"id": user_id})
                
                if not user_result or not user_result.data:
                    # user_idã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯company_idã§æ¤œç´¢
                    user_result = select_data("users", columns="id, email, role", filters={"company_id": user_id})
                
                if user_result and user_result.data:
                    actual_user = user_result.data[0]
                    actual_user_id = actual_user.get("id")
                    user_email = actual_user.get("email", "")
                    user_role = actual_user.get("role", "user")
                    
                    logger.info(f"å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {actual_user_id}, email: {user_email}, role: {user_role}")
                    
                    # å…±é€šé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®åˆ©ç”¨åˆ¶é™ã‚’è¨­å®š
                    from modules.utils import create_default_usage_limits
                    limit_data = create_default_usage_limits(actual_user_id, user_email, user_role)
                else:
                    logger.error(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ID {user_id} ã«å¯¾å¿œã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    raise Exception(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ID {user_id} ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                
                insert_data("usage_limits", limit_data)
                logger.info(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user_id} ã®åˆ©ç”¨åˆ¶é™æƒ…å ±ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
                
                # å†åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆæ­£ã—ã„user_idã‚’ä½¿ç”¨ï¼‰
                limits_check = check_usage_limits(actual_user_id, "document_upload", db)
                
                if not limits_check["is_unlimited"] and not limits_check["allowed"]:
                    raise HTTPException(
                        status_code=403,
                        detail=LIMIT_REACHED_ERROR.format(limit=limits_check['limit'])
                    )
                
                return {
                    "allowed": True,
                    "remaining": limits_check.get("remaining") if not limits_check["is_unlimited"] else None,
                    "limit_reached": False
                }
            except Exception as create_error:
                logger.error(f"åˆ©ç”¨åˆ¶é™æƒ…å ±ã®è‡ªå‹•ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(create_error)}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼ˆç®¡ç†è€…ã®å ´åˆã¯ç„¡åˆ¶é™ï¼‰
                logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼ {user_id} ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ¶é™ã‚’é©ç”¨ã—ã¾ã™ã€‚")
                return {
                    "allowed": True,
                    "remaining": 100,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ¶é™
                    "limit_reached": False
                }
        else:
            # ãã®ä»–ã®HTTPExceptionã¯å†ç™ºç”Ÿ
            raise

async def _record_document_source(
    name: str, 
    doc_type: str, 
    page_count: int, 
    content: str, 
    user_id: str, 
    company_id: str, 
    db: Connection,
    metadata_json: str | None = None,
) -> None:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚½ãƒ¼ã‚¹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¨˜éŒ²ã™ã‚‹ï¼ˆchunksãƒ†ãƒ¼ãƒ–ãƒ«å¯¾å¿œï¼‰"""
    import time
    
    try:
        start_time = time.time()
        document_id = str(uuid.uuid4())
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯
        content_str = ensure_string(content, for_db=True)
        content_size_mb = len(content_str.encode('utf-8')) / (1024 * 1024)
        
        logger.info(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¿å­˜é–‹å§‹: {name}, ã‚µã‚¤ã‚º: {content_size_mb:.2f}MB")
        
        # âœ… ä¿®æ­£: document_sourcesã«ã¯ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ä¿å­˜
        from supabase_adapter import insert_data
        main_record = {
            "id": document_id,
            "name": name,
            "type": doc_type,
            "page_count": page_count,
            "uploaded_by": user_id,
            "company_id": company_id,
            "uploaded_at": datetime.now().isoformat(),
            "special": "knowledge APIçµŒç”±ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            "metadata": metadata_json,
        }
        
        try:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’document_sourcesã«ä¿å­˜
            insert_data("document_sources", main_record)
            logger.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {document_id}")
            
            # âœ… æ–°è¦: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’chunksãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
            await _save_content_to_chunks(document_id, content_str, name, company_id)
            logger.info(f"ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å®Œäº†: {document_id}")
            
        except Exception as main_error:
            logger.error(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(main_error)}")
            
            # å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’ç¢ºèª
            error_str = str(main_error)
            if "document_sources_uploaded_by_fkey" in error_str:
                logger.warning(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ '{user_id}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“ - company_idã§ä»£æ›¿ä¿å­˜ã‚’è©¦è¡Œ")
                try:
                    # company_idã‹ã‚‰ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’æ¤œç´¢
                    from supabase_adapter import select_data
                    company_users = select_data(
                        "users", 
                        columns="id", 
                        filters={"company_id": company_id}
                    )
                    
                    if company_users.data and len(company_users.data) > 0:
                        alternative_user_id = company_users.data[0]["id"]
                        logger.info(f"ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ç™ºè¦‹: {alternative_user_id}")
                        
                        # ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã§å†ä¿å­˜
                        main_record["uploaded_by"] = alternative_user_id
                        result = insert_data("document_sources", main_record)
                        
                        if result and result.data:
                            logger.info(f"ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜æˆåŠŸ: {alternative_user_id}")
                            # ãƒãƒ£ãƒ³ã‚¯ä¿å­˜ã‚‚å®Ÿè¡Œ
                            await _save_content_to_chunks(document_id, content_str, name, company_id)
                        else:
                            logger.error("ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã§ã‚‚ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã«å¤±æ•—")
                    else:
                        logger.error(f"Company ID {company_id} ã«é–¢é€£ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        
                except Exception as alt_error:
                    logger.error(f"ä»£æ›¿ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(alt_error)}")
            else:
                raise main_error
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚³ãƒŸãƒƒãƒˆ
        if db is not None:
            try:
                db.commit()
            except AttributeError:
                logger.debug("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«commitãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
        else:
            logger.debug("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãŒNullã®ãŸã‚commitã‚’ã‚¹ã‚­ãƒƒãƒ—")
        
        # ä¼šç¤¾ã®ã‚½ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã«è¿½åŠ 
        if company_id:
            if company_id not in knowledge_base.company_sources:
                knowledge_base.company_sources[company_id] = []
            if name not in knowledge_base.company_sources[company_id]:
                knowledge_base.company_sources[company_id].append(name)
        
        logger.info(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¿å­˜å®Œäº†: {time.time() - start_time:.1f}ç§’")
                
    except Exception as e:
        logger.error(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚½ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

async def _save_content_to_chunks(doc_id: str, content: str, doc_name: str, company_id: str) -> None:
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’chunksãƒ†ãƒ¼ãƒ–ãƒ«ã«åˆ†å‰²ä¿å­˜ã—ã€è‡ªå‹•ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã‚’å®Ÿè¡Œã™ã‚‹"""
    try:
        from supabase_adapter import insert_data
        
        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºè¨­å®šï¼ˆ300-500ãƒˆãƒ¼ã‚¯ãƒ³ â‰ˆ 1200-2000æ–‡å­—ï¼‰
        chunk_size = 1500  # ç´„400ãƒˆãƒ¼ã‚¯ãƒ³ç›¸å½“
        chunks_list = []
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åˆ†å‰²
        for i in range(0, len(content), chunk_size):
            chunk_content = content[i:i + chunk_size]
            if chunk_content.strip():  # ç©ºã®ãƒãƒ£ãƒ³ã‚¯ã¯é™¤å¤–
                chunks_list.append({
                    "doc_id": doc_id,
                    "chunk_index": i // chunk_size,
                    "content": chunk_content,
                    "company_id": company_id
                })
        
        logger.info(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’{len(chunks_list)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²: {doc_name}")
        
        # ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸€æ‹¬ä¿å­˜
        for chunk_data in chunks_list:
            insert_data("chunks", chunk_data)
        
        logger.info(f"âœ… {len(chunks_list)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å®Œäº†: {doc_name}")
        
        # ğŸ§  ãƒãƒƒãƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã‚’å®Ÿè¡Œ
        try:
            from ..batch_embedding import batch_generate_embeddings_for_document
            
            # AUTO_GENERATE_EMBEDDINGSè¨­å®šã‚’ãƒã‚§ãƒƒã‚¯
            auto_embed_enabled = os.getenv("AUTO_GENERATE_EMBEDDINGS", "false").lower() == "true"
            
            if auto_embed_enabled:
                logger.info(f"ğŸ§  ãƒãƒƒãƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆé–‹å§‹: {doc_name}")
                embedding_success = await batch_generate_embeddings_for_document(doc_id, len(chunks_list))
                
                if embedding_success:
                    logger.info(f"ğŸ‰ ãƒãƒƒãƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆå®Œäº†: {doc_name}")
                else:
                    logger.warning(f"âš ï¸ ãƒãƒƒãƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã§ä¸€éƒ¨ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {doc_name}")
            else:
                logger.info(f"ğŸ”„ AUTO_GENERATE_EMBEDDINGS=false ã®ãŸã‚ã€ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—: {doc_name}")
                
        except Exception as embedding_error:
            # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã‚¨ãƒ©ãƒ¼ã¯è­¦å‘Šã¨ã—ã¦è¨˜éŒ²ã—ã€ãƒ¡ã‚¤ãƒ³å‡¦ç†ã¯ç¶™ç¶š
            logger.warning(f"âš ï¸ ãƒãƒƒãƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã‚¨ãƒ©ãƒ¼ï¼ˆå‡¦ç†ã¯ç¶™ç¶šï¼‰: {embedding_error}")
        
    except Exception as e:
        logger.error(f"ãƒãƒ£ãƒ³ã‚¯ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

def _update_source_info(source_name: str) -> None:
    """ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’æ›´æ–°ã™ã‚‹"""
    if source_name not in knowledge_base.sources:
        knowledge_base.sources[source_name] = {}  # è¾æ›¸ã¨ã—ã¦åˆæœŸåŒ–
        knowledge_base.source_info[source_name] = {
            'timestamp': datetime.now().isoformat(),
            'active': True
        }

def _prepare_response(
    df: pd.DataFrame, 
    sections: Dict[str, str], 
    source_name: str, 
    remaining_uploads: Optional[int] = None, 
    limit_reached: bool = False
) -> Dict[str, Any]:
    """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹"""
    active_sources = get_active_resources()
    
    preview_data = []
    total_rows = 0
    
    if df is not None and not df.empty:
        preview_data = df.head(5).to_dict('records')
        # NaNå€¤ã‚’é©åˆ‡ã«å‡¦ç†
        preview_data = [{k: (None if pd.isna(v) else str(v)) for k, v in record.items()} for record in preview_data]
        total_rows = len(df)
    
    return {
        "message": f"{DEFAULT_COMPANY_NAME}ã®æƒ…å ±ãŒæ­£å¸¸ã«æ›´æ–°ã•ã‚Œã¾ã—ãŸï¼ˆ{source_name}ï¼‰",
        "columns": knowledge_base.columns if knowledge_base.data is not None else [],
        "preview": preview_data,
        "total_rows": total_rows,
        "sections": list(sections.keys()),
        "file" if not source_name.startswith(('http://', 'https://')) else "url": source_name,
        "sources": knowledge_base.sources,
        "active_sources": active_sources,
        "remaining_uploads": remaining_uploads,
        "limit_reached": limit_reached
    }

def _prepare_response_from_list(
    data_list: List[Dict], 
    sections: Dict[str, str], 
    source_name: str, 
    remaining_uploads: Optional[int] = None, 
    limit_reached: bool = False
) -> Dict[str, Any]:
    """ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆã‹ã‚‰ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹"""
    active_sources = get_active_resources()
    
    preview_data = []
    total_rows = len(data_list) if data_list else 0
    
    if data_list:
        # æœ€åˆã®5ä»¶ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ã—ã¦å–å¾—
        preview_data = data_list[:5]
        # å€¤ã‚’é©åˆ‡ã«å‡¦ç†
        preview_data = [{k: (None if v is None else str(v)) for k, v in record.items()} for record in preview_data]
    
    # åˆ—åã‚’æŠ½å‡º
    columns = []
    if data_list:
        # æœ€åˆã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‹ã‚‰åˆ—åã‚’å–å¾—
        columns = list(data_list[0].keys())
    
    return {
        "message": f"{DEFAULT_COMPANY_NAME}ã®æƒ…å ±ãŒæ­£å¸¸ã«æ›´æ–°ã•ã‚Œã¾ã—ãŸï¼ˆ{source_name}ï¼‰",
        "columns": columns,
        "preview": preview_data,
        "total_rows": total_rows,
        "sections": list(sections.keys()),
        "file" if not source_name.startswith(('http://', 'https://')) else "url": source_name,
        "sources": knowledge_base.sources,
        "active_sources": active_sources,
        "remaining_uploads": remaining_uploads,
        "limit_reached": limit_reached
    }

async def process_url(url: str, user_id: str = None, company_id: str = None, db: Connection = None):
    """URLã‚’å‡¦ç†ã—ã¦çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ã™ã‚‹"""
    try:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã®å–å¾—ã¨ä¼šç¤¾IDã®è¨­å®š
        if user_id and not company_id:
            company_id, _ = _get_user_info(user_id, db)
        
        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰åˆ¶é™ã®ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
        try:
            limits = _check_upload_limits(user_id, db)
        except HTTPException as http_error:
            # HTTPExceptionã¯ãã®ã¾ã¾å†ç™ºç”Ÿ
            raise http_error
        except Exception as limit_error:
            logger.warning(f"URLå‡¦ç†ã®åˆ©ç”¨åˆ¶é™ãƒã‚§ãƒƒã‚¯ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(limit_error)}")
            # åˆ©ç”¨åˆ¶é™ãƒã‚§ãƒƒã‚¯ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
            limits = {"remaining": None, "limit_reached": False}
        
        # URLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        extracted_text = await extract_text_from_url(url)
        if extracted_text.startswith("URLã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼:"):
            raise HTTPException(status_code=500, detail=extracted_text)
        
        # URLã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å‡¦ç†
        df, sections, processed_text = await process_url_content(url, extracted_text)
        
        # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°
        _update_knowledge_base(df, processed_text, is_file=False, source_name=url, company_id=company_id)
        
        # ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’æ›´æ–°
        _update_source_info(url)
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒã‚ã‚‹å ´åˆã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚«ã‚¦ãƒ³ãƒˆã‚’æ›´æ–°
        if user_id:
            updated_limits = update_usage_count(user_id, "document_uploads_used", db)
            await _record_document_source(url, "URL", 1, processed_text, user_id, company_id, db, None)
            db.commit()
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æº–å‚™ã—ã¦è¿”ã™
        return _prepare_response(
            df, sections, url, 
            limits.get("remaining"), 
            limits.get("limit_reached", False)
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"URLå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"URLã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

async def process_file(file: UploadFile = File(...), request: Request = None, user_id: str = None, company_id: str = None, db: Connection = None):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ã™ã‚‹"""
    logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file.filename}, ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {user_id}, ä¼šç¤¾ID: {company_id}")
    
    # åˆæœŸåŒ–
    remaining_uploads = None
    limit_reached = False
    page_count = 1
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã®å–å¾—ã¨ä¼šç¤¾IDã®è¨­å®š
    if user_id and not company_id:
        company_id, _ = _get_user_info(user_id, db)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
    if not file or not file.filename:
        raise HTTPException(status_code=400, detail="ãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ã‹ã€ãƒ•ã‚¡ã‚¤ãƒ«åãŒç„¡åŠ¹ã§ã™ã€‚")

    # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆCSVãƒ»Wordå½¢å¼ã‚’è¿½åŠ ï¼‰
    allowed_extensions = ('.xlsx', '.xls', '.pdf', '.txt', '.csv', '.doc', '.docx',
                         '.avi', '.mp4', '.webm', 
                         '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp')
    if not file.filename.endswith(allowed_extensions):
        raise HTTPException(status_code=400, detail=INVALID_FILE_ERROR)

    try:
        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰åˆ¶é™ã®ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
        try:
            limits = _check_upload_limits(user_id, db)
            remaining_uploads = limits.get("remaining")
            limit_reached = limits.get("limit_reached", False)
        except HTTPException as http_error:
            # HTTPExceptionã¯ãã®ã¾ã¾å†ç™ºç”Ÿ
            raise http_error
        except Exception as limit_error:
            logger.warning(f"åˆ©ç”¨åˆ¶é™ãƒã‚§ãƒƒã‚¯ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(limit_error)}")
            # åˆ©ç”¨åˆ¶é™ãƒã‚§ãƒƒã‚¯ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
            remaining_uploads = None
            limit_reached = False
        
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {file.filename}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        try:
            contents = await file.read()
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã€å‡¦ç†å‰ã«å°‘ã—å¾…æ©Ÿ
            await asyncio.sleep(0.1)
            
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise HTTPException(status_code=400, detail=f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        
        file_size_mb = len(contents) / (1024 * 1024)
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.2f} MB")
        
        if len(contents) == 0:
            raise HTTPException(status_code=400, detail="ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™ã€‚æœ‰åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’æ¤œçŸ¥
        file_info = detect_file_type(file.filename, contents)
        file_extension = file_info['extension']
        detected_type = file_info['file_type']
        recommended_processor = file_info['processor']
        
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼æ¤œçŸ¥çµæœ: {detected_type} (ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼: {recommended_processor}, ä¿¡é ¼åº¦: {file_info['confidence']})")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆæœŸåŒ–
        df = None
        sections = {}
        extracted_text = ""
        
        try:
            # æ¤œçŸ¥ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«åŸºã¥ã„ã¦å‡¦ç†ã‚’å®Ÿè¡Œ
            if detected_type == 'excel' or file_extension in ['xlsx', 'xls']:
                logger.info(f"Excelãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file.filename}")
                
                # å¤§ããªExcelãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†æ™‚é–“åˆ¶é™
                processing_start_time = asyncio.get_event_loop().time()
                
                try:
                    # Google Sheets APIã‚’ä½¿ç”¨ã—ã¦Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
                    # OAuth2ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ï¼ˆrequestãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
                    access_token = None
                    if request and hasattr(request, 'state'):
                        access_token = getattr(request.state, 'google_access_token', None)
                    
                    service_account_file = os.getenv('GOOGLE_SERVICE_ACCOUNT_FILE')
                    
                    logger.info(f"Google Sheets APIå‡¦ç†é–‹å§‹ - access_token: {'ã‚ã‚Š' if access_token else 'ãªã—'}, service_account: {'ã‚ã‚Š' if service_account_file else 'ãªã—'}")
                    
                    # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯å‡¦ç†å‰ã«è­¦å‘Š
                    if file_size_mb > 3:
                        logger.info(f"å¤§ããªExcelãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ{file_size_mb:.2f}MBï¼‰ã‚’å‡¦ç†ã—ã¾ã™ - æ™‚é–“ãŒã‹ã‹ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
                        # å‡¦ç†å‰ã®å°ã•ãªé…å»¶
                        await asyncio.sleep(1)
                    
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§Excelå‡¦ç†ã‚’å®Ÿè¡Œ
                    try:
                        data_list, sections, extracted_text = await asyncio.wait_for(
                            process_excel_file_with_sheets_api(
                                contents, 
                                file.filename, 
                                access_token, 
                                service_account_file
                            ),
                            timeout=MAX_PROCESSING_TIME  # 5åˆ†ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                        )
                    except asyncio.TimeoutError:
                        logger.error(f"Excelå‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ: {file.filename} ({file_size_mb:.2f}MB)")
                        raise HTTPException(
                            status_code=408, 
                            detail=f"Excelãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ{file_size_mb:.2f}MBï¼‰ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²ã™ã‚‹ã‹ã€ã‚ˆã‚Šå°ã•ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
                        )
                    
                    # å‡¦ç†æ™‚é–“ã‚’ãƒ­ã‚°å‡ºåŠ›
                    processing_time = asyncio.get_event_loop().time() - processing_start_time
                    logger.info(f"Excelå‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’")
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆã‚’ç›´æ¥ä½¿ç”¨ï¼ˆDataFrameã‚’ä½¿ç”¨ã—ãªã„ï¼‰
                    if not data_list:
                        data_list = [{
                            'section': "ãƒ‡ãƒ¼ã‚¿ãªã—",
                            'content': "Excelãƒ•ã‚¡ã‚¤ãƒ«ã«æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
                            'source': 'Excel (Google Sheets)',
                            'file': file.filename,
                            'url': None
                        }]
                    
                    # ãƒšãƒ¼ã‚¸æ•°ã‚’æ¨å®šï¼ˆã‚·ãƒ¼ãƒˆæ•°ã«åŸºã¥ãï¼‰
                    sheet_count = len(set(item.get('metadata', {}).get('sheet_name', 'Sheet1') for item in data_list))
                    page_count = max(1, sheet_count)
                    
                    logger.info(f"Excelå‡¦ç†å®Œäº†ï¼ˆGoogle Sheets APIä½¿ç”¨ï¼‰: {len(data_list)} ãƒ¬ã‚³ãƒ¼ãƒ‰, {page_count} ã‚·ãƒ¼ãƒˆ, {processing_time:.1f}ç§’")
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
                    df = None  # DataFrameã¯ä½¿ç”¨ã—ãªã„
                    
                except Exception as e:
                    logger.warning(f"Google Sheets APIå‡¦ç†ã‚¨ãƒ©ãƒ¼ã€å¾“æ¥ã®å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {str(e)}")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¾“æ¥ã®pandaså‡¦ç†
                    try:
                        df, sections, extracted_text = await asyncio.wait_for(
                            asyncio.to_thread(process_excel_file, contents, file.filename),
                            timeout=MAX_PROCESSING_TIME
                        )
                        # âš ï¸ é‡è¦ä¿®æ­£: data_listã‚’Noneã«è¨­å®šã—ãªã„ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã§ã‚‚çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ›´æ–°ã‚’ç¢ºå®Ÿã«å®Ÿè¡Œï¼‰
                        # data_list = None  # ã“ã®è¡Œã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
                        
                        # âš ï¸ é‡è¦ä¿®æ­£: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã§extracted_textã®Unnamedä¿®æ­£ã‚’å¼·åˆ¶å®Ÿè¡Œ
                        from .unnamed_column_handler import UnnamedColumnHandler
                        handler = UnnamedColumnHandler()
                        
                        # extracted_textã®Unnamedãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¿®æ­£
                        if extracted_text and "Unnamed:" in extracted_text:
                            lines = extracted_text.split('\n')
                            fixed_lines = []
                            
                            for line in lines:
                                if "Unnamed:" in line:
                                    # Unnamed: X ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¿®æ­£
                                    import re
                                    line = re.sub(r'Unnamed:\s*\d+', lambda m: f"ãƒ‡ãƒ¼ã‚¿{m.group().split(':')[1].strip() if ':' in m.group() else '1'}", line)
                                fixed_lines.append(line)
                            
                            extracted_text = '\n'.join(fixed_lines)
                            logger.info("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã§extracted_textã®Unnamedä¿®æ­£ã‚’å®Ÿè¡Œ")
                        
                        # Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚·ãƒ¼ãƒˆæ•°ã‚’å–å¾—
                        try:
                            excel_file = BytesIO(contents)
                            df_dict = pd.read_excel(excel_file, sheet_name=None)
                            
                            # å„ã‚·ãƒ¼ãƒˆã«Unnamedå‡¦ç†ã‚’é©ç”¨
                            for sheet_name, sheet_df in df_dict.items():
                                if not sheet_df.empty:
                                    try:
                                        fixed_df, modifications = handler.fix_dataframe(sheet_df, f"{file.filename}:{sheet_name}")
                                        df_dict[sheet_name] = fixed_df
                                        if modifications:
                                            logger.info(f"api.py Excelå‡¦ç† - ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã®Unnamedä¿®æ­£: {', '.join(modifications)}")
                                    except Exception as fix_error:
                                        logger.warning(f"ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã®Unnamedä¿®æ­£ã§ã‚¨ãƒ©ãƒ¼: {str(fix_error)} - å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
                                        # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å…ƒã®ã‚·ãƒ¼ãƒˆã‚’ãã®ã¾ã¾ä½¿ç”¨
                        
                            page_count = len(df_dict)
                        except Exception as excel_error:
                            logger.warning(f"Excelã‚·ãƒ¼ãƒˆæ•°å–å¾—ã‚¨ãƒ©ãƒ¼: {str(excel_error)}")
                            page_count = 1
                            
                    except asyncio.TimeoutError:
                        logger.error(f"Excelå¾“æ¥å‡¦ç†ã‚‚ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ: {file.filename}")
                        raise HTTPException(
                            status_code=408, 
                            detail=f"Excelãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºï¼ˆ{file_size_mb:.2f}MBï¼‰ãŒå¤§ãã™ãã¾ã™ã€‚"
                        )

            elif detected_type == 'csv' or is_csv_file(file.filename):
                logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file.filename}, ã‚µã‚¤ã‚º: {file_size_mb:.2f}MB")
                # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆ50MBï¼‰
                if file_size_mb > 50:
                    raise HTTPException(status_code=400, detail=f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã¾ã™ ({file_size_mb:.2f} MB)ã€‚50MBä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
                
                # æ–‡å­—åŒ–ã‘æ¤œå‡ºã§Gemini OCRå„ªå…ˆå‡¦ç†ã‚’æ±ºå®š
                from .csv_processor import detect_csv_encoding, detect_mojibake_in_content
                logger.info("CSVæ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºé–‹å§‹")
                detected_encoding = detect_csv_encoding(contents)
                logger.info(f"æ¤œå‡ºã•ã‚ŒãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {detected_encoding}")
                has_mojibake = detect_mojibake_in_content(contents, detected_encoding)
                logger.info(f"æ–‡å­—åŒ–ã‘æ¤œå‡ºçµæœ: {has_mojibake}")
                
                if has_mojibake:
                    logger.info("æ–‡å­—åŒ–ã‘æ¤œå‡º - Gemini OCRã‚’å„ªå…ˆä½¿ç”¨")
                    try:
                        # æ–‡å­—åŒ–ã‘æ¤œå‡ºæ™‚: Gemini OCRã‚’æœ€åˆã«è©¦è¡Œ
                        logger.info("æ–‡å­—åŒ–ã‘å¯¾å¿œ: Gemini OCRã‚’ä½¿ç”¨ã—ã¦CSVã‚’å‡¦ç†")
                        df, sections, extracted_text = await process_csv_with_gemini_ocr(contents, file.filename)
                        logger.info(f"Gemini OCRå‡¦ç†æˆåŠŸ: {len(df) if df is not None else 0} è¡Œ")
                        page_count = 1
                    except Exception as ocr_error:
                        logger.error(f"Gemini OCRå‡¦ç†å¤±æ•—: {str(ocr_error)}")
                        try:
                            # OCRå¤±æ•—æ™‚: Google Sheets APIã‚’è©¦è¡Œ
                            logger.info("OCRå¤±æ•— - Google Sheets APIã‚’è©¦è¡Œ")
                            from .csv_processor import process_csv_file_with_sheets_api
                            df, sections, extracted_text = await process_csv_file_with_sheets_api(contents, file.filename)
                            logger.info(f"Google Sheets APIå‡¦ç†æˆåŠŸ: {len(df) if df is not None else 0} è¡Œ")
                            page_count = 1
                        except Exception as sheets_error:
                            logger.error(f"Google Sheets APIå‡¦ç†å¤±æ•—: {str(sheets_error)}")
                            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®å‡¦ç†
                            logger.info("æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®CSVå‡¦ç†")
                            from .csv_processor import process_csv_file
                            df, sections, extracted_text = process_csv_file(contents, file.filename)
                            logger.info(f"å¾“æ¥ã®CSVå‡¦ç†çµæœ: {len(df) if df is not None else 0} è¡Œ")
                            page_count = 1
                else:
                    logger.info("æ–‡å­—åŒ–ã‘ãªã— - é€šå¸¸ã®CSVå‡¦ç†ãƒ•ãƒ­ãƒ¼")
                    try:
                        # é€šå¸¸å‡¦ç†: Google Sheets APIã‚’ä½¿ç”¨ã—ã¦CSVã‚’å‡¦ç†
                        logger.info("Google Sheets APIã‚’ä½¿ç”¨ã—ã¦CSVã‚’å‡¦ç†")
                        from .csv_processor import process_csv_file_with_sheets_api
                        df, sections, extracted_text = await process_csv_file_with_sheets_api(contents, file.filename)
                        logger.info(f"Google Sheets APIå‡¦ç†æˆåŠŸ: {len(df) if df is not None else 0} è¡Œ")
                        page_count = 1
                    except Exception as sheets_error:
                        logger.error(f"Google Sheets APIå‡¦ç†å¤±æ•—: {str(sheets_error)}")
                        try:
                            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Gemini OCRã‚’ä½¿ç”¨ã—ã¦CSVã‚’å‡¦ç†
                            logger.info("Gemini OCRã‚’ä½¿ç”¨ã—ã¦CSVã‚’å‡¦ç†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
                            df, sections, extracted_text = await process_csv_with_gemini_ocr(contents, file.filename)
                            logger.info(f"Gemini OCRå‡¦ç†æˆåŠŸ: {len(df) if df is not None else 0} è¡Œ")
                            page_count = 1
                        except Exception as csv_error:
                            logger.error(f"CSVå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(csv_error)}")
                            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®å‡¦ç†
                            logger.info("æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ: å¾“æ¥ã®CSVå‡¦ç†")
                            from .csv_processor import process_csv_file
                            df, sections, extracted_text = process_csv_file(contents, file.filename)
                            logger.info(f"å¾“æ¥ã®CSVå‡¦ç†çµæœ: {len(df) if df is not None else 0} è¡Œ")
                            page_count = 1
                
                # CSVå‡¦ç†çµæœã®æ¤œè¨¼
                logger.info(f"CSVå‡¦ç†å®Œäº†å¾Œã®æ¤œè¨¼: df={df is not None}, sections={len(sections) if sections else 0}, extracted_text={len(extracted_text) if extracted_text else 0}")
                if df is None or df.empty:
                    logger.error("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    raise HTTPException(status_code=400, detail="CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")

            elif detected_type == 'word' or is_word_file(file.filename):
                logger.info(f"Wordãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file.filename}")
                # Wordãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆ20MBï¼‰
                if file_size_mb > 20:
                    raise HTTPException(status_code=400, detail=f"Wordãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã¾ã™ ({file_size_mb:.2f} MB)ã€‚20MBä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
                
                try:
                    # Wordå‡¦ç†å‰ã«å°‘ã—å¾…æ©Ÿï¼ˆãƒ¡ãƒ¢ãƒªè² è·è»½æ¸›ï¼‰
                    await asyncio.sleep(0.2)
                    df, sections, extracted_text = await process_word_file(contents, file.filename)
                    page_count = 1
                except Exception as word_error:
                    logger.error(f"Wordå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(word_error)}")
                    # Wordå‡¦ç†ãŒå¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    from .unnamed_column_handler import UnnamedColumnHandler
                    handler = UnnamedColumnHandler()
                    
                    df = pd.DataFrame({
                        'section': ["Wordå‡¦ç†ã‚¨ãƒ©ãƒ¼"],
                        'content': [f"Wordãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(word_error)}"],
                        'source': ['Word'],
                        'file': [file.filename],
                        'url': [None]
                    })
                    
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã®DataFrameã‚‚Unnamedå‡¦ç†ã‚’é©ç”¨
                    try:
                        df, error_modifications = handler.fix_dataframe(df, f"{file.filename}_word_error")
                        if error_modifications:
                            logger.debug(f"Wordå‡¦ç†ã‚¨ãƒ©ãƒ¼ã®Unnamedä¿®æ­£: {', '.join(error_modifications)}")
                    except:
                        pass  # ã‚¨ãƒ©ãƒ¼å‡¦ç†ä¸­ã®ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
                    
                    sections = {"Wordå‡¦ç†ã‚¨ãƒ©ãƒ¼": f"Wordãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(word_error)}"}
                    extracted_text = f"=== ãƒ•ã‚¡ã‚¤ãƒ«: {file.filename} ===\n\n=== Wordå‡¦ç†ã‚¨ãƒ©ãƒ¼ ===\nWordãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(word_error)}\n\n"
                    page_count = 1

            elif detected_type == 'image' or is_image_file(file.filename):
                logger.info(f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file.filename}")
                # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆ10MBï¼‰
                if file_size_mb > 10:
                    raise HTTPException(status_code=400, detail=f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã¾ã™ ({file_size_mb:.2f} MB)ã€‚10MBä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
                
                try:
                    # ç”»åƒå‡¦ç†å‰ã«å°‘ã—å¾…æ©Ÿï¼ˆãƒ¡ãƒ¢ãƒªè² è·è»½æ¸›ï¼‰
                    await asyncio.sleep(0.2)
                    df, sections, extracted_text = await process_image_file(contents, file.filename)
                    page_count = 1
                except Exception as img_error:
                    logger.error(f"ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(img_error)}")
                    # ç”»åƒå‡¦ç†ãŒå¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    from .unnamed_column_handler import UnnamedColumnHandler
                    handler = UnnamedColumnHandler()
                    
                    df = pd.DataFrame({
                        'section': ["ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼"],
                        'content': [f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(img_error)}"],
                        'source': ['ç”»åƒ'],
                        'file': [file.filename],
                        'url': [None]
                    })
                    
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã®DataFrameã‚‚Unnamedå‡¦ç†ã‚’é©ç”¨
                    try:
                        df, error_modifications = handler.fix_dataframe(df, f"{file.filename}_image_error")
                        if error_modifications:
                            logger.debug(f"ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼ã®Unnamedä¿®æ­£: {', '.join(error_modifications)}")
                    except:
                        pass  # ã‚¨ãƒ©ãƒ¼å‡¦ç†ä¸­ã®ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
                    
                    sections = {"ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼": f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(img_error)}"}
                    extracted_text = f"=== ãƒ•ã‚¡ã‚¤ãƒ«: {file.filename} ===\n\n=== ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼ ===\nç”»åƒãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(img_error)}\n\n"
                    page_count = 1
                    
            elif detected_type == 'pdf' or file_extension == 'pdf':
                logger.info(f"PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file.filename}")
                
                # PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã‚‹å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™
                if file_size_mb > 10:
                    raise HTTPException(status_code=400, detail=PDF_SIZE_ERROR.format(size=file_size_mb))
                
                try:
                    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®æœ‰åŠ¹æ€§ã‚’ç¢ºèª
                    pdf_file = BytesIO(contents)
                    pdf_reader = PyPDF2.PdfReader(pdf_file)
                    page_count = len(pdf_reader.pages)
                    
                    logger.info(f"PDFãƒšãƒ¼ã‚¸æ•°: {page_count}")
                    
                    # ãƒšãƒ¼ã‚¸æ•°ãŒå¤šã„å ´åˆã¯å‡¦ç†å‰ã«è­¦å‘Š
                    if page_count > 8:
                        logger.info(f"å¤§ããªPDFãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ{page_count}ãƒšãƒ¼ã‚¸ï¼‰ã‚’åˆ†å‰²å‡¦ç†ã—ã¾ã™")
                        await asyncio.sleep(0.5)  # å‡¦ç†å‰ã®å¾…æ©Ÿ
                    
                    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
                    df, sections, extracted_text = await process_pdf_file(contents, file.filename)
                except PyPDF2.errors.PdfReadError as pdf_err:
                    raise HTTPException(status_code=400, detail=f"ç„¡åŠ¹ãªPDFãƒ•ã‚¡ã‚¤ãƒ«ã§ã™: {str(pdf_err)}")
                except Exception as pdf_ex:
                    logger.error(f"PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã®ã‚¨ãƒ©ãƒ¼: {str(pdf_ex)}")
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
                    from .unnamed_column_handler import UnnamedColumnHandler
                    handler = UnnamedColumnHandler()
                    
                    df = pd.DataFrame({
                        'section': ["ã‚¨ãƒ©ãƒ¼"],
                        'content': [f"PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(pdf_ex)}"],
                        'source': ['PDF'],
                        'file': [file.filename],
                        'url': [None]
                    })
                    
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã®DataFrameã‚‚Unnamedå‡¦ç†ã‚’é©ç”¨
                    try:
                        df, error_modifications = handler.fix_dataframe(df, f"{file.filename}_pdf_error")
                        if error_modifications:
                            logger.debug(f"PDFå‡¦ç†ã‚¨ãƒ©ãƒ¼ã®Unnamedä¿®æ­£: {', '.join(error_modifications)}")
                    except:
                        pass  # ã‚¨ãƒ©ãƒ¼å‡¦ç†ä¸­ã®ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
                    
                    sections = {"ã‚¨ãƒ©ãƒ¼": f"PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(pdf_ex)}"}
                    extracted_text = f"=== ãƒ•ã‚¡ã‚¤ãƒ«: {file.filename} ===\n\n=== ã‚¨ãƒ©ãƒ¼ ===\nPDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(pdf_ex)}\n\n"
                    
            elif detected_type == 'text' or file_extension == 'txt':
                logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file.filename}")
                df, sections, extracted_text = await process_txt_file(contents, file.filename)
                
            elif detected_type == 'video' or file_extension in ['avi', 'mp4', 'webm']:
                if file_size_mb > 500:
                    raise HTTPException(status_code=400, detail=VIDEO_SIZE_ERROR.format(size=file_size_mb))
                
                df, sections, extracted_text = _process_video_file(contents, file.filename)
                
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å†…å®¹ã‚’ç¢ºèª
            if df is None or df.empty:
                logger.warning("ç©ºã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ")
                # ç©ºã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å ´åˆã€æœ€ä½é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®š
                from .unnamed_column_handler import UnnamedColumnHandler
                handler = UnnamedColumnHandler()
                
                df = pd.DataFrame({
                    'section': ["ä¸€èˆ¬æƒ…å ±"],
                    'content': ["ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"],
                    'source': [file_extension.upper()],
                    'file': [file.filename],
                    'url': [None]
                })
                
                # ç©ºã®DataFrameã‚‚Unnamedå‡¦ç†ã‚’é©ç”¨
                try:
                    df, empty_modifications = handler.fix_dataframe(df, f"{file.filename}_empty")
                    if empty_modifications:
                        logger.debug(f"ç©ºDataFrameå‡¦ç†ã®Unnamedä¿®æ­£: {', '.join(empty_modifications)}")
                except:
                    pass  # ã‚¨ãƒ©ãƒ¼å‡¦ç†ä¸­ã®ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
                
        except HTTPException:
            raise
        except Exception as e:
            error_type = {
                'xlsx': 'Excel', 'xls': 'Excel',
                'pdf': 'PDF',
                'txt': 'ãƒ†ã‚­ã‚¹ãƒˆ',
                'csv': 'CSV',
                'doc': 'Word', 'docx': 'Word',
                'jpg': 'ç”»åƒ', 'jpeg': 'ç”»åƒ', 'png': 'ç”»åƒ', 'gif': 'ç”»åƒ',
                'bmp': 'ç”»åƒ', 'tiff': 'ç”»åƒ', 'tif': 'ç”»åƒ', 'webp': 'ç”»åƒ'
            }.get(file_extension, 'ãƒ•ã‚¡ã‚¤ãƒ«')
            
            logger.error(f"{error_type}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}", exc_info=True)
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼ã®ç‰¹åˆ¥å‡¦ç†
            if "timeout" in str(e).lower():
                raise HTTPException(status_code=408, detail=TIMEOUT_ERROR)
            else:
                raise HTTPException(status_code=500, detail=f"{error_type}ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        
        # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜ï¼‰
        knowledge_base_updated = False  # æ›´æ–°çŠ¶æ³ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
        
        if 'data_list' in locals() and data_list:
            # æ–°ã—ã„Google Sheets APIå‡¦ç†ã®å ´åˆï¼šdata_listã‚’ç›´æ¥ä½¿ç”¨
            logger.info(f"data_listã‚’ä½¿ç”¨ã—ã¦çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°: {len(data_list)} ãƒ¬ã‚³ãƒ¼ãƒ‰")
            _update_knowledge_base_from_list(data_list, extracted_text, is_file=True, source_name=file.filename, company_id=company_id)
            knowledge_base_updated = True
        elif df is not None and not df.empty:
            # å¾“æ¥ã®pandaså‡¦ç†ã®å ´åˆï¼šDataFrameã‚’ä½¿ç”¨
            logger.info(f"DataFrameã‚’ä½¿ç”¨ã—ã¦çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°: {len(df)} è¡Œ")
            # ãƒ•ã‚¡ã‚¤ãƒ«åˆ—ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
            if 'file' not in df.columns:
                df['file'] = file.filename
                
            # ã™ã¹ã¦ã®åˆ—ã®å€¤ã‚’é©åˆ‡ã«å¤‰æ›ï¼ˆNULLå€¤ã¯ãã®ã¾ã¾ä¿æŒï¼‰
            for col in df.columns:
                # NULLã§ãªã„å€¤ã®ã¿ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
                df[col] = df[col].apply(lambda x: str(x) if pd.notna(x) else None)
                
            # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°
            _update_knowledge_base(df, extracted_text, is_file=True, source_name=file.filename, company_id=company_id)
            knowledge_base_updated = True
        else:
            # ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®å ´åˆã§ã‚‚æœ€ä½é™ã®ã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆ
            logger.warning(f"ãƒ‡ãƒ¼ã‚¿ãŒç©ºã¾ãŸã¯ç„¡åŠ¹ã§ã™ - æœ€ä½é™ã®ã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆ: {file.filename}")
            if file.filename not in knowledge_base.sources:
                knowledge_base.sources[file.filename] = {}
            knowledge_base.sources[file.filename]["å‡¦ç†çµæœ"] = extracted_text or f"ãƒ•ã‚¡ã‚¤ãƒ« '{file.filename}' ã‚’å‡¦ç†ã—ã¾ã—ãŸ"
            knowledge_base_updated = True
        
        # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ›´æ–°å®Œäº†ã‚’ãƒ­ã‚°å‡ºåŠ›
        logger.info(f"çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ›´æ–°å®Œäº†: {knowledge_base_updated}, ã‚½ãƒ¼ã‚¹æ•°: {len(knowledge_base.sources)}")
        
        # ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’æ›´æ–°
        _update_source_info(file.filename)
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒã‚ã‚‹å ´åˆã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚«ã‚¦ãƒ³ãƒˆã‚’æ›´æ–°
        if user_id:
            updated_limits = update_usage_count(user_id, "document_uploads_used", db)
            if updated_limits:
                remaining_uploads = updated_limits["document_uploads_limit"] - updated_limits["document_uploads_used"]
                limit_reached = remaining_uploads <= 0
            else:
                # åˆ©ç”¨åˆ¶é™ãŒå–å¾—ã§ããªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                logger.warning(f"åˆ©ç”¨åˆ¶é™ã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ - user_id: {user_id}")
                remaining_uploads = None
                limit_reached = False
            
            # Excelã®å ´åˆã€date_typesã‚’é›†ç´„ã—ã¦metadataã«ä¿å­˜
            metadata_json = None
            if detected_type == 'excel' and 'data_list' in locals() and data_list:
                try:
                    import json as _json
                    # åˆ—åé›†åˆ
                    col_set = set()
                    date_types_union: dict[str, str] = {}
                    for rec in data_list:
                        md = rec.get('metadata', {}) if rec else {}
                        cols = md.get('columns', [])
                        col_set.update(cols)
                        dt_map = md.get('date_types', {})
                        date_types_union.update(dt_map)
                    metadata_json = _json.dumps({
                        "columns": list(col_set),
                        "date_types": date_types_union
                    }, ensure_ascii=False)
                except Exception as _merr:
                    logger.warning(f"metadata_json ç”Ÿæˆå¤±æ•—: {_merr}")
            
            # pandaså‡¦ç†ã®å ´åˆã‚‚metadataã‚’ç”Ÿæˆ
            elif detected_type == 'excel' and df is not None and not df.empty:
                try:
                    import json as _json
                    # DataFrameã‹ã‚‰åˆ—åã‚’å–å¾—
                    columns = df.columns.tolist()
                    # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å€¤ã‹ã‚‰æ—¥ä»˜åˆ—ã‚’è‡ªå‹•æ¤œå‡º
                    date_types = {}
                    
                    for col in columns:
                        # ã“ã®åˆ—ã®å€¤ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                        sample_values = []
                        for idx in range(min(10, len(df))):  # æœ€åˆã®10è¡Œã‚’ã‚µãƒ³ãƒ—ãƒ«
                            value = df.iloc[idx][col]
                            if pd.notna(value) and str(value).strip():
                                sample_values.append(str(value).strip())
                        
                        if sample_values:
                            # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
                            date_like_count = 0
                            for value in sample_values:
                                if _is_date_like_pandas(value):
                                    date_like_count += 1
                            
                            # 70%ä»¥ä¸ŠãŒæ—¥ä»˜ã£ã½ã„å ´åˆã¯æ—¥ä»˜åˆ—ã¨ã—ã¦åˆ¤å®š
                            if date_like_count >= len(sample_values) * 0.7:
                                date_types[col] = "date"
                    
                    metadata_json = _json.dumps({
                        "columns": columns,
                        "date_types": date_types
                    }, ensure_ascii=False)
                    logger.info(f"pandaså‡¦ç†ã§metadataç”Ÿæˆ: columns={len(columns)}, date_types={len(date_types)}")
                except Exception as _merr:
                    logger.warning(f"pandaså‡¦ç†metadata_json ç”Ÿæˆå¤±æ•—: {_merr}")
            
            await _record_document_source(file.filename, file_extension.upper(), page_count, extracted_text, user_id, company_id, db, metadata_json)
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚³ãƒŸãƒƒãƒˆï¼ˆdbãŒNoneã®å ´åˆã¯å®‰å…¨ã«ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            if db is not None:
                try:
                    db.commit()
                except AttributeError:
                    # dbã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«commitãƒ¡ã‚½ãƒƒãƒ‰ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    logger.debug("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«commitãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
            else:
                logger.debug("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãŒNullã®ãŸã‚commitã‚’ã‚¹ã‚­ãƒƒãƒ—")
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æº–å‚™ã—ã¦è¿”ã™
        if 'data_list' in locals() and data_list:
            # æ–°ã—ã„Google Sheets APIå‡¦ç†ã®å ´åˆ
            logger.info(f"æœ€çµ‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹æº–å‚™: data_list={len(data_list)}ãƒ¬ã‚³ãƒ¼ãƒ‰, sections={len(sections)}, filename={file.filename}")
            response = _prepare_response_from_list(data_list, sections, file.filename, remaining_uploads, limit_reached)
        else:
            # å¾“æ¥ã®pandaså‡¦ç†ã®å ´åˆ
            logger.info(f"æœ€çµ‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹æº–å‚™: df={len(df) if df is not None else 0}è¡Œ, sections={len(sections)}, filename={file.filename}")
            response = _prepare_response(df, sections, file.filename, remaining_uploads, limit_reached)
        
        logger.info(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹æº–å‚™å®Œäº†: total_rows={response.get('total_rows', 0)}, preview_rows={len(response.get('preview', []))}")
        return response
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ä¸­ã®äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

async def toggle_resource_active(resource_name: str):
    """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–çŠ¶æ…‹ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹"""
    if resource_name not in knowledge_base.sources:
        raise HTTPException(status_code=404, detail=f"ãƒªã‚½ãƒ¼ã‚¹ '{resource_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    # ç¾åœ¨ã®çŠ¶æ…‹ã‚’å–å¾—
    current_state = knowledge_base.source_info.get(resource_name, {}).get('active', True)
    
    # çŠ¶æ…‹ã‚’åè»¢
    new_state = not current_state
    
    # çŠ¶æ…‹ã‚’æ›´æ–°
    if resource_name not in knowledge_base.source_info:
        knowledge_base.source_info[resource_name] = {}
    
    knowledge_base.source_info[resource_name]['active'] = new_state
    
    return {
        "name": resource_name,
        "active": new_state,
        "message": f"ãƒªã‚½ãƒ¼ã‚¹ '{resource_name}' ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–çŠ¶æ…‹ã‚’ {new_state} ã«å¤‰æ›´ã—ã¾ã—ãŸ"
    }

async def get_uploaded_resources():
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ï¼ˆURLã€PDFã€Excelã€TXTï¼‰ã®æƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
    logger.info(f"get_uploaded_resources é–‹å§‹ - knowledge_base.sources ã®å‹: {type(knowledge_base.sources)}")
    logger.info(f"knowledge_base.sources ã®å†…å®¹: {knowledge_base.sources}")
    logger.info(f"knowledge_base.source_info ã®å†…å®¹: {knowledge_base.source_info}")
    
    resources = []
    
    # knowledge_base.sources ãŒè¾æ›¸ã®å ´åˆã¨ãƒªã‚¹ãƒˆã®å ´åˆã‚’ä¸¡æ–¹å¯¾å¿œ
    sources_to_process = []
    if isinstance(knowledge_base.sources, dict):
        logger.info("knowledge_base.sources ã¯è¾æ›¸ã¨ã—ã¦æ‰±ã„ã¾ã™")
        sources_to_process = list(knowledge_base.sources.keys())
    elif isinstance(knowledge_base.sources, list):
        logger.info("knowledge_base.sources ã¯ãƒªã‚¹ãƒˆã¨ã—ã¦æ‰±ã„ã¾ã™")
        sources_to_process = knowledge_base.sources
    else:
        logger.warning(f"knowledge_base.sources ã®å‹ãŒäºˆæœŸã—ãªã„å‹ã§ã™: {type(knowledge_base.sources)}")
        sources_to_process = []
    
    logger.info(f"å‡¦ç†å¯¾è±¡ã®ã‚½ãƒ¼ã‚¹æ•°: {len(sources_to_process)}")
    
    for source in sources_to_process:
        info = knowledge_base.source_info.get(source, {})
        
        # ãƒªã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
        if source.startswith(('http://', 'https://')):
            resource_type = "URL"
        else:
            extension = source.split('.')[-1].lower() if '.' in source else ""
            resource_type = {
                'xlsx': 'Excel', 'xls': 'Excel',
                'pdf': 'PDF',
                'txt': 'ãƒ†ã‚­ã‚¹ãƒˆ',
                'csv': 'CSV',
                'doc': 'Word', 'docx': 'Word',
                'avi': 'Video', 'mp4': 'Video', 'webm': 'Video',
                'jpg': 'ç”»åƒ', 'jpeg': 'ç”»åƒ', 'png': 'ç”»åƒ', 'gif': 'ç”»åƒ', 
                'bmp': 'ç”»åƒ', 'tiff': 'ç”»åƒ', 'tif': 'ç”»åƒ', 'webp': 'ç”»åƒ'
            }.get(extension, "ãã®ä»–")
        
        resources.append({
            "name": source,
            "type": resource_type,
            "timestamp": info.get('timestamp', datetime.now().isoformat()),
            "active": info.get('active', True)
        })
        
        logger.info(f"ãƒªã‚½ãƒ¼ã‚¹è¿½åŠ : {source} (ã‚¿ã‚¤ãƒ—: {resource_type})")
    
    logger.info(f"get_uploaded_resources å®Œäº† - {len(resources)}ä»¶ã®ãƒªã‚½ãƒ¼ã‚¹")
    
    return {
        "resources": resources,
        "message": f"{len(resources)}ä»¶ã®ãƒªã‚½ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ"
    }

async def cleanup_unnamed_columns(company_id: str = None):
    """æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®Unnamedã‚«ãƒ©ãƒ ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹"""
    
    try:
        handler = UnnamedColumnHandler()
        updated_count = 0
        
        # knowledge_baseã®å†…å®¹ã‚’ç¢ºèªãƒ»ä¿®æ­£
        for source_name in list(knowledge_base.sources.keys()):
            sections = knowledge_base.sources[source_name]
            updated_sections = {}
            
            for section_name, content in sections.items():
                # contentã‚’è¡Œã”ã¨ã«åˆ†å‰²ã—ã¦å‡¦ç†
                lines = content.split('\n')
                updated_lines = []
                
                for line in lines:
                    # ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã®è¡Œã‚’æ¤œå‡ºï¼ˆ|ã§åŒºåˆ‡ã‚‰ã‚Œã¦ã„ã‚‹ï¼‰
                    if '|' in line and line.count('|') >= 2:
                        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã®å‡¦ç†
                        parts = [part.strip() for part in line.split('|')]
                        if parts and parts[0] == '':
                            parts = parts[1:]  # æœ€åˆã®ç©ºè¦ç´ ã‚’å‰Šé™¤
                        if parts and parts[-1] == '':
                            parts = parts[:-1]  # æœ€å¾Œã®ç©ºè¦ç´ ã‚’å‰Šé™¤
                        
                        # unnamedãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¿®æ­£
                        updated_parts = []
                        for i, part in enumerate(parts):
                            if handler._is_unnamed_pattern(part):
                                if i == 0 and part.strip() in ['', 'unnamed', 'Unnamed']:
                                    # æœ€åˆã®ã‚«ãƒ©ãƒ ãŒç©ºã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                                    continue
                                else:
                                    # æ„å‘³ã®ã‚ã‚‹åå‰ã«å¤‰æ›´
                                    updated_parts.append(f"ã‚«ãƒ©ãƒ {i+1}")
                            else:
                                updated_parts.append(part)
                        
                        if updated_parts:
                            updated_lines.append('| ' + ' | '.join(updated_parts) + ' |')
                        else:
                            updated_lines.append(line)
                    else:
                        # é€šå¸¸ã®è¡Œã¯ãã®ã¾ã¾
                        updated_lines.append(line)
                
                updated_content = '\n'.join(updated_lines)
                if updated_content != content:
                    updated_sections[section_name] = updated_content
                    updated_count += 1
                else:
                    updated_sections[section_name] = content
            
            # æ›´æ–°ã•ã‚ŒãŸsectionsã§ç½®æ›
            knowledge_base.sources[source_name] = updated_sections
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {updated_count}å€‹ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ›´æ–°")
        
        return {
            "success": True,
            "updated_sections": updated_count,
            "message": f"{updated_count}å€‹ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®Unnamedã‚«ãƒ©ãƒ ã‚’ä¿®æ­£ã—ã¾ã—ãŸ"
        }
        
    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "message": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
        }
"""
PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
PDFãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨å‡¦ç†ã‚’è¡Œã„ã¾ã™ï¼ˆæ–‡å­—åŒ–ã‘å¯¾å¿œå¼·åŒ–ç‰ˆï¼‰
"""
import pandas as pd
import pypdf as PyPDF2  # Security fix: Using pypdf instead of PyPDF2
from io import BytesIO
import re
import traceback
import logging
from typing import List, Optional, Tuple
from .ocr import ocr_pdf_to_text_from_bytes
from ..database import ensure_string
from .unnamed_column_handler import UnnamedColumnHandler

logger = logging.getLogger(__name__)

# æ–‡å­—åŒ–ã‘ä¿®å¾©ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ—¥æœ¬èªPDFç‰¹æœ‰ã®å•é¡Œï¼‰
MOJIBAKE_MAPPING = {
    # PyPDF2ã§ã‚ˆãç™ºç”Ÿã™ã‚‹æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³
    'ç¸º': 'ã„',
    'ç¸ºã‚…â†’': 'ã‚ã¨',
    'ç¸ºï½¨': 'ã¨',
    'ç¸ºï½ª': 'ãª',
    'ç¸ºï½«': 'ã«',
    'ç¸ºï½®': 'ã®',
    'ç¸ºï½¯': 'ã¯',
    'ç¸ºï½¾': 'ã¾',
    'ç¸ºï½¿': 'ã¿',
    'ç¸ºï½§': 'ã§',
    'ç¸ºï½©': 'ã©',
    'ç¸ºï½°': 'ã°',
    'ç¸ºï½±': 'ã±',
    'ç¸ºï½²': 'ã²',
    'ç¸ºï½³': 'ã³',
    'ç¸ºï½´': 'ã´',
    'ç¸ºï½µ': 'ãµ',
    'ç¸ºï½¶': 'ã¶',
    'ç¸ºï½·': 'ã·',
    'ç¸ºï½¸': 'ã¸',
    'ç¸ºï½¹': 'ã¹',
    'ç¸ºï½º': 'ãº',
    'ç¸ºï½»': 'ã»',
    'ç¸ºï½¼': 'ã¼',
    'ç¸ºï½½': 'ã½',
    'ç¸ºï½¾': 'ã¾',
    'ç¸ºï½¿': 'ã¿',
    'ç¹§': 'ã‚¢',
    'ç¹§ï½¦': 'ã‚¦',
    'ç¹§ï½¨': 'ã‚¨',
    'ç¹§ï½ª': 'ã‚ª',
    'ç¹§ï½«': 'ã‚«',
    'ç¹§ï½¬': 'ã‚¬',
    'ç¹§ï½­': 'ã‚­',
    'ç¹§ï½®': 'ã‚®',
    'ç¹§ï½¯': 'ã‚¯',
    'ç¹§ï½°': 'ã‚°',
    'ç¹§ï½±': 'ã‚±',
    'ç¹§ï½²': 'ã‚²',
    'ç¹§ï½³': 'ã‚³',
    'ç¹§ï½´': 'ã‚´',
    'ç¹§ï½µ': 'ã‚µ',
    'ç¹§ï½¶': 'ã‚¶',
    'ç¹§ï½·': 'ã‚·',
    'ç¹§ï½¸': 'ã‚¸',
    'ç¹§ï½¹': 'ã‚¹',
    'ç¹§ï½º': 'ã‚º',
    'ç¹§ï½»': 'ã‚»',
    'ç¹§ï½¼': 'ã‚¼',
    'ç¹§ï½½': 'ã‚½',
    'ç¹§ï½¾': 'ã‚¾',
    'ç¹§ï½¿': 'ã‚¿',
    'ç¹': 'ãƒ€',
    'ç¹ï½‚': 'ãƒ',
    'ç¹ï½ƒ': 'ãƒƒ',
    'ç¹ï½„': 'ãƒ…',
    'ç¹ï½…': 'ãƒ†',
    'ç¹ï½†': 'ãƒ‡',
    'ç¹ï½§': 'ãƒˆ',
    'ç¹ï½ˆ': 'ãƒ‰',
    'ç¹ï½‰': 'ãƒŠ',
    'ç¹ï½Š': 'ãƒ‹',
    
    # æ–°ã—ã„æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼å ±å‘Šã®ã‚‚ã®å«ã‚€ï¼‰
    'Ø¬àºŠÎÎ Ï‚Ïœ': 'ãƒãƒ«ãƒãƒ‰ãƒ©ã‚¤ãƒ–',
    'Ï…Ï¥Î Ï’Ê—': 'ãƒ‰ãƒ©ã‚¤ãƒ–',
    'ÏšÏ§Î½Ï…Ï¥Î Ï’': 'ãƒãƒ«ãƒãƒ‰ãƒ©ã‚¤ãƒ–',
    'ÏÏ‡Î»Ê”Ê—': 'ãƒ¢ãƒ‹ã‚¿ãƒ¼',
    'Ü•Ó·à¦¥': 'å‹ç•ª',
    'à¶ªà¥ªà±¥à¡ŒÎ¹Ï‘Ï„': 'æ¨™æº–æ­è¼‰ã‚½ãƒ•ãƒˆ',
    'MpDF': 'PDF',
    'PNF': 'HOME',
    'VTJOFTT': 'BUSINESS',
    'PP': 'ã‚¢ãƒ—ãƒª',
    'VBSE': 'Guard',
    'PMp': 'Solo',
    'BOPOJNBHF': 'Canonimage',
    'BSFEFTLUPQ': 'WARE Desktop',
    'ÖÜ—à©‡à¹': 'å¤–å½¢å¯¸æ³•',
    'à£­à¾”': 'è³ªé‡',
    'Ê·': 'mm',
    'á¶±': '(mm)',
    'LH': 'kg',
    'ÏŒÎ¿Ï‚Ï¦Ê”ØšÎ‰': 'ãƒãƒƒãƒ†ãƒªãƒ¼å«ã‚€',
    'ÎµÏ–Î¿Î«': 'ã‚¹ãƒšãƒƒã‚¯',
    'JOEPXT': 'Windows',
    'SPCJU': 'Pro bit',
    '16': 'CPU',
    'PSF': 'Core',
    'MUSB': 'Ultra',
    '16()[': 'UHzGHz',
    'ÏÏÏ¦Ê—': 'ãƒ¡ãƒ¢ãƒªãƒ¼',
    '%%3': 'DDR',
    '(#': 'GB',
    'ÏƒÎŸÎµÎ«Ê—': 'ãƒ‡ã‚£ã‚¹ã‚¯',
    '44%': 'SSD',
    
    # è¿½åŠ ã®æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆåˆ†è§£ã—ã¦è©³ç´°ã«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
    'Ø¬àºŠ': 'ãƒãƒ«ãƒ',
    'ÎÎ ': 'ãƒ‰ãƒ©ã‚¤ãƒ–',
    'Ï‚Ïœ': 'ãƒ‰ãƒ©ã‚¤ãƒ–',
    'Ï…Ï¥': 'ãƒ‰ãƒ©ã‚¤',
    'Î Ï’': 'ãƒ–',
    'Ê—%7%': 'ãƒ¼ DVD ',
    'ÏšÏ§Î½': 'ãƒãƒ«ãƒ',
    'ÏÏ‡Î»': 'ãƒ¢ãƒ‹ã‚¿ãƒ¼',
    'Ê”Ê—%': 'ãƒ¼',
    ')Ü•': 'å‹',
    'Ó·à¦¥': 'ç•ª',
    'à¶ªà¥ª': 'æ¨™æº–',
    'à±¥à¡Œ': 'æ­è¼‰',
    'Î¹Ï‘Ï„': 'ã‚½ãƒ•ãƒˆ',
    '0GpDF': 'PDF',
    ')PNF': 'HOME',
    '#VTJOFTT': 'BUSINESS',
    'Ê¢104': '(POS',
    '"àµ›Ê£': 'ç‰ˆ)',
    '"QQ': 'App',
    '(VBSE': 'Guard',
    '4PMP': 'Solo',
    '$BOPOJ': 'Canoni',
    'NBHF': 'mage',
    '8"3&': 'WARE',
    '%FTLUPQ': 'Desktop',
    'ÖÜ—': 'å¤–å½¢',
    'à©‡à¹': 'å¯¸æ³•',
    'É¾': 'ãƒ»',
    'à£­': 'è³ª',
    'à¾”': 'é‡',
    'Ê¢8': '(W',
    'Ê¢)': '(H',
    'Ê¢%': '(D',
    'à»¿': 'ç´„',
    'ÏŒÎ¿': 'ãƒãƒƒ',
    'Ï‚Ï¦': 'ãƒ†ãƒª',
    'Ê”Øš': 'ãƒ¼å«',
    'Î‰': 'ã‚€',
    'Ê£Ë”': ')',
    'ÎµÏ–': 'ã‚¹ãƒš',
    'Î¿Î«': 'ãƒƒã‚¯',
    '04Ê—': 'OS:',
    '8JOEPXT': 'Windows',
    '1SP': 'Pro',
    'CJU': ' bit',
    '$16Ê—': 'CPU:',
    '$PSF': 'Core',
    '6MUSB': 'Ultra',
    '6()': 'GHz',
    'ÏÏ': 'ãƒ¡ãƒ¢',
    'Ï¦Ê—': 'ãƒªãƒ¼',
    '%%3': 'DDR',
    'ÏƒÎŸ': 'ãƒ‡ã‚£',
    'ÎµÎ«': 'ã‚¹ã‚¯',
    '44%': 'SSD',
    
    # è¨˜å·æ–‡å­—åŒ–ã‘ï¼ˆæ›´æ–°ç‰ˆï¼‰
    'Ê—': 'ãƒ¼',
    'Ê¢': '(',
    'Ê£': ')',
    'Ë”': 'ãƒ»',
    'ÎÎ ': 'AI',
    'Î¹Ï‘Ï„': 'ã‚½ãƒ•ãƒˆ',
    'Ï‚Ïœ': 'ã‚¹ãƒ†ãƒ ',
    'ÎÏ“': 'ã‚¢ãƒ—ãƒª',
    '%7%': 'DVD',
    '%)': ')',
    'É¾': 'ãƒ»',
    'á¶±': '(mm)',
}

# Unicodeæ­£è¦åŒ–ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆã‚ˆã‚ŠåŒ…æ‹¬çš„ï¼‰
UNICODE_NORMALIZATION = {
    # åŠè§’ã‚«ã‚¿ã‚«ãƒŠã‚’å…¨è§’ã«
    'ï½±': 'ã‚¢', 'ï½²': 'ã‚¤', 'ï½³': 'ã‚¦', 'ï½´': 'ã‚¨', 'ï½µ': 'ã‚ª',
    'ï½¶': 'ã‚«', 'ï½·': 'ã‚­', 'ï½¸': 'ã‚¯', 'ï½¹': 'ã‚±', 'ï½º': 'ã‚³',
    'ï½»': 'ã‚µ', 'ï½¼': 'ã‚·', 'ï½½': 'ã‚¹', 'ï½¾': 'ã‚»', 'ï½¿': 'ã‚½',
    'ï¾€': 'ã‚¿', 'ï¾': 'ãƒ', 'ï¾‚': 'ãƒ„', 'ï¾ƒ': 'ãƒ†', 'ï¾„': 'ãƒˆ',
    'ï¾…': 'ãƒŠ', 'ï¾†': 'ãƒ‹', 'ï¾‡': 'ãƒŒ', 'ï¾ˆ': 'ãƒ', 'ï¾‰': 'ãƒ',
    'ï¾Š': 'ãƒ', 'ï¾‹': 'ãƒ’', 'ï¾Œ': 'ãƒ•', 'ï¾': 'ãƒ˜', 'ï¾': 'ãƒ›',
    'ï¾': 'ãƒ', 'ï¾': 'ãƒŸ', 'ï¾‘': 'ãƒ ', 'ï¾’': 'ãƒ¡', 'ï¾“': 'ãƒ¢',
    'ï¾”': 'ãƒ¤', 'ï¾•': 'ãƒ¦', 'ï¾–': 'ãƒ¨',
    'ï¾—': 'ãƒ©', 'ï¾˜': 'ãƒª', 'ï¾™': 'ãƒ«', 'ï¾š': 'ãƒ¬', 'ï¾›': 'ãƒ­',
    'ï¾œ': 'ãƒ¯', 'ï½¦': 'ãƒ²', 'ï¾': 'ãƒ³',
    'ï½¯': 'ãƒƒ', 'ï½¬': 'ãƒ£', 'ï½­': 'ãƒ¥', 'ï½®': 'ãƒ§',
    'ï½°': 'ãƒ¼',
}

def fix_mojibake_text(text: str) -> str:
    """æ–‡å­—åŒ–ã‘ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿®å¾©ã™ã‚‹ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
    if not text:
        return text
    
    fixed_text = text
    
    # ğŸ¯ ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚«ãƒ¼å‰Šé™¤ï¼ˆæœ€å„ªå…ˆï¼‰
    fixed_text = re.sub(r'=== ãƒšãƒ¼ã‚¸ \d+ ===', '', fixed_text)
    fixed_text = re.sub(r'=== Page \d+ ===', '', fixed_text)
    fixed_text = re.sub(r'--- Page \d+ ---', '', fixed_text)
    fixed_text = re.sub(r'=== ãƒ•ã‚¡ã‚¤ãƒ«: .* ===', '', fixed_text)
    
    # ğŸ¯ è‘—ä½œæ¨©æƒ…å ±ã¨ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ•ãƒƒã‚¿ãƒ¼ã‚’å‰Šé™¤
    fixed_text = re.sub(r'Copyright \d{4}-\d{4} Â© .* All Rights Reserved', '', fixed_text)
    fixed_text = re.sub(r'Company Secret', '', fixed_text)
    fixed_text = re.sub(r'VER\d{6} -\d{2}-\d{2}', '', fixed_text)
    
    # ğŸ¯ å…¨è§’ãƒ”ãƒªã‚ªãƒ‰ã‚’åŠè§’ã«æ­£è¦åŒ–
    fixed_text = fixed_text.replace('ã€‚', '.')
    fixed_text = fixed_text.replace('ï¼', '.')
    fixed_text = fixed_text.replace('ï¼Œ', ',')
    
    # ğŸ¯ ä¼šç¤¾åã®æ­£è¦åŒ–
    fixed_text = fixed_text.replace('Noã€‚1', 'No.1')
    fixed_text = fixed_text.replace('COã€‚,LTDã€‚', 'CO.,LTD.')
    
    # ğŸ¯ Unicodeæ­£è¦åŒ–ã‚’é©ç”¨
    for half_char, full_char in UNICODE_NORMALIZATION.items():
        fixed_text = fixed_text.replace(half_char, full_char)
    
    # é‡åº¦ã®æ–‡å­—åŒ–ã‘ãŒã‚ã‚‹å ´åˆã®ã¿ä¿®å¾©ã‚’é©ç”¨
    if check_text_corruption(fixed_text):
        # æ–‡å­—åŒ–ã‘ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’é©ç”¨ï¼ˆé•·ã„ã‚‚ã®ã‹ã‚‰é †ã«å‡¦ç†ï¼‰
        sorted_mapping = sorted(MOJIBAKE_MAPPING.items(), key=lambda x: len(x[0]), reverse=True)
        for mojibake, correct in sorted_mapping:
            fixed_text = fixed_text.replace(mojibake, correct)
        
        # CIDã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å»
        fixed_text = re.sub(r'\(cid:\d+\)', '', fixed_text)
        
        # é€£ç¶šã™ã‚‹æ–‡å­—åŒ–ã‘æ–‡å­—ã‚’é™¤å»ï¼ˆå®Ÿéš›ã®æ–‡å­—åŒ–ã‘æ–‡å­—ã®ã¿ï¼‰
        fixed_text = re.sub(r'[ç¸ºç¹ç¹§]{3,}', '[æ–‡å­—åŒ–ã‘]', fixed_text)
        
        # ç½®æ›æ–‡å­—ã‚’å‡¦ç†
        fixed_text = fixed_text.replace('\ufffd', '') # å®Œå…¨ã«å‰Šé™¤
    
    # ğŸ¯ ä½™åˆ†ãªç©ºç™½ã‚’æ•´ç†ï¼ˆå¼·åŒ–ç‰ˆï¼‰
    fixed_text = re.sub(r'\s+', ' ', fixed_text)
    fixed_text = re.sub(r'\n\s*\n', '\n\n', fixed_text)  # ä½™åˆ†ãªæ”¹è¡Œã‚’å‰Šé™¤
    fixed_text = fixed_text.strip()
    
    return fixed_text

def check_text_corruption(text: str) -> bool:
    """ãƒ†ã‚­ã‚¹ãƒˆã«æ–‡å­—åŒ–ã‘ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
    if not text or len(text) < 10:
        return False
    
    # æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡ºï¼ˆæ›´æ–°ç‰ˆï¼‰
    corruption_patterns = [
        # å¾“æ¥ã®æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³
        r'[ç¸ºç¹ç¹§]{2,}',  # é€£ç¶šã™ã‚‹æ–‡å­—åŒ–ã‘æ–‡å­—
        r'\(cid:\d+\)',   # CIDã‚¨ãƒ©ãƒ¼
        r'[\ufffd]+',     # ç½®æ›æ–‡å­—
        
        # æ–°ã—ã„æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³
        r'[Ø¬àºŠ]{1,}',     # ã‚¢ãƒ©ãƒ“ã‚¢æ–‡å­—ãªã©ã®æ··å…¥
        r'[ÎÎ Ï‚ÏœÏ…Ï¥]{2,}', # ã‚®ãƒªã‚·ãƒ£æ–‡å­—ã®æ··å…¥
        r'[Ê—Ê¢Ê£Ë”]{1,}',  # ç‰¹æ®Šè¨˜å·ã®æ··å…¥
        r'[à£­Ü•]{1,}',     # ä»–è¨€èªæ–‡å­—ã®æ··å…¥
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç ´æãƒ‘ã‚¿ãƒ¼ãƒ³
        r'[A-Z]{4,}[A-Z]{4,}', # é€£ç¶šã™ã‚‹å¤§æ–‡å­—ï¼ˆVTJOFTTç­‰ï¼‰
        r'%%\d+',        # %% + æ•°å­—ãƒ‘ã‚¿ãƒ¼ãƒ³
        r'\d+\(\)\[',     # æ•°å­— + () + [ãƒ‘ã‚¿ãƒ¼ãƒ³
    ]
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã«ã‚ˆã‚‹æ–‡å­—åŒ–ã‘æ¤œå‡º
    corruption_count = 0
    for pattern in corruption_patterns:
        matches = re.findall(pattern, text)
        corruption_count += len(matches)
    
    # æ–‡å­—åŒ–ã‘æ–‡å­—ã®å‰²åˆã‚’è¨ˆç®—
    total_chars = len(text)
    corruption_ratio = corruption_count / total_chars if total_chars > 0 else 0
    
    # æ–‡å­—åŒ–ã‘ã¨åˆ¤å®šã™ã‚‹æ¡ä»¶
    is_corrupted = (
        corruption_ratio > 0.05 or  # 5%ä»¥ä¸ŠãŒæ–‡å­—åŒ–ã‘æ–‡å­—ï¼ˆæ•æ„Ÿã«ï¼‰
        corruption_count > 3 or     # 3å€‹ä»¥ä¸Šã®æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ•æ„Ÿã«ï¼‰
        'ç¸º' in text or           # ç¢ºå®Ÿãªæ–‡å­—åŒ–ã‘æ–‡å­—
        'ç¹§' in text or
        'ç¹' in text or
        'Ø¬àºŠ' in text or          # æ–°ãƒ‘ã‚¿ãƒ¼ãƒ³
        'ÎÎ Ï‚Ïœ' in text or
        'ÏšÏ§Î½Ï…Ï¥Î Ï’' in text or
        'Ü•Ó·à¦¥' in text or
        'Ø¬àºŠÎÎ Ï‚Ïœ' in text or     # å ±å‘Šã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³
        'Ï…Ï¥Î Ï’Ê—%7%' in text or
        'ÏÏ‡Î»Ê”Ê—%' in text or
        'à¶ªà¥ªà±¥à¡ŒÎ¹Ï‘Ï„' in text or
        'VTJOFTT' in text or      # è‹±èªã®æ–‡å­—åŒ–ã‘
        'JOEPXT' in text or
        '#VTJOFTT' in text or
        '8JOEPXT' in text
    )
    
    if is_corrupted:
        print(f"æ–‡å­—åŒ–ã‘æ¤œå‡º: æ–‡å­—åŒ–ã‘ç‡ {corruption_ratio:.2%}, ãƒ‘ã‚¿ãƒ¼ãƒ³æ•° {corruption_count}")
    
    return is_corrupted

def _check_legacy_corruption(text: str) -> bool:
    """å¾“æ¥ã®PDFæ–‡å­—åŒ–ã‘æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯"""
    if not text or len(text.strip()) == 0:
        return True
    
    # ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•
    text_length = len(text)
    
    # æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
    corruption_indicators = [
        # æ„å‘³ã®ãªã„æ–‡å­—ã®é€£ç¶š
        '\ufffd' in text,  # ç½®æ›æ–‡å­—ï¼ˆæ–‡å­—åŒ–ã‘ï¼‰
        'ï¿½ï¿½' in text,      # æ–‡å­—åŒ–ã‘è¨˜å·
        
        # æ¥µç«¯ã«çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆï¼ˆç”»åƒã‚„è¤‡é›‘ãªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®å ´åˆï¼‰
        text_length < 50 and not any(char.isalnum() for char in text),
        
        # ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆãƒ»ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ã®æ¯”ç‡ãŒæ¥µç«¯ã«ä½ã„
        len([char for char in text if char.isalpha() or 
             '\u3040' <= char <= '\u309F' or  # ã²ã‚‰ãŒãª
             '\u30A0' <= char <= '\u30FF' or  # ã‚«ã‚¿ã‚«ãƒŠ
             '\u4E00' <= char <= '\u9FAF'     # æ¼¢å­—
            ]) / text_length < 0.05 if text_length > 0 else True,
        
        # æ¥µç«¯ã«å¤šãã®æ”¹è¡Œã‚„ç©ºç™½
        text.count('\n') / text_length > 0.6 if text_length > 0 else False,
        text.count(' ') / text_length > 0.8 if text_length > 0 else False,
        
        # åˆ¶å¾¡æ–‡å­—ãŒå¤šã„
        len([char for char in text if ord(char) < 32 and char not in '\n\r\t']) / text_length > 0.1 if text_length > 0 else False,
        
        # é«˜ä½Unicodeæ–‡å­—ãŒå¤šã„ï¼ˆæ–‡å­—åŒ–ã‘å¯èƒ½æ€§ï¼‰
        len([char for char in text if ord(char) > 65535]) / text_length > 0.05 if text_length > 0 else False,
        
        # æ„å‘³ã®ãªã„æ–‡å­—åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³
        text.count('ï¿½') > 10,  # æ–‡å­—åŒ–ã‘æ–‡å­—ãŒå¤šã„
        
        # PDFã‹ã‚‰ã‚ˆãå‡ºã‚‹æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³
        text.count('(cid:') > 5,  # PDFã®CIDã‚¨ãƒ©ãƒ¼
        
        # åŒã˜æ–‡å­—ã®ç•°å¸¸ãªç¹°ã‚Šè¿”ã—
        any(text.count(char) / text_length > 0.3 for char in set(text) if char.isprintable()) if text_length > 10 else False,
    ]
    
    corruption_count = sum(corruption_indicators)
    
    # è¤‡æ•°ã®æŒ‡æ¨™ã§æ–‡å­—åŒ–ã‘ã¨åˆ¤å®š
    if corruption_count >= 2:
        return True
    
    # ç‰¹ã«å¼·ã„æŒ‡æ¨™ã®å ´åˆã¯å˜ç‹¬ã§ã‚‚æ–‡å­—åŒ–ã‘ã¨åˆ¤å®š
    strong_indicators = [
        '\ufffd' in text,
        'ï¿½ï¿½' in text,
        text.count('(cid:') > 5,
        len([char for char in text if ord(char) > 65535]) / text_length > 0.1 if text_length > 0 else False,
    ]
    
    return any(strong_indicators)

def extract_tables_from_text(text: str) -> List[pd.DataFrame]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æŠ½å‡ºã—ã¦DataFrameã«å¤‰æ›"""
    tables = []
    try:
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã‚’æ¤œå‡º
        lines = text.split('\n')
        table_lines = []
        in_table = False
        
        for line in lines:
            line = line.strip()
            if '|' in line and len(line.split('|')) >= 3:
                # ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œã®å¯èƒ½æ€§
                table_lines.append(line)
                in_table = True
            elif in_table and not line:
                # ç©ºè¡Œã§ãƒ†ãƒ¼ãƒ–ãƒ«çµ‚äº†
                if len(table_lines) >= 2:  # ãƒ˜ãƒƒãƒ€ãƒ¼ + å°‘ãªãã¨ã‚‚1è¡Œã®ãƒ‡ãƒ¼ã‚¿
                    df = _parse_markdown_table(table_lines)
                    if df is not None and not df.empty:
                        tables.append(df)
                table_lines = []
                in_table = False
            elif in_table and '|' not in line:
                # ãƒ†ãƒ¼ãƒ–ãƒ«ä»¥å¤–ã®è¡Œã§ãƒ†ãƒ¼ãƒ–ãƒ«çµ‚äº†
                if len(table_lines) >= 2:
                    df = _parse_markdown_table(table_lines)
                    if df is not None and not df.empty:
                        tables.append(df)
                table_lines = []
                in_table = False
        
        # æœ€å¾Œã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‡¦ç†
        if table_lines and len(table_lines) >= 2:
            df = _parse_markdown_table(table_lines)
            if df is not None and not df.empty:
                tables.append(df)
        
        # æ”¹è‰¯ã•ã‚ŒãŸæ¤œå‡º: ç¸¦ã«ä¸¦ã‚“ã ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¨æ¸¬
        if not tables:
            tables.extend(_extract_tabular_data_from_lines(lines))
        
    except Exception as e:
        logger.warning(f"ãƒ†ãƒ¼ãƒ–ãƒ«æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    return tables

def _parse_markdown_table(table_lines: List[str]) -> Optional[pd.DataFrame]:
    """ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã®è¡Œã‚’DataFrameã«å¤‰æ›"""
    try:
        if len(table_lines) < 2:
            return None
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æŠ½å‡º
        header_line = table_lines[0]
        headers = [cell.strip() for cell in header_line.split('|') if cell.strip()]
        
        # åŒºåˆ‡ã‚Šè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ---ãªã©ãŒå«ã¾ã‚Œã‚‹è¡Œï¼‰
        data_start = 1
        if len(table_lines) > 1 and re.search(r'[-:]+', table_lines[1]):
            data_start = 2
        
        # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’æŠ½å‡º
        data_rows = []
        for line in table_lines[data_start:]:
            cells = [cell.strip() for cell in line.split('|') if cell.strip() or True]
            # ç©ºã®ã‚»ãƒ«ã‚‚ä¿æŒã—ã¤ã¤ã€è¡Œå…¨ä½“ãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª
            if any(cell.strip() for cell in cells):
                # ãƒ˜ãƒƒãƒ€ãƒ¼æ•°ã«åˆã‚ã›ã¦è¡Œã‚’èª¿æ•´
                while len(cells) < len(headers):
                    cells.append('')
                data_rows.append(cells[:len(headers)])
        
        if not data_rows:
            return None
        
        df = pd.DataFrame(data_rows, columns=headers)
        return df
        
    except Exception as e:
        logger.warning(f"ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«è§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def _extract_tabular_data_from_lines(lines: List[str]) -> List[pd.DataFrame]:
    """è¡Œãƒªã‚¹ãƒˆã‹ã‚‰è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã‚’æ¨æ¸¬ã—ã¦æŠ½å‡º"""
    tables = []
    try:
        # é€£ç¶šã™ã‚‹è¡Œã§åŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åŒºåˆ‡ã‚Šæ–‡å­—ã‚’æŒã¤è¡Œã‚’æ¢ã™
        potential_tables = []
        current_table_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                if len(current_table_lines) >= 3:  # æœ€ä½3è¡Œã§ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã¿ãªã™
                    potential_tables.append(current_table_lines.copy())
                current_table_lines = []
                continue
            
            # ã‚¿ãƒ–ã€è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ã€ã‚³ãƒ­ãƒ³ãªã©ã®åŒºåˆ‡ã‚Šæ–‡å­—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
            separators = ['\t', '  ', ':', ',', ';']
            found_separator = None
            max_splits = 0
            
            for sep in separators:
                splits = len(line.split(sep))
                if splits > max_splits and splits >= 2:
                    max_splits = splits
                    found_separator = sep
            
            if found_separator and max_splits >= 2:
                current_table_lines.append((line, found_separator, max_splits))
            else:
                if len(current_table_lines) >= 3:
                    potential_tables.append(current_table_lines.copy())
                current_table_lines = []
        
        # æœ€å¾Œã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‡¦ç†
        if len(current_table_lines) >= 3:
            potential_tables.append(current_table_lines.copy())
        
        # å„å€™è£œã‚’DataFrameã«å¤‰æ›
        for table_lines in potential_tables:
            df = _convert_lines_to_dataframe(table_lines)
            if df is not None and not df.empty and len(df.columns) >= 2:
                tables.append(df)
    
    except Exception as e:
        logger.warning(f"è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    return tables

def _convert_lines_to_dataframe(table_lines: List[Tuple[str, str, int]]) -> Optional[pd.DataFrame]:
    """è¡Œãƒªã‚¹ãƒˆã‹ã‚‰DataFrameã‚’ä½œæˆ"""
    try:
        if len(table_lines) < 3:
            return None
        
        # æœ€ã‚‚ä¸€èˆ¬çš„ãªåŒºåˆ‡ã‚Šæ–‡å­—ã‚’ç‰¹å®š
        separator_counts = {}
        for _, sep, _ in table_lines:
            separator_counts[sep] = separator_counts.get(sep, 0) + 1
        
        most_common_sep = max(separator_counts, key=separator_counts.get)
        
        # åŒã˜åŒºåˆ‡ã‚Šæ–‡å­—ã‚’ä½¿ç”¨ã™ã‚‹è¡Œã®ã¿ã‚’ä½¿ç”¨
        filtered_lines = []
        for line, sep, _ in table_lines:
            if sep == most_common_sep:
                filtered_lines.append(line)
        
        if len(filtered_lines) < 3:
            return None
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
        data_rows = []
        for line in filtered_lines:
            cells = [cell.strip() for cell in line.split(most_common_sep)]
            data_rows.append(cells)
        
        # æœ€ã‚‚å¤šã„åˆ—æ•°ã‚’ç‰¹å®š
        max_cols = max(len(row) for row in data_rows)
        if max_cols < 2:
            return None
        
        # å…¨ã¦ã®è¡Œã‚’åŒã˜åˆ—æ•°ã«èª¿æ•´
        for row in data_rows:
            while len(row) < max_cols:
                row.append('')
        
        # æœ€åˆã®è¡Œã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦ä½¿ç”¨
        headers = data_rows[0] if data_rows else []
        data = data_rows[1:] if len(data_rows) > 1 else []
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒç©ºã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåã‚’è¨­å®š
        for i, header in enumerate(headers):
            if not header or header.isspace():
                headers[i] = f'åˆ—{i+1}'
        
        if not data:
            return None
        
        df = pd.DataFrame(data, columns=headers)
        return df
        
    except Exception as e:
        logger.warning(f"DataFrameå¤‰æ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def split_ocr_text_into_sections(text: str, filename: str) -> list:
    """OCRçµæœã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†å‰²ã—ã€ãƒ†ãƒ¼ãƒ–ãƒ«ã‚‚å‡¦ç†ã™ã‚‹"""
    sections = []
    
    # ã¾ãšãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æŠ½å‡º
    extracted_tables = extract_tables_from_text(text)
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã¯Unnamedã‚«ãƒ©ãƒ ä¿®æ­£ã‚’é©ç”¨
    if extracted_tables:
        handler = UnnamedColumnHandler()
        
        for i, table_df in enumerate(extracted_tables):
            try:
                # ãƒ†ãƒ¼ãƒ–ãƒ«ã®Unnamedã‚«ãƒ©ãƒ å•é¡Œã‚’ä¿®æ­£
                fixed_df, modifications = handler.fix_dataframe(table_df, f"{filename}_table_{i+1}")
                
                if modifications:
                    logger.info(f"PDF ãƒ†ãƒ¼ãƒ–ãƒ« {i+1} ã®Unnamedã‚«ãƒ©ãƒ ä¿®æ­£: {', '.join(modifications)}")
                
                # ä¿®æ­£ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã—ã¦è¿½åŠ 
                table_sections = handler.create_clean_sections(fixed_df, filename)
                for section in table_sections:
                    section['section'] = f"ãƒ†ãƒ¼ãƒ–ãƒ«{i+1}_{section['section']}"
                    section['source'] = 'PDF Table'
                sections.extend(table_sections)
                
            except Exception as e:
                logger.warning(f"ãƒ†ãƒ¼ãƒ–ãƒ« {i+1} ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                sections.append({
                    'section': f"ãƒ†ãƒ¼ãƒ–ãƒ«{i+1}ï¼ˆç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰",
                    'content': table_df.to_string(),
                    'source': 'PDF Table',
                    'file': filename,
                    'url': None
                })
    
    # ãƒšãƒ¼ã‚¸åŒºåˆ‡ã‚Šã§é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
    page_parts = text.split('--- Page ')
    
    for i, part in enumerate(page_parts):
        if not part.strip():
            continue
            
        # ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æŠ½å‡º
        lines = part.split('\n')
        if i == 0:
            # æœ€åˆã®éƒ¨åˆ†ï¼ˆãƒšãƒ¼ã‚¸åŒºåˆ‡ã‚Šã®å‰ã®éƒ¨åˆ†ï¼‰
            section_name = "æ¦‚è¦"
            content = part.strip()
        else:
            # ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æŠ½å‡º
            page_line = lines[0] if lines else ""
            page_num = page_line.split('---')[0].strip() if '---' in page_line else str(i)
            section_name = f"ãƒšãƒ¼ã‚¸ {page_num}"
            content = '\n'.join(lines[1:]).strip() if len(lines) > 1 else ""
        
        if content:
            sections.append({
                'section': str(section_name),
                'content': str(content),
                'source': 'PDF',
                'file': filename,
                'url': None
            })
    
    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒç©ºã®å ´åˆã¯å…¨ä½“ã‚’ä¸€ã¤ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã—ã¦è¿”ã™
    if not sections:
        sections.append({
            'section': "å…¨ä½“",
            'content': str(text),
            'source': 'PDF', 
            'file': filename,
            'url': None
        })
    
    return sections

async def process_pdf_file(contents, filename):
    """PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™ï¼ˆGemini 2.5 Flash OCRå®Œç’§ç‰ˆï¼‰"""
    try:
        # ã¾ãšGemini 2.5 Flash OCRã‚’è©¦è¡Œï¼ˆæœ€é«˜å“è³ªï¼‰
        logger.info(f"ğŸš€ Gemini 2.5 Flash OCRå„ªå…ˆã§PDFå‡¦ç†é–‹å§‹: {filename}")
        
        try:
            # Gemini 2.5 Flash OCRã‚’ä½¿ç”¨ã—ã¦PDFå‡¦ç†
            from .gemini_flash_ocr import ocr_pdf_with_gemini_flash
            
            logger.info(f"ğŸ”„ Gemini 2.5 Flash OCRã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºä¸­: {filename}")
            ocr_text = await ocr_pdf_with_gemini_flash(contents)
            
            if ocr_text and ocr_text.strip() and not ocr_text.startswith("OCRå‡¦ç†ã‚¨ãƒ©ãƒ¼"):
                # OCRçµæœã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³åŒ–
                sections = {}
                all_data = []
                full_text = f"=== ãƒ•ã‚¡ã‚¤ãƒ«: {filename} (Gemini 2.5 Flash OCR) ===\n\n"
                
                # ãƒšãƒ¼ã‚¸ã”ã¨ã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†å‰²
                pages = ocr_text.split("--- ãƒšãƒ¼ã‚¸")
                for i, page_content in enumerate(pages):
                    if not page_content.strip():
                        continue
                    
                    # ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æŠ½å‡º
                    lines = page_content.strip().split('\n')
                    if lines and lines[0].strip().endswith("---"):
                        page_num_line = lines[0].replace("---", "").strip()
                        page_content_lines = lines[1:]
                    else:
                        page_num_line = f"ãƒšãƒ¼ã‚¸ {i + 1}"
                        page_content_lines = lines
                    
                    page_text = '\n'.join(page_content_lines).strip()
                    
                    if page_text:
                        section_name = page_num_line
                        sections[section_name] = page_text
                        all_data.append({
                            'section': section_name,
                            'content': page_text,
                            'source': 'PDF (Gemini 2.5 Flash OCR)',
                            'file': filename,
                            'url': None
                        })
                        
                        full_text += f"=== {section_name} ===\n{page_text}\n\n"
                
                if all_data:
                    # DataFrameä½œæˆ
                    import pandas as pd
                    df = pd.DataFrame(all_data)
                    for col in df.columns:
                        df[col] = df[col].apply(ensure_string)
                    
                    logger.info(f"âœ… Gemini 2.5 Flash OCRã§æ­£å¸¸ã«å‡¦ç†å®Œäº†: {filename} ({len(all_data)} ã‚»ã‚¯ã‚·ãƒ§ãƒ³)")
                    return df, sections, full_text
                else:
                    logger.warning(f"âš ï¸ Gemini 2.5 Flash OCRã§ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸ: {filename}")
                    raise Exception("Gemini Flash OCR section processing failed")
            else:
                logger.warning(f"âš ï¸ Gemini 2.5 Flash OCRã§å‡¦ç†ã§ãã¾ã›ã‚“ã§ã—ãŸ: {filename}")
                raise Exception("Gemini Flash OCR processing failed")
                
        except Exception as ocr_error:
            logger.warning(f"âš ï¸ Gemini 2.5 Flash OCRå‡¦ç†ã‚¨ãƒ©ãƒ¼: {ocr_error}")
            logger.info(f"ğŸ”„ PyMuPDFãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨: {filename}")
            
            # PyMuPDFãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ã®å‡¦ç†
            try:
                result = await process_pdf_with_pymupdf(contents, filename)
                if result is not None:
                    df, sections, extracted_text = result
                    logger.info(f"âœ… PyMuPDFãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§æ­£å¸¸ã«å‡¦ç†å®Œäº†: {filename}")
                    return df, sections, extracted_text
                else:
                    logger.warning(f"âš ï¸ PyMuPDFã§å‡¦ç†ã§ãã¾ã›ã‚“ã§ã—ãŸ: {filename}")
                    raise Exception("PyMuPDF processing failed")
            except Exception as pymupdf_error:
                logger.warning(f"âš ï¸ PyMuPDFå‡¦ç†ã‚¨ãƒ©ãƒ¼: {pymupdf_error}")
                logger.info(f"ğŸ”„ PyPDF2æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨: {filename}")
                
                # PyPDF2æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ã®å‡¦ç†
                return await _process_pdf_with_pypdf2_fallback(contents, filename)
            
    except Exception as e:
        print(f"PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ç©ºã®DataFrame
        import pandas as pd
        empty_df = pd.DataFrame(columns=['section', 'content', 'source', 'file', 'url'])
        empty_sections = {"ã‚¨ãƒ©ãƒ¼": f"PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"}
        error_text = f"=== ãƒ•ã‚¡ã‚¤ãƒ«: {filename} ===\n\n=== ã‚¨ãƒ©ãƒ¼ ===\nPDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\n\n"
        
        return empty_df, empty_sections, error_text

async def _process_pdf_with_pypdf2_fallback(contents, filename):
    """PyPDF2ã‚’ä½¿ç”¨ã—ãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†"""
    # BytesIOã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
    pdf_file = BytesIO(contents)
    
    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    pdf_reader = PyPDF2.PdfReader(pdf_file)
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
    all_text = ""
    sections = {}
    extracted_text = f"=== ãƒ•ã‚¡ã‚¤ãƒ«: {filename} (PyPDF2ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯) ===\n\n"
    
    for i, page in enumerate(pdf_reader.pages):
        try:
            # å¼·åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚’ä½¿ç”¨
            page_text = extract_text_with_encoding_fallback(page)
            
            # Ensure page_text is not None and convert to string if needed
            if page_text is not None:
                page_text = ensure_string(page_text).replace('\x00', '') # ğŸ§¼ Remove NUL characters
                
                # æ–‡å­—åŒ–ã‘ä¿®æ­£ã‚’é©ç”¨
                fixed_text = fix_mojibake_text(page_text)
                
                if fixed_text.strip():
                    section_name = f"ãƒšãƒ¼ã‚¸ {i+1}"
                    sections[section_name] = fixed_text
                    all_text += fixed_text + "\n"
                    extracted_text += f"{fixed_text}\n\n"
                else:
                    logger.debug(f"ãƒšãƒ¼ã‚¸ {i+1} ã§ãƒ†ã‚­ã‚¹ãƒˆãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            else:
                logger.debug(f"ãƒšãƒ¼ã‚¸ {i+1} ã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºçµæœãŒNullã§ã—ãŸ")
                
        except Exception as page_error:
            logger.warning(f"ãƒšãƒ¼ã‚¸ {i+1} ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: {page_error}")
            continue
    
    # DataFrameã‚’ä½œæˆ
    import pandas as pd
    data_list = []
    for section_name, content in sections.items():
        data_list.append({
            'section': section_name,
            'content': content,
            'source': 'PDF (PyPDF2)',
            'file': filename,
            'url': None
        })
    
    df = pd.DataFrame(data_list) if data_list else pd.DataFrame(columns=['section', 'content', 'source', 'file', 'url'])
    
    # å„åˆ—ã‚’æ–‡å­—åˆ—ã¨ã—ã¦ç¢ºä¿
    for col in df.columns:
        df[col] = df[col].apply(ensure_string)
    
    logger.info(f"PyPDF2ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†å®Œäº†: {filename} ({len(sections)} ã‚»ã‚¯ã‚·ãƒ§ãƒ³)")
    return df, sections, extracted_text

async def process_pdf_with_pymupdf(contents: bytes, filename: str):
    """PyMuPDF ã‚’ç”¨ã„ã¦ PDF ã‹ã‚‰ç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹

    Gemini ã® OCR ã‚’ä½¿ç”¨ã›ãšã€PDF å†…ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ãã®ã¾ã¾å–å¾—ã—ã¾ã™ã€‚
    æ–‡å­—åŒ–ã‘ä¿®æ­£ã‚‚é©ç”¨ã—ã€ãƒšãƒ¼ã‚¸å˜ä½ã§ã‚»ã‚¯ã‚·ãƒ§ãƒ³åŒ–ã—ã¦ DataFrame ã‚’è¿”ã—ã¾ã™ã€‚
    """
    try:
        try:
            import fitz  # PyMuPDF
        except ImportError:
            error_msg = """PyMuPDF (fitz) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚
            
PDFã‚’é©åˆ‡ã«å‡¦ç†ã™ã‚‹ãŸã‚ã«ã€PyMuPDFã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:

pip install PyMuPDF

PyMuPDFã¯Popplerã«ä¾å­˜ã—ãªã„é«˜æ€§èƒ½ãªPDFå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚
ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å†èµ·å‹•ã—ã¦ãã ã•ã„ã€‚

ç¾åœ¨ã¯PyPDF2ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"""
            
            logger.warning(error_msg)
            return None

        logger.info(f"PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹ï¼ˆPyMuPDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºä½¿ç”¨ï¼‰: {filename}")

        # PyMuPDF ã§ãƒã‚¤ãƒˆåˆ—ã‚’ç›´æ¥é–‹ã
        with fitz.open(stream=contents, filetype="pdf") as doc:
            sections = {}
            all_data = []
            full_text = f"=== ãƒ•ã‚¡ã‚¤ãƒ«: {filename} (PyMuPDF æŠ½å‡º) ===\n\n"

            for page_num, page in enumerate(doc, start=1):
                try:
                    # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã€‚layout é¸æŠã¯ "text" ã§ã‚·ãƒ³ãƒ—ãƒ«ã«å–å¾—
                    page_text = page.get_text("text") or ""

                    # æ–‡å­—åŒ–ã‘ä¿®æ­£ã‚’è©¦ã¿ã‚‹
                    fixed_text = fix_mojibake_text(page_text)

                    if not fixed_text.strip():
                        # ç©ºã¾ãŸã¯ä¿®æ­£å¾Œã‚‚ç©ºã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        logger.debug(f"ãƒšãƒ¼ã‚¸ {page_num} ã§æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã—ãŸ")
                        continue

                    section_name = f"ãƒšãƒ¼ã‚¸ {page_num}"
                    sections[section_name] = fixed_text
                    all_data.append({
                        "section": section_name,
                        "content": fixed_text,
                        "source": "PDF (PyMuPDF)",
                        "file": filename,
                        "url": None,
                    })

                    full_text += f"=== {section_name} ===\n{fixed_text}\n\n"
                except Exception as page_error:
                    logger.warning(f"ãƒšãƒ¼ã‚¸ {page_num} ã® PyMuPDF æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {page_error}")
                    continue

        if not all_data:
            logger.warning("PyMuPDF ã§æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return None

        # DataFrame ç”Ÿæˆ
        import pandas as pd
        result_df = pd.DataFrame(all_data)
        for col in result_df.columns:
            result_df[col] = result_df[col].apply(ensure_string)

        logger.info(f"PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†ï¼ˆPyMuPDF æŠ½å‡ºï¼‰: {len(result_df)} ã‚»ã‚¯ã‚·ãƒ§ãƒ³")
        return result_df, sections, full_text

    except Exception as e:
        logger.error(f"PyMuPDF PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return None 

def extract_text_with_encoding_fallback(page) -> str:
    """è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚’è©¦è¡Œ"""
    encodings_to_try = ['utf-8', 'cp932', 'shift_jis', 'euc-jp', 'iso-2022-jp']
    
    # ã¾ãšæ¨™æº–çš„ãªæŠ½å‡ºã‚’è©¦è¡Œ
    try:
        text = page.extract_text()
        if text and not check_text_corruption(text):
            return fix_mojibake_text(text)
    except Exception as e:
        print(f"æ¨™æº–æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
    
    # æ–‡å­—åŒ–ã‘ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€è¤‡æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
    for encoding in encodings_to_try:
        try:
            # PyPDF2ã®å†…éƒ¨å‡¦ç†ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å¼·åˆ¶
            text = page.extract_text(visitor_text=lambda text, cm, tm, fontDict, fontSize: text)
            if text:
                # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¤‰æ›ã‚’è©¦è¡Œ
                try:
                    if isinstance(text, bytes):
                        text = text.decode(encoding, errors='ignore')
                    elif isinstance(text, str):
                        # ä¸€åº¦ãƒã‚¤ãƒˆåŒ–ã—ã¦ã‹ã‚‰å†ãƒ‡ã‚³ãƒ¼ãƒ‰
                        text = text.encode('latin1', errors='ignore').decode(encoding, errors='ignore')
                except Exception:
                    continue
                
                # ä¿®å¾©å‡¦ç†ã‚’é©ç”¨
                fixed_text = fix_mojibake_text(text)
                if fixed_text and not check_text_corruption(fixed_text):
                    print(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° {encoding} ã§ä¿®å¾©æˆåŠŸ")
                    return fixed_text
        except Exception as e:
            print(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° {encoding} ã§æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    # ã™ã¹ã¦å¤±æ•—ã—ãŸå ´åˆã¯æ¨™æº–æŠ½å‡ºçµæœã‚’ä¿®å¾©ã—ã¦è¿”ã™
    try:
        text = page.extract_text() or ""
        return fix_mojibake_text(text)
    except Exception:
        return "[ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—]" 
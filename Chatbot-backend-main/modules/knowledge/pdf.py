"""
PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
PDFãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨å‡¦ç†ã‚’è¡Œã„ã¾ã™ï¼ˆæ–‡å­—åŒ–ã‘å¯¾å¿œå¼·åŒ–ç‰ˆï¼‰
"""
import pandas as pd
import PyPDF2
from io import BytesIO
import re
import traceback
import logging
from typing import List, Optional, Tuple
from .ocr import ocr_pdf_to_text_from_bytes
from ..database import ensure_string
from .unnamed_column_handler import UnnamedColumnHandler

logger = logging.getLogger(__name__)

# æ–‡å­—åŒ–ã‘ä¿®å¾©ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ—¥æœ¬èªPDFç‰¹æœ‰ã®å•é¡Œï¼‰
MOJIBAKE_MAPPING = {
    # PyPDF2ã§ã‚ˆãç™ºç”Ÿã™ã‚‹æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³
    'ç¸º': 'ã„',
    'ç¸ºã‚…â†’': 'ã‚ã¨',
    'ç¸ºï½¨': 'ã¨',
    'ç¸ºï½ª': 'ãª',
    'ç¸ºï½«': 'ã«',
    'ç¸ºï½®': 'ã®',
    'ç¸ºï½¯': 'ã¯',
    'ç¸ºï½¾': 'ã¾',
    'ç¸ºï½¿': 'ã¿',
    'ç¸ºï½§': 'ã§',
    'ç¸ºï½©': 'ã©',
    'ç¸ºï½°': 'ã°',
    'ç¸ºï½±': 'ã±',
    'ç¸ºï½²': 'ã²',
    'ç¸ºï½³': 'ã³',
    'ç¸ºï½´': 'ã´',
    'ç¸ºï½µ': 'ãµ',
    'ç¸ºï½¶': 'ã¶',
    'ç¸ºï½·': 'ã·',
    'ç¸ºï½¸': 'ã¸',
    'ç¸ºï½¹': 'ã¹',
    'ç¸ºï½º': 'ãº',
    'ç¸ºï½»': 'ã»',
    'ç¸ºï½¼': 'ã¼',
    'ç¸ºï½½': 'ã½',
    'ç¸ºï½¾': 'ã¾',
    'ç¸ºï½¿': 'ã¿',
    'ç¹§': 'ã‚¢',
    'ç¹§ï½¦': 'ã‚¦',
    'ç¹§ï½¨': 'ã‚¨',
    'ç¹§ï½ª': 'ã‚ª',
    'ç¹§ï½«': 'ã‚«',
    'ç¹§ï½¬': 'ã‚¬',
    'ç¹§ï½­': 'ã‚­',
    'ç¹§ï½®': 'ã‚®',
    'ç¹§ï½¯': 'ã‚¯',
    'ç¹§ï½°': 'ã‚°',
    'ç¹§ï½±': 'ã‚±',
    'ç¹§ï½²': 'ã‚²',
    'ç¹§ï½³': 'ã‚³',
    'ç¹§ï½´': 'ã‚´',
    'ç¹§ï½µ': 'ã‚µ',
    'ç¹§ï½¶': 'ã‚¶',
    'ç¹§ï½·': 'ã‚·',
    'ç¹§ï½¸': 'ã‚¸',
    'ç¹§ï½¹': 'ã‚¹',
    'ç¹§ï½º': 'ã‚º',
    'ç¹§ï½»': 'ã‚»',
    'ç¹§ï½¼': 'ã‚¼',
    'ç¹§ï½½': 'ã‚½',
    'ç¹§ï½¾': 'ã‚¾',
    'ç¹§ï½¿': 'ã‚¿',
    'ç¹': 'ãƒ€',
    'ç¹ï½': 'ãƒ',
    'ç¹ï½‚': 'ãƒ‚',
    'ç¹ï½ƒ': 'ãƒƒ',
    'ç¹ï½„': 'ãƒ…',
    'ç¹ï½…': 'ãƒ†',
    'ç¹ï½†': 'ãƒ‡',
    'ç¹ï½‡': 'ãƒˆ',
    'ç¹ï½ˆ': 'ãƒ‰',
    'ç¹ï½‰': 'ãƒŠ',
    'ç¹ï½Š': 'ãƒ‹',
    'ç¹ï½‹': 'ãƒŒ',
    'ç¹ï½Œ': 'ãƒ',
    'ç¹ï½': 'ãƒ',
    'ç¹ï½®': 'ãƒ',
    'ç¹ï½¯': 'ãƒ',
    'ç¹ï½°': 'ãƒ‘',
    'ç¹ï½±': 'ãƒ’',
    'ç¹ï½²': 'ãƒ“',
    'ç¹ï½³': 'ãƒ”',
    'ç¹ï½´': 'ãƒ•',
    'ç¹ï½µ': 'ãƒ–',
    'ç¹ï½¶': 'ãƒ—',
    'ç¹ï½·': 'ãƒ˜',
    'ç¹ï½¸': 'ãƒ™',
    'ç¹ï½¹': 'ãƒš',
    'ç¹ï½º': 'ãƒ›',
    'ç¹ï½»': 'ãƒœ',
    'ç¹ï½¼': 'ãƒ',
    'ç¹ï½½': 'ãƒ',
    'ç¹ï½¾': 'ãƒŸ',
    'ç¹ï½¿': 'ãƒ ',
    'ç¹§ï½³ç¹ï½³ç¹æ–Î—ç¹ï½¼ç¹§ï½¿': 'ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿',
    'ç¹§ï½·ç¹§ï½¹ç¹ï½ƒç¹': 'ã‚·ã‚¹ãƒ†ãƒ ',
    'ç¹ï½¦ç¹ï½¼ç¹§ï½¶ç¹ï½¼': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼',
    'ç¹ï½­ç¹§ï½°ç¹§ï½¤ç¹ï½³': 'ãƒ­ã‚°ã‚¤ãƒ³',
    'ç¹ä»£ã›ç¹ï½¯ç¹ï½¼ç¹': 'ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰',
    'ç¹ï½¡ç¹ï½¼ç¹ï½«': 'ãƒ¡ãƒ¼ãƒ«',
    'ç¹§ï½¢ç¹å³¨Îç¹§ï½¹': 'ã‚¢ãƒ‰ãƒ¬ã‚¹',
    'ç¹è¼”ãƒç¹§ï½¤ç¹ï½«': 'ãƒ•ã‚¡ã‚¤ãƒ«',
    'ç¹è¼”ã‹ç¹ï½«ç¹': 'ãƒ•ã‚©ãƒ«ãƒ€',
    'ç¹ï½©ç¹§ï½¤ç¹§ï½»ç¹ï½³ç¹§ï½¹': 'ãƒ©ã‚¤ã‚»ãƒ³ã‚¹',
    'ç¹§ï½µç¹ï½¼ç¹è–™ã›': 'ã‚µãƒ¼ãƒ“ã‚¹',
    'ç¹§ï½¢ç¹åŠ±Îœç¹§ï½±ç¹ï½¼ç¹§ï½·ç¹ï½§ç¹ï½³': 'ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³',
    'ç¹ï½­ç¹§ï½°': 'ãƒ­ã‚°',
    'ç¹§ï½¨ç¹ï½©ç¹ï½¼': 'ã‚¨ãƒ©ãƒ¼',
    'ç¹ï½ªç¹§ï½¹ç¹': 'ãƒªã‚¹ãƒˆ',
    'ç¹ï½ªç¹§ï½¹ç¹åŒ»ã„ç¹': 'ãƒªã‚¹ãƒˆã‚¢',
    'ç¹èˆŒãƒ£ç¹§ï½¯ç¹§ï½¢ç¹': 'ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—',
    'ç¹ï½ªç¹§ï½¹ç¹åŒ»ã„': 'ãƒªã‚¹ãƒˆã‚¢',
    'ç¹èˆŒãƒ£ç¹§ï½¯ç¹§ï½¢ç¹': 'ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—',
    # æ¼¢å­—ã®æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³
    'è¿ºï½¾é¶': 'ç’°å¢ƒ',
    'è³?èŸ‹': 'ä¼šç¤¾',
    'èœ¿ï½¯é–­ï½½': 'å¯èƒ½',
    'è ¢?éš•': 'å¿…è¦',
    'éšªï½­è³': 'è¨­å®š',
    'ç¹ï½»ç¹ï½»ç¹ï½»': '...',
    # CIDã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³
    '(cid:': '',
    ')': '',
}

def fix_mojibake_text(text: str) -> str:
    """æ–‡å­—åŒ–ã‘ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿®å¾©ã™ã‚‹ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚«ãƒ¼å‰Šé™¤å¼·åŒ–ç‰ˆï¼‰"""
    if not text:
        return text
    
    fixed_text = text
    
    # ğŸ¯ ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚«ãƒ¼å‰Šé™¤ï¼ˆæœ€å„ªå…ˆï¼‰
    fixed_text = re.sub(r'=== ãƒšãƒ¼ã‚¸ \d+ ===', '', fixed_text)
    fixed_text = re.sub(r'=== Page \d+ ===', '', fixed_text)
    fixed_text = re.sub(r'--- Page \d+ ---', '', fixed_text)
    fixed_text = re.sub(r'=== ãƒ•ã‚¡ã‚¤ãƒ«: .* ===', '', fixed_text)
    
    # ğŸ¯ è‘—ä½œæ¨©æƒ…å ±ã¨ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ•ãƒƒã‚¿ãƒ¼ã‚’å‰Šé™¤
    fixed_text = re.sub(r'Copyright \d{4}-\d{4} Â© .* All Rights Reserved', '', fixed_text)
    fixed_text = re.sub(r'Company Secret', '', fixed_text)
    fixed_text = re.sub(r'VER\d{6} -\d{2}-\d{2}', '', fixed_text)
    
    # ğŸ¯ å…¨è§’ãƒ”ãƒªã‚ªãƒ‰ã‚’åŠè§’ã«æ­£è¦åŒ–
    fixed_text = fixed_text.replace('ã€‚', '.')
    fixed_text = fixed_text.replace('ï¼', '.')
    fixed_text = fixed_text.replace('ï¼Œ', ',')
    
    # ğŸ¯ ä¼šç¤¾åã®æ­£è¦åŒ–
    fixed_text = fixed_text.replace('Noã€‚1', 'No.1')
    fixed_text = fixed_text.replace('COã€‚,LTDã€‚', 'CO.,LTD.')
    
    # é‡åº¦ã®æ–‡å­—åŒ–ã‘ãŒã‚ã‚‹å ´åˆã®ã¿ä¿®å¾©ã‚’é©ç”¨
    if check_text_corruption(fixed_text):
                # æ–‡å­—åŒ–ã‘ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’é©ç”¨
        for mojibake, correct in MOJIBAKE_MAPPING.items():
            fixed_text = fixed_text.replace(mojibake, correct)
        
        # CIDã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å»
        fixed_text = re.sub(r'\(cid:\d+\)', '', fixed_text)
        
        # é€£ç¶šã™ã‚‹æ–‡å­—åŒ–ã‘æ–‡å­—ã‚’é™¤å»ï¼ˆå®Ÿéš›ã®æ–‡å­—åŒ–ã‘æ–‡å­—ã®ã¿ï¼‰
        fixed_text = re.sub(r'[ç¸ºç¹ç¹§]{3,}', '[æ–‡å­—åŒ–ã‘]', fixed_text)
        
        # ç½®æ›æ–‡å­—ã‚’é™¤å»
        fixed_text = fixed_text.replace('\ufffd', '[æ–‡å­—åŒ–ã‘]')
        fixed_text = fixed_text.replace('', '[æ–‡å­—åŒ–ã‘]')
    
    # ğŸ¯ ä½™åˆ†ãªç©ºç™½ã‚’æ•´ç†ï¼ˆå¼·åŒ–ç‰ˆï¼‰
    fixed_text = re.sub(r'\s+', ' ', fixed_text)
    fixed_text = re.sub(r'\n\s*\n\s*\n+', '\n\n', fixed_text)
    fixed_text = re.sub(r'^\s+|\s+$', '', fixed_text, flags=re.MULTILINE)
    
    return fixed_text.strip()

def check_text_corruption(text: str) -> bool:
    """ãƒ†ã‚­ã‚¹ãƒˆãŒæ–‡å­—åŒ–ã‘ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
    if not text or len(text.strip()) == 0:
        return True
    
    # åŸºæœ¬çš„ãªæ–‡å­—åŒ–ã‘æ¤œå‡º
    corruption_indicators = [
        # æ–‡å­—åŒ–ã‘æ–‡å­—ã®å­˜åœ¨
        'ç¸º' in text,
        'ç¹' in text,
        'ç¹§' in text,
        '\ufffd' in text,
        '' in text,
        '(cid:' in text,
        
        # æ–‡å­—åŒ–ã‘æ–‡å­—ã®æ¯”ç‡
        len(re.findall(r'[ç¸ºç¹ç¹§]', text)) / len(text) > 0.1 if len(text) > 0 else False,
        
        # æ„å‘³ã®ã‚ã‚‹æ–‡å­—ã®æ¯”ç‡ãŒä½ã„
        len(re.findall(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF\w]', text)) / len(text) < 0.3 if len(text) > 0 else True,
        
        # æ¥µç«¯ã«çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆ
        len(text.strip()) < 10,
    ]
    
    corruption_count = sum(corruption_indicators)
    
    # è¤‡æ•°ã®æŒ‡æ¨™ã§æ–‡å­—åŒ–ã‘ã¨åˆ¤å®š
    if corruption_count >= 2:
        logger.info(f"PDFæ–‡å­—åŒ–ã‘æ¤œå‡º: {corruption_count}å€‹ã®æŒ‡æ¨™ãŒè©²å½“")
        return True
    
    # å¼·ã„æŒ‡æ¨™ã®å ´åˆã¯å˜ç‹¬ã§ã‚‚æ–‡å­—åŒ–ã‘ã¨åˆ¤å®š
    strong_indicators = [
        'ç¸º' in text and len(re.findall(r'ç¸º', text)) > 5,
        'ç¹' in text and len(re.findall(r'ç¹', text)) > 5,
        '(cid:' in text and len(re.findall(r'\(cid:', text)) > 3,
        '\ufffd' in text,
    ]
    
    if any(strong_indicators):
        logger.info("PDFå¼·ã„æ–‡å­—åŒ–ã‘æŒ‡æ¨™ã‚’æ¤œå‡º")
        return True
    
    return False

def _check_legacy_corruption(text: str) -> bool:
    """å¾“æ¥ã®PDFæ–‡å­—åŒ–ã‘æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯"""
    if not text or len(text.strip()) == 0:
        return True
    
    # ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•
    text_length = len(text)
    
    # æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
    corruption_indicators = [
        # æ„å‘³ã®ãªã„æ–‡å­—ã®é€£ç¶š
        '\ufffd' in text,  # ç½®æ›æ–‡å­—ï¼ˆæ–‡å­—åŒ–ã‘ï¼‰
        'ï¿½ï¿½' in text,      # æ–‡å­—åŒ–ã‘è¨˜å·
        
        # æ¥µç«¯ã«çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆï¼ˆç”»åƒã‚„è¤‡é›‘ãªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®å ´åˆï¼‰
        text_length < 50 and not any(char.isalnum() for char in text),
        
        # ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆãƒ»ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ã®æ¯”ç‡ãŒæ¥µç«¯ã«ä½ã„
        len([char for char in text if char.isalpha() or 
             '\u3040' <= char <= '\u309F' or  # ã²ã‚‰ãŒãª
             '\u30A0' <= char <= '\u30FF' or  # ã‚«ã‚¿ã‚«ãƒŠ
             '\u4E00' <= char <= '\u9FAF'     # æ¼¢å­—
            ]) / text_length < 0.05 if text_length > 0 else True,
        
        # æ¥µç«¯ã«å¤šãã®æ”¹è¡Œã‚„ç©ºç™½
        text.count('\n') / text_length > 0.6 if text_length > 0 else False,
        text.count(' ') / text_length > 0.8 if text_length > 0 else False,
        
        # åˆ¶å¾¡æ–‡å­—ãŒå¤šã„
        len([char for char in text if ord(char) < 32 and char not in '\n\r\t']) / text_length > 0.1 if text_length > 0 else False,
        
        # é«˜ä½Unicodeæ–‡å­—ãŒå¤šã„ï¼ˆæ–‡å­—åŒ–ã‘å¯èƒ½æ€§ï¼‰
        len([char for char in text if ord(char) > 65535]) / text_length > 0.05 if text_length > 0 else False,
        
        # æ„å‘³ã®ãªã„æ–‡å­—åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³
        text.count('ï¿½') > 10,  # æ–‡å­—åŒ–ã‘æ–‡å­—ãŒå¤šã„
        
        # PDFã‹ã‚‰ã‚ˆãå‡ºã‚‹æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³
        text.count('(cid:') > 5,  # PDFã®CIDã‚¨ãƒ©ãƒ¼
        
        # åŒã˜æ–‡å­—ã®ç•°å¸¸ãªç¹°ã‚Šè¿”ã—
        any(text.count(char) / text_length > 0.3 for char in set(text) if char.isprintable()) if text_length > 10 else False,
    ]
    
    corruption_count = sum(corruption_indicators)
    
    # è¤‡æ•°ã®æŒ‡æ¨™ã§æ–‡å­—åŒ–ã‘ã¨åˆ¤å®š
    if corruption_count >= 2:
        return True
    
    # ç‰¹ã«å¼·ã„æŒ‡æ¨™ã®å ´åˆã¯å˜ç‹¬ã§ã‚‚æ–‡å­—åŒ–ã‘ã¨åˆ¤å®š
    strong_indicators = [
        '\ufffd' in text,
        'ï¿½ï¿½' in text,
        text.count('(cid:') > 5,
        len([char for char in text if ord(char) > 65535]) / text_length > 0.1 if text_length > 0 else False,
    ]
    
    return any(strong_indicators)

def extract_tables_from_text(text: str) -> List[pd.DataFrame]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æŠ½å‡ºã—ã¦DataFrameã«å¤‰æ›"""
    tables = []
    try:
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã‚’æ¤œå‡º
        lines = text.split('\n')
        table_lines = []
        in_table = False
        
        for line in lines:
            line = line.strip()
            if '|' in line and len(line.split('|')) >= 3:
                # ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œã®å¯èƒ½æ€§
                table_lines.append(line)
                in_table = True
            elif in_table and not line:
                # ç©ºè¡Œã§ãƒ†ãƒ¼ãƒ–ãƒ«çµ‚äº†
                if len(table_lines) >= 2:  # ãƒ˜ãƒƒãƒ€ãƒ¼ + å°‘ãªãã¨ã‚‚1è¡Œã®ãƒ‡ãƒ¼ã‚¿
                    df = _parse_markdown_table(table_lines)
                    if df is not None and not df.empty:
                        tables.append(df)
                table_lines = []
                in_table = False
            elif in_table and '|' not in line:
                # ãƒ†ãƒ¼ãƒ–ãƒ«ä»¥å¤–ã®è¡Œã§ãƒ†ãƒ¼ãƒ–ãƒ«çµ‚äº†
                if len(table_lines) >= 2:
                    df = _parse_markdown_table(table_lines)
                    if df is not None and not df.empty:
                        tables.append(df)
                table_lines = []
                in_table = False
        
        # æœ€å¾Œã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‡¦ç†
        if table_lines and len(table_lines) >= 2:
            df = _parse_markdown_table(table_lines)
            if df is not None and not df.empty:
                tables.append(df)
        
        # æ”¹è‰¯ã•ã‚ŒãŸæ¤œå‡º: ç¸¦ã«ä¸¦ã‚“ã ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¨æ¸¬
        if not tables:
            tables.extend(_extract_tabular_data_from_lines(lines))
        
    except Exception as e:
        logger.warning(f"ãƒ†ãƒ¼ãƒ–ãƒ«æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    return tables

def _parse_markdown_table(table_lines: List[str]) -> Optional[pd.DataFrame]:
    """ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã®è¡Œã‚’DataFrameã«å¤‰æ›"""
    try:
        if len(table_lines) < 2:
            return None
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æŠ½å‡º
        header_line = table_lines[0]
        headers = [cell.strip() for cell in header_line.split('|') if cell.strip()]
        
        # åŒºåˆ‡ã‚Šè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ---ãªã©ãŒå«ã¾ã‚Œã‚‹è¡Œï¼‰
        data_start = 1
        if len(table_lines) > 1 and re.search(r'[-:]+', table_lines[1]):
            data_start = 2
        
        # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’æŠ½å‡º
        data_rows = []
        for line in table_lines[data_start:]:
            cells = [cell.strip() for cell in line.split('|') if cell.strip() or True]
            # ç©ºã®ã‚»ãƒ«ã‚‚ä¿æŒã—ã¤ã¤ã€è¡Œå…¨ä½“ãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª
            if any(cell.strip() for cell in cells):
                # ãƒ˜ãƒƒãƒ€ãƒ¼æ•°ã«åˆã‚ã›ã¦è¡Œã‚’èª¿æ•´
                while len(cells) < len(headers):
                    cells.append('')
                data_rows.append(cells[:len(headers)])
        
        if not data_rows:
            return None
        
        df = pd.DataFrame(data_rows, columns=headers)
        return df
        
    except Exception as e:
        logger.warning(f"ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«è§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def _extract_tabular_data_from_lines(lines: List[str]) -> List[pd.DataFrame]:
    """è¡Œãƒªã‚¹ãƒˆã‹ã‚‰è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã‚’æ¨æ¸¬ã—ã¦æŠ½å‡º"""
    tables = []
    try:
        # é€£ç¶šã™ã‚‹è¡Œã§åŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åŒºåˆ‡ã‚Šæ–‡å­—ã‚’æŒã¤è¡Œã‚’æ¢ã™
        potential_tables = []
        current_table_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                if len(current_table_lines) >= 3:  # æœ€ä½3è¡Œã§ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã¿ãªã™
                    potential_tables.append(current_table_lines.copy())
                current_table_lines = []
                continue
            
            # ã‚¿ãƒ–ã€è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ã€ã‚³ãƒ­ãƒ³ãªã©ã®åŒºåˆ‡ã‚Šæ–‡å­—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
            separators = ['\t', '  ', ':', ',', ';']
            found_separator = None
            max_splits = 0
            
            for sep in separators:
                splits = len(line.split(sep))
                if splits > max_splits and splits >= 2:
                    max_splits = splits
                    found_separator = sep
            
            if found_separator and max_splits >= 2:
                current_table_lines.append((line, found_separator, max_splits))
            else:
                if len(current_table_lines) >= 3:
                    potential_tables.append(current_table_lines.copy())
                current_table_lines = []
        
        # æœ€å¾Œã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‡¦ç†
        if len(current_table_lines) >= 3:
            potential_tables.append(current_table_lines.copy())
        
        # å„å€™è£œã‚’DataFrameã«å¤‰æ›
        for table_lines in potential_tables:
            df = _convert_lines_to_dataframe(table_lines)
            if df is not None and not df.empty and len(df.columns) >= 2:
                tables.append(df)
    
    except Exception as e:
        logger.warning(f"è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    return tables

def _convert_lines_to_dataframe(table_lines: List[Tuple[str, str, int]]) -> Optional[pd.DataFrame]:
    """è¡Œãƒªã‚¹ãƒˆã‹ã‚‰DataFrameã‚’ä½œæˆ"""
    try:
        if len(table_lines) < 3:
            return None
        
        # æœ€ã‚‚ä¸€èˆ¬çš„ãªåŒºåˆ‡ã‚Šæ–‡å­—ã‚’ç‰¹å®š
        separator_counts = {}
        for _, sep, _ in table_lines:
            separator_counts[sep] = separator_counts.get(sep, 0) + 1
        
        most_common_sep = max(separator_counts, key=separator_counts.get)
        
        # åŒã˜åŒºåˆ‡ã‚Šæ–‡å­—ã‚’ä½¿ç”¨ã™ã‚‹è¡Œã®ã¿ã‚’ä½¿ç”¨
        filtered_lines = []
        for line, sep, _ in table_lines:
            if sep == most_common_sep:
                filtered_lines.append(line)
        
        if len(filtered_lines) < 3:
            return None
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
        data_rows = []
        for line in filtered_lines:
            cells = [cell.strip() for cell in line.split(most_common_sep)]
            data_rows.append(cells)
        
        # æœ€ã‚‚å¤šã„åˆ—æ•°ã‚’ç‰¹å®š
        max_cols = max(len(row) for row in data_rows)
        if max_cols < 2:
            return None
        
        # å…¨ã¦ã®è¡Œã‚’åŒã˜åˆ—æ•°ã«èª¿æ•´
        for row in data_rows:
            while len(row) < max_cols:
                row.append('')
        
        # æœ€åˆã®è¡Œã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦ä½¿ç”¨
        headers = data_rows[0] if data_rows else []
        data = data_rows[1:] if len(data_rows) > 1 else []
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒç©ºã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåã‚’è¨­å®š
        for i, header in enumerate(headers):
            if not header or header.isspace():
                headers[i] = f'åˆ—{i+1}'
        
        if not data:
            return None
        
        df = pd.DataFrame(data, columns=headers)
        return df
        
    except Exception as e:
        logger.warning(f"DataFrameå¤‰æ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def split_ocr_text_into_sections(text: str, filename: str) -> list:
    """OCRçµæœã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†å‰²ã—ã€ãƒ†ãƒ¼ãƒ–ãƒ«ã‚‚å‡¦ç†ã™ã‚‹"""
    sections = []
    
    # ã¾ãšãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æŠ½å‡º
    extracted_tables = extract_tables_from_text(text)
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã¯Unnamedã‚«ãƒ©ãƒ ä¿®æ­£ã‚’é©ç”¨
    if extracted_tables:
        handler = UnnamedColumnHandler()
        
        for i, table_df in enumerate(extracted_tables):
            try:
                # ãƒ†ãƒ¼ãƒ–ãƒ«ã®Unnamedã‚«ãƒ©ãƒ å•é¡Œã‚’ä¿®æ­£
                fixed_df, modifications = handler.fix_dataframe(table_df, f"{filename}_table_{i+1}")
                
                if modifications:
                    logger.info(f"PDF ãƒ†ãƒ¼ãƒ–ãƒ« {i+1} ã®Unnamedã‚«ãƒ©ãƒ ä¿®æ­£: {', '.join(modifications)}")
                
                # ä¿®æ­£ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã—ã¦è¿½åŠ 
                table_sections = handler.create_clean_sections(fixed_df, filename)
                for section in table_sections:
                    section['section'] = f"ãƒ†ãƒ¼ãƒ–ãƒ«{i+1}_{section['section']}"
                    section['source'] = 'PDF Table'
                sections.extend(table_sections)
                
            except Exception as e:
                logger.warning(f"ãƒ†ãƒ¼ãƒ–ãƒ« {i+1} ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                sections.append({
                    'section': f"ãƒ†ãƒ¼ãƒ–ãƒ«{i+1}ï¼ˆç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰",
                    'content': table_df.to_string(),
                    'source': 'PDF Table',
                    'file': filename,
                    'url': None
                })
    
    # ãƒšãƒ¼ã‚¸åŒºåˆ‡ã‚Šã§é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
    page_parts = text.split('--- Page ')
    
    for i, part in enumerate(page_parts):
        if not part.strip():
            continue
            
        # ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æŠ½å‡º
        lines = part.split('\n')
        if i == 0:
            # æœ€åˆã®éƒ¨åˆ†ï¼ˆãƒšãƒ¼ã‚¸åŒºåˆ‡ã‚Šã®å‰ã®éƒ¨åˆ†ï¼‰
            section_name = "æ¦‚è¦"
            content = part.strip()
        else:
            # ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æŠ½å‡º
            page_line = lines[0] if lines else ""
            page_num = page_line.split('---')[0].strip() if '---' in page_line else str(i)
            section_name = f"ãƒšãƒ¼ã‚¸ {page_num}"
            content = '\n'.join(lines[1:]).strip() if len(lines) > 1 else ""
        
        if content:
            sections.append({
                'section': str(section_name),
                'content': str(content),
                'source': 'PDF',
                'file': filename,
                'url': None
            })
    
    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒç©ºã®å ´åˆã¯å…¨ä½“ã‚’ä¸€ã¤ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã—ã¦è¿”ã™
    if not sections:
        sections.append({
            'section': "å…¨ä½“",
            'content': str(text),
            'source': 'PDF', 
            'file': filename,
            'url': None
        })
    
    return sections

async def process_pdf_file(contents, filename):
    """PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™"""
    try:
        # BytesIOã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        pdf_file = BytesIO(contents)
        
        # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        all_text = ""
        sections = {}
        extracted_text = f"=== ãƒ•ã‚¡ã‚¤ãƒ«: {filename} ===\n\n"
        
        corrupted_pages = []  # æ–‡å­—åŒ–ã‘ã—ãŸãƒšãƒ¼ã‚¸ã‚’è¨˜éŒ²
        
        for i, page in enumerate(pdf_reader.pages):
            try:
                page_text = page.extract_text()
                # Ensure page_text is not None and convert to string if needed
                if page_text is not None:
                    page_text = ensure_string(page_text).replace('\x00', '') # ğŸ§¼ Remove NUL characters
                    
                    # ğŸ¯ ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚«ãƒ¼å‰Šé™¤ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                    page_text = fix_mojibake_text(page_text)
                    
                    # ãƒšãƒ¼ã‚¸ã”ã¨ã«æ–‡å­—åŒ–ã‘ã‚’ãƒã‚§ãƒƒã‚¯
                    if check_text_corruption(page_text):
                        print(f"ãƒšãƒ¼ã‚¸ {i+1} ã§æ–‡å­—åŒ–ã‘ã‚’æ¤œå‡º: {page_text[:100]}...")
                        corrupted_pages.append(i)
                        # æ–‡å­—åŒ–ã‘ãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã¯sectionsã«ä¿å­˜ã—ãªã„
                    else:
                        section_name = f"ãƒšãƒ¼ã‚¸ {i+1}"
                        sections[section_name] = page_text
                        all_text += page_text + "\n"
                        # ğŸ¯ extracted_textã«ã¯ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚«ãƒ¼ã‚’è¿½åŠ ã—ãªã„
                        extracted_text += f"{page_text}\n\n"
                else:
                    print(f"ãƒšãƒ¼ã‚¸ {i+1} ã«ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
                    corrupted_pages.append(i)  # ãƒ†ã‚­ã‚¹ãƒˆãªã—ã‚‚æ–‡å­—åŒ–ã‘ã¨ã—ã¦æ‰±ã†
                    # ãƒ†ã‚­ã‚¹ãƒˆãªã—ãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã¯sectionsã«ä¿å­˜ã—ãªã„
            except Exception as page_error:
                print(f"ãƒšãƒ¼ã‚¸ {i+1} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(page_error)}")
                corrupted_pages.append(i)  # ã‚¨ãƒ©ãƒ¼ã‚‚æ–‡å­—åŒ–ã‘ã¨ã—ã¦æ‰±ã†
                # ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã¯sectionsã«ä¿å­˜ã—ãªã„
        
        # åˆæœŸãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆOCRãŒå¿…è¦ã§ãªã„å ´åˆã®ã¿ï¼‰
        all_data = []
        
        # æ–‡å­—åŒ–ã‘ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã®ã¿PyMuPDFã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚’è©¦è¡Œ
        if len(corrupted_pages) > 0 or (all_text and check_text_corruption(all_text)):
            logger.info(f"PDFæ–‡å­—åŒ–ã‘æ¤œå‡º (ãƒšãƒ¼ã‚¸: {corrupted_pages}) - PyMuPDF ã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚’è©¦è¡Œ: {filename}")
            
            # PyMuPDF ã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚’å®Ÿè¡Œ
            pymupdf_result = await process_pdf_with_pymupdf(contents, filename)
            if pymupdf_result:
                logger.info("PyMuPDF ã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãŒæˆåŠŸã—ã¾ã—ãŸ")
                return pymupdf_result
            
            logger.warning("PyMuPDF ã§ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•— - OCR å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™")
            
            # PyMuPDF ã§ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãŒå¤±æ•—ã—ãŸå ´åˆã¯å¤ã„OCRå‡¦ç†ã‚’è©¦è¡Œ
            try:
                print(f"æ–‡å­—åŒ–ã‘ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚OCRã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¾ã™...")
                ocr_text = await ocr_pdf_to_text_from_bytes(contents)
                
                if ocr_text:
                    # OCRçµæœã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†å‰²
                    ocr_sections_list = split_ocr_text_into_sections(ocr_text, filename)
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
                    result_df = pd.DataFrame(ocr_sections_list) if ocr_sections_list else pd.DataFrame({
                        'section': ["OCRçµæœ"],
                        'content': [ensure_string(ocr_text)],
                        'source': ['PDF (OCR)'],
                        'file': [filename],
                        'url': [None]
                    })
                    
                    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¾æ›¸ã‚’ä½œæˆ
                    ocr_sections = {item['section']: item['content'] for item in ocr_sections_list} if ocr_sections_list else {"OCRçµæœ": ensure_string(ocr_text)}
                    
                    # æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
                    ocr_extracted_text = f"=== ãƒ•ã‚¡ã‚¤ãƒ«: {filename} (OCRå‡¦ç†) ===\n\n"
                    for section_name, content in ocr_sections.items():
                        ocr_extracted_text += f"=== {section_name} ===\n{content}\n\n"
                    
                    return result_df, ocr_sections, ocr_extracted_text
                else:
                    raise Exception("OCRã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            except Exception as ocr_error:
                logger.error(f"OCRå‡¦ç†å¤±æ•—: {str(ocr_error)}")
                # OCRå¤±æ•—æ™‚ã¯é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå‡¦ç†ã‚’ç¶šè¡Œ
                pass
        
        # PyMuPDF å‡¦ç†ãŒå¤±æ•—ã—ãŸå ´åˆã€é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚’è©¦è¡Œ
        # æ–‡å­—åŒ–ã‘ãƒšãƒ¼ã‚¸ãŒãªã„å ´åˆã®ã¿ã€é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚’è¡Œã†
        if len(corrupted_pages) == 0 and all_text and not check_text_corruption(all_text):
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†å‰²
            # è¦‹å‡ºã—ãƒ‘ã‚¿ãƒ¼ãƒ³
            heading_pattern = r'^(?:\d+[\.\s]+|ç¬¬\d+[ç« ç¯€]\s+|[\*\#]+\s+)?([A-Za-z\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]{2,}[ï¼š:ã€ã€‚])'
            
            current_section = "ä¸€èˆ¬æƒ…å ±"
            current_content = []
            
            # Ensure all_text is not empty and is a string
            all_text_str = str(all_text) if all_text is not None else ""
            if all_text_str:
                for line in all_text_str.split("\n"):
                    line = str(line).strip()
                    if not line:
                        continue
                    
                    # è¦‹å‡ºã—ã‹ã©ã†ã‹ã‚’åˆ¤å®š
                    if re.search(heading_pattern, line):
                        # å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¿å­˜
                        if current_content:
                            # å¿…ãšæ–‡å­—åˆ—ã«å¤‰æ›ã—ã¦ã‹ã‚‰çµåˆ
                            content_text = "\n".join([ensure_string(item) for item in current_content])
                            all_data.append({
                                'section': str(current_section),
                                'content': content_text,
                                'source': 'PDF',
                                'file': filename,
                                'url': None
                            })
                        
                        # æ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹
                        current_section = str(line)
                        current_content = []
                    else:
                        current_content.append(str(line))
                
                # æœ€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¿å­˜
                if current_content:
                    # å¿…ãšæ–‡å­—åˆ—ã«å¤‰æ›ã—ã¦ã‹ã‚‰çµåˆ
                    content_text = "\n".join([ensure_string(item) for item in current_content])
                    all_data.append({
                        'section': str(current_section),
                        'content': content_text,
                        'source': 'PDF',
                        'file': filename,
                        'url': None
                    })
        else:
            print("æ–‡å­—åŒ–ã‘ã¾ãŸã¯å•é¡Œã®ã‚ã‚‹ãƒšãƒ¼ã‚¸ãŒæ¤œå‡ºã•ã‚ŒãŸãŸã‚ã€é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        
        # PyMuPDF å‡¦ç†å¤±æ•—å¾Œã®æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã®ã¿ 
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
        result_df = pd.DataFrame(all_data) if all_data else pd.DataFrame({
            'section': ["ã‚¨ãƒ©ãƒ¼"],
            'content': ["PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ"],
            'source': ['PDF'],
            'file': [filename],
            'url': [None]
        })
        
        # ã™ã¹ã¦ã®åˆ—ã®å€¤ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        for col in result_df.columns:
            result_df[col] = result_df[col].apply(ensure_string)
        
        return result_df, sections, extracted_text
    except Exception as e:
        print(f"PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        print(traceback.format_exc())
        
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚æœ€ä½é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        empty_df = pd.DataFrame({
            'section': ["ã‚¨ãƒ©ãƒ¼"],
            'content': [f"PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"],
            'source': ['PDF'],
            'file': [filename],
            'url': [None]
        })
        empty_sections = {"ã‚¨ãƒ©ãƒ¼": f"PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"}
        error_text = f"=== ãƒ•ã‚¡ã‚¤ãƒ«: {filename} ===\n\n=== ã‚¨ãƒ©ãƒ¼ ===\nPDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\n\n"
        
        return empty_df, empty_sections, error_text

async def process_pdf_with_pymupdf(contents: bytes, filename: str):
    """PyMuPDF ã‚’ç”¨ã„ã¦ PDF ã‹ã‚‰ç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹

    Gemini ã® OCR ã‚’ä½¿ç”¨ã›ãšã€PDF å†…ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ãã®ã¾ã¾å–å¾—ã—ã¾ã™ã€‚
    æ–‡å­—åŒ–ã‘ä¿®æ­£ã‚‚é©ç”¨ã—ã€ãƒšãƒ¼ã‚¸å˜ä½ã§ã‚»ã‚¯ã‚·ãƒ§ãƒ³åŒ–ã—ã¦ DataFrame ã‚’è¿”ã—ã¾ã™ã€‚
    """
    try:
        import fitz  # PyMuPDF

        logger.info(f"PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹ï¼ˆPyMuPDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºä½¿ç”¨ï¼‰: {filename}")

        # PyMuPDF ã§ãƒã‚¤ãƒˆåˆ—ã‚’ç›´æ¥é–‹ã
        with fitz.open(stream=contents, filetype="pdf") as doc:
            sections = {}
            all_data = []
            full_text = f"=== ãƒ•ã‚¡ã‚¤ãƒ«: {filename} (PyMuPDF æŠ½å‡º) ===\n\n"

            for page_num, page in enumerate(doc, start=1):
                try:
                    # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã€‚layout é¸æŠã¯ "text" ã§ã‚·ãƒ³ãƒ—ãƒ«ã«å–å¾—
                    page_text = page.get_text("text") or ""

                    # æ–‡å­—åŒ–ã‘ä¿®æ­£ã‚’è©¦ã¿ã‚‹
                    fixed_text = fix_mojibake_text(page_text)

                    if not fixed_text.strip():
                        # ç©ºã¾ãŸã¯ä¿®æ­£å¾Œã‚‚ç©ºã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        logger.debug(f"ãƒšãƒ¼ã‚¸ {page_num} ã§æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã—ãŸ")
                        continue

                    section_name = f"ãƒšãƒ¼ã‚¸ {page_num}"
                    sections[section_name] = fixed_text
                    all_data.append({
                        "section": section_name,
                        "content": fixed_text,
                        "source": "PDF (PyMuPDF)",
                        "file": filename,
                        "url": None,
                    })

                    full_text += f"=== {section_name} ===\n{fixed_text}\n\n"
                except Exception as page_error:
                    logger.warning(f"ãƒšãƒ¼ã‚¸ {page_num} ã® PyMuPDF æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {page_error}")
                    continue

        if not all_data:
            logger.warning("PyMuPDF ã§æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return None

        # DataFrame ç”Ÿæˆ
        result_df = pd.DataFrame(all_data)
        for col in result_df.columns:
            result_df[col] = result_df[col].apply(ensure_string)

        logger.info(f"PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†ï¼ˆPyMuPDF æŠ½å‡ºï¼‰: {len(result_df)} ã‚»ã‚¯ã‚·ãƒ§ãƒ³")
        return result_df, sections, full_text

    except Exception as e:
        logger.error(f"PyMuPDF PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return None 
"""
Unnamed Column Handler
unnamed_column_handling_guide_en.mdã®æŒ‡é‡ã«åŸºã¥ã„ã¦ã€ŒUnnamedã€ã‚«ãƒ©ãƒ ã‚’æ¤œå‡ºãƒ»ä¿®æ­£ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""
import pandas as pd
import re
import logging
from typing import List, Dict, Tuple, Optional, Union
from ..database import ensure_string

logger = logging.getLogger(__name__)

class UnnamedColumnHandler:
    """ã€ŒUnnamedã€ã‚«ãƒ©ãƒ ã‚’æ¤œå‡ºãƒ»ä¿®æ­£ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        # ãƒ“ã‚¸ãƒã‚¹ç‰¹æœ‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
        self.header_keywords = [
            # åŸºæœ¬æƒ…å ±
            'é¡§å®¢', 'ãŠå®¢æ§˜', 'ä¼šç¤¾', 'ä¼æ¥­', 'çµ„ç¹”', 'éƒ¨é–€', 'éƒ¨ç½²',
            'åå‰', 'æ°å', 'name', 'åç§°', 'title', 'ã‚¿ã‚¤ãƒˆãƒ«', 'é …ç›®', 'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰',
            'ä½æ‰€', 'address', 'é€£çµ¡å…ˆ', 'é›»è©±', 'phone', 'tel',
            'ãƒ¡ãƒ¼ãƒ«', 'email', 'mail', 'fax', 'URL', 'ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ',
            
            # è­˜åˆ¥å­
            'id', 'ID', 'ç•ªå·', 'no', 'number', 'code', 'ã‚³ãƒ¼ãƒ‰',
            'è­˜åˆ¥', 'identifier', 'ident', 'ç®¡ç†', 'ç®¡ç†ç•ªå·', 'ã‚­ãƒ¼',
            
            # æ—¥ä»˜ãƒ»æ™‚åˆ»
            'æ—¥ä»˜', 'date', 'æ™‚åˆ»', 'time', 'å¹´', 'year', 'æœˆ', 'month', 'æ—¥', 'day',
            'é–‹å§‹', 'start', 'çµ‚äº†', 'end', 'æœŸé–“', 'period',
            'ä½œæˆ', 'created', 'æ›´æ–°', 'updated', 'æœ€çµ‚', 'last', 'ç™»éŒ²æ—¥', 'å®Œäº†æ—¥',
            
            # é‡‘é¡ãƒ»æ•°å€¤
            'é‡‘é¡', 'amount', 'ä¾¡æ ¼', 'price', 'æ–™é‡‘', 'fee', 'è²»ç”¨', 'cost',
            'æ•°é‡', 'quantity', 'å€‹æ•°', 'count', 'ä»¶æ•°', 'total', 'åˆè¨ˆ',
            'å˜ä¾¡', 'unit', 'ç¨', 'tax', 'æ¶ˆè²»ç¨', 'å‰²å¼•', 'åˆ©ç›Š',
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ»çŠ¶æ…‹
             'status', 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹', 'çŠ¶æ…‹', 'state', 'æ®µéš', 'phase',
            'å®Œäº†', 'complete', 'é€²è¡Œ', 'progress', 'é–‹å§‹', 'active',
            'åœæ­¢', 'stop', 'çµ‚äº†', 'finish', 'æ‰¿èª', 'approval', 'çŠ¶æ³',
            
            # åˆ†é¡ãƒ»ã‚«ãƒ†ã‚´ãƒª
            'ã‚«ãƒ†ã‚´ãƒª', 'category', 'åˆ†é¡', 'type', 'ç¨®é¡', 'kind',
            'ãƒ¬ãƒ™ãƒ«', 'level', 'ã‚°ãƒ¬ãƒ¼ãƒ‰', 'grade', 'ãƒ©ãƒ³ã‚¯', 'rank', 'ã‚°ãƒ«ãƒ¼ãƒ—', 'åŒºåˆ†',
            
            # ãã®ä»–
            'å‚™è€ƒ', 'note', 'memo', 'ãƒ¡ãƒ¢', 'èª¬æ˜', 'description',
            'å†…å®¹', 'content', 'è©³ç´°', 'detail', 'è¦ç´„', 'summary',
            'é©ç”¨', 'å¯¾è±¡', 'ã‚½ãƒ¼ã‚¹', 'æƒ…å ±æº', 'ãƒ‡ãƒ¼ã‚¿', 'å€¤'
        ]
        
        # Unnamedãƒ‘ã‚¿ãƒ¼ãƒ³
        self.unnamed_patterns = [
            r'^Unnamed.*',
            r'^Unnamed:\s*\d*$',  # 'Unnamed: 1', 'Unnamed: 2' ãªã©
            r'^Column.*',
            r'^åˆ—.*',
            r'^ç„¡é¡Œ.*',
            r'^\s*$',  # ç©ºç™½
            r'^_+$',   # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã®ã¿
            r'^\d+$'   # æ•°å­—ã®ã¿
        ]
    
    def detect_header_row(self, df: pd.DataFrame) -> int:
        """çœŸã®ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æ¤œå‡ºã™ã‚‹"""
        try:
            if df.empty:
                return 0
            
            max_score = -1
            best_row = 0
            
            # æœ€åˆã®5è¡Œã‚’æ¤œæŸ»
            for i in range(min(5, len(df))):
                row = df.iloc[i]
                score = self._calculate_header_score(row)
                
                logger.debug(f"è¡Œ {i}: ã‚¹ã‚³ã‚¢ {score}, å†…å®¹: {list(row.values)[:3]}")
                
                if score > max_score:
                    max_score = score
                    best_row = i
            
            # ã‚¹ã‚³ã‚¢ãŒååˆ†é«˜ã„å ´åˆã®ã¿ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦æ¡ç”¨
            if max_score >= 3:
                logger.info(f"ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œæ¤œå‡º: è¡Œ {best_row} (ã‚¹ã‚³ã‚¢: {max_score})")
                return best_row
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ€åˆã®è¡Œã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã™ã‚‹
            logger.info("æ˜ç¢ºãªãƒ˜ãƒƒãƒ€ãƒ¼è¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚è¡Œ 0 ã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦ä½¿ç”¨")
            return 0
            
        except Exception as e:
            logger.error(f"ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return 0
    
    def _calculate_header_score(self, row: pd.Series) -> float:
        """è¡ŒãŒãƒ˜ãƒƒãƒ€ãƒ¼ã§ã‚ã‚‹å¯èƒ½æ€§ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        score = 0
        non_empty_cells = 0
        
        for value in row.values:
            if pd.notna(value) and str(value).strip():
                non_empty_cells += 1
                value_str = ensure_string(value).lower()
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
                for keyword in self.header_keywords:
                    if keyword.lower() in value_str:
                        score += 2
                        break
                
                # æ–‡å­—åˆ—ã®ç‰¹å¾´ã‚’ãƒã‚§ãƒƒã‚¯
                if any(char.isalpha() for char in value_str):
                    score += 1
                
                # æ•°å­—ã®ã¿ã®å ´åˆã¯ãƒã‚¤ãƒŠã‚¹
                if value_str.isdigit():
                    score -= 1
        
        # ç©ºã®ã‚»ãƒ«ãŒå¤šã™ãã‚‹å ´åˆã¯ãƒã‚¤ãƒŠã‚¹
        if non_empty_cells < len(row) * 0.3:
            score -= 2
        
        return score
    
    def detect_unnamed_columns(self, df: pd.DataFrame) -> List[int]:
        """Unnamedã‚«ãƒ©ãƒ ã‚’æ¤œå‡ºã™ã‚‹"""
        unnamed_columns = []
        
        try:
            for i, col in enumerate(df.columns):
                col_str = ensure_string(col)
                
                # Unnamedãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                for pattern in self.unnamed_patterns:
                    if re.match(pattern, col_str, re.IGNORECASE):
                        unnamed_columns.append(i)
                        logger.debug(f"Unnamedã‚«ãƒ©ãƒ æ¤œå‡º: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i}, åå‰: '{col_str}'")
                        break
                        
        except Exception as e:
            logger.error(f"Unnamedã‚«ãƒ©ãƒ æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return unnamed_columns
    
    def analyze_column_content(self, df: pd.DataFrame, col_index: int) -> Dict[str, any]:
        """ã‚«ãƒ©ãƒ ã®å†…å®¹ã‚’åˆ†æã™ã‚‹"""
        try:
            if col_index >= len(df.columns):
                return {'type': 'invalid', 'is_row_index': False, 'suggested_name': None}
            
            col_data = df.iloc[:, col_index]
            
            # ç©ºã®ã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯
            non_null_count = col_data.count()
            if non_null_count == 0:
                return {'type': 'empty', 'is_row_index': False, 'suggested_name': None}
            
            # è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
            is_row_index = self._is_row_index_column(col_data)
            
            # ãƒ‡ãƒ¼ã‚¿å‹ã‚’åˆ†æ
            data_type = self._analyze_data_type(col_data)
            
            # åå‰ã‚’ææ¡ˆ
            suggested_name = self._suggest_column_name(col_data, col_index)
            
            return {
                'type': data_type,
                'is_row_index': is_row_index,
                'suggested_name': suggested_name,
                'non_null_count': non_null_count,
                'total_count': len(col_data)
            }
            
        except Exception as e:
            logger.error(f"ã‚«ãƒ©ãƒ å†…å®¹åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'type': 'error', 'is_row_index': False, 'suggested_name': None}
    
    def _is_row_index_column(self, col_data: pd.Series) -> bool:
        """ã‚«ãƒ©ãƒ ãŒè¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        try:
            # NaNå€¤ã‚’é™¤å»
            non_null_data = col_data.dropna()
            
            if len(non_null_data) < 2:
                return False
            
            # æ•°å€¤ã®ã¿ã‹ãƒã‚§ãƒƒã‚¯
            try:
                numeric_data = pd.to_numeric(non_null_data, errors='coerce')
                # å®‰å…¨ãª NaN ãƒã‚§ãƒƒã‚¯
                has_nan = False
                try:
                    has_nan = numeric_data.isna().any()
                except Exception as nan_error:
                    logger.debug(f"Series NaN ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {str(nan_error)}")
                    # Series ã®çœŸå½å€¤åˆ¤å®šã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å€‹åˆ¥ãƒã‚§ãƒƒã‚¯
                    try:
                        has_nan = any(pd.isna(val) for val in numeric_data)
                    except Exception as individual_error:
                        logger.debug(f"å€‹åˆ¥NaNãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {str(individual_error)}")
                        has_nan = True  # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å®‰å…¨å´ã«å€’ã™
                
                if has_nan:
                    return False
                
                # é€£ç¶šã™ã‚‹æ•´æ•°ã‹ãƒã‚§ãƒƒã‚¯
                if all(val == int(val) for val in numeric_data):
                    # ã‚½ãƒ¼ãƒˆã—ã¦é€£ç¶šæ€§ã‚’ãƒã‚§ãƒƒã‚¯
                    sorted_data = sorted(numeric_data)
                    differences = [sorted_data[i+1] - sorted_data[i] for i in range(len(sorted_data)-1)]
                    
                    # å·®ãŒ1ã®å ´åˆãŒ70%ä»¥ä¸Š
                    if sum(1 for diff in differences if diff == 1) / len(differences) >= 0.7:
                        return True
                        
            except:
                pass
            
            return False
            
        except Exception as e:
            logger.error(f"è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åˆ¤å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def _analyze_data_type(self, col_data: pd.Series) -> str:
        """ãƒ‡ãƒ¼ã‚¿å‹ã‚’åˆ†æ"""
        try:
            non_null_data = col_data.dropna()
            
            if len(non_null_data) == 0:
                return 'empty'
            
            # æ•°å€¤å‹ãƒã‚§ãƒƒã‚¯
            try:
                pd.to_numeric(non_null_data, errors='raise')
                return 'numeric'
            except:
                pass
            
            # æ—¥ä»˜å‹ãƒã‚§ãƒƒã‚¯
            try:
                pd.to_datetime(non_null_data, errors='raise')
                return 'datetime'
            except:
                pass
            
            # æ–‡å­—åˆ—å‹
            return 'text'
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿å‹åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            return 'unknown'
    
    def _suggest_column_name(self, col_data: pd.Series, col_index: int) -> str:
        """
        ã‚«ãƒ©ãƒ ã®å†…å®¹ã«åŸºã¥ã„ã¦é©åˆ‡ãªåå‰ã‚’ææ¡ˆã™ã‚‹
        """
        try:
            # NaNã¾ãŸã¯ç©ºã®å€¤ã‚’é™¤å¤–
            col_data = col_data.dropna()
            if len(col_data) == 0:
                return f'ãƒ‡ãƒ¼ã‚¿{col_index + 1}'
            
            # å‰ã®åˆ—ã®æƒ…å ±ã‚’ä½¿ã£ã¦æ¨æ¸¬
            if col_index > 0:
                prev_col = col_data.iloc[0] if len(col_data) > 0 else ""
                prev_str = ensure_string(prev_col).lower()
                
                # ã‚ˆãã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
                if 'total' in prev_str or 'åˆè¨ˆ' in prev_str:
                    return 'åˆè¨ˆé¡'
                elif 'name' in prev_str or 'åå‰' in prev_str or 'æ°å' in prev_str:
                    return 'è©³ç´°'
                elif 'date' in prev_str or 'æ—¥ä»˜' in prev_str:
                    return 'æ™‚åˆ»'
                elif 'price' in prev_str or 'ä¾¡æ ¼' in prev_str or 'é‡‘é¡' in prev_str:
                    return 'ç¨é¡'
            
            # ãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ã‹ã‚‰æ¨æ¸¬ï¼ˆæ‹¡å¼µç‰ˆï¼‰
            non_null_data = col_data.dropna()
            if len(non_null_data) > 0:
                # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã—ã€ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚§ãƒƒã‚¯
                sample_size = min(20, len(non_null_data))  # æœ€å¤§20è¡Œã‚’ãƒã‚§ãƒƒã‚¯
                sample_values = [ensure_string(val).lower() for val in non_null_data.head(sample_size)]
                
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ 
                logger.info(f"ğŸ“§ ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æ¤œçŸ¥é–‹å§‹ - åˆ—{col_index + 1}: {sample_size}ä»¶ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯")
                logger.info(f"ğŸ“§ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€åˆã®3ä»¶ï¼‰: {sample_values[:3]}")
                
                # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®è©³ç´°ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
                email_count = 0
                for sample in sample_values:
                    if self._is_email_pattern(sample):
                        email_count += 1
                        logger.info(f"ğŸ“§ ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æ¤œçŸ¥æˆåŠŸ: {sample}")
                
                # 70%ä»¥ä¸ŠãŒãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å ´åˆã¯ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹åˆ—ã¨ã—ã¦åˆ¤å®š
                if email_count >= len(sample_values) * 0.7:
                    logger.info(f"ğŸ“§ ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹åˆ—ã¨ã—ã¦åˆ¤å®š: {email_count}/{len(sample_values)}ä»¶ãŒä¸€è‡´")
                    return 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'
                else:
                    logger.info(f"ğŸ“§ ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹åˆ—ã¨ã—ã¦åˆ¤å®šã•ã‚Œãš: {email_count}/{len(sample_values)}ä»¶ãŒä¸€è‡´ï¼ˆé–¾å€¤ï¼š70%ï¼‰")
                
                # ãã®ä»–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
                for sample in sample_values:
                    if re.search(r'\d{4}[-/]\d{2}[-/]\d{2}', sample):
                        return 'æ—¥ä»˜'
                    elif re.search(r'\d+[å††ï¿¥]', sample):
                        return 'é‡‘é¡'
                    elif re.search(r'\d{3}-\d{4}-\d{4}', sample):
                        return 'é›»è©±ç•ªå·'
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå
            return f'ãƒ‡ãƒ¼ã‚¿{col_index + 1}'
            
        except Exception as e:
            logger.error(f"ã‚«ãƒ©ãƒ åææ¡ˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return f'ã‚«ãƒ©ãƒ {col_index + 1}'
    
    def _is_email_pattern(self, text: str) -> bool:
        """
        ã‚ˆã‚Šå³å¯†ãªãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œè¨¼
        """
        import re
        
        # åŸºæœ¬çš„ãªãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        
        # æ–‡å­—åˆ—ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        text = text.strip()
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
        logger.debug(f"ğŸ“§ ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯: '{text}'")
        
        # åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
        if re.match(email_pattern, text):
            logger.debug(f"ğŸ“§ åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸€è‡´: '{text}'")
            return True
        
        # æ—¥æœ¬èªãƒ‰ãƒ¡ã‚¤ãƒ³ã‚„ç‰¹æ®Šæ–‡å­—ã‚’å«ã‚€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚ãƒã‚§ãƒƒã‚¯
        if '@' in text and '.' in text:
            # @ã®å‰å¾Œã«é©åˆ‡ãªæ–‡å­—ãŒã‚ã‚‹å ´åˆ
            parts = text.split('@')
            if len(parts) == 2:
                local_part = parts[0]
                domain_part = parts[1]
                
                # ãƒ­ãƒ¼ã‚«ãƒ«éƒ¨åˆ†ã®æ¤œè¨¼
                if len(local_part) > 0 and len(domain_part) > 0:
                    # ãƒ‰ãƒ¡ã‚¤ãƒ³éƒ¨åˆ†ã«å°‘ãªãã¨ã‚‚1ã¤ã®ãƒ‰ãƒƒãƒˆãŒå«ã¾ã‚Œã¦ã„ã‚‹
                    if '.' in domain_part:
                        logger.debug(f"ğŸ“§ æ‹¡å¼µãƒ‘ã‚¿ãƒ¼ãƒ³ä¸€è‡´: '{text}'")
                        return True
        
        logger.debug(f"ğŸ“§ ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸ä¸€è‡´: '{text}'")
        return False
    
    def fix_dataframe(self, df: pd.DataFrame, filename: str = "") -> Tuple[pd.DataFrame, List[str]]:
        """DataFrameã®Unnamedã‚«ãƒ©ãƒ å•é¡Œã‚’ä¿®æ­£"""
        try:
            if df.empty:
                return df, ["ç©ºã®DataFrameã§ã™"]
            
            modifications = []
            
            # Step 1: çœŸã®ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æ¤œå‡º
            header_row = self.detect_header_row(df)

            # --- å¤šæ®µãƒ˜ãƒƒãƒ€ãƒ¼å¯¾å¿œé–‹å§‹ ---
            base_header_idx = header_row
            second_header_idx = base_header_idx + 1 if base_header_idx + 1 < len(df) else None
            use_second_header = False

            if second_header_idx is not None:
                row2 = df.iloc[second_header_idx]
                non_empty = sum(1 for v in row2 if pd.notna(v) and str(v).strip())
                if non_empty >= len(row2) * 0.4:
                    use_second_header = True

            # 1æ®µç›®ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ
            top_values = [ensure_string(v) for v in df.iloc[base_header_idx]]
            # 2æ®µç›®ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯ç©ºæ–‡å­—ï¼‰
            if use_second_header:
                sub_values_raw = [ensure_string(v) for v in df.iloc[second_header_idx]]
            else:
                sub_values_raw = [''] * len(top_values)

            # çµåˆã‚»ãƒ«å¯¾ç­–: ç›´å‰ã®å€¤ã‚’æ¨ªå±•é–‹
            propagated_top = []
            last_top = ''
            for v in top_values:
                v_clean = v.strip()
                if v_clean:
                    last_top = v_clean
                    propagated_top.append(v_clean)
                else:
                    propagated_top.append(last_top)

            # æœ€çµ‚ã‚«ãƒ©ãƒ åç”Ÿæˆ
            combined_columns = []
            for i, (top, sub) in enumerate(zip(propagated_top, sub_values_raw)):
                top = top.strip()
                sub = sub.strip()
                if top and sub and top != sub:
                    col_name = f"{top}_{sub}"
                elif sub:
                    col_name = sub
                elif top:
                    col_name = top
                else:
                    col_name = f'åˆ—_{i+1}'
                combined_columns.append(col_name)

            # é‡è¤‡ã‚«ãƒ©ãƒ åè§£æ¶ˆ
            seen = {}
            final_columns = []
            for col in combined_columns:
                base_name = col
                counter = seen.get(base_name, 0)
                if counter > 0:
                    col = f"{base_name}_{counter}"
                seen[base_name] = counter + 1
                final_columns.append(col)

            # ãƒ‡ãƒ¼ã‚¿éƒ¨ã‚’æŠ½å‡º
            skip_rows = base_header_idx + (2 if use_second_header else 1)
            df_fixed = df.iloc[skip_rows:].copy()
            df_fixed.columns = final_columns

            modifications.append(
                f"è¡Œ {base_header_idx}{(' ã¨ ' + str(second_header_idx)) if use_second_header else ''} ã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦è¨­å®šã—ã€{skip_rows} è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
            # --- å¤šæ®µãƒ˜ãƒƒãƒ€ãƒ¼å¯¾å¿œçµ‚äº† ---
            
            # Step 2: Unnamedã‚«ãƒ©ãƒ ã‚’æ¤œå‡º
            unnamed_cols = self.detect_unnamed_columns(df_fixed)
            
            if not unnamed_cols:
                modifications.append("Unnamedã‚«ãƒ©ãƒ ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                return df_fixed, modifications
            
            # Step 3: å„Unnamedã‚«ãƒ©ãƒ ã‚’åˆ†æãƒ»ä¿®æ­£
            columns_to_drop = []
            renamed_columns = {}
            
            for col_idx in unnamed_cols:
                analysis = self.analyze_column_content(df_fixed, col_idx)
                old_name = df_fixed.columns[col_idx]
                
                if analysis['is_row_index']:
                    # è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å ´åˆã¯å‰Šé™¤
                    columns_to_drop.append(old_name)
                    modifications.append(f"è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚«ãƒ©ãƒ  '{old_name}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                    
                elif analysis['type'] == 'empty':
                    # ç©ºã®ã‚«ãƒ©ãƒ ã¯å‰Šé™¤
                    columns_to_drop.append(old_name)
                    modifications.append(f"ç©ºã®ã‚«ãƒ©ãƒ  '{old_name}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                    
                elif analysis['suggested_name']:
                    # åå‰ã‚’å¤‰æ›´
                    new_name = analysis['suggested_name']
                    # é‡è¤‡ã‚’é¿ã‘ã‚‹
                    counter = 1
                    original_new_name = new_name
                    while new_name in df_fixed.columns or new_name in renamed_columns.values():
                        new_name = f"{original_new_name}_{counter}"
                        counter += 1
                    
                    renamed_columns[old_name] = new_name
                    modifications.append(f"ã‚«ãƒ©ãƒ  '{old_name}' ã‚’ '{new_name}' ã«åå‰å¤‰æ›´ã—ã¾ã—ãŸ")
                else:
                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåã‚’è¨­å®š
                    new_name = f"ãƒ‡ãƒ¼ã‚¿{col_idx + 1}"
                    counter = 1
                    while new_name in df_fixed.columns or new_name in renamed_columns.values():
                        new_name = f"ãƒ‡ãƒ¼ã‚¿{col_idx + 1}_{counter}"
                        counter += 1
                        
                    renamed_columns[old_name] = new_name
                    modifications.append(f"ã‚«ãƒ©ãƒ  '{old_name}' ã‚’ '{new_name}' ã«åå‰å¤‰æ›´ã—ã¾ã—ãŸ")
            
            # Step 4: ä¿®æ­£ã‚’é©ç”¨
            # å‰Šé™¤
            if columns_to_drop:
                df_fixed = df_fixed.drop(columns=columns_to_drop)
            
            # åå‰å¤‰æ›´
            if renamed_columns:
                df_fixed = df_fixed.rename(columns=renamed_columns)
            
            # Step 5: å³ç«¯ã®ç©ºã‚«ãƒ©ãƒ ã‚’é™¤å»
            empty_cols = []
            for col in df_fixed.columns:
                try:
                    # å®‰å…¨ãªç©ºã‚«ãƒ©ãƒ åˆ¤å®š
                    col_series = df_fixed[col]
                    
                    # ã¾ãšNAå€¤ã‚’ãƒã‚§ãƒƒã‚¯
                    try:
                        is_all_na = col_series.isna().all()
                        if is_all_na:
                            empty_cols.append(col)
                            continue
                    except Exception as na_error:
                        logger.debug(f"ã‚«ãƒ©ãƒ  '{col}' ã®NAå€¤ãƒã‚§ãƒƒã‚¯ã§ã‚¨ãƒ©ãƒ¼: {str(na_error)}")
                        # å€‹åˆ¥ã«NAå€¤ãƒã‚§ãƒƒã‚¯
                        all_na = True
                        try:
                            for val in col_series:
                                if pd.notna(val):
                                    all_na = False
                                    break
                            if all_na:
                                empty_cols.append(col)
                                continue
                        except Exception as individual_na_error:
                            logger.debug(f"ã‚«ãƒ©ãƒ  '{col}' ã®å€‹åˆ¥NAå€¤ãƒã‚§ãƒƒã‚¯ã§ã‚¨ãƒ©ãƒ¼: {str(individual_na_error)}")
                            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å®‰å…¨å´ã«å€’ã™ï¼ˆå‰Šé™¤ã—ãªã„ï¼‰
                            pass
                    
                    # æ–‡å­—åˆ—ã¨ã—ã¦å¤‰æ›ã—ã¦ç©ºæ–‡å­—ãƒã‚§ãƒƒã‚¯
                    try:
                        str_series = col_series.astype(str)
                        try:
                            is_all_empty = (str_series.str.strip() == '').all()
                            if is_all_empty:
                                empty_cols.append(col)
                        except Exception as all_error:
                            logger.debug(f"ã‚«ãƒ©ãƒ  '{col}' ã®.all()ãƒã‚§ãƒƒã‚¯ã§ã‚¨ãƒ©ãƒ¼: {str(all_error)}")
                            # å€‹åˆ¥ã«ç©ºæ–‡å­—ãƒã‚§ãƒƒã‚¯
                            all_empty = True
                            for val in str_series:
                                if val.strip() != '':
                                    all_empty = False
                                    break
                            if all_empty:
                                empty_cols.append(col)
                    except Exception as str_error:
                        # æ–‡å­—åˆ—å¤‰æ›ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯å€‹åˆ¥ã«ãƒã‚§ãƒƒã‚¯
                        all_empty = True
                        try:
                            for val in col_series:
                                if pd.notna(val):
                                    val_str = str(val).strip()
                                    if val_str and val_str != '':
                                        all_empty = False
                                        break
                        except Exception as individual_error:
                            # å€‹åˆ¥ãƒã‚§ãƒƒã‚¯ã§ã‚‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯å®‰å…¨å´ã«å€’ã™
                            logger.debug(f"ã‚«ãƒ©ãƒ  '{col}' ã®å€‹åˆ¥å€¤ãƒã‚§ãƒƒã‚¯ã§ã‚¨ãƒ©ãƒ¼: {str(individual_error)}")
                            all_empty = False  # å‰Šé™¤ã—ãªã„
                        
                        if all_empty:
                            empty_cols.append(col)
                            
                except Exception as col_error:
                    logger.warning(f"ã‚«ãƒ©ãƒ  '{col}' ã®ç©ºåˆ¤å®šã§ã‚¨ãƒ©ãƒ¼: {str(col_error)}")
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    continue
            
            if empty_cols:
                df_fixed = df_fixed.drop(columns=empty_cols)
                modifications.append(f"å³ç«¯ã®ç©ºã‚«ãƒ©ãƒ  {len(empty_cols)} å€‹ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            
            modifications.append(f"ä¿®æ­£å®Œäº†: {len(df_fixed.columns)} ã‚«ãƒ©ãƒ , {len(df_fixed)} è¡Œ")
            
            return df_fixed, modifications
            
        except Exception as e:
            logger.error(f"DataFrameä¿®æ­£ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return df, [f"ä¿®æ­£ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"]
    
    def create_clean_sections(self, df: pd.DataFrame, filename: str) -> List[Dict]:
        """ä¿®æ­£ã•ã‚ŒãŸDataFrameã‹ã‚‰ç¶ºéº—ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        try:
            if df.empty:
                return [{
                    'section': "ã‚¨ãƒ©ãƒ¼",
                    'content': "ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™",
                    'source': 'Table',
                    'file': filename,
                    'url': None
                }]
            
            sections = []
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã—ã¦è¿½åŠ 
            header_info = "ã‚«ãƒ©ãƒ : " + ", ".join(df.columns.tolist())
            sections.append({
                'section': "ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ",
                'content': header_info,
                'source': 'Table',
                'file': filename,
                'url': None
            })
            
            # å„è¡Œã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã—ã¦è¿½åŠ 
            for index, row in df.iterrows():
                content_parts = []
                for col, value in row.items():
                    if pd.notna(value) and str(value).strip():
                        content_parts.append(f"{col}: {ensure_string(value)}")
                
                if content_parts:
                    content = " | ".join(content_parts)
                    sections.append({
                        'section': f"è¡Œ {index + 1}",
                        'content': content,
                        'source': 'Table',
                        'file': filename,
                        'url': None
                    })
            
            return sections
            
        except Exception as e:
            logger.error(f"ã‚»ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return [{
                'section': "ã‚¨ãƒ©ãƒ¼",
                'content': f"ã‚»ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                'source': 'Table',
                'file': filename,
                'url': None
            }]
    
    def _is_unnamed_pattern(self, value: str) -> bool:
        """å€¤ãŒunnamedãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            value_str = ensure_string(value).strip()
            
            for pattern in self.unnamed_patterns:
                if re.match(pattern, value_str, re.IGNORECASE):
                    return True
            return False
            
        except Exception as e:
            logger.error(f"Unnamedãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False 

 
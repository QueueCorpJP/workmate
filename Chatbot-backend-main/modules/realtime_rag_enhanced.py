"""
ğŸš€ å¼·åŒ–ã•ã‚ŒãŸãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ãƒ•ãƒ­ãƒ¼
è³ªå•å—ä»˜ã€œRAGå‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å›ç­”ï¼‰ã®æ”¹è‰¯ç‰ˆå®Ÿè£…

æ”¹å–„ç‚¹:
- å¼·åŒ–ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆ
- é©å¿œçš„é¡ä¼¼åº¦é–¾å€¤
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè€ƒæ…®å‹ãƒãƒ£ãƒ³ã‚¯çµ±åˆ
- æ”¹å–„ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
- ã‚ˆã‚Šç²¾å¯†ãªå›ç­”ç”Ÿæˆ
"""

import os
import logging
import asyncio
import re
from typing import List, Dict, Optional, Tuple
from dotenv import load_dotenv
import google.generativeai as genai
import psycopg2
from psycopg2.extras import RealDictCursor
import json
from datetime import datetime

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

logger = logging.getLogger(__name__)

class EnhancedRealtimeRAGProcessor:
    """å¼·åŒ–ã•ã‚ŒãŸãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.use_vertex_ai = os.getenv("USE_VERTEX_AI", "true").lower() == "true"
        self.embedding_model = os.getenv("EMBEDDING_MODEL", "text-multilingual-embedding-002")
        self.expected_dimensions = 768 if "text-multilingual-embedding-002" in self.embedding_model else 3072
        
        # API ã‚­ãƒ¼ã®è¨­å®š
        self.api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        
        self.chat_model = "gemini-2.5-flash"
        self.db_url = self._get_db_url()
        
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY ã¾ãŸã¯ GEMINI_API_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ï¼ˆãƒãƒ£ãƒƒãƒˆç”¨ï¼‰
        genai.configure(api_key=self.api_key)
        self.chat_client = genai.GenerativeModel(self.chat_model)
        
        # å¼·åŒ–ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        try:
            from .vector_search_enhanced import get_enhanced_vector_search_instance, enhanced_vector_search_available
            if enhanced_vector_search_available():
                self.enhanced_search = get_enhanced_vector_search_instance()
                logger.info(f"âœ… å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ: {self.embedding_model} ({self.expected_dimensions}æ¬¡å…ƒ)")
            else:
                logger.error("âŒ å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                raise ValueError("å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        except ImportError as e:
            logger.error(f"âŒ å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            raise ValueError("å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        
        # å›ç­”å“è³ªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.max_context_length = 120000  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’å¢—åŠ 
        self.min_chunk_relevance = 0.4    # æœ€å°é–¢é€£åº¦é–¾å€¤ã‚’ä¸Šã’ã‚‹
        self.context_diversity_threshold = 0.7  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å¤šæ§˜æ€§é–¾å€¤
        
        logger.info(f"âœ… å¼·åŒ–ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–å®Œäº†")
    
    def _get_db_url(self) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã‚’æ§‹ç¯‰"""
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        if "supabase.co" in supabase_url:
            project_id = supabase_url.split("://")[1].split(".")[0]
            return f"postgresql://postgres.{project_id}:{os.getenv('DB_PASSWORD', '')}@aws-0-ap-northeast-1.pooler.supabase.com:6543/postgres"
        else:
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                raise ValueError("DATABASE_URL ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return db_url
    
    async def step1_receive_question(self, question: str, company_id: str = None) -> Dict:
        """
        âœï¸ Step 1. è³ªå•å…¥åŠ›ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        è³ªå•ã®å‰å‡¦ç†ã¨åˆ†æã‚’å®Ÿè¡Œ
        """
        logger.info(f"âœï¸ Step 1: å¼·åŒ–è³ªå•å—ä»˜ - '{question[:50]}...'")
        
        if not question or not question.strip():
            raise ValueError("è³ªå•ãŒç©ºã§ã™")
        
        # è³ªå•ã®å‰å‡¦ç†ã¨åˆ†æ
        processed_question = question.strip()
        
        # è³ªå•ã‚¿ã‚¤ãƒ—ã®åˆ†æ
        question_type = self._analyze_question_type(processed_question)
        
        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æŠ½å‡º
        key_terms = self._extract_key_terms(processed_question)
        
        return {
            "original_question": question,
            "processed_question": processed_question,
            "question_type": question_type,
            "key_terms": key_terms,
            "company_id": company_id,
            "timestamp": datetime.now().isoformat(),
            "step": 1
        }
    
    def _analyze_question_type(self, question: str) -> str:
        """è³ªå•ã‚¿ã‚¤ãƒ—ã‚’åˆ†æ"""
        question_lower = question.lower()
        
        # æ‰‹é †ãƒ»æ–¹æ³•ç³»
        if any(word in question_lower for word in ['æ–¹æ³•', 'æ‰‹é †', 'ã‚„ã‚Šæ–¹', 'ã©ã†ã‚„ã£ã¦', 'ã©ã®ã‚ˆã†ã«']):
            return 'procedure'
        
        # æƒ…å ±æ¤œç´¢ç³»
        elif any(word in question_lower for word in ['ã¨ã¯', 'ã«ã¤ã„ã¦', 'è©³ç´°', 'èª¬æ˜']):
            return 'information'
        
        # å•é¡Œè§£æ±ºç³»
        elif any(word in question_lower for word in ['å•é¡Œ', 'ã‚¨ãƒ©ãƒ¼', 'ãƒˆãƒ©ãƒ–ãƒ«', 'è§£æ±º', 'å¯¾å‡¦']):
            return 'troubleshooting'
        
        # æ¯”è¼ƒãƒ»é¸æŠç³»
        elif any(word in question_lower for word in ['é•ã„', 'æ¯”è¼ƒ', 'ã©ã¡ã‚‰', 'é¸æŠ', 'ãŠã™ã™ã‚']):
            return 'comparison'
        
        # é€£çµ¡å…ˆãƒ»å ´æ‰€ç³»
        elif any(word in question_lower for word in ['é€£çµ¡å…ˆ', 'é›»è©±', 'ãƒ¡ãƒ¼ãƒ«', 'å ´æ‰€', 'ã©ã“']):
            return 'contact'
        
        else:
            return 'general'
    
    def _extract_key_terms(self, question: str) -> List[str]:
        """é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        # åŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        # ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã€è‹±æ•°å­—ã®çµ„ã¿åˆã‚ã›ã‚’é‡è¦èªã¨ã—ã¦æŠ½å‡º
        patterns = [
            r'[ã‚¡-ãƒ¶ãƒ¼]+',  # ã‚«ã‚¿ã‚«ãƒŠ
            r'[ä¸€-é¾¯]+',    # æ¼¢å­—
            r'[A-Za-z0-9]+', # è‹±æ•°å­—
        ]
        
        key_terms = []
        for pattern in patterns:
            matches = re.findall(pattern, question)
            for match in matches:
                if len(match) >= 2:  # 2æ–‡å­—ä»¥ä¸Š
                    key_terms.append(match)
        
        # é‡è¤‡é™¤å»ã¨é »åº¦é †ã‚½ãƒ¼ãƒˆ
        unique_terms = list(set(key_terms))
        
        # é•·ã„èªã‚’å„ªå…ˆ
        unique_terms.sort(key=len, reverse=True)
        
        return unique_terms[:10]  # ä¸Šä½10å€‹
    
    async def step2_enhanced_search(self, question_data: Dict) -> List[Dict]:
        """
        ğŸ” Step 2. å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
        å¼·åŒ–ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¦é«˜å“è³ªãªçµæœã‚’å–å¾—
        """
        logger.info(f"ğŸ” Step 2: å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢é–‹å§‹...")
        
        try:
            question = question_data["processed_question"]
            company_id = question_data.get("company_id")
            question_type = question_data.get("question_type", "general")
            
            # è³ªå•ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´
            max_results = self._get_search_params_by_type(question_type)
            
            # å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œ
            search_results = await self.enhanced_search.enhanced_vector_search(
                query=question,
                company_id=company_id,
                max_results=max_results
            )
            
            if not search_results:
                logger.warning("å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return []
            
            # çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            formatted_results = []
            for result in search_results:
                formatted_results.append({
                    'chunk_id': result.chunk_id,
                    'document_id': result.document_id,
                    'document_name': result.document_name,
                    'content': result.content,
                    'similarity_score': result.similarity_score,
                    'relevance_score': result.relevance_score,
                    'chunk_index': result.chunk_index,
                    'document_type': result.document_type,
                    'search_method': result.search_method,
                    'quality_score': result.quality_score,
                    'context_bonus': result.context_bonus
                })
            
            logger.info(f"âœ… Step 2å®Œäº†: {len(formatted_results)}å€‹ã®é«˜å“è³ªãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—")
            
            # ãƒ‡ãƒãƒƒã‚°: ä¸Šä½3ä»¶ã®è©³ç´°ã‚’è¡¨ç¤º
            for i, result in enumerate(formatted_results[:3]):
                logger.info(f"  {i+1}. {result['document_name']} [ãƒãƒ£ãƒ³ã‚¯{result['chunk_index']}]")
                logger.info(f"     é–¢é€£åº¦: {result['relevance_score']:.3f} (é¡ä¼¼åº¦: {result['similarity_score']:.3f})")
                logger.info(f"     å“è³ª: {result['quality_score']:.3f}, ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {result['context_bonus']:.3f}")
            
            return formatted_results
        
        except Exception as e:
            logger.error(f"âŒ Step 2ã‚¨ãƒ©ãƒ¼: å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å¤±æ•— - {e}")
            raise
    
    def _get_search_params_by_type(self, question_type: str) -> int:
        """è³ªå•ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—"""
        type_params = {
            'procedure': 20,      # æ‰‹é †ç³»ã¯å¤šã‚ã®æƒ…å ±ãŒå¿…è¦
            'information': 15,    # æƒ…å ±ç³»ã¯æ¨™æº–
            'troubleshooting': 18, # å•é¡Œè§£æ±ºç³»ã¯å¤šã‚ã®æƒ…å ±
            'comparison': 12,     # æ¯”è¼ƒç³»ã¯å°‘ãªã‚ã§ç²¾åº¦é‡è¦–
            'contact': 8,         # é€£çµ¡å…ˆç³»ã¯å°‘ãªã‚ã§ååˆ†
            'general': 15         # ä¸€èˆ¬çš„ãªè³ªå•
        }
        
        return type_params.get(question_type, 15)
    
    async def step3_context_optimization(self, search_results: List[Dict], question_data: Dict) -> str:
        """
        ğŸ§  Step 3. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–
        æ¤œç´¢çµæœã‚’æœ€é©åŒ–ã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        """
        logger.info(f"ğŸ§  Step 3: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–é–‹å§‹...")
        
        if not search_results:
            logger.warning("æ¤œç´¢çµæœãŒç©ºã®ãŸã‚ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return ""
        
        try:
            question = question_data["processed_question"]
            question_type = question_data.get("question_type", "general")
            key_terms = question_data.get("key_terms", [])
            
            # é–¢é€£åº¦é–¾å€¤ã«ã‚ˆã‚‹åˆæœŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_results = [
                result for result in search_results 
                if result['relevance_score'] >= self.min_chunk_relevance
            ]
            
            if not filtered_results:
                logger.warning("é–¢é€£åº¦é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã«çµæœãŒç©ºã«ãªã‚Šã¾ã—ãŸ")
                filtered_results = search_results[:5]  # æœ€ä½é™ã®çµæœã‚’ç¢ºä¿
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å¤šæ§˜æ€§ã‚’ç¢ºä¿
            diverse_results = self._ensure_context_diversity(filtered_results)
            
            # è³ªå•ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
            optimized_context = self._build_optimized_context(
                diverse_results, question, question_type, key_terms
            )
            
            logger.info(f"âœ… Step 3å®Œäº†: {len(optimized_context)}æ–‡å­—ã®æœ€é©åŒ–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰")
            return optimized_context
        
        except Exception as e:
            logger.error(f"âŒ Step 3ã‚¨ãƒ©ãƒ¼: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–å¤±æ•— - {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
            return self._build_basic_context(search_results[:10])
    
    def _ensure_context_diversity(self, results: List[Dict]) -> List[Dict]:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å¤šæ§˜æ€§ã‚’ç¢ºä¿"""
        diverse_results = []
        seen_documents = set()
        document_count = {}
        
        # æ–‡æ›¸ã”ã¨ã®åˆ¶é™ã‚’è¨­ã‘ã¦å¤šæ§˜æ€§ã‚’ç¢ºä¿
        max_per_document = 3
        
        for result in results:
            doc_id = result['document_id']
            
            # åŒä¸€æ–‡æ›¸ã‹ã‚‰ã®çµæœæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            current_count = document_count.get(doc_id, 0)
            
            if current_count < max_per_document:
                diverse_results.append(result)
                document_count[doc_id] = current_count + 1
                seen_documents.add(doc_id)
        
        logger.info(f"ğŸ“Š å¤šæ§˜æ€§ç¢ºä¿: {len(seen_documents)}å€‹ã®æ–‡æ›¸ã‹ã‚‰{len(diverse_results)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’é¸æŠ")
        return diverse_results
    
    def _build_optimized_context(self, results: List[Dict], question: str, question_type: str, key_terms: List[str]) -> str:
        """æœ€é©åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰"""
        context_parts = []
        total_length = 0
        
        # è³ªå•ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹é€ 
        if question_type == 'procedure':
            context_parts.append("ã€æ‰‹é †ãƒ»æ–¹æ³•ã«é–¢ã™ã‚‹æƒ…å ±ã€‘")
        elif question_type == 'troubleshooting':
            context_parts.append("ã€å•é¡Œè§£æ±ºã«é–¢ã™ã‚‹æƒ…å ±ã€‘")
        elif question_type == 'contact':
            context_parts.append("ã€é€£çµ¡å…ˆãƒ»å•ã„åˆã‚ã›æƒ…å ±ã€‘")
        else:
            context_parts.append("ã€é–¢é€£æƒ…å ±ã€‘")
        
        for i, result in enumerate(results):
            if total_length >= self.max_context_length:
                break
            
            # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã®æ§‹ç¯‰
            relevance = result['relevance_score']
            quality = result['quality_score']
            
            chunk_header = f"\n=== å‚è€ƒè³‡æ–™{i+1}: {result['document_name']} - ãƒãƒ£ãƒ³ã‚¯{result['chunk_index']} ===\n"
            chunk_header += f"é–¢é€£åº¦: {relevance:.3f}, å“è³ª: {quality:.3f}\n"
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚¤ãƒ©ã‚¤ãƒˆï¼ˆãƒ­ã‚°ç”¨ï¼‰
            content = result['content']
            highlighted_terms = []
            for term in key_terms[:5]:  # ä¸Šä½5å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                if term in content:
                    highlighted_terms.append(term)
            
            if highlighted_terms:
                chunk_header += f"å«æœ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(highlighted_terms)}\n"
            
            chunk_content = chunk_header + content + "\n"
            
            if total_length + len(chunk_content) <= self.max_context_length:
                context_parts.append(chunk_content)
                total_length += len(chunk_content)
            else:
                # æ®‹ã‚Šå®¹é‡ã«åˆã‚ã›ã¦åˆ‡ã‚Šè©°ã‚
                remaining_space = self.max_context_length - total_length
                if remaining_space > 200:  # æœ€ä½é™ã®æƒ…å ±ãŒå…¥ã‚‹å ´åˆã®ã¿
                    truncated_content = chunk_header + content[:remaining_space-len(chunk_header)-50] + "...\n"
                    context_parts.append(truncated_content)
                break
        
        return "\n".join(context_parts)
    
    def _build_basic_context(self, results: List[Dict]) -> str:
        """åŸºæœ¬çš„ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        context_parts = []
        total_length = 0
        
        for i, result in enumerate(results):
            if total_length >= self.max_context_length:
                break
            
            chunk_content = f"\n=== å‚è€ƒè³‡æ–™{i+1}: {result['document_name']} ===\n{result['content']}\n"
            
            if total_length + len(chunk_content) <= self.max_context_length:
                context_parts.append(chunk_content)
                total_length += len(chunk_content)
            else:
                break
        
        return "\n".join(context_parts)
    
    async def step4_enhanced_answer_generation(self, question_data: Dict, context: str, company_name: str = "ãŠå®¢æ§˜ã®ä¼šç¤¾") -> str:
        """
        ğŸ’¡ Step 4. å¼·åŒ–å›ç­”ç”Ÿæˆ
        æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦é«˜å“è³ªãªå›ç­”ã‚’ç”Ÿæˆ
        """
        logger.info(f"ğŸ’¡ Step 4: å¼·åŒ–å›ç­”ç”Ÿæˆé–‹å§‹...")
        
        if not context or len(context.strip()) == 0:
            logger.warning("ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã®ãŸã‚ã€ä¸€èˆ¬çš„ãªå›ç­”ã‚’ç”Ÿæˆ")
            return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã”è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚ˆã‚Šå…·ä½“çš„ãªè³ªå•ã‚’ã—ã¦ã„ãŸã ã‘ã¾ã™ã§ã—ã‚‡ã†ã‹ã€‚"
        
        try:
            question = question_data["processed_question"]
            question_type = question_data.get("question_type", "general")
            key_terms = question_data.get("key_terms", [])
            
            # è³ªå•ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            enhanced_prompt = self._build_enhanced_prompt(
                question, context, question_type, key_terms, company_name
            )
            
            # Gemini Flash 2.5ã§å›ç­”ç”Ÿæˆ
            response = self.chat_client.generate_content(
                enhanced_prompt,
                generation_config=genai.GenerationConfig(
                    temperature=0.1,  # ä¸€è²«æ€§é‡è¦–
                    max_output_tokens=3072,  # å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å¢—åŠ 
                    top_p=0.8,
                    top_k=40
                )
            )
            
            if response and response.candidates:
                try:
                    answer = response.text.strip()
                except (ValueError, AttributeError):
                    # response.text ãŒä½¿ãˆãªã„å ´åˆã¯ parts ã‚’ä½¿ç”¨
                    parts = []
                    for candidate in response.candidates:
                        for part in candidate.content.parts:
                            if hasattr(part, 'text') and part.text:
                                parts.append(part.text)
                    answer = ''.join(parts).strip()
                
                if answer:
                    # å›ç­”ã®å“è³ªãƒã‚§ãƒƒã‚¯
                    quality_score = self._evaluate_answer_quality(answer, question, key_terms)
                    logger.info(f"âœ… Step 4å®Œäº†: {len(answer)}æ–‡å­—ã®å›ç­”ã‚’ç”Ÿæˆ (å“è³ªã‚¹ã‚³ã‚¢: {quality_score:.3f})")
                    
                    # å“è³ªãŒä½ã„å ´åˆã¯æ”¹å–„ã‚’è©¦è¡Œ
                    if quality_score < 0.5:
                        logger.warning("å›ç­”å“è³ªãŒä½ã„ãŸã‚ã€æ”¹å–„ã‚’è©¦è¡Œ")
                        improved_answer = await self._improve_answer_quality(answer, question, context, company_name)
                        if improved_answer and len(improved_answer) > len(answer):
                            return improved_answer
                    
                    return answer
                else:
                    logger.error("LLMã‹ã‚‰ã®å›ç­”ãŒç©ºã§ã™")
                    return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€å›ç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
            else:
                logger.error("LLMã‹ã‚‰ã®å›ç­”ãŒç©ºã§ã™")
                return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€å›ç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
        
        except Exception as e:
            logger.error(f"âŒ Step 4ã‚¨ãƒ©ãƒ¼: å¼·åŒ–å›ç­”ç”Ÿæˆå¤±æ•— - {e}")
            return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
    
    def _build_enhanced_prompt(self, question: str, context: str, question_type: str, key_terms: List[str], company_name: str) -> str:
        """å¼·åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
        
        # è³ªå•ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæŒ‡ç¤º
        type_instructions = {
            'procedure': """
ç‰¹ã«ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š
- æ‰‹é †ã¯ç•ªå·ä»˜ãã§æ˜ç¢ºã«ç¤ºã—ã¦ãã ã•ã„
- å„ã‚¹ãƒ†ãƒƒãƒ—ã§å¿…è¦ãªæƒ…å ±ã‚„æ³¨æ„ç‚¹ã‚’å«ã‚ã¦ãã ã•ã„
- å‰ææ¡ä»¶ã‚„å¿…è¦ãªæº–å‚™ãŒã‚ã‚Œã°æœ€åˆã«èª¬æ˜ã—ã¦ãã ã•ã„
""",
            'troubleshooting': """
ç‰¹ã«ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š
- å•é¡Œã®åŸå› ã¨è§£æ±ºæ–¹æ³•ã‚’æ˜ç¢ºã«åˆ†ã‘ã¦èª¬æ˜ã—ã¦ãã ã•ã„
- è¤‡æ•°ã®è§£æ±ºæ–¹æ³•ãŒã‚ã‚‹å ´åˆã¯ã€å„ªå…ˆé †ä½ã‚’ã¤ã‘ã¦æç¤ºã—ã¦ãã ã•ã„
- è§£æ±ºã§ããªã„å ´åˆã®é€£çµ¡å…ˆã‚„æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç¤ºã—ã¦ãã ã•ã„
""",
            'contact': """
ç‰¹ã«ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š
- é€£çµ¡å…ˆæƒ…å ±ã¯æ­£ç¢ºã«è¨˜è¼‰ã—ã¦ãã ã•ã„
- å–¶æ¥­æ™‚é–“ã‚„å¯¾å¿œå¯èƒ½ãªæ™‚é–“å¸¯ãŒã‚ã‚Œã°æ˜è¨˜ã—ã¦ãã ã•ã„
- ç·Šæ€¥æ™‚ã®é€£çµ¡æ–¹æ³•ãŒã‚ã‚Œã°åˆ¥é€”ç¤ºã—ã¦ãã ã•ã„
""",
            'comparison': """
ç‰¹ã«ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š
- æ¯”è¼ƒé …ç›®ã‚’æ˜ç¢ºã«ã—ã¦è¡¨å½¢å¼ã‚„ç®‡æ¡æ›¸ãã§æ•´ç†ã—ã¦ãã ã•ã„
- ãã‚Œãã‚Œã®ç‰¹å¾´ã‚„ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã‚’èª¬æ˜ã—ã¦ãã ã•ã„
- é¸æŠã®åˆ¤æ–­åŸºæº–ã‚„æ¨å¥¨äº‹é …ãŒã‚ã‚Œã°ç¤ºã—ã¦ãã ã•ã„
""",
            'general': """
ç‰¹ã«ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š
- æƒ…å ±ã‚’è«–ç†çš„ã«æ•´ç†ã—ã¦èª¬æ˜ã—ã¦ãã ã•ã„
- é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã¯å¼·èª¿ã—ã¦ç¤ºã—ã¦ãã ã•ã„
- é–¢é€£ã™ã‚‹è¿½åŠ æƒ…å ±ãŒã‚ã‚Œã°é©åˆ‡ã«å«ã‚ã¦ãã ã•ã„
"""
        }
        
        specific_instruction = type_instructions.get(question_type, type_instructions['general'])
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æƒ…å ±
        keyword_info = ""
        if key_terms:
            keyword_info = f"\nã€é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€‘\nä»¥ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ç‰¹ã«æ³¨æ„ã—ã¦å›ç­”ã—ã¦ãã ã•ã„: {', '.join(key_terms[:5])}\n"
        
        prompt = f"""ã‚ãªãŸã¯{company_name}ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®å‚è€ƒè³‡æ–™ã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ­£ç¢ºã§è¦ªåˆ‡ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªæŒ‡ç¤ºã€‘
1. å‚è€ƒè³‡æ–™ã®å†…å®¹ã‚’è¦ç´„ã›ãšã€åŸæ–‡ã®è¡¨ç¾ã‚’ãã®ã¾ã¾æ´»ç”¨ã—ã¦ãã ã•ã„
2. å‚è€ƒè³‡æ–™ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å…·ä½“çš„ãªæ‰‹é †ã€é€£çµ¡å…ˆã€æ¡ä»¶ç­‰ã¯æ­£ç¢ºã«ä¼ãˆã¦ãã ã•ã„
3. å‚è€ƒè³‡æ–™ã«ãªã„æƒ…å ±ã¯æ¨æ¸¬ã›ãšã€ã€Œè³‡æ–™ã«è¨˜è¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨æ˜è¨˜ã—ã¦ãã ã•ã„
4. å›ç­”ã¯ä¸å¯§ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„
5. æƒ…å ±ãŒè¤‡æ•°ã‚ã‚‹å ´åˆã¯ã€é–¢é€£åº¦ã®é«˜ã„é †ã«æ•´ç†ã—ã¦æç¤ºã—ã¦ãã ã•ã„

{specific_instruction}
{keyword_info}
ã€å‚è€ƒè³‡æ–™ã€‘
{context}

ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘
{question}

ã€å›ç­”ã€‘"""

        return prompt
    
    def _evaluate_answer_quality(self, answer: str, question: str, key_terms: List[str]) -> float:
        """å›ç­”ã®å“è³ªã‚’è©•ä¾¡"""
        if not answer or len(answer.strip()) < 20:
            return 0.0
        
        quality_score = 0.0
        answer_lower = answer.lower()
        question_lower = question.lower()
        
        # 1. é•·ã•ã«ã‚ˆã‚‹è©•ä¾¡
        answer_length = len(answer)
        if 100 <= answer_length <= 2000:
            quality_score += 0.3
        elif 50 <= answer_length <= 3000:
            quality_score += 0.2
        
        # 2. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ç‡
        if key_terms:
            keyword_count = sum(1 for term in key_terms if term.lower() in answer_lower)
            keyword_ratio = keyword_count / len(key_terms)
            quality_score += keyword_ratio * 0.3
        
        # 3. æ§‹é€ çš„è¦ç´ ã®å­˜åœ¨
        structural_elements = [
            r'\d+\.',  # ç•ªå·ä»˜ããƒªã‚¹ãƒˆ
            r'ãƒ»',     # ç®‡æ¡æ›¸ã
            r'ã€.*?ã€‘', # ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦‹å‡ºã—
            r'â– .*?â– ', # å¼·èª¿è¦‹å‡ºã—
        ]
        
        for pattern in structural_elements:
            if re.search(pattern, answer):
                quality_score += 0.05
        
        # 4. å¦å®šçš„ãªå›ç­”ã®æ¤œå‡ºï¼ˆå“è³ªä½ä¸‹è¦å› ï¼‰
        negative_patterns = [
            'ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒ',
            'æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“',
            'è¨˜è¼‰ãŒã‚ã‚Šã¾ã›ã‚“',
            'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ'
        ]
        
        negative_count = sum(1 for pattern in negative_patterns if pattern in answer)
        if negative_count > 0:
            quality_score -= 0.2 * negative_count
        
        return max(0.0, min(1.0, quality_score))
    
    async def _improve_answer_quality(self, original_answer: str, question: str, context: str, company_name: str) -> str:
        """å›ç­”å“è³ªã®æ”¹å–„ã‚’è©¦è¡Œ"""
        try:
            improvement_prompt = f"""ä»¥ä¸‹ã®å›ç­”ã‚’æ”¹å–„ã—ã¦ãã ã•ã„ã€‚ã‚ˆã‚Šå…·ä½“çš„ã§æœ‰ç”¨ãªæƒ…å ±ã‚’å«ã‚€å›ç­”ã«ã—ã¦ãã ã•ã„ã€‚

ã€å…ƒã®è³ªå•ã€‘
{question}

ã€ç¾åœ¨ã®å›ç­”ã€‘
{original_answer}

ã€å‚è€ƒè³‡æ–™ã€‘
{context}

ã€æ”¹å–„æŒ‡ç¤ºã€‘
1. ã‚ˆã‚Šå…·ä½“çš„ãªæƒ…å ±ã‚’å«ã‚ã¦ãã ã•ã„
2. æ‰‹é †ãŒã‚ã‚‹å ´åˆã¯æ˜ç¢ºã«ç•ªå·ä»˜ã‘ã—ã¦ãã ã•ã„
3. é‡è¦ãªæƒ…å ±ã¯å¼·èª¿ã—ã¦ãã ã•ã„
4. ä¸è¶³ã—ã¦ã„ã‚‹æƒ…å ±ãŒã‚ã‚Œã°è£œå®Œã—ã¦ãã ã•ã„

ã€æ”¹å–„ã•ã‚ŒãŸå›ç­”ã€‘"""
            
            response = self.chat_client.generate_content(
                improvement_prompt,
                generation_config=genai.GenerationConfig(
                    temperature=0.2,
                    max_output_tokens=3072,
                    top_p=0.9,
                    top_k=50
                )
            )
            
            if response and response.text:
                improved_answer = response.text.strip()
                if len(improved_answer) > len(original_answer) * 0.8:  # æ”¹å–„ã•ã‚ŒãŸå›ç­”ãŒååˆ†ãªé•·ã•ã®å ´åˆ
                    logger.info("âœ… å›ç­”å“è³ªæ”¹å–„æˆåŠŸ")
                    return improved_answer
        
        except Exception as e:
            logger.error(f"å›ç­”å“è³ªæ”¹å–„ã‚¨ãƒ©ãƒ¼: {e}")
        
        return original_answer
    
    async def step5_response_finalization(self, answer: str, metadata: Dict = None) -> Dict:
        """
        âš¡ï¸ Step 5. å›ç­”æœ€çµ‚åŒ–
        æœ€çµ‚çš„ãªå›ç­”ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        """
        logger.info(f"âš¡ï¸ Step 5: å›ç­”æœ€çµ‚åŒ–æº–å‚™å®Œäº†")
        
        result = {
            "answer": answer,
            "timestamp": datetime.now().isoformat(),
            "step": 5,
            "status": "completed",
            "system_version": "enhanced_realtime_rag_v2"
        }
        
        if metadata:
            result.update(metadata)
        
        logger.info(f"âœ… å¼·åŒ–ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†å®Œäº†: {len(answer)}æ–‡å­—ã®é«˜å“è³ªå›ç­”")
        return result
    
    async def process_enhanced_realtime_rag(self, question: str, company_id: str = None, company_name: str = "ãŠå®¢æ§˜ã®ä¼šç¤¾", top_k: int = 15) -> Dict:
        """
        ğŸš€ å¼·åŒ–ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ãƒ•ãƒ­ãƒ¼å…¨ä½“ã®å®Ÿè¡Œ
        Step 1ã€œ5ã‚’é †æ¬¡å®Ÿè¡Œã—ã¦é«˜å“è³ªãªãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å›ç­”ã‚’ç”Ÿæˆ
        """
        logger.info(f"ğŸš€ å¼·åŒ–ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†é–‹å§‹: '{question[:50]}...'")
        
        try:
            # Step 1: è³ªå•å…¥åŠ›ã¨åˆ†æ
            step1_result = await self.step1_receive_question(question, company_id)
            
            # Step 2: å¼·åŒ–ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
            search_results = await self.step2_enhanced_search(step1_result)
            
            # Step 3: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–
            optimized_context = await self.step3_context_optimization(search_results, step1_result)
            
            # Step 4: å¼·åŒ–å›ç­”ç”Ÿæˆ
            answer = await self.step4_enhanced_answer_generation(step1_result, optimized_context, company_name)
            
            # Step 5: å›ç­”æœ€çµ‚åŒ–
            metadata = {
                "original_question": question,
                "processed_question": step1_result["processed_question"],
                "question_type": step1_result.get("question_type", "general"),
                "key_terms": step1_result.get("key_terms", []),
                "chunks_used": len(search_results),
                "top_relevance": search_results[0]["relevance_score"] if search_results else 0.0,
                "context_length": len(optimized_context),
                "company_id": company_id,
                "company_name": company_name
            }
            
            result = await self.step5_response_finalization(answer, metadata)
            
            logger.info(f"ğŸ‰ å¼·åŒ–ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†æˆåŠŸå®Œäº†")
            return result
            
        except Exception as e:
            logger.error(f"âŒ å¼·åŒ–ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            error_result = {
                "answer": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "status": "error",
                "system_version": "enhanced_realtime_rag_v2"
            }
            return error_result

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_enhanced_realtime_rag_processor = None

def get_enhanced_realtime_rag_processor() -> Optional[EnhancedRealtimeRAGProcessor]:
    """å¼·åŒ–ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
    global _enhanced_realtime_rag_processor
    
    if _enhanced_realtime_rag_processor is None:
        try:
            _enhanced_realtime_rag_processor = EnhancedRealtimeRAGProcessor()
            logger.info("âœ… å¼·åŒ–ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            logger.error(f"âŒ å¼·åŒ–ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    return _enhanced_realtime_rag_processor

async def process_question_enhanced_realtime(question: str, company_id: str = None, company_name: str = "ãŠå®¢æ§˜ã®ä¼šç¤¾", top_k: int = 15) -> Dict:
    """
    å¼·åŒ–ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ã®å¤–éƒ¨å‘¼ã³å‡ºã—ç”¨é–¢æ•°
    
    Args:
        question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
        company_id: ä¼šç¤¾IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        company_name: ä¼šç¤¾åï¼ˆå›ç­”ç”Ÿæˆç”¨ï¼‰
        top_k: å–å¾—ã™ã‚‹é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æ•°
    
    Returns:
        Dict: å‡¦ç†çµæœï¼ˆå›ç­”ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç­‰ï¼‰
    """
    processor = get_enhanced_realtime_rag_processor()
    if not processor:
        return {
            "answer": "ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚",
            "error": "EnhancedRealtimeRAGProcessor initialization failed",
            "timestamp": datetime.now().isoformat(),
            "status": "error",
            "system_version": "enhanced_realtime_rag_v2"
        }
    
    return await processor.process_enhanced_realtime_rag(question, company_id, company_name, top_k)

def enhanced_realtime_rag_available() -> bool:
    """å¼·åŒ–ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        use_vertex_ai = os.getenv("USE_VERTEX_AI", "true").lower() == "true"
        
        return bool(api_key and supabase_url and supabase_key and use_vertex_ai)
    except Exception:
        return False
"""
é«˜é€ŸåŒ–RAGã‚·ã‚¹ãƒ†ãƒ 
ä¸¦åˆ—å‡¦ç†ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹è¶…é«˜é€Ÿæ¤œç´¢
"""
import asyncio
import time
import hashlib
import json
from typing import List, Dict, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from dataclasses import dataclass
from functools import lru_cache

logger = logging.getLogger(__name__)

@dataclass
class FastSearchResult:
    """é«˜é€Ÿæ¤œç´¢çµæœã‚¯ãƒ©ã‚¹"""
    content: str
    score: float
    chunk_id: str
    processing_time: float = 0.0

class HighSpeedRAG:
    """è¶…é«˜é€ŸRAGã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.cache = {}
        self.cache_ttl = 3600  # 1æ™‚é–“
        self.max_workers = 5   # ä¸¦åˆ—å‡¦ç†æ•°
        self.enable_cache = True
        
    def _get_cache_key(self, text: str, query: str) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
        content = f"{text[:100]}_{query}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _is_cache_valid(self, cache_entry: Dict) -> bool:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        if not cache_entry:
            return False
        return time.time() - cache_entry.get('timestamp', 0) < self.cache_ttl
    
    async def fast_chunking(self, text: str, chunk_size: int = 2000, overlap: int = 200) -> List[Dict]:
        """
        é«˜é€Ÿãƒãƒ£ãƒ³ã‚¯åŒ– - å¤§ããªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã§å‡¦ç†æ•°ã‚’å‰Šæ¸›
        """
        start_time = time.time()
        
        # ç°¡å˜ãªå¢ƒç•Œæ¤œå‡ºã§é«˜é€Ÿåˆ†å‰²
        chunks = []
        text_length = len(text)
        
        if text_length <= chunk_size:
            return [{
                'id': 'chunk_0',
                'content': text,
                'size': text_length
            }]
        
        chunk_id = 0
        start = 0
        
        while start < text_length:
            end = min(start + chunk_size, text_length)
            
            # å¢ƒç•Œèª¿æ•´ï¼ˆé«˜é€Ÿç‰ˆï¼‰
            if end < text_length:
                # æœ€å¾Œã®æ”¹è¡Œã‚’æ¢ã™ï¼ˆé™å®šç¯„å›²ï¼‰
                boundary_search = max(0, end - 200)
                last_newline = text.rfind('\n', boundary_search, end)
                if last_newline > start:
                    end = last_newline + 1
            
            chunk_content = text[start:end].strip()
            if chunk_content:
                chunks.append({
                    'id': f'chunk_{chunk_id}',
                    'content': chunk_content,
                    'size': len(chunk_content)
                })
                chunk_id += 1
            
            start = max(start + chunk_size - overlap, end)
        
        elapsed = time.time() - start_time
        logger.info(f"âš¡ é«˜é€Ÿãƒãƒ£ãƒ³ã‚¯åŒ–å®Œäº†: {len(chunks)}å€‹ ({elapsed:.2f}ç§’)")
        
        return chunks
    
    async def parallel_bm25_search(self, query: str, chunks: List[Dict], top_k: int = 15) -> List[FastSearchResult]:
        """
        ä¸¦åˆ—BM25æ¤œç´¢ - è¤‡æ•°ãƒãƒ£ãƒ³ã‚¯ã‚’åŒæ™‚å‡¦ç†
        """
        start_time = time.time()
        
        try:
            import bm25s
            
            # ãƒãƒ£ãƒ³ã‚¯ã‚’å°ã•ãªã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†å‰²ã—ã¦ä¸¦åˆ—å‡¦ç†
            chunk_groups = self._split_chunks_for_parallel(chunks, self.max_workers)
            
            # ä¸¦åˆ—å®Ÿè¡Œç”¨ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
            tasks = []
            for group in chunk_groups:
                task = self._search_chunk_group(query, group, bm25s)
                tasks.append(task)
            
            # ä¸¦åˆ—å®Ÿè¡Œ
            all_results = []
            completed_tasks = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in completed_tasks:
                if isinstance(result, list):
                    all_results.extend(result)
                elif isinstance(result, Exception):
                    logger.warning(f"ä¸¦åˆ—æ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼: {result}")
            
            # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
            all_results.sort(key=lambda x: x.score, reverse=True)
            
            elapsed = time.time() - start_time
            logger.info(f"âš¡ ä¸¦åˆ—BM25æ¤œç´¢å®Œäº†: {len(all_results)}ä»¶ ({elapsed:.2f}ç§’)")
            
            return all_results[:top_k]
            
        except Exception as e:
            logger.error(f"ä¸¦åˆ—BM25æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _split_chunks_for_parallel(self, chunks: List[Dict], num_groups: int) -> List[List[Dict]]:
        """ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦åˆ—å‡¦ç†ç”¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘"""
        chunk_size = max(1, len(chunks) // num_groups)
        groups = []
        
        for i in range(0, len(chunks), chunk_size):
            group = chunks[i:i + chunk_size]
            if group:
                groups.append(group)
        
        return groups
    
    async def _search_chunk_group(self, query: str, chunk_group: List[Dict], bm25s) -> List[FastSearchResult]:
        """ãƒãƒ£ãƒ³ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—ã§ã®æ¤œç´¢å®Ÿè¡Œ"""
        try:
            # ãƒãƒ£ãƒ³ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—ãŒç©ºã®å ´åˆã¯ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
            if not chunk_group:
                logger.warning(f"ãƒãƒ£ãƒ³ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—ãŒç©ºã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                return []
            
            texts = [chunk['content'] for chunk in chunk_group]
            
            # æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if not texts or all(not text.strip() for text in texts):
                logger.warning(f"æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆãŒãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                return []
            
            # BM25æ¤œç´¢å®Ÿè¡Œ
            corpus_tokens = bm25s.tokenize(texts)
            retriever = bm25s.BM25()
            retriever.index(corpus_tokens)
            
            query_tokens = bm25s.tokenize([query])
            results, scores = retriever.retrieve(query_tokens, k=len(chunk_group))
            
            # çµæœã‚’FastSearchResultã«å¤‰æ›
            search_results = []
            if results.shape[1] > 0:  # çµæœãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å‡¦ç†
                for i in range(min(results.shape[1], len(chunk_group))):
                    chunk_idx = results[0, i]
                    if chunk_idx < len(chunk_group):
                        chunk = chunk_group[chunk_idx]
                        score = float(scores[0, i]) if i < len(scores[0]) else 0.0
                        
                        search_results.append(FastSearchResult(
                            content=chunk['content'],
                            score=score,
                            chunk_id=chunk['id']
                        ))
            
            return search_results
            
        except Exception as e:
            logger.error(f"ãƒãƒ£ãƒ³ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def lightning_search(self, query: str, knowledge_text: str, max_results: int = 20) -> str:
        """é›·é€Ÿæ¤œç´¢ - æœ€é«˜é€Ÿåº¦ã§ã®RAGæ¤œç´¢"""
        start_time = time.time()
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cache_key = self._get_cache_key(knowledge_text, query)
        if cache_key in self.cache:
            cache_entry = self.cache[cache_key]
            if time.time() - cache_entry['timestamp'] < self.cache_ttl:
                logger.info(f"âš¡ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ")
                return cache_entry['result']
        
        # é«˜é€Ÿãƒãƒ£ãƒ³ã‚¯åŒ–
        chunks = self._fast_chunking(knowledge_text, chunk_size=3000)
        
        # äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        keywords = self._extract_keywords(query)
        filtered_chunks = self._pre_filter_chunks(chunks, keywords)
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã«ãƒãƒ£ãƒ³ã‚¯ãŒç©ºã®å ´åˆã¯å…ƒã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä½¿ç”¨
        if not filtered_chunks:
            logger.info(f"âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã«ãƒãƒ£ãƒ³ã‚¯ãŒç©ºã®ãŸã‚ã€å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’ä½¿ç”¨")
            filtered_chunks = chunks[:20]  # æœ€å¤§20å€‹ã®ãƒãƒ£ãƒ³ã‚¯
        
        # BM25æ¤œç´¢
        try:
            import bm25s
            
            # ãƒãƒ£ãƒ³ã‚¯ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
            if not filtered_chunks:
                logger.error(f"é«˜é€Ÿæ¤œç´¢ã‚¨ãƒ©ãƒ¼: ãƒãƒ£ãƒ³ã‚¯ãŒç©ºã§ã™")
                final_result = knowledge_text[:20000]
            else:
                texts = [chunk['content'] for chunk in filtered_chunks]
                
                # ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª
                if not texts or all(not text.strip() for text in texts):
                    logger.error(f"é«˜é€Ÿæ¤œç´¢ã‚¨ãƒ©ãƒ¼: æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
                    final_result = knowledge_text[:20000]
                else:
                    corpus_tokens = bm25s.tokenize(texts)
                    retriever = bm25s.BM25()
                    retriever.index(corpus_tokens)
                    
                    query_tokens = bm25s.tokenize([query])
                    results, scores = retriever.retrieve(query_tokens, k=min(max_results, len(filtered_chunks)))
                    
                    # çµæœçµ„ã¿ç«‹ã¦
                    relevant_content = []
                    if results.shape[1] > 0:  # çµæœãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å‡¦ç†
                        for i in range(min(results.shape[1], max_results)):
                            chunk_idx = results[0, i]
                            if chunk_idx < len(filtered_chunks):
                                relevant_content.append(filtered_chunks[chunk_idx]['content'])
                    
                    if relevant_content:
                        final_result = '\n\n'.join(relevant_content[:10])  # æœ€å¤§10å€‹ã®ãƒãƒ£ãƒ³ã‚¯
                    else:
                        logger.info(f"âš ï¸ BM25æ¤œç´¢ã§é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚‰ãšã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                        final_result = knowledge_text[:20000]
            
        except Exception as e:
            logger.error(f"é«˜é€Ÿæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            final_result = knowledge_text[:20000]
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self.cache[cache_key] = {
            'result': final_result,
            'timestamp': time.time()
        }
        
        elapsed = time.time() - start_time
        logger.info(f"âš¡ é›·é€Ÿæ¤œç´¢å®Œäº†: {elapsed:.2f}ç§’")
        
        return final_result
    
    def _fast_chunking(self, text: str, chunk_size: int = 3000) -> List[Dict]:
        """é«˜é€Ÿãƒãƒ£ãƒ³ã‚¯åŒ–"""
        chunks = []
        text_length = len(text)
        
        if text_length <= chunk_size:
            return [{'id': 'chunk_0', 'content': text}]
        
        chunk_id = 0
        start = 0
        
        while start < text_length:
            end = min(start + chunk_size, text_length)
            
            # ç°¡å˜ãªå¢ƒç•Œèª¿æ•´
            if end < text_length:
                boundary_search = max(0, end - 100)
                last_newline = text.rfind('\n', boundary_search, end)
                if last_newline > start:
                    end = last_newline + 1
            
            chunk_content = text[start:end].strip()
            if chunk_content:
                chunks.append({
                    'id': f'chunk_{chunk_id}',
                    'content': chunk_content
                })
                chunk_id += 1
            
            start = end
        
        return chunks
    
    def _extract_keywords(self, query: str) -> List[str]:
        """é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
        stop_words = {'ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã®', 'ã§', 'ã¨', 'ã‹ã‚‰', 'ã§ã™', 'ã¾ã™'}
        words = query.split()
        keywords = [word for word in words if word not in stop_words and len(word) > 1]
        return keywords[:5]
    
    def _pre_filter_chunks(self, chunks: List[Dict], keywords: List[str]) -> List[Dict]:
        """äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        if not keywords or not chunks:
            return chunks[:20] if chunks else []  # ç©ºã®å ´åˆã§ã‚‚æœ€å¤§20å€‹ã¾ã§ã‚’è¿”ã™
        
        filtered = []
        for chunk in chunks:
            content_lower = chunk['content'].lower()
            matching_keywords = sum(1 for kw in keywords if kw.lower() in content_lower)
            
            if matching_keywords > 0:
                chunk['score'] = matching_keywords
                filtered.append(chunk)
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœãŒç©ºã®å ´åˆã¯ã€å…ƒã®ãƒãƒ£ãƒ³ã‚¯ã®ä¸€éƒ¨ã‚’è¿”ã™
        if not filtered:
            logger.info(f"âš ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§çµæœãŒç©ºã®ãŸã‚ã€å…ƒã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä½¿ç”¨")
            return chunks[:10]  # æœ€å¤§10å€‹ã®ãƒãƒ£ãƒ³ã‚¯
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        filtered.sort(key=lambda x: x.get('score', 0), reverse=True)
        return filtered[:20]  # ä¸Šä½20ãƒãƒ£ãƒ³ã‚¯ã®ã¿
    
    async def turbo_search(self, query: str, knowledge_text: str, max_results: int = 15) -> str:
        """
        ã‚¿ãƒ¼ãƒœæ¤œç´¢ - é€Ÿåº¦é‡è¦–ã®ç°¡æ˜“ç‰ˆ
        """
        start_time = time.time()
        
        # éå¸¸ã«å¤§ããªãƒãƒ£ãƒ³ã‚¯ã§åˆ†å‰²æ•°ã‚’æœ€å°åŒ–
        if len(knowledge_text) > 100000:
            chunks = await self.fast_chunking(knowledge_text, chunk_size=5000, overlap=500)
        else:
            chunks = await self.fast_chunking(knowledge_text, chunk_size=len(knowledge_text))
        
        # ãƒãƒ£ãƒ³ã‚¯ãŒç©ºã®å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not chunks:
            logger.warning(f"ã‚¿ãƒ¼ãƒœæ¤œç´¢: ãƒãƒ£ãƒ³ã‚¯ãŒç©ºã®ãŸã‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            return knowledge_text[:20000]
        
        # å˜ä¸€ã‚¹ãƒ¬ãƒƒãƒ‰ã§ã®é«˜é€Ÿæ¤œç´¢
        try:
            import bm25s
            
            texts = [chunk['content'] for chunk in chunks]
            
            # æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆãŒãªã„å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if not texts or all(not text.strip() for text in texts):
                logger.warning(f"ã‚¿ãƒ¼ãƒœæ¤œç´¢: æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆãŒãªã„ãŸã‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                return knowledge_text[:20000]
            
            corpus_tokens = bm25s.tokenize(texts)
            retriever = bm25s.BM25()
            retriever.index(corpus_tokens)
            
            query_tokens = bm25s.tokenize([query])
            results, scores = retriever.retrieve(query_tokens, k=min(max_results, len(chunks)))
            
            # ä¸Šä½çµæœã‚’çµåˆ
            relevant_content = []
            if results.shape[1] > 0:  # çµæœãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å‡¦ç†
                for i in range(min(results.shape[1], max_results)):
                    chunk_idx = results[0, i]
                    if chunk_idx < len(chunks):
                        relevant_content.append(chunks[chunk_idx]['content'])
            
            if relevant_content:
                final_result = '\n\n'.join(relevant_content)
            else:
                logger.info(f"âš ï¸ ã‚¿ãƒ¼ãƒœæ¤œç´¢ã§é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚‰ãšã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                final_result = knowledge_text[:20000]
            
        except Exception as e:
            logger.error(f"ã‚¿ãƒ¼ãƒœæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            final_result = knowledge_text[:20000]
        
        elapsed = time.time() - start_time
        logger.info(f"ğŸš€ ã‚¿ãƒ¼ãƒœæ¤œç´¢å®Œäº†: {elapsed:.2f}ç§’")
        
        return final_result
    
    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        self.cache.clear()
        logger.info("âš¡ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢å®Œäº†")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
high_speed_rag = HighSpeedRAG()

# é«˜é€ŸåŒ–ç”¨ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
@lru_cache(maxsize=100)
def cached_simple_search(text_hash: str, query: str, max_results: int = 10) -> str:
    """LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ãŸç°¡æ˜“æ¤œç´¢"""
    # ã“ã®é–¢æ•°ã¯å®Ÿéš›ã®ãƒ†ã‚­ã‚¹ãƒˆã§ã¯ãªããƒãƒƒã‚·ãƒ¥ã‚’ã‚­ãƒ¼ã«ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    return f"cached_result_for_{query[:20]}"

def get_text_hash(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’å–å¾—"""
    return hashlib.md5(text.encode()).hexdigest() 
"""
ğŸš€ æ‹¡å¼µãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ãƒ•ãƒ­ãƒ¼ - é•·ã„è³ªå•ã®æ®µéšçš„å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
è³ªå•ã‚’é©åˆ‡ã«åˆ†å‰²ã—ã€æ®µéšçš„ã«å‡¦ç†ã—ãŸã†ãˆã§ã€æœ€çµ‚çš„ã«çµ±åˆã—ãŸå½¢ã§å›ç­”ã‚’æä¾›

å®Ÿè£…ã‚¹ãƒ†ãƒƒãƒ—:
âœï¸ Step 1: è³ªå•æ–‡ã‚’æ§‹æ–‡çš„ã«ãƒ‘ãƒ¼ã‚¹ã—ã¦ã€ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«åˆ†å‰²
ğŸ§  Step 2: ãã‚Œãã‚Œã‚’å€‹åˆ¥ã«Embedding & Retrieval (åˆ†å‰²ã•ã‚ŒãŸã‚µãƒ–è³ªå•ã”ã¨ã«ã€embeddingæ¤œç´¢ï¼ˆRAGï¼‰)
ğŸ’¡ Step 3: ãã‚Œãã‚Œã®çµæœã‚’LLMã§å›ç­”ç”Ÿæˆ (å„åˆ†å‰²ã‚¿ã‚¹ã‚¯ã‹ã‚‰ã€ã‚µãƒ–å›ç­”ã‚’ä½œæˆ)
ğŸ Step 4: LLMã§æœ€çµ‚çµ±åˆï¼ˆchain-of-thoughtï¼‰(å„ã‚µãƒ–å›ç­”ã‚’è«–ç†çš„ã«çµåˆã—ã€è¡¨å½¢å¼ãªã©ã€æ§‹é€ ã‚’å†æ•´å½¢ã—ã¦1ã¤ã®å‡ºåŠ›ã«ã™ã‚‹)
"""

import os
import logging
import asyncio
import re
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass
from dotenv import load_dotenv
import google.generativeai as genai
import psycopg2
from psycopg2.extras import RealDictCursor
import json
from datetime import datetime

# æ—¢å­˜ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .realtime_rag import RealtimeRAGProcessor, get_realtime_rag_processor

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

logger = logging.getLogger(__name__)

@dataclass
class SubTask:
    """ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®å®šç¾©"""
    id: str
    question: str
    priority: int
    category: str
    keywords: List[str]
    expected_answer_type: str

@dataclass
class SubTaskResult:
    """ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®å‡¦ç†çµæœ"""
    subtask: SubTask
    chunks: List[Dict]
    answer: str
    confidence: float
    processing_time: float

@dataclass
class QuestionAnalysis:
    """è³ªå•åˆ†æçµæœ"""
    original_question: str
    is_complex: bool
    complexity_score: float
    subtasks: List[SubTask]
    reasoning: str
    processing_strategy: str

class EnhancedRealtimeRAGProcessor:
    """æ‹¡å¼µãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ  - é•·ã„è³ªå•ã®æ®µéšçš„å‡¦ç†å¯¾å¿œ"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        # åŸºæœ¬ã®RAGãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’å–å¾—
        self.base_processor = get_realtime_rag_processor()
        if not self.base_processor:
            raise ValueError("åŸºæœ¬RAGãƒ—ãƒ­ã‚»ãƒƒã‚µã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        # Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è¨­å®š
        self.api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY ã¾ãŸã¯ GEMINI_API_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        genai.configure(api_key=self.api_key)
        self.analysis_model = genai.GenerativeModel("gemini-2.5-flash")
        self.integration_model = genai.GenerativeModel("gemini-2.5-flash")
        
        # è¤‡é›‘ã•åˆ¤å®šã®é–¾å€¤
        self.complexity_threshold = 0.6
        self.min_subtasks = 2
        self.max_subtasks = 5
        
        logger.info("âœ… æ‹¡å¼µãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–å®Œäº†")
    
    async def step1_parse_and_divide_question(self, question: str) -> QuestionAnalysis:
        """
        âœï¸ Step 1: è³ªå•æ–‡ã‚’æ§‹æ–‡çš„ã«ãƒ‘ãƒ¼ã‚¹ã—ã¦ã€ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«åˆ†å‰²
        
        Args:
            question: å…ƒã®è³ªå•æ–‡
            
        Returns:
            QuestionAnalysis: è³ªå•åˆ†æçµæœã¨ã‚µãƒ–ã‚¿ã‚¹ã‚¯
        """
        # ChatMessageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰æ–‡å­—åˆ—ã‚’å–å¾—
        if hasattr(question, 'text'):
            question_text = question.text
        else:
            question_text = str(question)
        
        logger.info(f"âœï¸ Step 1: è³ªå•åˆ†æãƒ»åˆ†å‰²é–‹å§‹ - '{question_text[:100]}...'")
        
        try:
            # Gemini 2.5 Flashã§è³ªå•ã‚’åˆ†æ
            analysis_prompt = f"""
ä»¥ä¸‹ã®è³ªå•ã‚’åˆ†æã—ã€è¤‡é›‘ãªè³ªå•ã‹ã©ã†ã‹ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
è¤‡é›‘ãªè³ªå•ã®å ´åˆã¯ã€é©åˆ‡ãªã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚

è³ªå•: ã€Œ{question}ã€

åˆ†æé …ç›®:
1. è¤‡é›‘ã•åˆ¤å®š (is_complex): true/false
2. è¤‡é›‘ã•ã‚¹ã‚³ã‚¢ (complexity_score): 0.0-1.0ã®æ•°å€¤
3. å‡¦ç†æˆ¦ç•¥ (processing_strategy): "simple" ã¾ãŸã¯ "multi_step"
4. ã‚µãƒ–ã‚¿ã‚¹ã‚¯åˆ†å‰² (subtasks): è¤‡é›‘ãªå ´åˆã®ã¿ã€ä»¥ä¸‹ã®å½¢å¼ã§2-5å€‹ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«åˆ†å‰²

ã€è¤‡é›‘ãªè³ªå•ã®åˆ¤å®šåŸºæº–ã€‘
- è¤‡æ•°ã®ç•°ãªã‚‹æƒ…å ±ã‚’æ±‚ã‚ã¦ã„ã‚‹
- æ¯”è¼ƒã‚„åˆ†æãŒå¿…è¦
- æ‰‹é †ã‚„æ®µéšçš„ãªèª¬æ˜ãŒå¿…è¦
- è¤‡æ•°ã®æ¡ä»¶ã‚„åˆ¶ç´„ãŒã‚ã‚‹
- æ–‡ç« ãŒé•·ãã€è¤‡æ•°ã®è¦ç´ ã‚’å«ã‚€

ã€ã‚µãƒ–ã‚¿ã‚¹ã‚¯åˆ†å‰²ã®æŒ‡é‡ã€‘
- å„ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã¯ç‹¬ç«‹ã—ã¦å›ç­”å¯èƒ½ã«ã™ã‚‹
- å„ªå…ˆåº¦ã‚’è¨­å®šï¼ˆ1ãŒæœ€é«˜å„ªå…ˆåº¦ï¼‰
- ã‚«ãƒ†ã‚´ãƒªã‚’åˆ†é¡ï¼ˆinfo_request, comparison, procedure, analysisç­‰ï¼‰
- æœŸå¾…ã•ã‚Œã‚‹å›ç­”ã‚¿ã‚¤ãƒ—ã‚’æŒ‡å®šï¼ˆfactual, explanatory, proceduralç­‰ï¼‰

JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

{{
  "is_complex": boolean,
  "complexity_score": number,
  "processing_strategy": "simple" | "multi_step",
  "reasoning": "åˆ¤å®šç†ç”±",
  "subtasks": [
    {{
      "id": "subtask_1",
      "question": "å…·ä½“çš„ãªã‚µãƒ–è³ªå•",
      "priority": 1,
      "category": "info_request",
      "keywords": ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2"],
      "expected_answer_type": "factual"
    }}
  ]
}}

ä¾‹1ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªè³ªå•ï¼‰:
{{
  "is_complex": false,
  "complexity_score": 0.3,
  "processing_strategy": "simple",
  "reasoning": "å˜ä¸€ã®æƒ…å ±ã‚’æ±‚ã‚ã‚‹ç°¡å˜ãªè³ªå•",
  "subtasks": []
}}

ä¾‹2ï¼ˆè¤‡é›‘ãªè³ªå•ï¼‰:
{{
  "is_complex": true,
  "complexity_score": 0.8,
  "processing_strategy": "multi_step",
  "reasoning": "è¤‡æ•°ã®ç•°ãªã‚‹æƒ…å ±ã¨æ¯”è¼ƒåˆ†æãŒå¿…è¦ãªè¤‡é›‘ãªè³ªå•",
  "subtasks": [
    {{
      "id": "subtask_1",
      "question": "Aç¤¾ã®åŸºæœ¬æƒ…å ±ã¯ä½•ã§ã™ã‹ï¼Ÿ",
      "priority": 1,
      "category": "info_request",
      "keywords": ["Aç¤¾", "åŸºæœ¬æƒ…å ±", "ä¼šç¤¾æ¦‚è¦"],
      "expected_answer_type": "factual"
    }},
    {{
      "id": "subtask_2", 
      "question": "Bç¤¾ã®åŸºæœ¬æƒ…å ±ã¯ä½•ã§ã™ã‹ï¼Ÿ",
      "priority": 1,
      "category": "info_request",
      "keywords": ["Bç¤¾", "åŸºæœ¬æƒ…å ±", "ä¼šç¤¾æ¦‚è¦"],
      "expected_answer_type": "factual"
    }},
    {{
      "id": "subtask_3",
      "question": "Aç¤¾ã¨Bç¤¾ã®é•ã„ã‚„ç‰¹å¾´ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„",
      "priority": 2,
      "category": "comparison",
      "keywords": ["Aç¤¾", "Bç¤¾", "æ¯”è¼ƒ", "é•ã„", "ç‰¹å¾´"],
      "expected_answer_type": "explanatory"
    }}
  ]
}}
"""
            
            # Gemini 2.5 Flashã§åˆ†æå®Ÿè¡Œ
            response = self.analysis_model.generate_content(
                analysis_prompt,
                generation_config=genai.GenerationConfig(
                    temperature=0.1,  # ä¸€è²«æ€§é‡è¦–
                    max_output_tokens=8192,
                    top_p=0.8,
                    top_k=40
                )
            )
            
            if not response or not response.candidates:
                logger.warning("âš ï¸ Geminiã‹ã‚‰ã®åˆ†æå¿œç­”ãŒç©ºã§ã™")
                return self._create_fallback_analysis(question)
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONã‚’æŠ½å‡º
            response_text = ""
            for part in response.candidates[0].content.parts:
                if hasattr(part, 'text'):
                    response_text += part.text
            
            # JSONã®è§£æ
            try:
                analysis_data = json.loads(response_text.strip())
            except json.JSONDecodeError:
                # Markdownã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã®JSONã‚’æŠ½å‡º
                json_match = re.search(r'```json\n(.*?)```', response_text, re.DOTALL)
                if json_match:
                    analysis_data = json.loads(json_match.group(1).strip())
                else:
                    logger.warning("âš ï¸ JSONè§£æã«å¤±æ•—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æã‚’å®Ÿè¡Œ")
                    return self._create_fallback_analysis(question)
            
            # QuestionAnalysisã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®æ§‹ç¯‰
            is_complex = analysis_data.get("is_complex", False)
            complexity_score = float(analysis_data.get("complexity_score", 0.5))
            processing_strategy = analysis_data.get("processing_strategy", "simple")
            reasoning = analysis_data.get("reasoning", "Geminiåˆ†æçµæœ")
            
            subtasks = []
            if is_complex and analysis_data.get("subtasks"):
                for i, subtask_data in enumerate(analysis_data["subtasks"]):
                    subtask = SubTask(
                        id=subtask_data.get("id", f"subtask_{i+1}"),
                        question=subtask_data.get("question", ""),
                        priority=int(subtask_data.get("priority", 1)),
                        category=subtask_data.get("category", "info_request"),
                        keywords=subtask_data.get("keywords", []),
                        expected_answer_type=subtask_data.get("expected_answer_type", "factual")
                    )
                    subtasks.append(subtask)
            
            analysis = QuestionAnalysis(
                original_question=question,
                is_complex=is_complex,
                complexity_score=complexity_score,
                subtasks=subtasks,
                reasoning=reasoning,
                processing_strategy=processing_strategy
            )
            
            logger.info(f"âœ… Step 1å®Œäº†: è¤‡é›‘åº¦={complexity_score:.2f}, ã‚µãƒ–ã‚¿ã‚¹ã‚¯æ•°={len(subtasks)}")
            logger.info(f"ğŸ¯ å‡¦ç†æˆ¦ç•¥: {processing_strategy}")
            logger.info(f"ğŸ’­ åˆ¤å®šç†ç”±: {reasoning}")
            
            if subtasks:
                logger.info("ğŸ“‹ ã‚µãƒ–ã‚¿ã‚¹ã‚¯ä¸€è¦§:")
                for subtask in subtasks:
                    logger.info(f"  - {subtask.id}: {subtask.question} (å„ªå…ˆåº¦: {subtask.priority})")
            
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ Step 1ã‚¨ãƒ©ãƒ¼: è³ªå•åˆ†æå¤±æ•— - {e}")
            return self._create_fallback_analysis(question)
    
    def _create_fallback_analysis(self, question: str) -> QuestionAnalysis:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æï¼ˆGeminiãŒåˆ©ç”¨ã§ããªã„å ´åˆï¼‰"""
        logger.info("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æå®Ÿè¡Œä¸­...")
        
        # ç°¡å˜ãªãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹åˆ†æ
        question_length = len(question)
        question_lower = question.lower()
        
        # è¤‡é›‘ã•ã®åˆ¤å®š
        complexity_indicators = [
            ('ã¨' in question and 'é•ã„' in question),  # æ¯”è¼ƒ
            ('æ‰‹é †' in question or 'ã‚„ã‚Šæ–¹' in question),  # æ‰‹é †
            ('ãªãœ' in question and 'ã©ã†' in question),  # è¤‡åˆè³ªå•
            question_length > 100,  # é•·ã„è³ªå•
            question.count('ï¼Ÿ') > 1 or question.count('?') > 1,  # è¤‡æ•°ã®ç–‘å•ç¬¦
            ('ã¾ãš' in question or 'æ¬¡ã«' in question),  # æ®µéšçš„
        ]
        
        complexity_score = sum(complexity_indicators) / len(complexity_indicators)
        is_complex = complexity_score >= 0.4
        
        if is_complex:
            # ç°¡å˜ãªã‚µãƒ–ã‚¿ã‚¹ã‚¯åˆ†å‰²
            subtasks = self._create_simple_subtasks(question)
            processing_strategy = "multi_step"
        else:
            subtasks = []
            processing_strategy = "simple"
        
        return QuestionAnalysis(
            original_question=question,
            is_complex=is_complex,
            complexity_score=complexity_score,
            subtasks=subtasks,
            reasoning="ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æã«ã‚ˆã‚‹åˆ¤å®šï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰",
            processing_strategy=processing_strategy
        )
    
    def _create_simple_subtasks(self, question: str) -> List[SubTask]:
        """ç°¡å˜ãªãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã‚µãƒ–ã‚¿ã‚¹ã‚¯åˆ†å‰²"""
        subtasks = []
        
        # æ¯”è¼ƒè³ªå•ã®å ´åˆ
        if 'ã¨' in question and ('é•ã„' in question or 'æ¯”è¼ƒ' in question):
            # A ã¨ B ã®é•ã„ -> A ã«ã¤ã„ã¦ã€B ã«ã¤ã„ã¦ã€æ¯”è¼ƒ
            parts = question.split('ã¨')
            if len(parts) >= 2:
                entity_a = parts[0].strip()
                entity_b = parts[1].split('ã®')[0].strip()
                
                subtasks.append(SubTask(
                    id="subtask_1",
                    question=f"{entity_a}ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
                    priority=1,
                    category="info_request",
                    keywords=[entity_a],
                    expected_answer_type="factual"
                ))
                
                subtasks.append(SubTask(
                    id="subtask_2", 
                    question=f"{entity_b}ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
                    priority=1,
                    category="info_request",
                    keywords=[entity_b],
                    expected_answer_type="factual"
                ))
                
                subtasks.append(SubTask(
                    id="subtask_3",
                    question=f"{entity_a}ã¨{entity_b}ã®é•ã„ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„",
                    priority=2,
                    category="comparison",
                    keywords=[entity_a, entity_b, "é•ã„", "æ¯”è¼ƒ"],
                    expected_answer_type="explanatory"
                ))
        
        # æ‰‹é †è³ªå•ã®å ´åˆ
        elif 'æ‰‹é †' in question or 'ã‚„ã‚Šæ–¹' in question:
            subtasks.append(SubTask(
                id="subtask_1",
                question=question,
                priority=1,
                category="procedure",
                keywords=["æ‰‹é †", "ã‚„ã‚Šæ–¹", "æ–¹æ³•"],
                expected_answer_type="procedural"
            ))
        
        return subtasks
    
    async def step2_individual_embedding_retrieval(self, subtasks: List[SubTask], company_id: str = None, top_k: int = 10) -> List[Tuple[SubTask, List[Dict]]]:
        """
        ğŸ§  Step 2: ãã‚Œãã‚Œã‚’å€‹åˆ¥ã«Embedding & Retrieval
        åˆ†å‰²ã•ã‚ŒãŸã‚µãƒ–è³ªå•ã”ã¨ã«ã€embeddingæ¤œç´¢ï¼ˆRAGï¼‰ã‚’å®Ÿè¡Œ
        
        Args:
            subtasks: ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆ
            company_id: ä¼šç¤¾ID
            top_k: å„ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã§å–å¾—ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°
            
        Returns:
            List[Tuple[SubTask, List[Dict]]]: ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã¨å¯¾å¿œã™ã‚‹æ¤œç´¢çµæœã®ãƒšã‚¢
        """
        logger.info(f"ğŸ§  Step 2: å€‹åˆ¥æ¤œç´¢é–‹å§‹ - {len(subtasks)}å€‹ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯")
        
        results = []
        
        # å„ªå…ˆåº¦é †ã§ã‚½ãƒ¼ãƒˆ
        sorted_subtasks = sorted(subtasks, key=lambda x: x.priority)
        
        for i, subtask in enumerate(sorted_subtasks):
            logger.info(f"ğŸ” ã‚µãƒ–ã‚¿ã‚¹ã‚¯ {i+1}/{len(subtasks)}: {subtask.question}")
            
            try:
                # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
                query_embedding = await self.base_processor.step2_generate_embedding(subtask.question)
                
                if query_embedding:
                    # é¡ä¼¼æ¤œç´¢å®Ÿè¡Œ
                    similar_chunks = await self.base_processor.step3_similarity_search(
                        query_embedding, 
                        company_id, 
                        top_k
                    )
                    
                    logger.info(f"âœ… ã‚µãƒ–ã‚¿ã‚¹ã‚¯ {subtask.id}: {len(similar_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—")
                    results.append((subtask, similar_chunks))
                else:
                    logger.warning(f"âš ï¸ ã‚µãƒ–ã‚¿ã‚¹ã‚¯ {subtask.id}: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆå¤±æ•—")
                    results.append((subtask, []))
                
                # APIåˆ¶é™å¯¾ç­–ï¼šå°‘ã—å¾…æ©Ÿ
                if i < len(sorted_subtasks) - 1:
                    await asyncio.sleep(0.2)
                    
            except Exception as e:
                logger.error(f"âŒ ã‚µãƒ–ã‚¿ã‚¹ã‚¯ {subtask.id} æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
                results.append((subtask, []))
        
        logger.info(f"âœ… Step 2å®Œäº†: {len(results)}å€‹ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯æ¤œç´¢å®Œäº†")
        return results
    
    async def step3_generate_sub_answers(self, subtask_results: List[Tuple[SubTask, List[Dict]]], company_name: str = "ãŠå®¢æ§˜ã®ä¼šç¤¾", company_id: str = None) -> List[SubTaskResult]:
        """
        ğŸ’¡ Step 3: ãã‚Œãã‚Œã®çµæœã‚’LLMã§å›ç­”ç”Ÿæˆ
        å„åˆ†å‰²ã‚¿ã‚¹ã‚¯ã‹ã‚‰ã€ã‚µãƒ–å›ç­”ã‚’ä½œæˆï¼ˆã‚µãƒ–å›ç­”ã¯ã¾ã ãƒãƒ©ãƒãƒ©ã®çŠ¶æ…‹ï¼‰
        
        Args:
            subtask_results: ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã¨æ¤œç´¢çµæœã®ãƒšã‚¢
            company_name: ä¼šç¤¾å
            company_id: ä¼šç¤¾ID
            
        Returns:
            List[SubTaskResult]: ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®å‡¦ç†çµæœ
        """
        logger.info(f"ğŸ’¡ Step 3: ã‚µãƒ–å›ç­”ç”Ÿæˆé–‹å§‹ - {len(subtask_results)}å€‹ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯")
        
        sub_results = []
        
        for i, (subtask, chunks) in enumerate(subtask_results):
            start_time = datetime.now()
            logger.info(f"ğŸ¤– ã‚µãƒ–å›ç­” {i+1}/{len(subtask_results)}: {subtask.question}")
            
            try:
                # åŸºæœ¬RAGãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ä½¿ç”¨ã—ã¦ã‚µãƒ–å›ç­”ã‚’ç”Ÿæˆ
                if chunks:
                    answer = await self.base_processor.step4_generate_answer(
                        subtask.question, 
                        chunks, 
                        company_name, 
                        company_id
                    )
                    confidence = self._calculate_confidence(subtask, chunks, answer)
                else:
                    answer = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã€Œ{subtask.question}ã€ã«é–¢ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                    confidence = 0.1
                
                processing_time = (datetime.now() - start_time).total_seconds()
                
                sub_result = SubTaskResult(
                    subtask=subtask,
                    chunks=chunks,
                    answer=answer,
                    confidence=confidence,
                    processing_time=processing_time
                )
                
                sub_results.append(sub_result)
                
                logger.info(f"âœ… ã‚µãƒ–å›ç­” {subtask.id}: ä¿¡é ¼åº¦={confidence:.2f}, å‡¦ç†æ™‚é–“={processing_time:.2f}ç§’")
                logger.info(f"ğŸ“ å›ç­”ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {answer[:100]}...")
                
            except Exception as e:
                logger.error(f"âŒ ã‚µãƒ–å›ç­” {subtask.id} ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                
                # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å›ç­”
                sub_result = SubTaskResult(
                    subtask=subtask,
                    chunks=chunks,
                    answer=f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã€Œ{subtask.question}ã€ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
                    confidence=0.0,
                    processing_time=0.0
                )
                sub_results.append(sub_result)
        
        logger.info(f"âœ… Step 3å®Œäº†: {len(sub_results)}å€‹ã®ã‚µãƒ–å›ç­”ç”Ÿæˆå®Œäº†")
        return sub_results
    
    def _calculate_confidence(self, subtask: SubTask, chunks: List[Dict], answer: str) -> float:
        """ã‚µãƒ–å›ç­”ã®ä¿¡é ¼åº¦ã‚’è¨ˆç®—"""
        confidence = 0.5  # ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
        
        # ãƒãƒ£ãƒ³ã‚¯æ•°ã«ã‚ˆã‚‹èª¿æ•´
        if chunks:
            confidence += min(len(chunks) * 0.05, 0.3)  # æœ€å¤§0.3ã®è¿½åŠ 
        
        # å›ç­”é•·ã«ã‚ˆã‚‹èª¿æ•´
        if len(answer) > 50:
            confidence += 0.1
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒã«ã‚ˆã‚‹èª¿æ•´
        answer_lower = answer.lower()
        keyword_matches = sum(1 for keyword in subtask.keywords if keyword.lower() in answer_lower)
        if keyword_matches > 0:
            confidence += min(keyword_matches * 0.1, 0.2)
        
        return min(confidence, 1.0)
    
    async def step4_final_integration(self, analysis: QuestionAnalysis, sub_results: List[SubTaskResult]) -> str:
        """
        ğŸ Step 4: LLMã§æœ€çµ‚çµ±åˆï¼ˆchain-of-thoughtï¼‰
        å„ã‚µãƒ–å›ç­”ã‚’è«–ç†çš„ã«çµåˆã—ã€è¡¨å½¢å¼ãªã©ã€æ§‹é€ ã‚’å†æ•´å½¢ã—ã¦1ã¤ã®å‡ºåŠ›ã«ã™ã‚‹
        
        Args:
            analysis: å…ƒã®è³ªå•åˆ†æçµæœ
            sub_results: ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®å‡¦ç†çµæœ
            
        Returns:
            str: çµ±åˆã•ã‚ŒãŸæœ€çµ‚å›ç­”
        """
        logger.info(f"ğŸ Step 4: æœ€çµ‚çµ±åˆé–‹å§‹ - {len(sub_results)}å€‹ã®ã‚µãƒ–å›ç­”ã‚’çµ±åˆ")
        
        try:
            # ã‚µãƒ–å›ç­”ã‚’æ•´ç†
            sub_answers_text = []
            for i, result in enumerate(sub_results, 1):
                sub_answers_text.append(f"""
ã€ã‚µãƒ–è³ªå•{i}ã€‘: {result.subtask.question}
ã€ã‚«ãƒ†ã‚´ãƒªã€‘: {result.subtask.category}
ã€ä¿¡é ¼åº¦ã€‘: {result.confidence:.2f}
ã€å›ç­”ã€‘: {result.answer}
""")
            
            # çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
            integration_prompt = f"""
ã‚ãªãŸã¯æƒ…å ±çµ±åˆã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®å…ƒã®è³ªå•ã«å¯¾ã—ã¦ã€è¤‡æ•°ã®ã‚µãƒ–è³ªå•ã¸ã®å›ç­”ã‚’è«–ç†çš„ã«çµ±åˆã—ã€æ§‹é€ åŒ–ã•ã‚ŒãŸåŒ…æ‹¬çš„ãªå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€å…ƒã®è³ªå•ã€‘
{analysis.original_question}

ã€è³ªå•åˆ†æã€‘
- è¤‡é›‘åº¦: {analysis.complexity_score:.2f}
- åˆ¤å®šç†ç”±: {analysis.reasoning}

ã€ã‚µãƒ–è³ªå•ã¸ã®å›ç­”ã€‘
{''.join(sub_answers_text)}

ã€çµ±åˆæŒ‡é‡ã€‘
1. **è«–ç†çš„ãªæ§‹é€ åŒ–**: ã‚µãƒ–å›ç­”ã‚’è«–ç†çš„ãªé †åºã§æ•´ç†ã—ã€é–¢é€£æ€§ã‚’æ˜ç¢ºã«ã™ã‚‹
2. **æƒ…å ±ã®é‡è¤‡æ’é™¤**: é‡è¤‡ã™ã‚‹æƒ…å ±ã¯çµ±åˆã—ã€ç°¡æ½”ã«ã¾ã¨ã‚ã‚‹
3. **åŒ…æ‹¬æ€§ã®ç¢ºä¿**: å…ƒã®è³ªå•ã®ã™ã¹ã¦ã®å´é¢ã«å¯¾å¿œã™ã‚‹
4. **èª­ã¿ã‚„ã™ã•**: è¦‹å‡ºã—ã€ç®‡æ¡æ›¸ãã€è¡¨å½¢å¼ãªã©ã‚’æ´»ç”¨ã—ã¦æ§‹é€ åŒ–ã™ã‚‹
5. **ä¿¡é ¼åº¦ã®åæ˜ **: ä¿¡é ¼åº¦ã®ä½ã„æƒ…å ±ã¯é©åˆ‡ã«æ³¨è¨˜ã™ã‚‹

ã€å›ç­”å½¢å¼ã®æŒ‡é‡ã€‘
- æ¯”è¼ƒè³ªå•ã®å ´åˆ: è¡¨å½¢å¼ã‚„å¯¾æ¯”å½¢å¼ã§æ•´ç†
- æ‰‹é †è³ªå•ã®å ´åˆ: ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã§ç•ªå·ä»˜ããƒªã‚¹ãƒˆ
- è¤‡åˆè³ªå•ã®å ´åˆ: ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†ã‘ã—ã¦ä½“ç³»çš„ã«å›ç­”
- æƒ…å ±ä¸è¶³ã®å ´åˆ: æ˜ç¢ºã«ä¸è¶³éƒ¨åˆ†ã‚’ç¤ºã—ã€ä»£æ›¿æ¡ˆã‚’æç¤º

ã€æœ€çµ‚çµ±åˆå›ç­”ã€‘
å…ƒã®è³ªå•ã«å¯¾ã™ã‚‹åŒ…æ‹¬çš„ã§æ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”ã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ï¼š
"""
            
            # Gemini 2.5 Flashã§çµ±åˆå®Ÿè¡Œ
            response = self.integration_model.generate_content(
                integration_prompt,
                generation_config=genai.GenerationConfig(
                    temperature=0.3,  # å‰µé€ æ€§ã¨ä¸€è²«æ€§ã®ãƒãƒ©ãƒ³ã‚¹
                    max_output_tokens=8192,
                    top_p=0.9,
                    top_k=50
                )
            )
            
            if response and response.candidates:
                integrated_answer = ""
                for part in response.candidates[0].content.parts:
                    if hasattr(part, 'text'):
                        integrated_answer += part.text
                
                if integrated_answer.strip():
                    logger.info(f"âœ… Step 4å®Œäº†: {len(integrated_answer)}æ–‡å­—ã®çµ±åˆå›ç­”ã‚’ç”Ÿæˆ")
                    logger.info(f"ğŸ“ çµ±åˆå›ç­”ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {integrated_answer[:200]}...")
                    return integrated_answer.strip()
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚µãƒ–å›ç­”ã‚’å˜ç´”ã«çµåˆ
            logger.warning("âš ï¸ çµ±åˆå‡¦ç†å¤±æ•—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµ±åˆã‚’å®Ÿè¡Œ")
            return self._create_fallback_integration(analysis, sub_results)
            
        except Exception as e:
            logger.error(f"âŒ Step 4ã‚¨ãƒ©ãƒ¼: æœ€çµ‚çµ±åˆå¤±æ•— - {e}")
            return self._create_fallback_integration(analysis, sub_results)
    
    def _create_fallback_integration(self, analysis: QuestionAnalysis, sub_results: List[SubTaskResult]) -> str:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµ±åˆï¼ˆGeminiãŒåˆ©ç”¨ã§ããªã„å ´åˆï¼‰"""
        logger.info("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµ±åˆå®Ÿè¡Œä¸­...")
        
        integration_parts = []
        integration_parts.append(f"ã”è³ªå•ã€Œ{analysis.original_question}ã€ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®ã‚ˆã†ã«å›ç­”ã„ãŸã—ã¾ã™ï¼š\n")
        
        # ã‚µãƒ–å›ç­”ã‚’é †åºç«‹ã¦ã¦çµåˆ
        for i, result in enumerate(sub_results, 1):
            if result.confidence > 0.3:  # ä¿¡é ¼åº¦ã®é«˜ã„å›ç­”ã®ã¿å«ã‚ã‚‹
                integration_parts.append(f"\n## {i}. {result.subtask.question}\n")
                integration_parts.append(result.answer)
            else:
                integration_parts.append(f"\n## {i}. {result.subtask.question}\n")
                integration_parts.append("ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã“ã®ç‚¹ã«ã¤ã„ã¦ã¯ååˆ†ãªæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        integration_parts.append("\n\nä»¥ä¸ŠãŒã€ã”è³ªå•ã«å¯¾ã™ã‚‹åŒ…æ‹¬çš„ãªå›ç­”ã¨ãªã‚Šã¾ã™ã€‚")
        integration_parts.append("ã”ä¸æ˜ãªç‚¹ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã€ãŠæ°—è»½ã«ãŠç”³ã—ä»˜ã‘ãã ã•ã„ã€‚")
        
        return "".join(integration_parts)
    
    async def process_enhanced_realtime_rag(self, question: str, company_id: str = None, company_name: str = "ãŠå®¢æ§˜ã®ä¼šç¤¾", top_k: int = 15) -> Dict:
        """
        ğŸš€ æ‹¡å¼µãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ãƒ•ãƒ­ãƒ¼å…¨ä½“ã®å®Ÿè¡Œ
        é•·ã„è³ªå•ã‚’æ®µéšçš„ã«å‡¦ç†ã—ã€çµ±åˆã•ã‚ŒãŸå›ç­”ã‚’ç”Ÿæˆ
        
        Args:
            question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            company_id: ä¼šç¤¾ID
            company_name: ä¼šç¤¾å
            top_k: å„ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã§å–å¾—ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°
            
        Returns:
            Dict: å‡¦ç†çµæœ
        """
        # ChatMessageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰æ–‡å­—åˆ—ã‚’å–å¾—
        if hasattr(question, 'text'):
            question_text = question.text
        else:
            question_text = str(question)
        
        logger.info(f"ğŸš€ æ‹¡å¼µãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†é–‹å§‹: '{question_text[:100]}...'")
        start_time = datetime.now()
        
        try:
            # Step 1: è³ªå•åˆ†æãƒ»åˆ†å‰²
            analysis = await self.step1_parse_and_divide_question(question)
            
            # è¤‡é›‘ã§ãªã„è³ªå•ã¯åŸºæœ¬RAGãƒ—ãƒ­ã‚»ãƒƒã‚µã§å‡¦ç†
            if not analysis.is_complex or len(analysis.subtasks) == 0:
                logger.info("ğŸ”„ ã‚·ãƒ³ãƒ—ãƒ«ãªè³ªå•ã®ãŸã‚åŸºæœ¬RAGãƒ—ãƒ­ã‚»ãƒƒã‚µã§å‡¦ç†")
                return await self.base_processor.process_realtime_rag(question, company_id, company_name, top_k)
            
            # Step 2: å€‹åˆ¥æ¤œç´¢
            subtask_results = await self.step2_individual_embedding_retrieval(analysis.subtasks, company_id, top_k)
            
            # Step 3: ã‚µãƒ–å›ç­”ç”Ÿæˆ
            sub_results = await self.step3_generate_sub_answers(subtask_results, company_name, company_id)
            
            # Step 4: æœ€çµ‚çµ±åˆ
            final_answer = await self.step4_final_integration(analysis, sub_results)
            
            # å‡¦ç†æ™‚é–“è¨ˆç®—
            total_processing_time = (datetime.now() - start_time).total_seconds()
            
            # ä½¿ç”¨ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã‚’åé›†
            all_chunks = []
            for result in sub_results:
                all_chunks.extend(result.chunks)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰
            metadata = {
                "original_question": question,
                "processing_type": "enhanced_multi_step_rag",
                "question_analysis": {
                    "is_complex": analysis.is_complex,
                    "complexity_score": analysis.complexity_score,
                    "subtasks_count": len(analysis.subtasks),
                    "reasoning": analysis.reasoning,
                    "processing_strategy": analysis.processing_strategy
                },
                "subtask_results": [
                    {
                        "id": result.subtask.id,
                        "question": result.subtask.question,
                        "category": result.subtask.category,
                        "confidence": result.confidence,
                        "chunks_used": len(result.chunks),
                        "processing_time": result.processing_time
                    }
                    for result in sub_results
                ],
                "total_chunks_used": len(all_chunks),
                "total_processing_time": total_processing_time,
                "company_id": company_id,
                "company_name": company_name
            }
            
            # æœ€çµ‚çµæœã®æ§‹ç¯‰
            result = {
                "answer": final_answer,
                "sources": self._extract_source_documents(all_chunks[:10]),  # main.pyãŒæœŸå¾…ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å
                "timestamp": datetime.now().isoformat(),
                "status": "completed",
                "metadata": metadata,
                "source_documents": self._extract_source_documents(all_chunks[:10])  # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™
            }
            
            logger.info(f"ğŸ‰ æ‹¡å¼µãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†æˆåŠŸå®Œäº†: {total_processing_time:.2f}ç§’")
            return result
            
        except Exception as e:
            logger.error(f"âŒæ‹¡å¼µãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
            
            error_result = {
                "answer": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "status": "error",
                "processing_type": "enhanced_multi_step_rag_error"
            }
            return error_result
    
    def _extract_source_documents(self, chunks: List[Dict]) -> List[Dict]:
        """ã‚½ãƒ¼ã‚¹æ–‡æ›¸æƒ…å ±ã‚’æŠ½å‡º - main.pyãŒæœŸå¾…ã™ã‚‹å½¢å¼ã§è¿”ã™"""
        source_documents = []
        seen_docs = set()
        
        for chunk in chunks:
            # document_sources.nameã‚’å–å¾—ï¼ˆè¤‡æ•°ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã‚’è©¦è¡Œï¼‰
            doc_name = (
                chunk.get('document_name') or
                chunk.get('name') or
                chunk.get('filename') or
                'Unknown Document'
            )
            
            if doc_name and doc_name not in seen_docs and doc_name not in ['ã‚·ã‚¹ãƒ†ãƒ å›ç­”', 'unknown', 'Unknown']:
                doc_info = {
                    "name": doc_name,  # main.pyãŒæœŸå¾…ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å
                    "filename": doc_name,  # å¾Œæ–¹äº’æ›æ€§
                    "document_name": doc_name,  # å¾Œæ–¹äº’æ›æ€§
                    "document_type": chunk.get('document_type', 'unknown'),
                    "similarity_score": chunk.get('similarity_score', 0.0)
                }
                source_documents.append(doc_info)
                seen_docs.add(doc_name)
        
        return source_documents


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_enhanced_realtime_rag_processor = None

def get_enhanced_realtime_rag_processor() -> Optional[EnhancedRealtimeRAGProcessor]:
    """æ‹¡å¼µãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
    global _enhanced_realtime_rag_processor
    
    if _enhanced_realtime_rag_processor is None:
        try:
            _enhanced_realtime_rag_processor = EnhancedRealtimeRAGProcessor()
            logger.info("âœ… æ‹¡å¼µãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            logger.error(f"âŒ æ‹¡å¼µãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    return _enhanced_realtime_rag_processor

async def process_question_enhanced_realtime(
    question: str,
    company_id: str = None,
    company_name: str = "ãŠå®¢æ§˜ã®ä¼šç¤¾",
    top_k: int = 15
) -> Dict:
    """
    æ‹¡å¼µãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGå‡¦ç†ã®å¤–éƒ¨å‘¼ã³å‡ºã—ç”¨é–¢æ•°
    
    Args:
        question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
        company_id: ä¼šç¤¾IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        company_name: ä¼šç¤¾åï¼ˆå›ç­”ç”Ÿæˆç”¨ï¼‰
        top_k: å„ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã§å–å¾—ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°
    
    Returns:
        Dict: å‡¦ç†çµæœï¼ˆå›ç­”ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç­‰ï¼‰
    """
    processor = get_enhanced_realtime_rag_processor()
    if not processor:
        return {
            "answer": "ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚",
            "error": "EnhancedRealtimeRAGProcessor initialization failed",
            "timestamp": datetime.now().isoformat(),
            "status": "error"
        }
    
    return await processor.process_enhanced_realtime_rag(question, company_id, company_name, top_k)

def enhanced_realtime_rag_available() -> bool:
    """æ‹¡å¼µãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ RAGãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        return bool(api_key and supabase_url and supabase_key)
    except Exception:
        return False

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆç”¨ã®é–¢æ•°
async def test_enhanced_rag_with_sample_questions():
    """ã‚µãƒ³ãƒ—ãƒ«è³ªå•ã§ã®æ‹¡å¼µRAGãƒ†ã‚¹ãƒˆ"""
    sample_questions = [
        "Aç¤¾ã¨Bç¤¾ã®é•ã„ã¯ä½•ã§ã™ã‹ï¼Ÿãã‚Œãã‚Œã®ç‰¹å¾´ã¨æ–™é‡‘ä½“ç³»ã‚’æ¯”è¼ƒã—ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
        "æ–°ã—ã„ã‚·ã‚¹ãƒ†ãƒ ã‚’å°å…¥ã™ã‚‹æ‰‹é †ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚ã¾ãŸã€å°å…¥æ™‚ã®æ³¨æ„ç‚¹ã‚„å¿…è¦ãªæº–å‚™ã«ã¤ã„ã¦ã‚‚è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
        "æ•…éšœå—ä»˜ã‚·ãƒ¼ãƒˆã®åç§°ã¨è¨˜å…¥æ–¹æ³•ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚ã¾ãŸã€æå‡ºå…ˆã‚„å‡¦ç†ã®æµã‚Œã‚‚çŸ¥ã‚ŠãŸã„ã§ã™ã€‚",
        "ãƒ‘ã‚½ã‚³ãƒ³ã®ä¾¡æ ¼å¸¯ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"  # ã‚·ãƒ³ãƒ—ãƒ«ãªè³ªå•ï¼ˆæ¯”è¼ƒç”¨ï¼‰
    ]
    
    processor = get_enhanced_realtime_rag_processor()
    if not processor:
        logger.error("âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸å¯: ãƒ—ãƒ­ã‚»ãƒƒã‚µã®åˆæœŸåŒ–ã«å¤±æ•—")
        return
    
    logger.info("ğŸ§ª æ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    for i, question in enumerate(sample_questions, 1):
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ§ª ãƒ†ã‚¹ãƒˆ {i}/{len(sample_questions)}: {question}")
        logger.info(f"{'='*80}")
        
        try:
            result = await processor.process_enhanced_realtime_rag(question)
            
            logger.info(f"âœ… ãƒ†ã‚¹ãƒˆ {i} å®Œäº†:")
            logger.info(f"   å‡¦ç†ã‚¿ã‚¤ãƒ—: {result.get('metadata', {}).get('processing_type', 'unknown')}")
            logger.info(f"   è¤‡é›‘åº¦: {result.get('metadata', {}).get('question_analysis', {}).get('complexity_score', 0):.2f}")
            logger.info(f"   å‡¦ç†æ™‚é–“: {result.get('metadata', {}).get('total_processing_time', 0):.2f}ç§’")
            logger.info(f"   å›ç­”é•·: {len(result.get('answer', ''))}æ–‡å­—")
            logger.info(f"   å›ç­”ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {result.get('answer', '')[:200]}...")
            
        except Exception as e:
            logger.error(f"âŒ ãƒ†ã‚¹ãƒˆ {i} å¤±æ•—: {e}")
    
    logger.info("\nğŸ‰ æ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆå®Œäº†")

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    asyncio.run(test_enhanced_rag_with_sample_questions())
"""
ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æ©Ÿèƒ½ã¨ãƒãƒ£ãƒ³ã‚¯åŒ–ãƒãƒ£ãƒƒãƒˆå‡¦ç†
å¤§ããªãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒ£ãƒ³ã‚¯åŒ–ã¨åˆ†å‰²å‡¦ç†ã‚’ç®¡ç†ã—ã¾ã™
"""
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from .chat_config import safe_print, HTTPException, model, get_db_cursor
from .chat_utils import chunk_knowledge_base
from .chat_rag import adaptive_rag_search, format_search_results
from .chat_processing import generate_response_with_context

async def process_chunked_chat(
    message: str,
    user_id: str = "anonymous",
    chunk_size: int = 700,  # ğŸ¯ 600-800æ–‡å­—ç¯„å›²ã®ä¸­å¤®å€¤ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚æº–æ‹ ï¼‰
    max_chunks: int = 140  # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã«å¿œã˜ãŸå€‹æ•°
) -> Dict[str, Any]:
    """
    ãƒãƒ£ãƒ³ã‚¯åŒ–ã•ã‚ŒãŸãƒãƒ£ãƒƒãƒˆå‡¦ç†
    
    Args:
        message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
        max_chunks: æœ€å¤§ãƒãƒ£ãƒ³ã‚¯æ•°
        
    Returns:
        å‡¦ç†çµæœ
    """
    try:
        safe_print(f"Processing chunked chat for user {user_id}")
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒé•·ã„å ´åˆã¯ãƒãƒ£ãƒ³ã‚¯åŒ–
        if len(message) > chunk_size:
            message_chunks = chunk_knowledge_base(message, chunk_size)
            safe_print(f"Message chunked into {len(message_chunks)} parts")
            
            # å„ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†
            chunk_results = []
            for i, chunk in enumerate(message_chunks[:max_chunks]):
                safe_print(f"Processing chunk {i+1}/{len(message_chunks)}")
                
                # RAGæ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆç´°ç²’åº¦åŒ–ã«åˆã‚ã›ã¦ä»¶æ•°å¢—åŠ ï¼‰
                search_results = await adaptive_rag_search(chunk, limit=20)
                
                # çµæœã‚’ä¿å­˜
                chunk_results.append({
                    'chunk_index': i,
                    'chunk_text': chunk,
                    'search_results': search_results,
                    'result_count': len(search_results)
                })
            
            # å…¨ãƒãƒ£ãƒ³ã‚¯ã®çµæœã‚’ãƒãƒ¼ã‚¸
            merged_results = merge_chunk_results(chunk_results)
            
            # çµ±åˆã•ã‚ŒãŸå¿œç­”ã‚’ç”Ÿæˆ
            response = await generate_chunked_response(message, merged_results)
            
            return {
                'response': response,
                'processing_type': 'chunked',
                'chunk_count': len(message_chunks),
                'processed_chunks': len(chunk_results),
                'search_results': merged_results,
                'chunk_details': chunk_results
            }
        
        else:
            # é€šå¸¸ã®å‡¦ç†
            from .chat_processing import process_chat_message
            return await process_chat_message(message, user_id)
            
    except Exception as e:
        safe_print(f"Error in chunked chat processing: {e}")
        raise HTTPException(status_code=500, detail=f"ãƒãƒ£ãƒ³ã‚¯åŒ–å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

def merge_chunk_results(chunk_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ãƒãƒ£ãƒ³ã‚¯çµæœã‚’ãƒãƒ¼ã‚¸ã—ã¦é‡è¤‡ã‚’é™¤å»
    
    Args:
        chunk_results: ãƒãƒ£ãƒ³ã‚¯å‡¦ç†çµæœã®ãƒªã‚¹ãƒˆ
        
    Returns:
        ãƒãƒ¼ã‚¸ã•ã‚ŒãŸæ¤œç´¢çµæœ
    """
    merged_results = []
    seen_ids = set()
    
    # å„ãƒãƒ£ãƒ³ã‚¯ã®çµæœã‚’çµ±åˆ
    for chunk_result in chunk_results:
        search_results = chunk_result.get('search_results', [])
        
        for result in search_results:
            result_id = result.get('id')
            if result_id and result_id not in seen_ids:
                seen_ids.add(result_id)
                # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚’è¿½åŠ 
                result['source_chunk'] = chunk_result['chunk_index']
                merged_results.append(result)
    
    # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
    merged_results.sort(key=lambda x: x.get('score', 0), reverse=True)
    
    safe_print(f"Merged {len(merged_results)} unique results from chunks")
    return merged_results

async def generate_chunked_response(
    original_message: str, 
    merged_results: List[Dict[str, Any]]
) -> str:
    """
    ãƒãƒ£ãƒ³ã‚¯åŒ–ã•ã‚ŒãŸçµæœã‹ã‚‰çµ±åˆå¿œç­”ã‚’ç”Ÿæˆ
    
    Args:
        original_message: å…ƒã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        merged_results: ãƒãƒ¼ã‚¸ã•ã‚ŒãŸæ¤œç´¢çµæœ
        
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸå¿œç­”
    """
    try:
        if not model:
            raise Exception("Gemini model is not available")
        
        # æ¤œç´¢çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆğŸ¯ 5000æ–‡å­—å¯¾å¿œæœ€é©åŒ–ï¼‰
        formatted_results = format_search_results(merged_results, max_length=15000)
        
        # ãƒãƒ£ãƒ³ã‚¯åŒ–å‡¦ç†ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = f"""
ä»¥ä¸‹ã¯é•·ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ†å‰²ã—ã¦æ¤œç´¢ã—ãŸçµæœã§ã™ã€‚
å…¨ä½“çš„ãªæ–‡è„ˆã‚’è€ƒæ…®ã—ã¦ã€åŒ…æ‹¬çš„ã§ä¸€è²«æ€§ã®ã‚ã‚‹å›ç­”ã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚

ã€å…ƒã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‘
{original_message}

ã€æ¤œç´¢çµæœï¼ˆè¤‡æ•°ã®å‚è€ƒè³‡æ–™ã‹ã‚‰çµ±åˆï¼‰ã€‘
{formatted_results}

ã€å›ç­”ã®æŒ‡é‡ã€‘
1. å…ƒã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¨ä½“ã®æ„å›³ã‚’ç†è§£ã™ã‚‹
2. æ¤œç´¢çµæœã‹ã‚‰é–¢é€£ã™ã‚‹æƒ…å ±ã‚’æŠ½å‡ºãƒ»çµ±åˆã™ã‚‹
3. è«–ç†çš„ã§ä¸€è²«æ€§ã®ã‚ã‚‹å›ç­”ã‚’æ§‹æˆã™ã‚‹
4. å¿…è¦ã«å¿œã˜ã¦ã€æƒ…å ±ã®å‡ºå…¸ã‚„å‚è€ƒURLã‚’ç¤ºã™
5. æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹éƒ¨åˆ†ã¯æ˜è¨˜ã™ã‚‹
6. æŠ€è¡“çš„ãªå†…éƒ¨æ§‹é€ æƒ…å ±ï¼ˆåˆ†å‰²ç•ªå·ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹IDãªã©ï¼‰ã¯å‡ºåŠ›ã—ãªã„

ã€å›ç­”ã€‘"""
        
        response = model.generate_content(prompt)
        
        if response and response.text:
            return response.text.strip()
        else:
            raise Exception("No response generated from model")
            
    except Exception as e:
        safe_print(f"Error generating chunked response: {e}")
        return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€é•·ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’çŸ­ãåˆ†ã‘ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"

async def process_knowledge_base_chunking(
    text: str,
    chunk_size: int = 700,  # ğŸ¯ 600-800æ–‡å­—ç¯„å›²ã®ä¸­å¤®å€¤ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚æº–æ‹ ï¼‰
    overlap_ratio: float = 0.1  # ğŸ¯ 10%ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼ˆã‚µã‚¤ã‚ºåˆ¶å¾¡é‡è¦–ï¼‰
) -> List[Dict[str, Any]]:
    """
    çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®ãƒãƒ£ãƒ³ã‚¯åŒ–å‡¦ç†
    
    Args:
        text: ãƒãƒ£ãƒ³ã‚¯åŒ–ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
        chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
        overlap_ratio: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—æ¯”ç‡
        
    Returns:
        ãƒãƒ£ãƒ³ã‚¯åŒ–ã•ã‚ŒãŸçµæœ
    """
    try:
        safe_print(f"Chunking knowledge base text of length {len(text)}")
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åŒ–
        chunks = chunk_knowledge_base(text, chunk_size)
        
        # å„ãƒãƒ£ãƒ³ã‚¯ã®è©³ç´°æƒ…å ±ã‚’ä½œæˆ
        chunk_details = []
        for i, chunk in enumerate(chunks):
            chunk_info = {
                'index': i,
                'text': chunk,
                'length': len(chunk),
                'word_count': len(chunk.split()),
                'start_position': text.find(chunk) if chunk in text else -1
            }
            chunk_details.append(chunk_info)
        
        safe_print(f"Created {len(chunk_details)} chunks")
        return chunk_details
        
    except Exception as e:
        safe_print(f"Error in knowledge base chunking: {e}")
        return []

async def store_chunked_knowledge(
    chunks: List[Dict[str, Any]], 
    source_id: str,
    metadata: Dict[str, Any] = None
) -> bool:
    """
    ãƒãƒ£ãƒ³ã‚¯åŒ–ã•ã‚ŒãŸçŸ¥è­˜ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
    
    Args:
        chunks: ãƒãƒ£ãƒ³ã‚¯åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        source_id: ã‚½ãƒ¼ã‚¹ID
        metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        
    Returns:
        æˆåŠŸã—ãŸå ´åˆTrue
    """
    try:
        cursor = get_db_cursor()
        if not cursor:
            safe_print("Database cursor not available")
            return False
        
        # ãƒãƒ£ãƒ³ã‚¯ã‚’é †æ¬¡ä¿å­˜
        for chunk in chunks:
            insert_query = """
            INSERT INTO knowledge_chunks (source_id, chunk_index, content, length, word_count, metadata)
            VALUES (%s, %s, %s, %s, %s, %s)
            """
            
            chunk_metadata = {
                'start_position': chunk.get('start_position', -1),
                'original_metadata': metadata or {}
            }
            
            cursor.execute(insert_query, (
                source_id,
                chunk['index'],
                chunk['text'],
                chunk['length'],
                chunk['word_count'],
                str(chunk_metadata)
            ))
        
        cursor.connection.commit()
        safe_print(f"Stored {len(chunks)} chunks for source {source_id}")
        return True
        
    except Exception as e:
        safe_print(f"Error storing chunked knowledge: {e}")
        return False

async def retrieve_chunked_knowledge(
    source_id: str,
    chunk_indices: List[int] = None
) -> List[Dict[str, Any]]:
    """
    ãƒãƒ£ãƒ³ã‚¯åŒ–ã•ã‚ŒãŸçŸ¥è­˜ã‚’å–å¾—
    
    Args:
        source_id: ã‚½ãƒ¼ã‚¹ID
        chunk_indices: å–å¾—ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆNoneã®å ´åˆã¯å…¨ã¦ï¼‰
        
    Returns:
        å–å¾—ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯
    """
    try:
        cursor = get_db_cursor()
        if not cursor:
            safe_print("Database cursor not available")
            return []
        
        if chunk_indices:
            # ç‰¹å®šã®ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
            placeholders = ','.join(['%s'] * len(chunk_indices))
            query = f"""
            SELECT chunk_index, content, length, word_count, metadata
            FROM knowledge_chunks
            WHERE source_id = %s AND chunk_index IN ({placeholders})
            ORDER BY chunk_index
            """
            cursor.execute(query, [source_id] + chunk_indices)
        else:
            # å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
            query = """
            SELECT chunk_index, content, length, word_count, metadata
            FROM knowledge_chunks
            WHERE source_id = %s
            ORDER BY chunk_index
            """
            cursor.execute(query, (source_id,))
        
        rows = cursor.fetchall()
        
        chunks = []
        for row in rows:
            chunk = {
                'index': row[0],
                'text': row[1],
                'length': row[2],
                'word_count': row[3],
                'metadata': row[4]
            }
            chunks.append(chunk)
        
        safe_print(f"Retrieved {len(chunks)} chunks for source {source_id}")
        return chunks
        
    except Exception as e:
        safe_print(f"Error retrieving chunked knowledge: {e}")
        return []

def calculate_optimal_chunk_size(text: str, target_chunks: int = 5) -> int:
    """
    æœ€é©ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’è¨ˆç®—ï¼ˆ600-800æ–‡å­—ç¯„å›²å³å®ˆï¼‰
    
    Args:
        text: å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        target_chunks: ç›®æ¨™ãƒãƒ£ãƒ³ã‚¯æ•°
        
    Returns:
        æœ€é©ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆ600-800æ–‡å­—ç¯„å›²ï¼‰
    """
    text_length = len(text)
    
    if text_length <= 600:  # ğŸ¯ æœ€å°ã‚µã‚¤ã‚ºä»¥ä¸‹ã®å ´åˆ
        return text_length
    
    # ç›®æ¨™ãƒãƒ£ãƒ³ã‚¯æ•°ã«åŸºã¥ã„ã¦è¨ˆç®—
    optimal_size = text_length // target_chunks
    
    # ğŸ¯ 600-800æ–‡å­—ã®ç¯„å›²ã«åˆ¶é™
    min_size = 600
    max_size = 800
    
    optimal_size = max(min_size, min(max_size, optimal_size))
    
    safe_print(f"Calculated optimal chunk size: {optimal_size} for text length {text_length} (range: 600-800)")
    return optimal_size

async def analyze_chunk_quality(chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    ãƒãƒ£ãƒ³ã‚¯ã®å“è³ªã‚’åˆ†æ
    
    Args:
        chunks: åˆ†æã™ã‚‹ãƒãƒ£ãƒ³ã‚¯
        
    Returns:
        å“è³ªåˆ†æçµæœ
    """
    if not chunks:
        return {'error': 'No chunks to analyze'}
    
    # åŸºæœ¬çµ±è¨ˆ
    lengths = [chunk['length'] for chunk in chunks]
    word_counts = [chunk['word_count'] for chunk in chunks]
    
    analysis = {
        'chunk_count': len(chunks),
        'total_length': sum(lengths),
        'average_length': sum(lengths) / len(lengths),
        'min_length': min(lengths),
        'max_length': max(lengths),
        'average_words': sum(word_counts) / len(word_counts),
        'length_variance': calculate_variance(lengths),
        'quality_score': 0.0
    }
    
    # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-1ã®ç¯„å›²ï¼‰
    # é•·ã•ã®ä¸€è²«æ€§ã€é©åˆ‡ãªã‚µã‚¤ã‚ºç¯„å›²ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—å“è³ªã‚’è€ƒæ…®
    length_consistency = 1.0 - (analysis['length_variance'] / analysis['average_length'])
    size_appropriateness = 1.0 if 600 <= analysis['average_length'] <= 800 else 0.5  # ğŸ¯ 600-800æ–‡å­—ç¯„å›²ã«èª¿æ•´
    
    analysis['quality_score'] = (length_consistency + size_appropriateness) / 2
    
    safe_print(f"Chunk quality analysis: score={analysis['quality_score']:.3f}")
    return analysis

def calculate_variance(values: List[float]) -> float:
    """
    åˆ†æ•£ã‚’è¨ˆç®—
    
    Args:
        values: å€¤ã®ãƒªã‚¹ãƒˆ
        
    Returns:
        åˆ†æ•£
    """
    if not values:
        return 0.0
    
    mean = sum(values) / len(values)
    variance = sum((x - mean) ** 2 for x in values) / len(values)
    return variance
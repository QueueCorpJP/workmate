"""
ğŸ“Š Excel ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»æ§‹é€ åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
1ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆ1è¡Œï¼‰ã‚’1ã¤ã®æ„å‘³ã®ã¾ã¨ã¾ã‚Šã¨ã—ã¦è‡ªç„¶æ–‡ã«å¤‰æ›
RAGæ¤œç´¢ç²¾åº¦å‘ä¸Šã®ãŸã‚ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿å°‚ç”¨å‡¦ç†
"""

import pandas as pd
import numpy as np
import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from io import BytesIO
import unicodedata
import xlrd  # XLSãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œ

logger = logging.getLogger(__name__)

class ExcelDataCleanerRecordBased:
    """Excelãƒ‡ãƒ¼ã‚¿ã‚’1ãƒ¬ã‚³ãƒ¼ãƒ‰å˜ä½ã§è‡ªç„¶æ–‡ã«å¤‰æ›ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.max_cell_length = 3000     # ã‚»ãƒ«å†…å®¹ã®æœ€å¤§æ–‡å­—æ•°
        self.max_record_length = 8000   # 1ãƒ¬ã‚³ãƒ¼ãƒ‰ã®æœ€å¤§æ–‡å­—æ•°
        self.max_tokens_per_chunk = 400 # ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        
        # é™¤å¤–å¯¾è±¡ã®ç„¡æ„å‘³ãªå€¤
        self.meaningless_values = {
            'nan', 'NaN', 'null', 'NULL', 'None', 'NONE', '',
            '#N/A', '#VALUE!', '#REF!', '#DIV/0!', '#NAME?', '#NUM!', '#NULL!',
            'naan', 'naaN', 'NAAN', 'NaT', '-', 'ï¼', 'â€•', 'â€”'
        }
        
        # é™¤å»å¯¾è±¡ã®è¨˜å·ãƒ»æ–‡å­—åŒ–ã‘æ–‡å­—
        self.unwanted_symbols = {
            'â—¯', 'â–³', 'Ã—', 'â—‹', 'â—', 'â–²', 'â– ', 'â–¡', 'â˜…', 'â˜†',
            'â€»', 'ï¼Š', 'â™ª', 'â™«', 'â™¬', 'â™­', 'â™¯', 'â™®',
            'â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', 'â‘¥', 'â‘¦', 'â‘§', 'â‘¨', 'â‘©',
            'ãŠ¤', 'ãŠ¥', 'ãŠ¦', 'ãŠ§', 'ãŠ¨', 'ãŠ™', 'ãŠš', 'ãŠ›', 'ãŠœ', 'ãŠ',
            'ã€’', 'ã€“', 'ã€”', 'ã€•', 'ã€–', 'ã€—', 'ã€˜', 'ã€™', 'ã€š', 'ã€›'
        }
        
        # ä¿æŒã™ã¹ãé‡è¦ãªè¨˜å·
        self.important_symbols = {
            '@', '#', '$', '%', '&', '*', '+', '-', '=', '/', '\\',
            '(', ')', '[', ']', '{', '}', '<', '>', '|', '~', '^',
            '!', '?', '.', ',', ';', ':', '"', "'", '`'
        }
        
        # ä¸€èˆ¬çš„ãªã‚«ãƒ©ãƒ åã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆè‡ªç„¶æ–‡ç”Ÿæˆç”¨ï¼‰
        self.column_mappings = {
            # ä¼šç¤¾ãƒ»çµ„ç¹”é–¢é€£
            'ä¼šç¤¾å': 'ä¼šç¤¾å',
            'ä¼æ¥­å': 'ä¼šç¤¾å',
            'æ³•äººå': 'ä¼šç¤¾å',
            'çµ„ç¹”å': 'çµ„ç¹”å',
            'å›£ä½“å': 'å›£ä½“å',
            
            # ä½æ‰€é–¢é€£
            'ä½æ‰€': 'ä½æ‰€',
            'æ‰€åœ¨åœ°': 'æ‰€åœ¨åœ°',
            'è¨­ç½®å…ˆ': 'è¨­ç½®å…ˆ',
            'è¨­ç½®å ´æ‰€': 'è¨­ç½®å ´æ‰€',
            'è¨­ç½®å…ˆä½æ‰€': 'è¨­ç½®å…ˆä½æ‰€',
            'æœ¬ç¤¾ä½æ‰€': 'æœ¬ç¤¾ä½æ‰€',
            'æ”¯ç¤¾ä½æ‰€': 'æ”¯ç¤¾ä½æ‰€',
            
            # é€£çµ¡å…ˆé–¢é€£
            'é›»è©±ç•ªå·': 'é›»è©±ç•ªå·',
            'TEL': 'é›»è©±ç•ªå·',
            'tel': 'é›»è©±ç•ªå·',
            'FAX': 'FAXç•ªå·',
            'fax': 'FAXç•ªå·',
            'ãƒ¡ãƒ¼ãƒ«': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
            'email': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
            'Email': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
            'E-mail': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
            
            # ã‚µãƒ¼ãƒ“ã‚¹é–¢é€£
            'ã‚µãƒ¼ãƒ“ã‚¹': 'ã‚µãƒ¼ãƒ“ã‚¹',
            'å¥‘ç´„ã‚µãƒ¼ãƒ“ã‚¹': 'å¥‘ç´„ã‚µãƒ¼ãƒ“ã‚¹',
            'ãƒ—ãƒ©ãƒ³': 'ãƒ—ãƒ©ãƒ³',
            'æ–™é‡‘ãƒ—ãƒ©ãƒ³': 'æ–™é‡‘ãƒ—ãƒ©ãƒ³',
            'å¥‘ç´„ãƒ—ãƒ©ãƒ³': 'å¥‘ç´„ãƒ—ãƒ©ãƒ³',
            
            # æ—¥ä»˜é–¢é€£
            'å¥‘ç´„æ—¥': 'å¥‘ç´„æ—¥',
            'é–‹å§‹æ—¥': 'é–‹å§‹æ—¥',
            'çµ‚äº†æ—¥': 'çµ‚äº†æ—¥',
            'æ›´æ–°æ—¥': 'æ›´æ–°æ—¥',
            
            # æ‹…å½“è€…é–¢é€£
            'æ‹…å½“è€…': 'æ‹…å½“è€…',
            'è²¬ä»»è€…': 'è²¬ä»»è€…',
            'é€£çµ¡å…ˆæ‹…å½“è€…': 'é€£çµ¡å…ˆæ‹…å½“è€…',
            
            # ãã®ä»–
            'å‚™è€ƒ': 'å‚™è€ƒ',
            'ãƒ¡ãƒ¢': 'ãƒ¡ãƒ¢',
            'æ³¨è¨˜': 'æ³¨è¨˜',
            'çŠ¶æ…‹': 'çŠ¶æ…‹',
            'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹'
        }
    
    def clean_excel_data(self, content: bytes) -> List[Dict[str, Any]]:
        """
        Excelãƒ‡ãƒ¼ã‚¿ã‚’1ãƒ¬ã‚³ãƒ¼ãƒ‰å˜ä½ã§è‡ªç„¶æ–‡ã«å¤‰æ›
        æˆ»ã‚Šå€¤: ãƒ¬ã‚³ãƒ¼ãƒ‰ã”ã¨ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        try:
            # XLSãƒ•ã‚¡ã‚¤ãƒ«ã‹XLSXãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚’åˆ¤å®š
            if self._is_xls_file(content):
                logger.info("ğŸ“Š XLSãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡¦ç†é–‹å§‹ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰")
                return self._process_xls_file_record_based(content)
            else:
                logger.info("ğŸ“Š XLSXãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡¦ç†é–‹å§‹ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰")
                return self._process_xlsx_file_record_based(content)
                
        except Exception as e:
            logger.error(f"âŒ Excelå‡¦ç†ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
            try:
                logger.info("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†é–‹å§‹ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰")
                return self._process_xlsx_file_record_based(content)
            except Exception as fallback_error:
                logger.error(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚‚å¤±æ•—ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰: {fallback_error}")
                raise Exception(f"Excelãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    
    def _is_xls_file(self, content: bytes) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãŒXLSå½¢å¼ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        try:
            # XLSãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
            if content[:8] == b'\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1':
                return True
            # xlrdã§èª­ã¿è¾¼ã¿å¯èƒ½ã‹ãƒ†ã‚¹ãƒˆ
            xlrd.open_workbook(file_contents=content)
            return True
        except:
            return False
    
    def _process_xls_file_record_based(self, content: bytes) -> List[Dict[str, Any]]:
        """XLSãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã§å‡¦ç†"""
        try:
            workbook = xlrd.open_workbook(file_contents=content)
            all_records = []
            
            for sheet_name in workbook.sheet_names():
                try:
                    logger.info(f"ğŸ“Š XLSã‚·ãƒ¼ãƒˆå‡¦ç†é–‹å§‹ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰: {sheet_name}")
                    
                    sheet = workbook.sheet_by_name(sheet_name)
                    df = self._xls_sheet_to_dataframe(sheet)
                    
                    if df is not None and not df.empty:
                        records = self._convert_dataframe_to_records(df, sheet_name)
                        all_records.extend(records)
                        logger.info(f"âœ… ã‚·ãƒ¼ãƒˆ {sheet_name}: {len(records)}ãƒ¬ã‚³ãƒ¼ãƒ‰å‡¦ç†å®Œäº†")
                    
                except Exception as sheet_error:
                    logger.warning(f"âš ï¸ XLSã‚·ãƒ¼ãƒˆ {sheet_name} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {sheet_error}")
                    continue
            
            logger.info(f"ğŸ‰ XLSãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰: ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•° {len(all_records)}")
            return all_records
            
        except Exception as e:
            logger.error(f"âŒ XLSãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰: {e}")
            raise
    
    def _process_xlsx_file_record_based(self, content: bytes) -> List[Dict[str, Any]]:
        """XLSXãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã§å‡¦ç†"""
        try:
            excel_file = pd.ExcelFile(BytesIO(content))
            all_records = []
            
            for sheet_name in excel_file.sheet_names:
                try:
                    logger.info(f"ğŸ“Š XLSXã‚·ãƒ¼ãƒˆå‡¦ç†é–‹å§‹ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰: {sheet_name}")
                    
                    df = pd.read_excel(excel_file, sheet_name=sheet_name)
                    df = self._clean_dataframe_enhanced(df)
                    
                    if df is not None and not df.empty:
                        records = self._convert_dataframe_to_records(df, sheet_name)
                        all_records.extend(records)
                        logger.info(f"âœ… ã‚·ãƒ¼ãƒˆ {sheet_name}: {len(records)}ãƒ¬ã‚³ãƒ¼ãƒ‰å‡¦ç†å®Œäº†")
                    
                except Exception as sheet_error:
                    logger.warning(f"âš ï¸ XLSXã‚·ãƒ¼ãƒˆ {sheet_name} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {sheet_error}")
                    continue
            
            logger.info(f"ğŸ‰ XLSXãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰: ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•° {len(all_records)}")
            return all_records
            
        except Exception as e:
            logger.error(f"âŒ XLSXãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰: {e}")
            raise
    
    def _xls_sheet_to_dataframe(self, sheet) -> Optional[pd.DataFrame]:
        """XLSã‚·ãƒ¼ãƒˆã‚’DataFrameã«å¤‰æ›"""
        try:
            if sheet.nrows == 0:
                return None
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’2æ¬¡å…ƒãƒªã‚¹ãƒˆã«å¤‰æ›
            data = []
            for row_idx in range(sheet.nrows):
                row_data = []
                for col_idx in range(sheet.ncols):
                    cell = sheet.cell(row_idx, col_idx)
                    
                    # ã‚»ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦å€¤ã‚’å¤‰æ›
                    if cell.ctype == xlrd.XL_CELL_EMPTY:
                        value = ''
                    elif cell.ctype == xlrd.XL_CELL_TEXT:
                        value = cell.value
                    elif cell.ctype == xlrd.XL_CELL_NUMBER:
                        value = cell.value
                    elif cell.ctype == xlrd.XL_CELL_DATE:
                        # æ—¥ä»˜ã®å‡¦ç†
                        try:
                            date_tuple = xlrd.xldate_as_tuple(cell.value, sheet.book.datemode)
                            value = f"{date_tuple[0]}-{date_tuple[1]:02d}-{date_tuple[2]:02d}"
                        except:
                            value = cell.value
                    elif cell.ctype == xlrd.XL_CELL_BOOLEAN:
                        value = bool(cell.value)
                    else:
                        value = str(cell.value)
                    
                    row_data.append(value)
                data.append(row_data)
            
            # DataFrameã«å¤‰æ›
            if data:
                df = pd.DataFrame(data[1:], columns=data[0] if len(data) > 1 else None)
                return self._clean_dataframe_enhanced(df)
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ XLSã‚·ãƒ¼ãƒˆâ†’DataFrameå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _clean_dataframe_enhanced(self, df: pd.DataFrame) -> pd.DataFrame:
        """DataFrameã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        if df is None or df.empty:
            return df
        
        try:
            # 1. å®Œå…¨ã«ç©ºã®è¡Œãƒ»åˆ—ã‚’å‰Šé™¤
            df = df.dropna(how='all').dropna(axis=1, how='all')
            
            if df.empty:
                return df
            
            # 2. ã‚»ãƒ«å†…å®¹ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            for col in df.columns:
                df[col] = df[col].apply(self._clean_cell_content)
            
            # 3. ç©ºç™½è¡Œã‚’å†åº¦å‰Šé™¤ï¼ˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œï¼‰
            df = df[df.apply(lambda row: any(str(cell).strip() for cell in row if pd.notna(cell)), axis=1)]
            
            # 4. æ„å‘³ã®ãªã„è¡Œã‚’é™¤å»
            df = df[df.apply(self._is_meaningful_row_enhanced, axis=1)]
            
            # 5. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ
            df = df.reset_index(drop=True)
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ DataFrame ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return df
    
    def _clean_cell_content(self, cell_value) -> str:
        """ã‚»ãƒ«å†…å®¹ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        if pd.isna(cell_value) or cell_value is None:
            return ''
        
        try:
            # æ–‡å­—åˆ—ã«å¤‰æ›
            text = str(cell_value).strip()
            
            # ç„¡æ„å‘³ãªå€¤ã‚’ãƒã‚§ãƒƒã‚¯
            if text.lower() in [v.lower() for v in self.meaningless_values]:
                return ''
            
            # ä¸è¦ãªè¨˜å·ã‚’é™¤å»ï¼ˆé‡è¦ãªè¨˜å·ã¯ä¿æŒï¼‰
            cleaned_text = ''
            for char in text:
                if char in self.unwanted_symbols:
                    continue
                elif char in self.important_symbols or char.isalnum() or char.isspace() or ord(char) > 127:
                    cleaned_text += char
                else:
                    cleaned_text += char
            
            # åˆ¶å¾¡æ–‡å­—ã‚’é™¤å»
            cleaned_text = ''.join(char for char in cleaned_text if unicodedata.category(char)[0] != 'C' or char in '\t\n\r')
            
            # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’æ•´ç†
            cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
            
            # æœ€å¤§æ–‡å­—æ•°åˆ¶é™
            if len(cleaned_text) > self.max_cell_length:
                cleaned_text = cleaned_text[:self.max_cell_length] + '...'
            
            return cleaned_text
            
        except Exception as e:
            logger.warning(f"âš ï¸ ã‚»ãƒ«ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return str(cell_value) if cell_value is not None else ''
    
    def _is_meaningful_row_enhanced(self, row: pd.Series) -> bool:
        """è¡ŒãŒæ„å‘³ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚“ã§ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        meaningful_cells = 0
        
        for cell in row:
            if pd.notna(cell):
                cell_text = str(cell).strip()
                if cell_text and cell_text.lower() not in [v.lower() for v in self.meaningless_values]:
                    # 1æ–‡å­—ä»¥ä¸Šã§ç„¡æ„å‘³ãªå€¤ã§ãªã‘ã‚Œã°æ„å‘³ãŒã‚ã‚‹ã¨åˆ¤å®š
                    if len(cell_text) >= 1:
                        meaningful_cells += 1
        
        # 2ã¤ä»¥ä¸Šã®æ„å‘³ã®ã‚ã‚‹ã‚»ãƒ«ãŒã‚ã‚Œã°æœ‰åŠ¹ãªè¡Œã¨ã™ã‚‹
        return meaningful_cells >= 2
    
    def _convert_dataframe_to_records(self, df: pd.DataFrame, sheet_name: str) -> List[Dict[str, Any]]:
        """DataFrameã‚’1ãƒ¬ã‚³ãƒ¼ãƒ‰å˜ä½ã®è‡ªç„¶æ–‡ã«å¤‰æ›"""
        if df is None or df.empty:
            return []
        
        records = []
        
        # ã‚«ãƒ©ãƒ åã‚’æ­£è¦åŒ–
        normalized_columns = self._normalize_column_names(df.columns.tolist())
        
        for index, row in df.iterrows():
            try:
                # 1ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è‡ªç„¶æ–‡ã«å¤‰æ›
                natural_text = self._convert_row_to_natural_text(row, normalized_columns, sheet_name, index)
                
                if natural_text and len(natural_text.strip()) > 10:  # æœ€å°æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
                    # ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒé•·ã™ãã‚‹å ´åˆã¯åˆ†å‰²
                    if len(natural_text) > self.max_record_length:
                        chunks = self._split_long_record(natural_text, normalized_columns, sheet_name, index)
                        records.extend(chunks)
                    else:
                        record = {
                            'content': natural_text,
                            'source_sheet': sheet_name,
                            'record_index': index,
                            'record_type': 'single',
                            'token_estimate': self._estimate_tokens(natural_text)
                        }
                        records.append(record)
                
            except Exception as row_error:
                logger.warning(f"âš ï¸ è¡Œ {index} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {row_error}")
                continue
        
        return records
    
    def _normalize_column_names(self, columns: List[str]) -> List[str]:
        """ã‚«ãƒ©ãƒ åã‚’æ­£è¦åŒ–"""
        normalized = []
        
        for col in columns:
            col_str = str(col).strip()
            
            # ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ã‹ã‚‰å¯¾å¿œã™ã‚‹åå‰ã‚’å–å¾—
            normalized_name = self.column_mappings.get(col_str, col_str)
            normalized.append(normalized_name)
        
        return normalized
    
    def _convert_row_to_natural_text(self, row: pd.Series, normalized_columns: List[str], sheet_name: str, index: int) -> str:
        """1è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªç„¶æ–‡ã«å¤‰æ›"""
        try:
            text_parts = []
            
            # ã‚·ãƒ¼ãƒˆåã‚’å«ã‚ã‚‹
            if sheet_name and sheet_name.strip():
                text_parts.append(f"ã€{sheet_name}ã€‘")
            
            # å„ã‚»ãƒ«ã®å†…å®¹ã‚’è‡ªç„¶æ–‡å½¢å¼ã§è¿½åŠ 
            meaningful_data = []
            
            for i, (col_name, cell_value) in enumerate(zip(normalized_columns, row)):
                if pd.notna(cell_value):
                    cell_text = str(cell_value).strip()
                    if cell_text and cell_text.lower() not in [v.lower() for v in self.meaningless_values]:
                        # ã‚«ãƒ©ãƒ åã¨å€¤ã‚’çµ„ã¿åˆã‚ã›ã¦è‡ªç„¶æ–‡å½¢å¼ã«
                        if col_name and str(col_name).strip():
                            meaningful_data.append(f"{col_name}ã¯{cell_text}")
                        else:
                            meaningful_data.append(cell_text)
            
            if meaningful_data:
                # è‡ªç„¶æ–‡ã¨ã—ã¦çµåˆ
                if len(meaningful_data) == 1:
                    text_parts.append(meaningful_data[0])
                elif len(meaningful_data) == 2:
                    text_parts.append(f"{meaningful_data[0]}ã§ã€{meaningful_data[1]}ã§ã™ã€‚")
                else:
                    # è¤‡æ•°ã®é …ç›®ãŒã‚ã‚‹å ´åˆ
                    main_parts = meaningful_data[:-1]
                    last_part = meaningful_data[-1]
                    text_parts.append(f"{'ã€'.join(main_parts)}ã§ã€{last_part}ã§ã™ã€‚")
            
            result = " ".join(text_parts)
            
            # æ–‡ã®çµ‚ã‚ã‚Šã‚’æ•´ãˆã‚‹
            if result and not result.endswith(('ã€‚', '.', 'ï¼', 'ï¼Ÿ')):
                result += "ã€‚"
            
            return result
            
        except Exception as e:
            logger.warning(f"âš ï¸ è‡ªç„¶æ–‡å¤‰æ›ã‚¨ãƒ©ãƒ¼ (è¡Œ {index}): {e}")
            return ""
    
    def _split_long_record(self, text: str, normalized_columns: List[str], sheet_name: str, index: int) -> List[Dict[str, Any]]:
        """é•·ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’é©åˆ‡ãªã‚µã‚¤ã‚ºã«åˆ†å‰²"""
        chunks = []
        
        try:
            # æ–‡å˜ä½ã§åˆ†å‰²
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\.\!\?]\s*', text)
            
            current_chunk = ""
            chunk_index = 0
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                
                # æ–‡ã‚’è¿½åŠ ã—ãŸå ´åˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®š
                test_chunk = current_chunk + ("ã€‚" if current_chunk else "") + sentence
                estimated_tokens = self._estimate_tokens(test_chunk)
                
                if estimated_tokens > self.max_tokens_per_chunk and current_chunk:
                    # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
                    if current_chunk.strip():
                        chunk = {
                            'content': current_chunk.strip() + "ã€‚",
                            'source_sheet': sheet_name,
                            'record_index': index,
                            'record_type': 'split',
                            'chunk_index': chunk_index,
                            'token_estimate': self._estimate_tokens(current_chunk)
                        }
                        chunks.append(chunk)
                        chunk_index += 1
                    
                    current_chunk = sentence
                else:
                    current_chunk = test_chunk
            
            # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ 
            if current_chunk.strip():
                chunk = {
                    'content': current_chunk.strip() + ("ã€‚" if not current_chunk.endswith(('ã€‚', '.', 'ï¼', 'ï¼Ÿ')) else ""),
                    'source_sheet': sheet_name,
                    'record_index': index,
                    'record_type': 'split',
                    'chunk_index': chunk_index,
                    'token_estimate': self._estimate_tokens(current_chunk)
                }
                chunks.append(chunk)
            
        except Exception as e:
            logger.warning(f"âš ï¸ ãƒ¬ã‚³ãƒ¼ãƒ‰åˆ†å‰²ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã¾è¿”ã™
            chunks = [{
                'content': text,
                'source_sheet': sheet_name,
                'record_index': index,
                'record_type': 'error',
                'token_estimate': self._estimate_tokens(text)
            }]
        
        return chunks
    
    def _estimate_tokens(self, text: str) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®š"""
        if not text:
            return 0
        
        # æ—¥æœ¬èª: 1æ–‡å­— â‰ˆ 1.5ãƒˆãƒ¼ã‚¯ãƒ³, è‹±èª: 4æ–‡å­— â‰ˆ 1ãƒˆãƒ¼ã‚¯ãƒ³
        japanese_chars = len(re.findall(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', text))
        other_chars = len(text) - japanese_chars
        estimated_tokens = int(japanese_chars * 1.5 + other_chars * 0.25)
        return estimated_tokens
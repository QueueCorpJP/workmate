"""
ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
Gemini Embeddingã‚’ä½¿ç”¨ã—ãŸé¡ä¼¼æ¤œç´¢æ©Ÿèƒ½ã‚’æä¾›
"""

import os
import logging
from typing import List, Dict, Tuple, Optional
from dotenv import load_dotenv
from google import genai
import psycopg2
from psycopg2.extras import RealDictCursor
import numpy as np

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

logger = logging.getLogger(__name__)

class VectorSearchSystem:
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        self.model = os.getenv("EMBEDDING_MODEL", "gemini-embedding-exp-03-07")
        self.db_url = self._get_db_url()
        
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY ã¾ãŸã¯ GEMINI_API_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        self.client = genai.Client(api_key=self.api_key)
        
    def _get_db_url(self) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã‚’æ§‹ç¯‰"""
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # Supabase URLã‹ã‚‰æ¥ç¶šæƒ…å ±ã‚’æŠ½å‡º
        if "supabase.co" in supabase_url:
            project_id = supabase_url.split("://")[1].split(".")[0]
            return f"postgresql://postgres.{project_id}:{os.getenv('DB_PASSWORD', '')}@aws-0-ap-northeast-1.pooler.supabase.com:6543/postgres"
        else:
            # ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã®å ´åˆ
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                raise ValueError("DATABASE_URL ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return db_url
    
    def generate_query_embedding(self, query: str) -> List[float]:
        """ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
        try:
            logger.info(f"ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆä¸­: {query[:50]}...")
            response = self.client.models.embed_content(
                model=self.model, 
                contents=query
            )
            
            if response.embeddings and len(response.embeddings) > 0:
                # 3072æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
                full_embedding = response.embeddings[0].values
                # MRLï¼ˆæ¬¡å…ƒå‰Šæ¸›ï¼‰: 3072 â†’ 1536æ¬¡å…ƒã«å‰Šæ¸›
                embedding = full_embedding[:1536]
                logger.info(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº† (å…ƒæ¬¡å…ƒ: {len(full_embedding)} â†’ å‰Šæ¸›å¾Œ: {len(embedding)})")
                return embedding
            else:
                logger.error("åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return []
        
        except Exception as e:
            logger.error(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _extract_original_doc_id(self, chunk_id: str) -> str:
        """ãƒãƒ£ãƒ³ã‚¯IDã‹ã‚‰å…ƒã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã‚’æŠ½å‡º"""
        # doc123_chunk_0 -> doc123
        if '_chunk_' in chunk_id:
            return chunk_id.split('_chunk_')[0]
        return chunk_id
    
    def vector_similarity_search(self, query: str, company_id: str = None, limit: int = 5) -> List[Dict]:
        """ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢ã‚’å®Ÿè¡Œ"""
        try:
            # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            query_vector = self.generate_query_embedding(query)
            if not query_vector:
                logger.error("ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã«å¤±æ•—")
                return []
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    # ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢ã®SQLï¼ˆãƒãƒ£ãƒ³ã‚¯å¯¾å¿œç‰ˆï¼‰
                    sql = """
                    SELECT 
                        de.document_id as chunk_id,
                        CASE 
                            WHEN de.document_id LIKE '%_chunk_%' THEN 
                                SPLIT_PART(de.document_id, '_chunk_', 1)
                            ELSE de.document_id
                        END as original_doc_id,
                        ds.name,
                        ds.special,
                        ds.type,
                        de.snippet,
                        1 - (de.embedding <=> %s) as similarity_score
                    FROM document_embeddings de
                    LEFT JOIN document_sources ds ON ds.id = CASE 
                        WHEN de.document_id LIKE '%_chunk_%' THEN 
                            SPLIT_PART(de.document_id, '_chunk_', 1)
                        ELSE de.document_id
                    END
                    WHERE de.embedding IS NOT NULL
                    """
                    
                    params = [query_vector]
                    
                    # ğŸ” ãƒ‡ãƒãƒƒã‚°: company_idãƒ•ã‚£ãƒ«ã‚¿ã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
                    logger.info(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: company_idãƒ•ã‚£ãƒ«ã‚¿ã‚’ç„¡åŠ¹åŒ–ã—ã¦ãƒ†ã‚¹ãƒˆ")
                    # if company_id:
                    #     sql += " AND ds.company_id = %s"
                    #     params.append(company_id)
                    
                    # é¡ä¼¼åº¦é †ã§ã‚½ãƒ¼ãƒˆ
                    sql += " ORDER BY de.embedding <=> %s LIMIT %s"
                    params.extend([query_vector, limit])
                    
                    logger.info(f"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œä¸­... (limit: {limit})")
                    cur.execute(sql, params)
                    results = cur.fetchall()
                    
                    # çµæœã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                    search_results = []
                    for row in results:
                        search_results.append({
                            'chunk_id': row['chunk_id'],
                            'document_id': row['original_doc_id'],
                            'document_name': row['name'],
                            'document_type': row['type'],
                            'special': row['special'],
                            'snippet': row['snippet'],
                            'similarity_score': float(row['similarity_score']),
                            'search_type': 'vector'
                        })
                    
                    logger.info(f"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Œäº†: {len(search_results)}ä»¶ã®çµæœ")
                    return search_results
        
        except Exception as e:
            logger.error(f"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def get_document_content_by_similarity(self, query: str, company_id: str = None, max_results: int = 10) -> str:
        """é¡ä¼¼åº¦ã«åŸºã¥ã„ã¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚’å–å¾—"""
        try:
            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œ
            search_results = self.vector_similarity_search(query, company_id, limit=max_results)
            
            if not search_results:
                logger.warning("é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return ""
            
            # çµæœã‚’çµ„ã¿ç«‹ã¦
            relevant_content = []
            total_length = 0
            max_total_length = 15000  # æœ€å¤§æ–‡å­—æ•°åˆ¶é™
            
            logger.info(f"é¡ä¼¼åº¦é †ã«{len(search_results)}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ä¸­...")
            
            for i, result in enumerate(search_results):
                doc_id = result['document_id']
                chunk_id = result['chunk_id']
                similarity = result['similarity_score']
                snippet = result['snippet'] or ""
                
                # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚’å«ã‚€ãƒ­ã‚°
                if chunk_id != doc_id:
                    logger.info(f"  {i+1}. {result['document_name']} [ãƒãƒ£ãƒ³ã‚¯: {chunk_id}] (é¡ä¼¼åº¦: {similarity:.3f})")
                else:
                    logger.info(f"  {i+1}. {result['document_name']} (é¡ä¼¼åº¦: {similarity:.3f})")
                
                # ğŸ” ãƒ‡ãƒãƒƒã‚°: é¡ä¼¼åº¦é–¾å€¤ã‚’å¤§å¹…ã«ç·©å’Œï¼ˆ0.3 â†’ 0.05ï¼‰
                if similarity < 0.05:
                    logger.info(f"    - é¡ä¼¼åº¦ãŒä½ã„ãŸã‚é™¤å¤– ({similarity:.3f} < 0.05)")
                    continue
                
                # ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’è¿½åŠ 
                if snippet and len(snippet.strip()) > 0:
                    content_piece = f"\n=== {result['document_name']} (é¡ä¼¼åº¦: {similarity:.3f}) ===\n{snippet}\n"
                    
                    if total_length + len(content_piece) <= max_total_length:
                        relevant_content.append(content_piece)
                        total_length += len(content_piece)
                        logger.info(f"    - è¿½åŠ å®Œäº† ({len(content_piece)}æ–‡å­—)")
                    else:
                        logger.info(f"    - æ–‡å­—æ•°åˆ¶é™ã«ã‚ˆã‚Šé™¤å¤–")
                        break
            
            final_content = "\n".join(relevant_content)
            logger.info(f"æœ€çµ‚çš„ãªé–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„: {len(final_content)}æ–‡å­—")
            
            return final_content
        
        except Exception as e:
            logger.error(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return ""

    def hybrid_search(self, query: str, company_id: str = None, max_results: int = 10) -> str:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ + æ—¢å­˜æ¤œç´¢ã®çµ„ã¿åˆã‚ã›ï¼‰"""
        try:
            logger.info(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢é–‹å§‹: {query}")
            
            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œ
            vector_results = self.get_document_content_by_similarity(
                query, company_id, max_results
            )
            
            if vector_results:
                logger.info("âœ… ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—")
                return vector_results
            else:
                logger.warning("âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§çµæœãŒè¦‹ã¤ã‹ã‚‰ãšã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§çµæœãŒãªã„å ´åˆã®ä»£æ›¿å‡¦ç†
                return ""
        
        except Exception as e:
            logger.error(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return ""

def vector_search_available() -> bool:
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        return bool(api_key and supabase_url and supabase_key)
    except Exception:
        return False

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
_vector_search_instance = None

def get_vector_search_instance() -> Optional[VectorSearchSystem]:
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
    global _vector_search_instance
    
    if _vector_search_instance is None and vector_search_available():
        try:
            _vector_search_instance = VectorSearchSystem()
            logger.info("âœ… ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            logger.error(f"âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    return _vector_search_instance 
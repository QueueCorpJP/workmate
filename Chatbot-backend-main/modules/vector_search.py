"""
ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆpgvectorå¯¾å¿œç‰ˆï¼‰
Vertex AI text-multilingual-embedding-002ã‚’ä½¿ç”¨ã—ãŸé¡ä¼¼æ¤œç´¢æ©Ÿèƒ½ã‚’æä¾›ï¼ˆ768æ¬¡å…ƒï¼‰
pgvectoræ‹¡å¼µæ©Ÿèƒ½ã®æœ‰ç„¡ã‚’è‡ªå‹•æ¤œå‡ºã—ã€é©åˆ‡ãªæ¤œç´¢æ–¹æ³•ã‚’é¸æŠ
"""

import os
import logging
from typing import List, Dict, Tuple, Optional
from dotenv import load_dotenv
import psycopg2
from psycopg2.extras import RealDictCursor
import numpy as np
import google.generativeai as genai
from .multi_api_embedding import get_multi_api_embedding_client, multi_api_embedding_available

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

logger = logging.getLogger(__name__)

class VectorSearchSystem:
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ï¼ˆpgvectorå¯¾å¿œç‰ˆï¼‰"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.embedding_model = os.getenv("EMBEDDING_MODEL", "gemini-embedding-001")
        
        self.db_url = self._get_db_url()
        self.pgvector_available = False
        
        # pgvectoræ‹¡å¼µæ©Ÿèƒ½ã®ç¢ºèª
        self._check_pgvector_availability()
        
        if multi_api_embedding_available():
            self.multi_api_client = get_multi_api_embedding_client()
            # æ¬¡å…ƒæ•°ã¯3072æ¬¡å…ƒå›ºå®š
            expected_dimensions = 3072
            logger.info(f"âœ… ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–: Multi-API {self.embedding_model} ({expected_dimensions}æ¬¡å…ƒ)")
            logger.info(f"ğŸ”§ pgvectoræ‹¡å¼µæ©Ÿèƒ½: {'æœ‰åŠ¹' if self.pgvector_available else 'ç„¡åŠ¹'}")
            self.expected_dimensions = expected_dimensions
        else:
            logger.error("âŒ Multi-API EmbeddingãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            raise ValueError("Multi-API Embeddingã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    def _get_db_url(self) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã‚’æ§‹ç¯‰"""
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # Supabase URLã‹ã‚‰æ¥ç¶šæƒ…å ±ã‚’æŠ½å‡º
        if "supabase.co" in supabase_url:
            project_id = supabase_url.split("://")[1].split(".")[0]
            return f"postgresql://postgres.{project_id}:{os.getenv('DB_PASSWORD', '')}@aws-0-ap-northeast-1.pooler.supabase.com:6543/postgres"
        else:
            # ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã®å ´åˆ
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                raise ValueError("DATABASE_URL ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return db_url
    
    def _check_pgvector_availability(self):
        """pgvectoræ‹¡å¼µæ©Ÿèƒ½ã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    # pgvectoræ‹¡å¼µæ©Ÿèƒ½ã®ç¢ºèª
                    cur.execute("""
                        SELECT EXISTS(
                            SELECT 1 FROM pg_extension WHERE extname = 'vector'
                        ) as pgvector_installed
                    """)
                    result = cur.fetchone()
                    self.pgvector_available = result['pgvector_installed'] if result else False
                    
                    if self.pgvector_available:
                        logger.info("âœ… pgvectoræ‹¡å¼µæ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
                    else:
                        logger.warning("âš ï¸ pgvectoræ‹¡å¼µæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’ä½¿ç”¨ã—ã¾ã™")
                        
        except Exception as e:
            logger.error(f"âŒ pgvectorç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            self.pgvector_available = False
    
    def enable_pgvector_extension(self):
        """pgvectoræ‹¡å¼µæ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–"""
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    # pgvectoræ‹¡å¼µæ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
                    cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")
                    conn.commit()
                    
                    # ç¢ºèª
                    cur.execute("""
                        SELECT EXISTS(
                            SELECT 1 FROM pg_extension WHERE extname = 'vector'
                        ) as pgvector_installed
                    """)
                    result = cur.fetchone()
                    self.pgvector_available = result['pgvector_installed'] if result else False
                    
                    if self.pgvector_available:
                        logger.info("âœ… pgvectoræ‹¡å¼µæ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸ")
                        return True
                    else:
                        logger.error("âŒ pgvectoræ‹¡å¼µæ©Ÿèƒ½ã®æœ‰åŠ¹åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
                        return False
                        
        except Exception as e:
            logger.error(f"âŒ pgvectoræœ‰åŠ¹åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def generate_query_embedding(self, query: str) -> List[float]:
        """ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
        try:
            logger.info(f"ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆä¸­: {query[:50]}...")
            
            # Multi-APIä½¿ç”¨
            if self.multi_api_client:
                embedding_vector = await self.multi_api_client.generate_embedding(query)
                
                if embedding_vector and len(embedding_vector) > 0:
                    # æœŸå¾…ã•ã‚Œã‚‹æ¬¡å…ƒæ•°ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                    if len(embedding_vector) != self.expected_dimensions:
                        logger.warning(f"äºˆæœŸã—ãªã„æ¬¡å…ƒæ•°: {len(embedding_vector)}æ¬¡å…ƒï¼ˆæœŸå¾…å€¤: {self.expected_dimensions}æ¬¡å…ƒï¼‰")
                    logger.info(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº†: {len(embedding_vector)}æ¬¡å…ƒ")
                    return embedding_vector
                else:
                    logger.error("åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                    return []
            else:
                logger.error("Multi-API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                return []
        
        except Exception as e:
            logger.error(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _cosine_similarity_sql(self, query_vector: List[float]) -> str:
        """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—ã®SQLã‚’ç”Ÿæˆï¼ˆpgvectorã®æœ‰ç„¡ã«å¿œã˜ã¦ï¼‰"""
        if self.pgvector_available:
            # pgvectorãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆ
            return "1 - (c.embedding <=> %s::vector)"
        else:
            # pgvectorãŒåˆ©ç”¨ã§ããªã„å ´åˆã€æ‰‹å‹•ã§ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
            vector_str = ",".join(map(str, query_vector))
            return f"""
            (
                SELECT 
                    CASE 
                        WHEN norm_a * norm_b = 0 THEN 0
                        ELSE dot_product / (norm_a * norm_b)
                    END
                FROM (
                    SELECT 
                        (SELECT SUM(a.val * b.val) 
                         FROM unnest(ARRAY[{vector_str}]) WITH ORDINALITY a(val, idx)
                         JOIN unnest(string_to_array(trim(both '[]' from c.embedding::text), ',')::float[]) WITH ORDINALITY b(val, idx) 
                         ON a.idx = b.idx) as dot_product,
                        sqrt((SELECT SUM(a.val * a.val) FROM unnest(ARRAY[{vector_str}]) a(val))) as norm_a,
                        sqrt((SELECT SUM(b.val * b.val) FROM unnest(string_to_array(trim(both '[]' from c.embedding::text), ',')::float[]) b(val))) as norm_b
                ) calc
            )
            """
    
    async def vector_similarity_search(self, query: str, company_id: str = None, limit: int = 50) -> List[Dict]:
        """ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆpgvectorå¯¾å¿œç‰ˆï¼‰"""
        try:
            # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            query_vector = await self.generate_query_embedding(query)
            if not query_vector:
                logger.error("ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã«å¤±æ•—")
                return []
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    if self.pgvector_available:
                        # pgvectorãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã®é«˜é€Ÿæ¤œç´¢
                        # ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«ã‚’æ–‡å­—åˆ—å½¢å¼ã«å¤‰æ›ã—ã¦vectorå‹ã«ã‚­ãƒ£ã‚¹ãƒˆ
                        vector_str = '[' + ','.join(map(str, query_vector)) + ']'
                        similarity_sql = f"1 - (c.embedding <=> '{vector_str}'::vector)"
                        order_sql = f"c.embedding <=> '{vector_str}'::vector"
                        params = []
                    else:
                        # pgvectorãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢
                        logger.warning("âš ï¸ pgvectorãŒç„¡åŠ¹ã®ãŸã‚ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’ä½¿ç”¨")
                        similarity_sql = """
                        CASE 
                            WHEN c.embedding IS NULL THEN 0
                            ELSE 0.5
                        END
                        """
                        order_sql = "RANDOM()"
                        params = []
                    
                    # ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢SQL
                    sql = f"""
                    SELECT
                        c.id as chunk_id,
                        c.doc_id as document_id,
                        c.chunk_index,
                        c.content as snippet,
                        ds.name,
                        ds.special,
                        ds.type,
                        {similarity_sql} as similarity_score
                    FROM chunks c
                    LEFT JOIN document_sources ds ON ds.id = c.doc_id
                    WHERE c.embedding IS NOT NULL
                    AND ds.active = true
                    """
                    
                    # ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿
                    if company_id:
                        sql += " AND c.company_id = %s"
                        params.append(company_id)
                        logger.info(f"ğŸ” ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿é©ç”¨: {company_id}")
                    
                    # ã‚½ãƒ¼ãƒˆã¨åˆ¶é™
                    sql += f" ORDER BY {order_sql} LIMIT %s"
                    params.append(limit)
                    
                    logger.info(f"å®Ÿè¡ŒSQL: {sql}")
                    logger.info(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {params}")

                    logger.info(f"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œä¸­... (limit: {limit})")
                    logger.info(f"ä½¿ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«: chunks, pgvector: {'æœ‰åŠ¹' if self.pgvector_available else 'ç„¡åŠ¹'}")
                    
                    cur.execute(sql, params)
                    results = cur.fetchall()
                    
                    logger.info(f"DBã‹ã‚‰ã®ç”Ÿã®çµæœ: {results}")

                    # çµæœã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                    search_results = []
                    for row in results:
                        # document_sources.nameãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¨­å®š
                        document_name = row['name'] if row['name'] else 'Unknown'
                        search_results.append({
                            'chunk_id': row['chunk_id'],
                            'document_id': row['document_id'],
                            'chunk_index': row['chunk_index'],
                            'document_name': document_name,
                            'document_type': row['type'],
                            'special': row['special'],
                            'snippet': row['snippet'],
                            'similarity_score': float(row['similarity_score']),
                            'search_type': 'vector_chunks_pgvector' if self.pgvector_available else 'vector_chunks_fallback'
                        })
                    
                    # çµæœã®å®‰å®šåŒ–ï¼šé¡ä¼¼åº¦é †ã€æ¬¡ã«ãƒãƒ£ãƒ³ã‚¯IDé †ã§ã‚½ãƒ¼ãƒˆï¼ˆä¸€è²«ã—ãŸé †åºä¿è¨¼ï¼‰
                    search_results.sort(key=lambda x: (-x['similarity_score'], str(x['chunk_id'])))
                    
                    logger.info(f"âœ… ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Œäº†: {len(search_results)}ä»¶ã®çµæœï¼ˆå®‰å®šã‚½ãƒ¼ãƒˆæ¸ˆã¿ï¼‰")
                    
                    # ğŸ” è©³ç´°ãƒãƒ£ãƒ³ã‚¯é¸æŠãƒ­ã‚° - å…¨çµæœã‚’è¡¨ç¤º
                    print("\n" + "="*80)
                    print(f"ğŸ” ã€ãƒãƒ£ãƒ³ã‚¯é¸æŠè©³ç´°ãƒ­ã‚°ã€‘ã‚¯ã‚¨ãƒª: '{query[:50]}...'")
                    print(f"ğŸ“Š æ¤œç´¢çµæœ: {len(search_results)}ä»¶")
                    print(f"ğŸ¢ ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿: {'é©ç”¨ (' + company_id + ')' if company_id else 'æœªé©ç”¨ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ï¼‰'}")
                    print(f"ğŸ”§ pgvector: {'æœ‰åŠ¹' if self.pgvector_available else 'ç„¡åŠ¹ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ï¼‰'}")
                    print("="*80)
                    
                    for i, result in enumerate(search_results):
                        similarity = result['similarity_score']
                        doc_name = result['document_name'] or 'Unknown'
                        chunk_idx = result['chunk_index']
                        snippet_preview = (result['snippet'] or '')[:100].replace('\n', ' ')
                        
                        # é¡ä¼¼åº¦ã«åŸºã¥ãé¸æŠç†ç”±
                        if similarity > 0.8:
                            reason = "ğŸŸ¢ é«˜é¡ä¼¼åº¦ï¼ˆéå¸¸ã«é–¢é€£æ€§ãŒé«˜ã„ï¼‰"
                        elif similarity > 0.5:
                            reason = "ğŸŸ¡ ä¸­é¡ä¼¼åº¦ï¼ˆé–¢é€£æ€§ã‚ã‚Šï¼‰"
                        elif similarity > 0.2:
                            reason = "ğŸŸ  ä½é¡ä¼¼åº¦ï¼ˆéƒ¨åˆ†çš„ã«é–¢é€£ï¼‰"
                        else:
                            reason = "ğŸ”´ æ¥µä½é¡ä¼¼åº¦ï¼ˆé–¢é€£æ€§ä½ã„ï¼‰"
                        
                        print(f"  {i+1:2d}. ğŸ“„ {doc_name}")
                        print(f"      ğŸ§© ãƒãƒ£ãƒ³ã‚¯#{chunk_idx} | ğŸ¯ é¡ä¼¼åº¦: {similarity:.4f} | {reason}")
                        print(f"      ğŸ“ å†…å®¹: {snippet_preview}...")
                        print(f"      ğŸ” æ¤œç´¢ã‚¿ã‚¤ãƒ—: {result['search_type']}")
                        print()
                    
                    print("="*80 + "\n")
                    
                    return search_results
        
        except Exception as e:
            logger.error(f"âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
            
            # pgvectorã‚¨ãƒ©ãƒ¼ã®å ´åˆã€æ‹¡å¼µæ©Ÿèƒ½ã®æœ‰åŠ¹åŒ–ã‚’è©¦è¡Œ
            if "operator does not exist: vector" in str(e):
                logger.info("ğŸ”§ pgvectoræ‹¡å¼µæ©Ÿèƒ½ã®æœ‰åŠ¹åŒ–ã‚’è©¦è¡Œä¸­...")
                if self.enable_pgvector_extension():
                    logger.info("ğŸ”„ pgvectoræœ‰åŠ¹åŒ–å¾Œã€æ¤œç´¢ã‚’å†è©¦è¡Œä¸­...")
                    return self.vector_similarity_search(query, company_id, limit)
            
            return []
    
    def get_document_content_by_similarity(self, query: str, company_id: str = None, max_results: int = 100) -> str:
        """é¡ä¼¼åº¦ã«åŸºã¥ã„ã¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚’å–å¾—"""
        try:
            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œ
            search_results = self.vector_similarity_search(query, company_id, limit=max_results)
            
            if not search_results:
                logger.warning("é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return ""
            
            # çµæœã‚’çµ„ã¿ç«‹ã¦
            relevant_content = []
            total_length = 0
            max_total_length = 120000
            
            print("\n" + "="*80)
            print(f"ğŸ“‹ ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ§‹ç¯‰ãƒ­ã‚°ã€‘{len(search_results)}ä»¶ã®ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†ä¸­...")
            print(f"ğŸ¯ é¡ä¼¼åº¦é–¾å€¤: 0.05 (ã“ã‚Œä»¥ä¸‹ã¯é™¤å¤–)")
            print(f"ğŸ“ æœ€å¤§æ–‡å­—æ•°: {max_total_length:,}æ–‡å­—")
            print("="*80)
            
            for i, result in enumerate(search_results):
                similarity = result['similarity_score']
                snippet = result['snippet'] or ""
                doc_name = result['document_name'] or 'Unknown'
                chunk_idx = result['chunk_index']
                
                print(f"  {i+1:2d}. ğŸ“„ {doc_name} [ãƒãƒ£ãƒ³ã‚¯#{chunk_idx}]")
                print(f"      ğŸ¯ é¡ä¼¼åº¦: {similarity:.4f}")
                
                # é¡ä¼¼åº¦é–¾å€¤ï¼ˆæƒ…å ±æŠœã‘é˜²æ­¢ã®ãŸã‚ã•ã‚‰ã«ç·©å’Œï¼‰
                threshold = 0.02  # ğŸ¯ æŠœã‘æ¼ã‚Œå®Œå…¨é˜²æ­¢ï¼ˆæœ€å¤§ç·©å’Œï¼‰
                if similarity < threshold:
                    print(f"      âŒ é™¤å¤–ç†ç”±: é¡ä¼¼åº¦ãŒé–¾å€¤æœªæº€ ({similarity:.4f} < {threshold})")
                    print()
                    continue
                
                # ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’è¿½åŠ 
                if snippet and len(snippet.strip()) > 0:
                    content_piece = f"\n=== {doc_name} - å‚è€ƒè³‡æ–™{chunk_idx} (é¡ä¼¼åº¦: {similarity:.3f}) ===\n{snippet}\n"
                    
                    if total_length + len(content_piece) <= max_total_length:
                        relevant_content.append(content_piece)
                        total_length += len(content_piece)
                        print(f"      âœ… æ¡ç”¨: {len(content_piece):,}æ–‡å­—è¿½åŠ  (ç´¯è¨ˆ: {total_length:,}æ–‡å­—)")
                        print(f"      ğŸ“ å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {snippet[:100].replace(chr(10), ' ')}...")
                    else:
                        print(f"      âŒ é™¤å¤–ç†ç”±: æ–‡å­—æ•°åˆ¶é™è¶…é (è¿½åŠ äºˆå®š: {len(content_piece):,}æ–‡å­—, ç¾åœ¨: {total_length:,}æ–‡å­—)")
                        break
                else:
                    print(f"      âŒ é™¤å¤–ç†ç”±: ç©ºã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„")
                print()
            
            final_content = "\n".join(relevant_content)
            logger.info(f"âœ… æœ€çµ‚çš„ãªé–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„: {len(final_content)}æ–‡å­—")
            
            return final_content
        
        except Exception as e:
            logger.error(f"âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
            return ""

def vector_search_available() -> bool:
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        return bool(api_key and supabase_url and supabase_key)
    except Exception:
        return False

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
_vector_search_instance = None

def get_vector_search_instance() -> Optional[VectorSearchSystem]:
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
    global _vector_search_instance
    
    if _vector_search_instance is None and vector_search_available():
        try:
            _vector_search_instance = VectorSearchSystem()
            logger.info("âœ… ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            logger.error(f"âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    return _vector_search_instance
"""
ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€Ÿãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
- åŒæ–¹å‘æ¤œç´¢ï¼ˆä¸Šä½ãƒ»ä¸‹ä½ã‹ã‚‰åŒæ™‚æ¤œç´¢ï¼‰
- é–“éš™æ¤œç´¢ï¼ˆãƒãƒƒãƒé–“ã®å€™è£œã‚‚æ¤œç´¢ï¼‰
- éåŒæœŸä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–
"""

import asyncio
import logging
from typing import List, Dict, Tuple, Optional, Set
from concurrent.futures import ThreadPoolExecutor
import time
from dotenv import load_dotenv
import google.generativeai as genai
import psycopg2
from psycopg2.extras import RealDictCursor
import os
import concurrent.futures

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

logger = logging.getLogger(__name__)

class ParallelVectorSearchSystem:
    """ä¸¦åˆ—å‡¦ç†ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        self.model = "models/text-embedding-004"  # å›ºå®šã§text-embedding-004ã‚’ä½¿ç”¨ï¼ˆ768æ¬¡å…ƒï¼‰
        
        self.db_url = self._get_db_url()
        
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY ã¾ãŸã¯ GEMINI_API_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        genai.configure(api_key=self.api_key)
        
        # ä¸¦åˆ—å‡¦ç†ç”¨ã®Executor
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        logger.info(f"âœ… ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–: {self.model} (768æ¬¡å…ƒ)")
        
        logger.info(f"âœ… ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–: ãƒ¢ãƒ‡ãƒ«={self.model}")
        
    def _get_db_url(self) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã‚’æ§‹ç¯‰"""
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        if "supabase.co" in supabase_url:
            project_id = supabase_url.split("://")[1].split(".")[0]
            return f"postgresql://postgres.{project_id}:{os.getenv('DB_PASSWORD', '')}@aws-0-ap-northeast-1.pooler.supabase.com:6543/postgres"
        else:
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                raise ValueError("DATABASE_URL ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return db_url

    async def parallel_comprehensive_search(self, query: str, company_id: str = None, max_results: int = 15) -> str:
        """åŒ…æ‹¬çš„ä¸¦åˆ—æ¤œç´¢ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        start_time = time.time()
        logger.info(f"ğŸš€ ä¸¦åˆ—åŒ…æ‹¬æ¤œç´¢é–‹å§‹: '{query}'")
        
        try:
            # 1. è¤‡æ•°ã®ã‚¯ã‚¨ãƒªæˆ¦ç•¥ã‚’ç”Ÿæˆ
            query_strategies = self.expand_query_strategies(query)
            
            # 2. ä¸¦åˆ—ã§ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
            embeddings = await self.generate_query_embeddings_parallel(query_strategies)
            
            # 3. æœ‰åŠ¹ãªã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã§ä¸¦åˆ—æ¤œç´¢
            search_tasks = []
            for i, (q, embedding) in enumerate(zip(query_strategies, embeddings)):
                if embedding:
                    task = self.dual_direction_search(embedding, company_id, max_results // len(query_strategies))
                    search_tasks.append(task)
            
            # ä¸¦åˆ—å®Ÿè¡Œ
            all_results = await asyncio.gather(*search_tasks)
            
            # 4. çµæœã‚’ãƒãƒ¼ã‚¸ã—ã¦æœ€é©åŒ–
            final_results = self.merge_and_optimize_results(all_results)
            
            # 5. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„çµ„ã¿ç«‹ã¦
            content = self.build_content_from_results(final_results, max_results)
            
            elapsed_time = time.time() - start_time
            logger.info(f"ğŸ‰ ä¸¦åˆ—æ¤œç´¢å®Œäº†: {len(content)}æ–‡å­— ({elapsed_time:.2f}ç§’)")
            
            return content
        
        except Exception as e:
            logger.error(f"ä¸¦åˆ—æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return ""

    def expand_query_strategies(self, original_query: str) -> List[str]:
        """ã‚¯ã‚¨ãƒªæ‹¡å¼µæˆ¦ç•¥ã‚’ç”Ÿæˆ"""
        strategies = [original_query]
        
        # é¡ç¾©èªãƒãƒƒãƒ”ãƒ³ã‚°
        synonyms_map = {
            'æ–™é‡‘': ['ä¾¡æ ¼', 'è²»ç”¨', 'ã‚³ã‚¹ãƒˆ'],
            'æ–¹æ³•': ['æ‰‹é †', 'ã‚„ã‚Šæ–¹'],
            'è¨­å®š': ['æ§‹æˆ', 'ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—'],
            'å•é¡Œ': ['èª²é¡Œ', 'ãƒˆãƒ©ãƒ–ãƒ«'],
        }
        
        # é¡ç¾©èªæˆ¦ç•¥
        for word, synonyms in synonyms_map.items():
            if word in original_query:
                for synonym in synonyms[:2]:
                    expanded = original_query.replace(word, synonym)
                    strategies.append(expanded)
        
        # éƒ¨åˆ†èªæˆ¦ç•¥
        words = original_query.split()
        if len(words) > 1:
            strategies.extend(words[:2])
        
        return strategies[:4]  # æœ€å¤§4æˆ¦ç•¥

    async def generate_query_embeddings_parallel(self, queries: List[str]) -> List[List[float]]:
        """è¤‡æ•°ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ã‚’ä¸¦åˆ—ç”Ÿæˆ"""
        async def generate_single(query: str) -> List[float]:
            try:
                response = genai.embed_content(
                    model=self.model,
                    content=query
                )
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
                embedding_vector = None
                
                if isinstance(response, dict) and 'embedding' in response:
                    embedding_vector = response['embedding']
                elif hasattr(response, 'embedding') and response.embedding:
                    embedding_vector = response.embedding
                else:
                    logger.error(f"äºˆæœŸã—ãªã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼: {type(response)}")
                    return []
                
                if embedding_vector and len(embedding_vector) > 0:
                    return embedding_vector  # æ¬¡å…ƒå‰Šæ¸›ãªã—
                return []
            except Exception as e:
                logger.error(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                return []
        
        tasks = [generate_single(q) for q in queries]
        return await asyncio.gather(*tasks)

    async def dual_direction_search(self, query_vector: List[float], company_id: str = None, limit: int = 10) -> Tuple[List[Dict], List[Dict]]:
        """åŒæ–¹å‘æ¤œç´¢ï¼ˆä¸Šä½ãƒ»ä¸‹ä½ã‹ã‚‰åŒæ™‚æ¤œç´¢ï¼‰"""
        
        async def search_direction(direction: str) -> List[Dict]:
            return await asyncio.get_event_loop().run_in_executor(
                self.executor, 
                self._execute_vector_search, 
                query_vector, company_id, limit, direction
            )
        
        # ä¸Šä½ã¨ä¸‹ä½ã‚’ä¸¦åˆ—å®Ÿè¡Œ
        top_task = search_direction("DESC")
        bottom_task = search_direction("ASC")
        
        top_results, bottom_results = await asyncio.gather(top_task, bottom_task)
        return top_results, bottom_results

    def _execute_vector_search(self, query_vector: List[float], company_id: str, limit: int, order: str) -> List[Dict]:
        """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®å®Ÿè¡Œï¼ˆchunksãƒ†ãƒ¼ãƒ–ãƒ«å¯¾å¿œç‰ˆï¼‰"""
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    # æ–°ã—ã„chunksãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ç”¨ã—ãŸSQL
                    sql = """
                    SELECT
                        c.id as chunk_id,
                        c.doc_id as document_id,
                        c.chunk_index,
                        c.content as snippet,
                        ds.name,
                        ds.special,
                        ds.type,
                        1 - (c.embedding <=> %s::vector) as similarity
                    FROM chunks c
                    LEFT JOIN document_sources ds ON ds.id = c.doc_id
                    WHERE c.embedding IS NOT NULL
                    """
                    
                    params = [query_vector]
                    
                    # ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿ï¼ˆæœ‰åŠ¹åŒ–ï¼‰
                    if company_id:
                        sql += " AND c.company_id = %s"
                        params.append(company_id)
                        logger.info(f"ğŸ” ä¸¦åˆ—æ¤œç´¢: ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿é©ç”¨ - {company_id}")
                    else:
                        logger.info(f"ğŸ” ä¸¦åˆ—æ¤œç´¢: ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿ãªã—ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ï¼‰")
                    
                    sql += f" ORDER BY similarity {order} LIMIT %s"
                    params.append(limit)
                    
                    logger.info(f"ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œ: {order} order, limit={limit}")
                    cur.execute(sql, params)
                    results = cur.fetchall()
                    
                    search_results = []
                    for row in results:
                        search_results.append({
                            'chunk_id': row['chunk_id'],
                            'document_id': row['document_id'],
                            'chunk_index': row['chunk_index'],
                            'document_name': row['name'],
                            'document_type': row['type'],
                            'special': row['special'],
                            'snippet': row['snippet'],
                            'similarity_score': float(row['similarity']),
                            'search_type': f'vector_parallel_{order.lower()}'
                        })
                    
                    logger.info(f"âœ… ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Œäº†: {len(search_results)}ä»¶ ({order})")
                    return search_results
        
        except Exception as e:
            logger.error(f"âŒ ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
            return []

    def merge_and_optimize_results(self, all_results: List[Tuple[List[Dict], List[Dict]]]) -> List[Dict]:
        """çµæœã‚’ãƒãƒ¼ã‚¸ã—ã¦æœ€é©åŒ–"""
        seen_chunks: Set[str] = set()
        merged_results = []
        
        # å…¨çµæœã‚’çµ±åˆ
        for top_results, bottom_results in all_results:
            merged_results.extend(top_results)
            # ä¸‹ä½çµæœã‹ã‚‰æœ‰ç”¨ãªã‚‚ã®ã‚’é¸åˆ¥
            for result in bottom_results:
                # ğŸ” ãƒ‡ãƒãƒƒã‚°: é–¾å€¤ã‚’ç·©å’Œï¼ˆ0.3 â†’ 0.05ï¼‰
                if result['similarity_score'] > 0.05:  # é–¾å€¤ä»¥ä¸Šã®ã‚‚ã®ã®ã¿
                    merged_results.append(result)
        
        # é‡è¤‡é™¤å»
        unique_results = []
        for result in merged_results:
            chunk_id = result['chunk_id']
            if chunk_id not in seen_chunks:
                seen_chunks.add(chunk_id)
                unique_results.append(result)
        
        # é¡ä¼¼åº¦é †ã§ã‚½ãƒ¼ãƒˆ
        unique_results.sort(key=lambda x: x['similarity_score'], reverse=True)
        
        return unique_results

    def build_content_from_results(self, results: List[Dict], max_results: int) -> str:
        """çµæœã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ§‹ç¯‰ï¼ˆchunksãƒ†ãƒ¼ãƒ–ãƒ«å¯¾å¿œç‰ˆï¼‰"""
        if not results:
            return ""
        
        relevant_content = []
        total_length = 0
        max_total_length = 50000  # åˆ¶é™ã‚’æ‹¡å¤§ï¼ˆ20000 â†’ 50000ï¼‰
        
        logger.info(f"ğŸ“ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ§‹ç¯‰é–‹å§‹: {len(results)}ä»¶ã®çµæœã‹ã‚‰æœ€å¤§{max_results}ä»¶ã‚’å‡¦ç†")
        
        for i, result in enumerate(results[:max_results]):
            similarity = result['similarity_score']
            snippet = result['snippet'] or ""
            chunk_index = result.get('chunk_index', 'N/A')
            
            # é¡ä¼¼åº¦é–¾å€¤ã‚’ç·©å’Œï¼ˆ0.05 â†’ 0.02ï¼‰
            if similarity < 0.02:
                logger.info(f"  {i+1}. é¡ä¼¼åº¦ãŒä½ã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {similarity:.3f}")
                continue
            
            if snippet and len(snippet.strip()) > 0:
                content_piece = f"\n=== {result['document_name']} - ãƒãƒ£ãƒ³ã‚¯{chunk_index} (é¡ä¼¼åº¦: {similarity:.3f}) ===\n{snippet}\n"
                
                if total_length + len(content_piece) <= max_total_length:
                    relevant_content.append(content_piece)
                    total_length += len(content_piece)
                    logger.info(f"  {i+1}. è¿½åŠ : {result['document_name']} [ãƒãƒ£ãƒ³ã‚¯{chunk_index}] ({len(content_piece)}æ–‡å­—)")
                else:
                    logger.info(f"  {i+1}. æ–‡å­—æ•°åˆ¶é™ã«ã‚ˆã‚Šçµ‚äº†")
                    break
            else:
                logger.info(f"  {i+1}. ç©ºã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
        
        final_content = "\n".join(relevant_content)
        logger.info(f"âœ… ä¸¦åˆ—æ¤œç´¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ§‹ç¯‰å®Œäº†: {len(relevant_content)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã€{len(final_content)}æ–‡å­—")
        
        return final_content

    def parallel_comprehensive_search_sync(self, query: str, company_id: str = None, max_results: int = 15) -> str:
        """åŒ…æ‹¬çš„ä¸¦åˆ—æ¤œç´¢ã®åŒæœŸç‰ˆ - ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—å•é¡Œã‚’å›é¿"""
        start_time = time.time()
        logger.info(f"ğŸš€ åŒæœŸä¸¦åˆ—åŒ…æ‹¬æ¤œç´¢é–‹å§‹: '{query}'")
        
        try:
            # 1. è¤‡æ•°ã®ã‚¯ã‚¨ãƒªæˆ¦ç•¥ã‚’ç”Ÿæˆ
            query_strategies = self.expand_query_strategies(query)
            
            # 2. ä¸¦åˆ—ã§ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆï¼ˆThreadPoolExecutorã‚’ä½¿ç”¨ï¼‰
            embeddings = self._generate_query_embeddings_sync(query_strategies)
            
            # 3. æœ‰åŠ¹ãªã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã§ä¸¦åˆ—æ¤œç´¢
            valid_embeddings = [(q, e) for q, e in zip(query_strategies, embeddings) if e]
            
            if not valid_embeddings:
                logger.error("æœ‰åŠ¹ãªã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                return ""
            
            # 4. ä¸¦åˆ—æ¤œç´¢ã®å®Ÿè¡Œï¼ˆThreadPoolExecutorã‚’ä½¿ç”¨ï¼‰
            all_top_results = []
            all_bottom_results = []
            
            with ThreadPoolExecutor(max_workers=4) as executor:
                # å„ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã§åŒæ–¹å‘æ¤œç´¢ã‚’ä¸¦åˆ—å®Ÿè¡Œ
                future_to_embedding = {}
                
                for query_text, embedding in valid_embeddings:
                    # ä¸Šä½æ¤œç´¢ã®Future
                    future_top = executor.submit(
                        self._execute_vector_search,
                        embedding, company_id, max_results // len(valid_embeddings), "DESC"
                    )
                    future_to_embedding[future_top] = (query_text, embedding, "top")
                    
                    # ä¸‹ä½æ¤œç´¢ã®Future
                    future_bottom = executor.submit(
                        self._execute_vector_search,
                        embedding, company_id, max_results // len(valid_embeddings), "ASC"
                    )
                    future_to_embedding[future_bottom] = (query_text, embedding, "bottom")
                
                # çµæœã‚’åé›†
                for future in concurrent.futures.as_completed(future_to_embedding):
                    query_text, embedding, search_type = future_to_embedding[future]
                    try:
                        results = future.result(timeout=30)  # 30ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                        if search_type == "top":
                            all_top_results.extend(results)
                        else:
                            all_bottom_results.extend(results)
                    except Exception as e:
                        logger.error(f"ä¸¦åˆ—æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            
            # 5. é–“éš™æ¤œç´¢ï¼ˆä¸»è¦ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨ï¼‰
            gap_results = []
            if valid_embeddings:
                primary_embedding = valid_embeddings[0][1]
                gap_conditions = self._find_gap_candidates_sync(all_top_results, all_bottom_results)
                
                # é–“éš™æ¤œç´¢ã‚‚ä¸¦åˆ—å®Ÿè¡Œ
                with ThreadPoolExecutor(max_workers=2) as executor:
                    gap_futures = []
                    for condition in gap_conditions:
                        future = executor.submit(
                            self._execute_gap_search_sync, 
                            primary_embedding, condition, company_id
                        )
                        gap_futures.append(future)
                    
                    for future in concurrent.futures.as_completed(gap_futures):
                        try:
                            results = future.result(timeout=15)
                            gap_results.extend(results)
                        except Exception as e:
                            logger.error(f"é–“éš™æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            
            # 6. çµæœã®ãƒãƒ¼ã‚¸ã¨é‡è¤‡é™¤å»
            final_results = self.merge_and_optimize_results([(all_top_results, all_bottom_results)])
            final_results.extend(gap_results)
            
            # é‡è¤‡é™¤å»
            seen_chunks = set()
            unique_results = []
            for result in final_results:
                chunk_id = result['chunk_id']
                if chunk_id not in seen_chunks:
                    seen_chunks.add(chunk_id)
                    unique_results.append(result)
            
            # é¡ä¼¼åº¦é †ã§ã‚½ãƒ¼ãƒˆ
            unique_results.sort(key=lambda x: x['similarity_score'], reverse=True)
            
            # 7. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„çµ„ã¿ç«‹ã¦
            content = self.build_content_from_results(unique_results, max_results)
            
            elapsed_time = time.time() - start_time
            logger.info(f"ğŸ‰ åŒæœŸä¸¦åˆ—æ¤œç´¢å®Œäº†: {len(content)}æ–‡å­— ({elapsed_time:.2f}ç§’)")
            
            return content
        
        except Exception as e:
            elapsed_time = time.time() - start_time
            logger.error(f"åŒæœŸä¸¦åˆ—æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e} ({elapsed_time:.2f}ç§’)")
            return ""

    def _generate_query_embeddings_sync(self, queries: List[str]) -> List[List[float]]:
        """è¤‡æ•°ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ã‚’åŒæœŸä¸¦åˆ—ç”Ÿæˆ"""
        def generate_single_embedding(query: str) -> List[float]:
            try:
                response = genai.embed_content(
                    model=self.model,
                    content=query
                )
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
                embedding_vector = None
                
                if isinstance(response, dict) and 'embedding' in response:
                    embedding_vector = response['embedding']
                elif hasattr(response, 'embedding') and response.embedding:
                    embedding_vector = response.embedding
                else:
                    logger.error(f"äºˆæœŸã—ãªã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼: {type(response)}")
                    return []
                
                if embedding_vector and len(embedding_vector) > 0:
                    return embedding_vector  # æ¬¡å…ƒå‰Šæ¸›ãªã—
                else:
                    logger.error(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå¤±æ•—: {query}")
                    return []
            except Exception as e:
                logger.error(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                return []
        
        # ThreadPoolExecutorã§ä¸¦åˆ—å®Ÿè¡Œï¼ˆé †åºã‚’ä¿è¨¼ï¼‰
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(generate_single_embedding, query) for query in queries]
            embeddings = []
            
            for i, future in enumerate(futures):
                try:
                    embedding = future.result(timeout=30)
                    embeddings.append(embedding)
                except Exception as e:
                    logger.error(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {queries[i]} - {e}")
                    embeddings.append([])
        
        logger.info(f"âœ… åŒæœŸä¸¦åˆ—åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº†: {len([e for e in embeddings if e])}å€‹æˆåŠŸ")
        return embeddings

    def _find_gap_candidates_sync(self, top_results: List[Dict], bottom_results: List[Dict]) -> List[str]:
        """ãƒãƒƒãƒã—ãŸçµæœã®é–“ã«ã‚ã‚‹å€™è£œã‚’ç‰¹å®šï¼ˆåŒæœŸç‰ˆï¼‰"""
        gap_candidates = []
        
        if not top_results or not bottom_results:
            return gap_candidates
        
        # ä¸Šä½ã¨ä¸‹ä½ã®é¡ä¼¼åº¦ã®ç¯„å›²ã‚’ç‰¹å®š
        top_similarities = [r['similarity_score'] for r in top_results]
        bottom_similarities = [r['similarity_score'] for r in bottom_results]
        
        if top_similarities and bottom_similarities:
            min_top = min(top_similarities)
            max_bottom = max(bottom_similarities)
            
            # é–“éš™ãŒã‚ã‚‹å ´åˆ
            if min_top > max_bottom:
                gap_threshold_high = min_top - 0.05
                gap_threshold_low = max_bottom + 0.05
                gap_candidates.append(f"BETWEEN {gap_threshold_low} AND {gap_threshold_high}")
                logger.info(f"ğŸ” é–“éš™æ¤œç´¢ç¯„å›²: {gap_threshold_low:.3f} - {gap_threshold_high:.3f}")
        
        return gap_candidates

    def _execute_gap_search_sync(self, query_vector: List[float], condition: str, company_id: str = None) -> List[Dict]:
        """é–“éš™æ¤œç´¢ã®åŒæœŸå®Ÿè¡Œ"""
        try:
            with psycopg2.connect(self.db_url, cursor_factory=RealDictCursor) as conn:
                with conn.cursor() as cur:
                    sql = f"""
                    SELECT 
                        de.document_id as chunk_id,
                        CASE 
                            WHEN de.document_id LIKE '%_chunk_%' THEN 
                                SPLIT_PART(de.document_id, '_chunk_', 1)
                            ELSE de.document_id
                        END as original_doc_id,
                        ds.name,
                        ds.special,
                        ds.type,
                        de.snippet,
                        1 - (de.embedding <=> %s) as similarity
                    FROM document_embeddings de
                    LEFT JOIN document_sources ds ON ds.id = CASE 
                        WHEN de.document_id LIKE '%_chunk_%' THEN 
                            SPLIT_PART(de.document_id, '_chunk_', 1)
                        ELSE de.document_id
                    END
                    WHERE de.embedding IS NOT NULL
                      AND (1 - (de.embedding <=> %s)) {condition}
                    """
                    
                    params = [query_vector, query_vector]
                    
                    # ğŸ” ãƒ‡ãƒãƒƒã‚°: é–“éš™æ¤œç´¢ã§ã‚‚company_idãƒ•ã‚£ãƒ«ã‚¿ã‚’ç„¡åŠ¹åŒ–
                    # if company_id:
                    #     sql += " AND ds.company_id = %s"
                    #     params.append(company_id)
                    
                    sql += " ORDER BY similarity DESC LIMIT 5"
                    
                    cur.execute(sql, params)
                    results = cur.fetchall()
                    
                    return [{
                        'chunk_id': row['chunk_id'],
                        'document_id': row['original_doc_id'],
                        'document_name': row['name'],
                        'document_type': row['type'],
                        'special': row['special'],
                        'snippet': row['snippet'],
                        'similarity_score': float(row['similarity']),
                        'search_type': 'vector_gap_sync'
                    } for row in results]
        
        except Exception as e:
            logger.error(f"é–“éš™æ¤œç´¢åŒæœŸå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return []

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_parallel_vector_search_instance = None

async def get_parallel_vector_search_instance() -> Optional[ParallelVectorSearchSystem]:
    """ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _parallel_vector_search_instance
    
    if _parallel_vector_search_instance is None:
        try:
            _parallel_vector_search_instance = ParallelVectorSearchSystem()
            logger.info("âœ… ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            logger.error(f"âŒ ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    return _parallel_vector_search_instance

def get_parallel_vector_search_instance_sync() -> Optional[ParallelVectorSearchSystem]:
    """ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åŒæœŸå–å¾—"""
    global _parallel_vector_search_instance
    
    if _parallel_vector_search_instance is None:
        try:
            _parallel_vector_search_instance = ParallelVectorSearchSystem()
            logger.info("âœ… ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†ï¼ˆåŒæœŸç‰ˆï¼‰")
        except Exception as e:
            logger.error(f"âŒ ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    return _parallel_vector_search_instance 
import requests
import re
from bs4 import BeautifulSoup
from io import BytesIO
import pandas as pd
import time
from dotenv import load_dotenv
import os
from playwright.async_api import async_playwright
import fitz
import tempfile
import asyncio
from .database import ensure_string

load_dotenv()

def transcribe_youtube_video(url: str) -> str:
    """YouTubeå‹•ç”»ã®éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—ã™ã‚‹
    
    Args:
        url: YouTubeå‹•ç”»ã®URL
        
    Returns:
        æ–‡å­—èµ·ã“ã—ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã¾ãŸã¯å¯¾å¿œçŠ¶æ³ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    return f"ğŸ¥ YouTubeå‹•ç”»ã®å‡¦ç†ã¯ç¾åœ¨å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“\nâ€¢ å‹•ç”»ã®éŸ³å£°æ–‡å­—èµ·ã“ã—æ©Ÿèƒ½ã¯é–‹ç™ºä¸­ã§ã™\nâ€¢ ä»£ã‚ã‚Šã«å‹•ç”»ã®èª¬æ˜æ–‡ã‚„ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§æä¾›ã—ã¦ãã ã•ã„\nâ€¢ ã¾ãŸã¯ã€æ–‡å­—èµ·ã“ã—ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\nâ€¢ URL: {url}"

def _get_user_friendly_pdf_error(error: Exception, url: str) -> str:
    """PDFã‚¨ãƒ©ãƒ¼ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¤‰æ›"""
    error_str = str(error).lower()
    
    # HTTP ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼
    if hasattr(error, 'response') and error.response is not None:
        status_code = error.response.status_code
        if status_code == 404:
            return f"âŒ ã“ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆ404ã‚¨ãƒ©ãƒ¼ï¼‰\nâ€¢ URLãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„\nâ€¢ ãƒ•ã‚¡ã‚¤ãƒ«ãŒå‰Šé™¤ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\nâ€¢ URL: {url}"
        elif status_code == 403:
            return f"âŒ ã“ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸï¼ˆ403ã‚¨ãƒ©ãƒ¼ï¼‰\nâ€¢ ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ã•ã‚Œã¦ã„ã¾ã™\nâ€¢ ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚Šã¾ã™\nâ€¢ URL: {url}"
    
    # PDFãƒ•ã‚¡ã‚¤ãƒ«å›ºæœ‰ã®ã‚¨ãƒ©ãƒ¼
    if 'pdf' in error_str and ('corrupt' in error_str or 'damaged' in error_str):
        return f"ğŸ“„ PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ã¾ã™\nâ€¢ ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\nâ€¢ å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\nâ€¢ URL: {url}"
    
    if 'password' in error_str or 'encrypted' in error_str:
        return f"ğŸ”’ ã“ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ä¿è­·ã•ã‚Œã¦ã„ã¾ã™\nâ€¢ ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒå¿…è¦ãªPDFãƒ•ã‚¡ã‚¤ãƒ«ã¯å‡¦ç†ã§ãã¾ã›ã‚“\nâ€¢ ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’è§£é™¤ã—ã¦ã‹ã‚‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\nâ€¢ URL: {url}"
    
    if 'timeout' in error_str:
        return f"â° PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ\nâ€¢ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\nâ€¢ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„\nâ€¢ URL: {url}"
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚¨ãƒ©ãƒ¼
    if 'not a pdf' in error_str or 'invalid pdf' in error_str:
        return f"ğŸ“„ ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯æœ‰åŠ¹ãªPDFãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ã‚ã‚Šã¾ã›ã‚“\nâ€¢ URLãŒæ­£ã—ã„PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„\nâ€¢ ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ãŒ.pdfã§ã‚‚å®Ÿéš›ã¯åˆ¥ã®å½¢å¼ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\nâ€¢ URL: {url}"
    
    return f"âŒ PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ\nâ€¢ è©³ç´°: {str(error)}\nâ€¢ ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ã„PDFã‹ç¢ºèªã—ã¦ãã ã•ã„\nâ€¢ åˆ¥ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã§è©¦ã—ã¦ã¿ã¦ãã ã•ã„\nâ€¢ URL: {url}"

async def extract_text_from_pdf(url: str) -> str:
    """URLã‹ã‚‰PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹
    
    Args:
        url: PDFãƒ•ã‚¡ã‚¤ãƒ«ã®URL
        
    Returns:
        æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã¾ãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    try:
        # URLã‹ã‚‰PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        response = requests.get(url, timeout=60, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        response.raise_for_status()
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©ºã§ãªã„ã‹ç¢ºèª
        if not response.content:
            return f"âŒ PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ\nâ€¢ ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™\nâ€¢ URLã‚’ç¢ºèªã—ã¦ãã ã•ã„\nâ€¢ URL: {url}"
        
        # Content-Typeã®ç¢ºèª
        content_type = response.headers.get('content-type', '').lower()
        if 'pdf' not in content_type and len(response.content) < 1000:
            return f"âŒ ã“ã®URLã¯æœ‰åŠ¹ãªPDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡ã—ã¦ã„ã¾ã›ã‚“\nâ€¢ Content-Type: {content_type}\nâ€¢ PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ç›´æ¥ãƒªãƒ³ã‚¯ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„\nâ€¢ URL: {url}"
        
        # PyMuPDFã‚’ä½¿ç”¨ã—ã¦PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        pdf_document = fitz.open(stream=response.content, filetype="pdf")
        
        # PDFãŒç©ºã§ãªã„ã‹ç¢ºèª
        if len(pdf_document) == 0:
            pdf_document.close()
            return f"âŒ ã“ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ãƒšãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“\nâ€¢ ç©ºã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã§ã™\nâ€¢ URL: {url}"
        
        text = ""
        for page_num in range(len(pdf_document)):
            page = pdf_document[page_num]
            page_text = page.get_text()
            text += page_text
        
        pdf_document.close()
        
        # æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ãªã„ã‹ç¢ºèª
        if not text or len(text.strip()) < 10:
            return f"âŒ PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ\nâ€¢ ç”»åƒã®ã¿ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\nâ€¢ ã‚¹ã‚­ãƒ£ãƒ³ã•ã‚ŒãŸPDFã¯ç¾åœ¨å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“\nâ€¢ URL: {url}"
        
        return text
    except Exception as e:
        error_message = _get_user_friendly_pdf_error(e, url)
        print(f"PDFæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return error_message

def extract_text_from_pdf_bytes(content: bytes) -> str:
    """PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚¤ãƒˆå†…å®¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹
    
    Args:
        content: PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚¤ãƒˆå†…å®¹
        
    Returns:
        æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    try:
        # PyMuPDFã‚’ä½¿ç”¨ã—ã¦PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        pdf_document = fitz.open(stream=content, filetype="pdf")
        text = ""
        for page_num in range(len(pdf_document)):
            page = pdf_document[page_num]
            text += page.get_text()
        pdf_document.close()
        return text
    except Exception as e:
        print(f"PDFæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return ""

def _get_user_friendly_url_error(error: Exception, url: str) -> str:
    """URLã‚¨ãƒ©ãƒ¼ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¤‰æ›"""
    import requests
    
    error_str = str(error).lower()
    
    # HTTP ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼
    if hasattr(error, 'response') and error.response is not None:
        status_code = error.response.status_code
        if status_code == 404:
            return f"âŒ ã“ã®URLã¯å­˜åœ¨ã—ã¾ã›ã‚“ï¼ˆ404ã‚¨ãƒ©ãƒ¼ï¼‰\nâ€¢ URLãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„\nâ€¢ ãƒšãƒ¼ã‚¸ãŒå‰Šé™¤ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\nâ€¢ URL: {url}"
        elif status_code == 403:
            return f"âŒ ã“ã®URLã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸï¼ˆ403ã‚¨ãƒ©ãƒ¼ï¼‰\nâ€¢ ã‚µã‚¤ãƒˆãŒã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ã‚’è¨­ã‘ã¦ã„ã¾ã™\nâ€¢ ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ãªãƒšãƒ¼ã‚¸ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\nâ€¢ URL: {url}"
        elif status_code == 401:
            return f"âŒ ã“ã®URLã¯èªè¨¼ãŒå¿…è¦ã§ã™ï¼ˆ401ã‚¨ãƒ©ãƒ¼ï¼‰\nâ€¢ ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ãªãƒšãƒ¼ã‚¸ã§ã™\nâ€¢ èªè¨¼æƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„\nâ€¢ URL: {url}"
        elif status_code == 500:
            return f"âŒ ã‚µãƒ¼ãƒãƒ¼ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ï¼ˆ500ã‚¨ãƒ©ãƒ¼ï¼‰\nâ€¢ ã‚µã‚¤ãƒˆå´ã®å•é¡Œã§ã™\nâ€¢ æ™‚é–“ã‚’ãŠã„ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„\nâ€¢ URL: {url}"
        else:
            return f"âŒ HTTPã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆ{status_code}ã‚¨ãƒ©ãƒ¼ï¼‰\nâ€¢ ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰äºˆæœŸã—ãªã„å¿œç­”ãŒè¿”ã•ã‚Œã¾ã—ãŸ\nâ€¢ URL: {url}"
    
    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼
    if 'timeout' in error_str or 'timed out' in error_str:
        return f"â° URLã®èª­ã¿è¾¼ã¿ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ\nâ€¢ ã‚µã‚¤ãƒˆã®å¿œç­”ãŒé…ã™ãã¾ã™\nâ€¢ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„\nâ€¢ å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™\nâ€¢ URL: {url}"
    
    # SSLè¨¼æ˜æ›¸ã‚¨ãƒ©ãƒ¼
    if 'ssl' in error_str or 'certificate' in error_str:
        return f"ğŸ”’ SSLè¨¼æ˜æ›¸ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ\nâ€¢ ã‚µã‚¤ãƒˆã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨¼æ˜æ›¸ã«å•é¡ŒãŒã‚ã‚Šã¾ã™\nâ€¢ HTTPSã§ã¯ãªãHTTPã§ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„\nâ€¢ URL: {url}"
    
    # æ¥ç¶šã‚¨ãƒ©ãƒ¼
    if 'connection' in error_str or 'resolve' in error_str or 'network' in error_str:
        return f"ğŸŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ\nâ€¢ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„\nâ€¢ URLãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„\nâ€¢ ã‚µã‚¤ãƒˆãŒä¸€æ™‚çš„ã«ãƒ€ã‚¦ãƒ³ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\nâ€¢ URL: {url}"
    
    # æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼
    if 'encoding' in error_str or 'decode' in error_str:
        return f"ğŸ“ æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ\nâ€¢ ãƒšãƒ¼ã‚¸ã®æ–‡å­—ã‚³ãƒ¼ãƒ‰ã«å•é¡ŒãŒã‚ã‚Šã¾ã™\nâ€¢ ä¸€éƒ¨ã®æ–‡å­—ãŒæ­£ã—ãèª­ã¿å–ã‚Œãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\nâ€¢ URL: {url}"
    
    # ãã®ä»–ã®ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼
    return f"âŒ URLå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ\nâ€¢ è©³ç´°: {str(error)}\nâ€¢ URLãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„\nâ€¢ åˆ¥ã®URLã§è©¦ã—ã¦ã¿ã¦ãã ã•ã„\nâ€¢ URL: {url}"

async def extract_text_from_html(url: str) -> str:
    """URLã‹ã‚‰HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹
    
    Args:
        url: æŠ½å‡ºå¯¾è±¡ã®URL
        
    Returns:
        æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã¾ãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    try:
        # URLã‹ã‚‰HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
        response = requests.get(url, timeout=30, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        response.raise_for_status()
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©ºã§ãªã„ã‹ç¢ºèª
        if not response.content:
            return f"âŒ ã“ã®URLã«ã¯å†…å®¹ãŒã‚ã‚Šã¾ã›ã‚“\nâ€¢ ãƒšãƒ¼ã‚¸ãŒç©ºç™½ã§ã™\nâ€¢ åˆ¥ã®URLã§è©¦ã—ã¦ã¿ã¦ãã ã•ã„\nâ€¢ URL: {url}"
        
        # BeautifulSoupã§HTMLã‚’è§£æ
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚¿ã‚°ã‚’å‰Šé™¤
        for script in soup(["script", "style"]):
            script.decompose()
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        text = soup.get_text()
        
        # æ”¹è¡Œã‚„ç©ºç™½ã‚’æ•´ç†
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = '\n'.join(chunk for chunk in chunks if chunk)
        
        # æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ãªã„ã‹ç¢ºèª
        if not text or len(text.strip()) < 10:
            return f"âŒ ã“ã®URLã‹ã‚‰æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ\nâ€¢ ãƒšãƒ¼ã‚¸ãŒJavaScriptã§å‹•çš„ã«ç”Ÿæˆã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\nâ€¢ ç”»åƒã‚„ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã®ãƒšãƒ¼ã‚¸ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\nâ€¢ URL: {url}"
        
        return text
    except Exception as e:
        error_message = _get_user_friendly_url_error(e, url)
        print(f"HTMLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return error_message

def safe_print(text):
    """å®‰å…¨ãªå‡ºåŠ›é–¢æ•°"""
    try:
        print(text)
    except Exception as e:
        print(f"å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)}")

def safe_safe_print(text):
    """ã‚ˆã‚Šå®‰å…¨ãªå‡ºåŠ›é–¢æ•°"""
    try:
        print(text)
    except Exception:
        pass  # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ç„¡è¦–

# ãƒ—ãƒ­ã‚­ã‚·è¨­å®šã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
WEBSHAREPROXY_USERNAME = os.getenv("WEBSHAREPROXY_USERNAME")
WEBSHAREPROXY_PASSWORD = os.getenv("WEBSHAREPROXY_PASSWORD")
ASSEMBLYAI_API_KEY = os.getenv("ASSEMBLYAI_API_KEY")
HTTP_PROXY = os.getenv("HTTP_PROXY")
HTTPS_PROXY = os.getenv("HTTPS_PROXY")

# requestsã®ãƒ—ãƒ­ã‚­ã‚·è¨­å®š
def get_proxies():
    """ç’°å¢ƒã«å¿œã˜ãŸãƒ—ãƒ­ã‚­ã‚·è¨­å®šã‚’å–å¾—"""
    proxies = {}
    if HTTP_PROXY:
        proxies['http'] = HTTP_PROXY
    if HTTPS_PROXY:
        proxies['https'] = HTTPS_PROXY
    return proxies if proxies else None

async def _check_and_throttle_url_request(url: str):
    """URLãƒªã‚¯ã‚¨ã‚¹ãƒˆå‰ã®ã‚µã‚¤ã‚ºæ¨å®šã¨ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°"""
    try:
        # å¤§ããªãƒ‡ãƒ¼ã‚¿ãŒäºˆæƒ³ã•ã‚Œã‚‹URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        large_data_indicators = [
            'drive.google.com',  # Google Drive
            'dropbox.com',       # Dropbox
            'onedrive.live.com', # OneDrive
            'mega.nz',           # Mega
            'archive.org',       # Internet Archive
            'youtube.com',       # YouTube (å‹•ç”»)
            'vimeo.com',         # Vimeo
            'slideshare.net',    # SlideShare
            'scribd.com',        # Scribd
            '.pdf',              # PDF files
            'docs.google.com',   # Google Docs
            'sheets.google.com', # Google Sheets
        ]
        
        url_lower = url.lower()
        
        # å¤§ããªãƒ‡ãƒ¼ã‚¿ã®å¯èƒ½æ€§ãŒã‚ã‚‹URLã®å ´åˆã¯äº‹å‰ã«é…å»¶
        for indicator in large_data_indicators:
            if indicator in url_lower:
                delay_seconds = 2.0
                print(f"å¤§ããªãƒ‡ãƒ¼ã‚¿ã®å¯èƒ½æ€§ãŒã‚ã‚‹URLæ¤œå‡º: {indicator} - {delay_seconds}ç§’å¾…æ©Ÿ")
                await asyncio.sleep(delay_seconds)
                break
        
        # ç‰¹ã«å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ãŒäºˆæƒ³ã•ã‚Œã‚‹å ´åˆã®è¿½åŠ é…å»¶
        high_risk_indicators = [
            'drive.google.com/file/d/',  # Google Driveç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ«
            'dropbox.com/s/',            # Dropboxå…±æœ‰ãƒ•ã‚¡ã‚¤ãƒ«
            '.pdf',                      # PDF
            'archive.org/download/',     # Archive.orgãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        ]
        
        for indicator in high_risk_indicators:
            if indicator in url_lower:
                additional_delay = 3.0
                print(f"é«˜ãƒªã‚¹ã‚¯URLæ¤œå‡º: {indicator} - è¿½åŠ {additional_delay}ç§’å¾…æ©Ÿ")
                await asyncio.sleep(additional_delay)
                break
                
    except Exception as e:
        print(f"URLäº‹å‰ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚å‡¦ç†ã¯ç¶šè¡Œ

# Function to extract video ID from a full YouTube URL
def get_video_id(youtube_url):
    """YouTube URLã‹ã‚‰å‹•ç”»IDã‚’æŠ½å‡ºã™ã‚‹ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    import re
    
    # ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã§å‹•ç”»IDã‚’æŠ½å‡º
    patterns = [
        r'(?:v=|\/)([0-9A-Za-z_-]{11}).*',  # æ¨™æº–çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³
        r'youtu\.be\/([0-9A-Za-z_-]{11})',  # çŸ­ç¸®URL
        r'embed\/([0-9A-Za-z_-]{11})',      # åŸ‹ã‚è¾¼ã¿URL
        r'watch\?v=([0-9A-Za-z_-]{11})'     # æ¨™æº–çš„ãªwatch URL
    ]
    
    for pattern in patterns:
        match = re.search(pattern, youtube_url)
        if match:
            video_id = match.group(1)
            print(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ '{pattern}' ã§å‹•ç”»IDæŠ½å‡º: {video_id}")
            return video_id
    
    print(f"å‹•ç”»IDæŠ½å‡ºå¤±æ•—: {youtube_url}")
    return None

def _process_video_file(contents, filename):
    """å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™"""
    try:
        video_file = BytesIO(contents)
        
        transcription = transcribe_video_file(video_file)
        # Ensure transcription is a string
        transcription_text = str(transcription) if transcription is not None else ""
        
        sections = {"ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³": transcription_text}
        extracted_text = f"=== ãƒ•ã‚¡ã‚¤ãƒ«: {filename} ===\n\n=== ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ ===\n{transcription_text}\n\n"

        result_df = pd.DataFrame({
            'section': ["ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³"],
            'content': [transcription_text],
            'source': ['Video'],
            'file': [filename],
            'url': [None]
        })
        
        return result_df, sections, extracted_text
    except Exception as e:
        print(f"Videoãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        print(traceback.format_exc())
        raise

# Headers for AssemblyAI API
HEADERS = {
    "authorization": ASSEMBLYAI_API_KEY,
    "content-type": "application/json"
}

def upload_to_assemblyai(video_file: BytesIO) -> str:
    """
    Uploads video file to AssemblyAI and returns the upload URL.
    """
    upload_url = "https://api.assemblyai.com/v2/upload"
    proxies = get_proxies()

    response = requests.post(
        upload_url,
        headers={"authorization": ASSEMBLYAI_API_KEY},
        data=video_file,
        proxies=proxies,
        timeout=300
    )

    response.raise_for_status()
    return response.json()['upload_url']

def start_transcription(upload_url: str) -> str:
    """
    Starts the transcription job and returns the transcript ID.
    """
    transcript_endpoint = "https://api.assemblyai.com/v2/transcript"
    json_data = {
        "audio_url": upload_url
    }
    proxies = get_proxies()

    response = requests.post(transcript_endpoint, json=json_data, headers=HEADERS, proxies=proxies, timeout=60)
    response.raise_for_status()
    return response.json()["id"]

def poll_transcription(transcript_id: str) -> dict:
    """
    Polls the transcript endpoint until transcription is completed.
    """
    polling_endpoint = f"https://api.assemblyai.com/v2/transcript/{transcript_id}"
    proxies = get_proxies()

    while True:
        response = requests.get(polling_endpoint, headers=HEADERS, proxies=proxies, timeout=60)
        response.raise_for_status()
        data = response.json()

        if data["status"] == "completed":
            return data
        elif data["status"] == "error":
            raise RuntimeError(f"Transcription failed: {data['error']}")

        time.sleep(3)  # Wait a few seconds before checking again

def transcribe_video_file(video_file: BytesIO) -> str:
    """
    Main function to handle the full transcription pipeline.
    """
    print("Uploading video...")
    upload_url = upload_to_assemblyai(video_file)

    print("Starting transcription...")
    transcript_id = start_transcription(upload_url)

    print("Waiting for transcription to complete...")
    transcript_data = poll_transcription(transcript_id)

    # Ensure we return a string, not None or an int
    text = transcript_data.get("text", "")
    return ensure_string(text)

def create_default_usage_limits(user_id: str, user_email: str, user_role: str = None) -> dict:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ©ç”¨åˆ¶é™ã‚’ç”Ÿæˆã™ã‚‹å…±é€šé–¢æ•°
    
    Args:
        user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        user_email: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹
        user_role: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    
    Returns:
        ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®åˆ©ç”¨åˆ¶é™è¨­å®š
    """
    # ç‰¹åˆ¥ç®¡ç†è€…ã®åˆ¤å®š
    is_unlimited = user_email == "queue@queueu-tech.jp"
    
    return {
        "user_id": user_id,
        "document_uploads_used": 0,
        "document_uploads_limit": 999999 if is_unlimited else 2,
        "questions_used": 0,
        "questions_limit": 999999 if is_unlimited else 10,
        "is_unlimited": is_unlimited
    }

def get_permission_flags(current_user: dict) -> dict:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ¨©é™ãƒ•ãƒ©ã‚°ã‚’ç”Ÿæˆã™ã‚‹å…±é€šé–¢æ•°
    
    Args:
        current_user: ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±
    
    Returns:
        æ¨©é™ãƒ•ãƒ©ã‚°ã®è¾æ›¸
    """
    # ç‰¹åˆ¥ç®¡ç†è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®çµ±ä¸€å®šç¾©
    special_admin_emails = ["queue@queuefood.co.jp", "queue@queueu-tech.jp"]
    
    return {
        "is_special_admin": current_user["email"] in special_admin_emails and current_user.get("is_special_admin", False),
        "is_admin_user": current_user["role"] == "admin_user",
        "is_user": current_user["role"] == "user",
        "user_email": current_user["email"],
        "user_role": current_user["role"]
    }


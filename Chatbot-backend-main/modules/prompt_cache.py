"""
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã®æœ€é©åŒ–ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥
é«˜é€Ÿãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã¨å†åˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥
Geminiã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œ
"""
from functools import lru_cache
import hashlib
import time
from typing import List, Optional, Dict, Any

def safe_print(text):
    """å®‰å…¨ãªprinté–¢æ•°"""
    try:
        print(text)
    except UnicodeEncodeError:
        try:
            safe_text = str(text).encode('utf-8', errors='replace').decode('utf-8')
            print(safe_text)
        except:
            print("[å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: Unicodeæ–‡å­—ã‚’å«ã‚€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸]")

# Geminiã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
class GeminiContextCache:
    """Geminiã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.cache_ttl = 3600  # 1æ™‚é–“ã®TTL
        self.min_context_size = 2000  # æœ€å°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºï¼ˆ2KBï¼‰
    
    def _generate_context_hash(self, content: str) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ç”Ÿæˆ"""
        # å†…å®¹ã®æ­£è¦åŒ–ï¼ˆæ”¹è¡Œã‚„ç©ºç™½ã®çµ±ä¸€ï¼‰
        normalized = content.strip().replace('\r\n', '\n').replace('\r', '\n')
        return hashlib.sha256(normalized.encode('utf-8')).hexdigest()[:16]
    
    def should_cache_context(self, content: str) -> bool:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã¹ãã‹ã‚’åˆ¤å®š"""
        return len(content) >= self.min_context_size
    
    def get_cached_content_id(self, knowledge_base_content: str) -> Optional[str]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„IDã‚’å–å¾—"""
        if not self.should_cache_context(knowledge_base_content):
            return None
        
        context_hash = self._generate_context_hash(knowledge_base_content)
        cache_entry = self.cache.get(context_hash)
        
        if cache_entry:
            # TTLãƒã‚§ãƒƒã‚¯
            if time.time() - cache_entry['timestamp'] < self.cache_ttl:
                safe_print(f"ğŸ¯ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {context_hash}")
                return cache_entry['content_id']
            else:
                # æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤
                del self.cache[context_hash]
                safe_print(f"â° æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤: {context_hash}")
        
        return None
    
    def store_context_cache(self, knowledge_base_content: str, content_id: str):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜"""
        if not self.should_cache_context(knowledge_base_content):
            return
        
        context_hash = self._generate_context_hash(knowledge_base_content)
        self.cache[context_hash] = {
            'content_id': content_id,
            'timestamp': time.time(),
            'size': len(knowledge_base_content)
        }
        safe_print(f"ğŸ’¾ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜: {context_hash} (ã‚µã‚¤ã‚º: {len(knowledge_base_content):,}æ–‡å­—)")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        valid_entries = 0
        expired_entries = 0
        total_size = 0
        
        current_time = time.time()
        for cache_entry in self.cache.values():
            if current_time - cache_entry['timestamp'] < self.cache_ttl:
                valid_entries += 1
                total_size += cache_entry['size']
            else:
                expired_entries += 1
        
        return {
            'valid_entries': valid_entries,
            'expired_entries': expired_entries,
            'total_cached_size': total_size,
            'cache_hit_potential': f"{(valid_entries / max(1, len(self.cache))) * 100:.1f}%"
        }
    
    def cleanup_expired_cache(self):
        """æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        current_time = time.time()
        expired_keys = [
            key for key, cache_entry in self.cache.items()
            if current_time - cache_entry['timestamp'] >= self.cache_ttl
        ]
        
        for key in expired_keys:
            del self.cache[key]
        
        if expired_keys:
            safe_print(f"ğŸ§¹ æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: {len(expired_keys)}ä»¶")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
gemini_context_cache = GeminiContextCache()

@lru_cache(maxsize=50)
def get_optimized_prompt_template(company_name: str, has_special_instructions: bool) -> str:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æœ€é©åŒ–ç‰ˆï¼ˆLRUã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ï¼‰"""
    
    base_instructions = f"""ã‚ãªãŸã¯è¦ªåˆ‡ã§ä¸å¯§ãªå¯¾å¿œãŒã§ãã‚‹{company_name}ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦å¯èƒ½ãªé™ã‚Šå…·ä½“çš„ã§å½¹ç«‹ã¤å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

å›ç­”ã®éš›ã®æ³¨æ„ç‚¹ï¼š
1. å¸¸ã«ä¸å¯§ãªè¨€è‘‰é£ã„ã‚’å¿ƒãŒã‘ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦æ•¬æ„ã‚’æŒã£ã¦æ¥ã—ã¦ãã ã•ã„
2. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«æƒ…å ±ãŒãªã„å ´åˆã§ã‚‚ã€ä¸€èˆ¬çš„ãªæ–‡è„ˆã§å›ç­”ã§ãã‚‹å ´åˆã¯é©åˆ‡ã«å¯¾å¿œã—ã¦ãã ã•ã„
3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œã‚‚ã£ã¨è©³ã—ãã€ãªã©ã¨è³ªå•ã—ãŸå ´åˆã¯ã€å‰å›ã®å›ç­”å†…å®¹ã«é–¢é€£ã™ã‚‹è©³ç´°æƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„
4. å¯èƒ½ãªé™ã‚Šå…·ä½“çš„ã§å®Ÿç”¨çš„ãªæƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„
5. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«OCRã§æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€æ–‡è„ˆã‹ã‚‰é©åˆ‡ã«è§£é‡ˆã—ã¦ãã ã•ã„
6. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã—ãŸå ´åˆã¯ã€å›ç­”ã®æœ€å¾Œã«ã€Œæƒ…å ±ã‚½ãƒ¼ã‚¹: [ãƒ•ã‚¡ã‚¤ãƒ«å]ã€ã®å½¢å¼ã§å‚ç…§ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„
7. å›ç­”ã¯**Markdownè¨˜æ³•**ã‚’ä½¿ç”¨ã—ã¦è¦‹ã‚„ã™ãæ•´ç†ã—ã¦ãã ã•ã„
8. é‡è¦ãªæƒ…å ±ã¯**å¤ªå­—**ã§å¼·èª¿ã—ã¦ãã ã•ã„
9. ã‚³ãƒ¼ãƒ‰ã‚„ãƒ•ã‚¡ã‚¤ãƒ«åã€è¨­å®šå€¤ãªã©ã¯`ãƒãƒƒã‚¯ã‚¯ã‚©ãƒ¼ãƒˆ`ã§å›²ã‚“ã§ãã ã•ã„"""
    
    if has_special_instructions:
        base_instructions += "\n10. ç‰¹åˆ¥ãªå›ç­”æŒ‡ç¤ºãŒã‚ã‚‹å ´åˆã¯ã€ãã®æŒ‡ç¤ºã«å¾“ã£ã¦ãã ã•ã„"
    
    return base_instructions

@lru_cache(maxsize=100)
def get_cached_conversation_format(history_length: int) -> str:
    """ä¼šè©±å±¥æ­´ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç‰ˆ"""
    if history_length == 0:
        return ""
    return "ç›´è¿‘ã®ä¼šè©±å±¥æ­´ï¼š\n{conversation_content}\n"

def build_conversation_history_fast(recent_messages: List[dict]) -> str:
    """é«˜é€Ÿä¼šè©±å±¥æ­´æ§‹ç¯‰"""
    if not recent_messages:
        return ""
    
    format_template = get_cached_conversation_format(len(recent_messages))
    
    conversation_parts = []
    for msg in reversed(recent_messages):  # å¤ã„é †ã«ä¸¦ã³æ›¿ãˆ
        try:
            user_msg = (msg.get('user_message', '') or '')[:100]
            bot_msg = (msg.get('bot_response', '') or '')[:100]
            
            # é•·ã„å ´åˆã¯çœç•¥è¨˜å·ã‚’è¿½åŠ 
            if len(msg.get('user_message', '')) > 100:
                user_msg += "..."
            if len(msg.get('bot_response', '')) > 100:
                bot_msg += "..."
            
            conversation_parts.append(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_msg}")
            conversation_parts.append(f"ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {bot_msg}")
            
        except Exception as e:
            safe_print(f"ä¼šè©±å±¥æ­´å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    if conversation_parts:
        return format_template.format(conversation_content="\n".join(conversation_parts))
    return ""

def build_optimized_prompt(
    company_name: str,
    active_resource_names: List[str],
    active_knowledge_text: str,
    conversation_history: str,
    message_text: str,
    special_instructions_text: str = ""
) -> str:
    """æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰"""
    
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—
    template = get_optimized_prompt_template(company_name, bool(special_instructions_text))
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆã®æ§‹ç¯‰ï¼ˆé«˜é€ŸåŒ–ï¼‰
    file_list = ', '.join(active_resource_names) if active_resource_names else ''
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®çµ„ã¿ç«‹ã¦
    prompt_parts = [
        template,
        f"\nåˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«: {file_list}" if file_list else "",
        special_instructions_text,
        f"\nçŸ¥è­˜ãƒ™ãƒ¼ã‚¹å†…å®¹ï¼š\n{active_knowledge_text}",
        f"\n{conversation_history}" if conversation_history else "",
        f"\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ï¼š\n{message_text}"
    ]
    
    # ç©ºã®éƒ¨åˆ†ã‚’é™¤å»ã—ã¦çµåˆ
    final_prompt = ''.join(part for part in prompt_parts if part)
    
    safe_print(f"âœ… æœ€é©åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰å®Œäº†: {len(final_prompt):,}æ–‡å­—")
    
    return final_prompt

def build_context_cached_prompt(
    company_name: str,
    active_resource_names: List[str],
    active_knowledge_text: str,
    conversation_history: str,
    message_text: str,
    special_instructions_text: str = ""
) -> tuple[str, Optional[str]]:
    """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰"""
    
    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
    cached_content_id = gemini_context_cache.get_cached_content_id(active_knowledge_text)
    
    if cached_content_id:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆï¼šçŸ¥è­˜ãƒ™ãƒ¼ã‚¹éƒ¨åˆ†ã‚’çœç•¥ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        template = get_optimized_prompt_template(company_name, bool(special_instructions_text))
        file_list = ', '.join(active_resource_names) if active_resource_names else ''
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‚ç…§ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt_parts = [
            template,
            f"\nåˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«: {file_list}" if file_list else "",
            special_instructions_text,
            "\n[æ³¨æ„: çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨]",
            f"\n{conversation_history}" if conversation_history else "",
            f"\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ï¼š\n{message_text}"
        ]
        
        final_prompt = ''.join(part for part in prompt_parts if part)
        safe_print(f"ğŸ¯ ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰: {len(final_prompt):,}æ–‡å­— (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ID: {cached_content_id})")
        
        return final_prompt, cached_content_id
    else:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ï¼šé€šå¸¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        full_prompt = build_optimized_prompt(
            company_name, active_resource_names, active_knowledge_text,
            conversation_history, message_text, special_instructions_text
        )
        safe_print(f"ğŸ’¾ æ–°è¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½œæˆå¯¾è±¡")
        
        return full_prompt, None

def generate_content_with_cache(model, prompt: str, cached_content_id: Optional[str] = None):
    """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ"""
    try:
        if cached_content_id:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨
            # æ³¨æ„: å®Ÿéš›ã®Gemini APIã§ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ã¯ã€APIã®ä»•æ§˜ã«ä¾å­˜
            # ã“ã“ã§ã¯æ¦‚å¿µçš„ãªå®Ÿè£…ã‚’ç¤ºã—ã¦ã„ã‚‹
            safe_print(f"ğŸš€ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½¿ç”¨: {cached_content_id}")
            response = model.generate_content(prompt)
        else:
            # é€šå¸¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ
            response = model.generate_content(prompt)
        
        return response
    except Exception as e:
        safe_print(f"âŒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

def estimate_prompt_size(
    company_name: str,
    active_resource_names: List[str],
    active_knowledge_text: str,
    conversation_history: str,
    message_text: str,
    special_instructions_text: str = ""
) -> int:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µã‚¤ã‚ºã®é«˜é€Ÿæ¨å®šï¼ˆå®Ÿéš›ã«æ§‹ç¯‰ã›ãšã«ã‚µã‚¤ã‚ºã‚’æ¨å®šï¼‰"""
    
    # åŸºæœ¬ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚µã‚¤ã‚º
    template_size = len(get_optimized_prompt_template(company_name, bool(special_instructions_text)))
    
    # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚µã‚¤ã‚º
    file_list_size = len(', '.join(active_resource_names)) if active_resource_names else 0
    knowledge_size = len(active_knowledge_text)
    history_size = len(conversation_history)
    message_size = len(message_text)
    special_size = len(special_instructions_text)
    
    # å›ºå®šæ–‡å­—åˆ—ã®æ¨å®šã‚µã‚¤ã‚ºï¼ˆæ”¹è¡Œã€ãƒ©ãƒ™ãƒ«ãªã©ï¼‰
    fixed_text_size = 200
    
    total_size = (
        template_size + 
        file_list_size + 
        knowledge_size + 
        history_size + 
        message_size + 
        special_size + 
        fixed_text_size
    )
    
    return total_size

def truncate_knowledge_for_size_limit(
    knowledge_text: str, 
    target_size: int, 
    other_content_size: int
) -> str:
    """ã‚µã‚¤ã‚ºåˆ¶é™ã«åˆã‚ã›ã¦çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’åˆ‡ã‚Šè©°ã‚"""
    
    available_size = target_size - other_content_size - 1000  # 1000æ–‡å­—ã®ãƒãƒƒãƒ•ã‚¡
    
    if available_size <= 0:
        return ""
    
    if len(knowledge_text) <= available_size:
        return knowledge_text
    
    # æ–‡ã®å¢ƒç•Œã§åˆ‡ã‚Šè©°ã‚
    truncated = knowledge_text[:available_size]
    last_sentence = max(
        truncated.rfind('ã€‚'),
        truncated.rfind('\n'),
        truncated.rfind('. '),
        available_size - 200  # æœ€ä½é™ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    )
    
    if last_sentence > 0:
        truncated = truncated[:last_sentence + 1]
    
    truncated += "\n\n[æ³¨æ„: ã‚µã‚¤ã‚ºåˆ¶é™ã®ãŸã‚ã€çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’çŸ­ç¸®ã—ã¦ã„ã¾ã™]"
    
    safe_print(f"âš ï¸ çŸ¥è­˜ãƒ™ãƒ¼ã‚¹åˆ‡ã‚Šè©°ã‚: {len(knowledge_text):,} â†’ {len(truncated):,}æ–‡å­—")
    
    return truncated

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®çµ±è¨ˆæƒ…å ±
def get_cache_stats() -> dict:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
    context_stats = gemini_context_cache.get_cache_stats()
    
    return {
        "template_cache_info": get_optimized_prompt_template.cache_info(),
        "conversation_format_cache_info": get_cached_conversation_format.cache_info(),
        "gemini_context_cache": context_stats
    }

def clear_prompt_caches():
    """å…¨ã¦ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
    get_optimized_prompt_template.cache_clear()
    get_cached_conversation_format.cache_clear()
    gemini_context_cache.cleanup_expired_cache()
    # å…¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
    gemini_context_cache.cache.clear()
    safe_print("âœ… å…¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å«ã‚€ï¼‰ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ") 
"""
Best Score Search System
ãƒ•ã‚¡ã‚¸ãƒ¼ã€ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã€å®Œå…¨ä¸€è‡´æ¤œç´¢ã‚’ä¸¦åˆ—å®Ÿè¡Œã—ã¦ä¸€ç•ªã‚¹ã‚³ã‚¢ãŒé«˜ã„ã‚‚ã®ã‚’æ¡ç”¨

s.mdã®ã€Œè¤‡æ•°ã‚¹ã‚³ã‚¢ã®rangeãŒæƒã£ã¦ã„ãªã„ã¨ãƒã‚¤ã‚¢ã‚¹ãŒå‡ºã‚‹ã€ã‚’è§£æ±ºã—ã€
å„æ¤œç´¢æ‰‹æ³•ã®æœ€é«˜ã‚¹ã‚³ã‚¢çµæœã‹ã‚‰æœ€è‰¯ã®ã‚‚ã®ã‚’é¸æŠã™ã‚‹
"""

import asyncio
import time
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import numpy as np
from supabase_adapter import execute_query, select_data
from .chat_config import safe_print
from .vector_search import get_vector_search_instance
from .unified_search_system import SearchResult, SearchType, ScoreNormalizer

logger = logging.getLogger(__name__)

class BestScoreSearchSystem:
    """æœ€é«˜ã‚¹ã‚³ã‚¢é¸æŠå‹æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.vector_search = get_vector_search_instance()
        self.score_normalizer = ScoreNormalizer()
    
    async def search_best_score(self, 
                               query: str, 
                               company_id: str = None,
                               limit: int = 10) -> List[Dict[str, Any]]:
        """
        ä¸¦åˆ—æ¤œç´¢ã—ã¦æœ€é«˜ã‚¹ã‚³ã‚¢ã®çµæœã‚’æ¡ç”¨
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            company_id: ä¼šç¤¾ID
            limit: çµæœæ•°åˆ¶é™
        """
        start_time = time.time()
        
        try:
            safe_print(f"ğŸš€ ä¸¦åˆ—æ¤œç´¢é–‹å§‹: {query}")
            
            # 3ã¤ã®æ¤œç´¢æ‰‹æ³•ã‚’ä¸¦åˆ—å®Ÿè¡Œ
            search_tasks = [
                self._exact_match_search(query, company_id, limit * 2),  # ã‚ˆã‚Šå¤šãå–å¾—
                self._fuzzy_search(query, company_id, limit * 2),
                self._vector_search(query, company_id, limit * 2)
            ]
            
            results_list = await asyncio.gather(*search_tasks, return_exceptions=True)
            
            # å„æ¤œç´¢æ‰‹æ³•ã®çµæœã‚’å‡¦ç†
            search_results = {}
            search_names = ["å®Œå…¨ä¸€è‡´", "Fuzzy", "ãƒ™ã‚¯ãƒˆãƒ«"]
            
            print(f"\nâ—â—â— ä¸¦åˆ—æ¤œç´¢çµæœå‡¦ç†é–‹å§‹ â—â—â—")
            
            for i, (results, name) in enumerate(zip(results_list, search_names)):
                print(f"\nâ— === {name}æ¤œç´¢çµæœå‡¦ç† ===")
                print(f"â— çµæœã‚¿ã‚¤ãƒ—: {type(results)}")
                
                if isinstance(results, Exception):
                    print(f"â—â—â— {name}æ¤œç´¢ã§ä¾‹å¤–ç™ºç”Ÿ: {results} â—â—â—")
                    safe_print(f"âŒ {name}æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {results}")
                    search_results[name] = []
                elif isinstance(results, list):
                    print(f"â— {name}æ¤œç´¢æˆåŠŸ: {len(results)}ä»¶å–å¾—")
                    if results:
                        print(f"â— {name}æ¤œç´¢ã‚µãƒ³ãƒ—ãƒ«:")
                        for j, item in enumerate(results[:2]):
                            if hasattr(item, 'to_dict'):
                                print(f"â—   [{j+1}] SearchResult: score={item.score}, type={item.search_type}")
                            else:
                                print(f"â—   [{j+1}] ä¸æ˜ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ: {type(item)}")
                    search_results[name] = results
                    safe_print(f"âœ… {name}æ¤œç´¢: {len(results)}ä»¶å–å¾—")
                else:
                    print(f"â—â—â— {name}æ¤œç´¢ã§äºˆæœŸã—ãªã„çµæœã‚¿ã‚¤ãƒ—: {type(results)} â—â—â—")
                    print(f"â— çµæœå†…å®¹: {results}")
                    search_results[name] = []
            
            print(f"\nâ— å…¨æ¤œç´¢æ‰‹æ³•ã®çµæœé›†è¨ˆ:")
            total_results = 0
            for name, results in search_results.items():
                count = len(results) if results else 0
                total_results += count
                print(f"â—   {name}: {count}ä»¶")
            print(f"â— ç·çµæœæ•°: {total_results}ä»¶")
            
            if total_results == 0:
                print(f"â—â—â— é‡å¤§ãªå•é¡Œ: å…¨ã¦ã®æ¤œç´¢æ‰‹æ³•ã§0ä»¶ã®çµæœ â—â—â—")
                print(f"â— è€ƒãˆã‚‰ã‚Œã‚‹åŸå› :")
                print(f"â—   1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„")
                print(f"â—   2. ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿ãŒå³ã—ã™ãã‚‹")
                print(f"â—   3. ã‚·ã‚¹ãƒ†ãƒ è¨­å®šã«å•é¡ŒãŒã‚ã‚‹")
                print(f"â—   4. ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã«å¤±æ•—ã—ã¦ã„ã‚‹")
                return []
            
            # ã‚¹ã‚³ã‚¢æ­£è¦åŒ–ã¨ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢é¸æŠ
            best_results = await self._select_best_scores(search_results, company_id, limit)
            
            execution_time = int((time.time() - start_time) * 1000)
            safe_print(f"ğŸ¯ æœ€é«˜ã‚¹ã‚³ã‚¢æ¤œç´¢å®Œäº†: {len(best_results)}ä»¶ã‚’{execution_time}msã§å–å¾—")
            
            # çµæœã‚’è©³ç´°è¡¨ç¤º
            self._display_search_comparison(search_results, best_results, query)
            
            return [result.to_dict() for result in best_results]
            
        except Exception as e:
            logger.error(f"æœ€é«˜ã‚¹ã‚³ã‚¢æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def _exact_match_search(self, query: str, company_id: str, limit: int) -> List[SearchResult]:
        """
        æ”¹è‰¯ã•ã‚ŒãŸå®Œå…¨ä¸€è‡´æ¤œç´¢ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰
        """
        print(f"â—â—â— å®Œå…¨ä¸€è‡´æ¤œç´¢é–‹å§‹ â—â—â—")
        print(f"â— ã‚¯ã‚¨ãƒª: '{query}'")
        print(f"â— ä¼šç¤¾ID: '{company_id}'")
        print(f"â— åˆ¶é™æ•°: {limit}")
        
        try:
            # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ç¢ºèª
            print(f"â— 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ç¢ºèª")
            print(f"â— SupabaseåŸºæœ¬æ“ä½œã§ãƒ‡ãƒ¼ã‚¿ç¢ºèªé–‹å§‹")
            
            # å…¨ä½“ãƒ‡ãƒ¼ã‚¿ç¢ºèª
            sample_result = select_data("chunks", columns="*", limit=1)
            print(f"â— å…¨ä½“ãƒ‡ãƒ¼ã‚¿ç¢ºèªçµæœ: {sample_result}")
            print(f"â— chunksãƒ†ãƒ¼ãƒ–ãƒ«å…¨ä½“: ãƒ‡ãƒ¼ã‚¿å­˜åœ¨ç¢ºèª")
            
            # ä¼šç¤¾IDã®ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª
            company_sample = select_data("chunks", columns="company_id,id", limit=5)
            print(f"â— ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ã®ä¼šç¤¾ID: {[item['company_id'] for item in company_sample.data]}")
            print(f"â— ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿: {company_sample.data[:2]}")
            
            # æŒ‡å®šä¼šç¤¾IDã§ã®ãƒ‡ãƒ¼ã‚¿ç¢ºèª
            company_filter_result = select_data("chunks", columns="*", filters={"company_id": company_id}, limit=1)
            print(f"â— ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿çµæœ: {company_filter_result}")
            
            if not company_filter_result.data:
                print(f"â—â—â— é‡è¦ç™ºè¦‹: ä¼šç¤¾IDã€Œ{company_id}ã€ã«è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ï¼ â—â—â—")
                print(f"â— chunksãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¯ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ãŒã€ã“ã®ä¼šç¤¾IDã§ã¯ãƒ‡ãƒ¼ã‚¿ãªã—")
                print(f"â— ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿ã‚’å¤–ã—ã¦æ¤œç´¢ç¶™ç¶š...")
                use_company_filter = False
            else:
                use_company_filter = True
            
            # 2. ãƒ¡ã‚¤ãƒ³æ¤œç´¢å®Ÿè¡Œ
            print(f"â— 2. SupabaseåŸºæœ¬æ“ä½œã§ãƒ¡ã‚¤ãƒ³æ¤œç´¢å®Ÿè¡Œ")
            
            if use_company_filter:
                filters = {"company_id": company_id}
                print(f"â— æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿: {filters}")
                
                result = select_data("chunks", columns="*", filters=filters, limit=min(limit * 10, 300))
            else:
                print(f"â— ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿ãªã—ã§æ¤œç´¢å®Ÿè¡Œ")
                result = select_data("chunks", columns="*", limit=min(limit * 10, 300))
            
            print(f"â— Supabaseæ¤œç´¢çµæœã‚¿ã‚¤ãƒ—: {type(result)}")
            print(f"â— Supabaseæ¤œç´¢çµæœãƒ‡ãƒ¼ã‚¿æ•°: {len(result.data) if result.data else 0}")
            
            if not result.data:
                print(f"â—â—â— ã‚¨ãƒ©ãƒ¼åŸå› ç‰¹å®š: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰0ä»¶å–å¾— â—â—â—")
                return []
            
            # 3. æ”¹è‰¯ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹æ¤œç´¢
            print(f"â— 3. Supabaseçµæœãƒ‡ãƒ¼ã‚¿å‡¦ç†ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹æ¤œç´¢")
            print(f"â— Supabaseã‹ã‚‰å–å¾—: {len(result.data)}ä»¶")
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            processed_results = self._improved_keyword_search(query, result.data)
            print(f"â— æ”¹è‰¯ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢çµæœ: {len(processed_results)}ä»¶")
            
            if not processed_results:
                print(f"â—â—â— ã‚¨ãƒ©ãƒ¼åŸå› ç‰¹å®š: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°å¾Œ0ä»¶ã§ã™ â—â—â—")
                print(f"â— åŸå› : ã‚¯ã‚¨ãƒªã€Œ{query}ã€ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã©ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ã‚‚ãƒãƒƒãƒã—ãªã„")
                return []
            
            # 4. SearchResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
            search_results = []
            for item in processed_results[:limit]:
                search_result = SearchResult(
                    id=item["id"],
                    content=item["content"],
                    title=item.get("file_name", "Unknown"),
                    score=item["score"],
                    search_type="exact_match",
                    metadata={
                        "doc_id": item.get("doc_id", ""),
                        "chunk_index": item.get("chunk_index", 0),
                        "query": query
                    }
                )
                search_results.append(search_result)
            
            print(f"â— å®Œå…¨ä¸€è‡´æ¤œç´¢æœ€çµ‚çµæœ: {len(search_results)}ä»¶")
            return search_results
            
        except Exception as e:
            print(f"â—â—â— å®Œå…¨ä¸€è‡´æ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)} â—â—â—")
            import traceback
            traceback.print_exc()
            return []

    def _improved_keyword_search(self, query: str, data: List[Dict]) -> List[Dict]:
        """
        æ”¹è‰¯ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹æ¤œç´¢
        """
        print(f"â— ğŸ” æ”¹è‰¯ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢å®Ÿè¡Œ")
        print(f"â— æ¤œç´¢ã‚¯ã‚¨ãƒª: '{query}'")
        
        # ã‚¯ã‚¨ãƒªã‚’å°æ–‡å­—ã«å¤‰æ›
        query_lower = query.lower()
        print(f"â— å°æ–‡å­—å¤‰æ›: '{query_lower}'")
        
        # é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ‹¡å¼µ
        keyword_expansions = {
            "ãƒã‚¦ã‚¹": ["ãƒã‚¦ã‚¹", "mouse", "ãƒã‚¤ãƒ³ãƒ†ã‚£ãƒ³ã‚°ãƒ‡ãƒã‚¤ã‚¹", "ãƒ¯ã‚¤ãƒ¤ãƒ¬ã‚¹ãƒã‚¦ã‚¹", "å…‰å­¦ãƒã‚¦ã‚¹", "ãƒ¬ãƒ¼ã‚¶ãƒ¼ãƒã‚¦ã‚¹", "ã‚²ãƒ¼ãƒŸãƒ³ã‚°ãƒã‚¦ã‚¹", "ã‚¨ãƒ«ã‚´ãƒãƒŸã‚¯ã‚¹"],
            "pc": ["pc", "ãƒ‘ã‚½ã‚³ãƒ³", "computer", "ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—", "ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³", "ãƒãƒ¼ãƒˆ", "ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—PC", "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼"],
            "ãŠã™ã™ã‚": ["ãŠã™ã™ã‚", "æ¨å¥¨", "é¸ã³æ–¹", "äººæ°—", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "æœ€é©", "é©ã—ãŸ", "è‰¯ã„"],
            "æ•™ãˆã¦": ["æ•™ãˆã¦", "ã«ã¤ã„ã¦", "æƒ…å ±", "è©³ç´°", "èª¬æ˜", "ç´¹ä»‹", "æ¡ˆå†…"]
        }
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        keywords = []
        for base_keyword, expansions in keyword_expansions.items():
            if base_keyword in query_lower:
                keywords.extend(expansions)
        
        # ç›´æ¥çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚‚è¿½åŠ ï¼ˆç‰©ä»¶ç•ªå·å¯¾å¿œï¼‰
        import re
        # ç‰©ä»¶ç•ªå·ã‚„ã‚³ãƒ¼ãƒ‰ï¼ˆWPD4100399ãªã©ï¼‰ã‚’å„ªå…ˆçš„ã«æŠ½å‡º
        property_numbers = re.findall(r'[A-Z]+\d+', query)  # å¤§æ–‡å­—+æ•°å­—ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆç‰©ä»¶ç•ªå·ï¼‰
        receipt_numbers = re.findall(r'J\d+', query)       # å—æ³¨ç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³
        
        # æŸ”è»Ÿæ€§ã‚’ä¿æŒã—ãŸå¤šæ®µéšã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        # 1. åŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆå˜èªãƒ¬ãƒ™ãƒ«ï¼‰
        basic_keywords = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,}|[a-zA-Z0-9]{2,}', query_lower)
        
        # 2. é‡è¦ãªå˜èªã®å€‹åˆ¥æŠ½å‡ºï¼ˆ1æ–‡å­—ã§ã‚‚é‡è¦ãªå ´åˆï¼‰
        important_single = re.findall(r'[æ©Ÿè£…ä¼šç¤¾è£½å“æŠ€è¡“]', query_lower)
        
        # 3. åŠ©è©ãƒ»æ¥ç¶šè©ã‚’é™¤ã„ãŸæ„å‘³ã®ã‚ã‚‹å˜èªã®ã¿æŠ½å‡º
        stop_words_extended = {'ã®', 'ã«', 'ã‚’', 'ã¯', 'ãŒ', 'ã§', 'ã¨', 'ã‹ã‚‰', 'ã¾ã§', 'ã‚ˆã‚Š', 'ã“ã¨', 'ã‚‚ã®', 'ã“ã‚Œ', 'ãã‚Œ', 'ã‚ã‚Œ', 'ã©ã®', 'ã©ã‚“ãª', 'ä½•', 'ã§ã™', 'ã§ã™ã‹', 'ã¾ã™', 'ã—ãŸ'}
        meaningful_keywords = [kw for kw in basic_keywords if kw not in stop_words_extended and len(kw) >= 1]
        
        # ç‰©ä»¶ç•ªå·ã‚’æœ€å„ªå…ˆã§è¿½åŠ ï¼ˆå®Œå…¨ä¸€è‡´ã®å„ªä½æ€§ä¿æŒï¼‰
        keywords.extend(property_numbers)      # æœ€å„ªå…ˆï¼šå®Œå…¨ä¸€è‡´ãŒå¿…è¦
        keywords.extend(receipt_numbers)       # æ¬¡ã«å„ªå…ˆï¼šå®Œå…¨ä¸€è‡´ãŒå¿…è¦
        keywords.extend(meaningful_keywords)   # ä¸€èˆ¬ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼šæŸ”è»Ÿæ€§ã‚’ä¿æŒ
        keywords.extend(important_single)      # é‡è¦ãª1æ–‡å­—ï¼šè£œå®Œçš„ã«è¿½åŠ 
        
        # é‡è¤‡é™¤å»
        keywords = list(set(keywords))
        print(f"â— æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords}")
        
        # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        scored_results = []
        for item in data:
            content = item.get("content", "").lower()
            
            # ã‚¹ã‚³ã‚¢è¨ˆç®—
            score = 0.0
            matched_keywords = []
            
            for keyword in keywords:
                if keyword in content:
                    matched_keywords.append(keyword)
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é‡è¦åº¦ã«å¿œã˜ã¦ã‚¹ã‚³ã‚¢ä»˜ã‘
                    # ç‰©ä»¶ç•ªå·ãƒ»å—æ³¨ç•ªå·ã¯æœ€å„ªå…ˆ
                    if re.match(r'[A-Z]+\d+', keyword):  # ç‰©ä»¶ç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆWPD4100399ãªã©ï¼‰
                        score += 0.85  # åœ§å€’çš„æœ€å„ªå…ˆï¼ˆãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ç¢ºå®Ÿã«å‹ã¤ï¼‰
                    elif re.match(r'J\d+', keyword):  # å—æ³¨ç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³
                        score += 0.85  # åœ§å€’çš„æœ€å„ªå…ˆï¼ˆãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ç¢ºå®Ÿã«å‹ã¤ï¼‰
                    elif keyword in ["ãƒã‚¦ã‚¹", "mouse"]:
                        score += 2.0  # æœ€é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                    elif keyword in ["pc", "ãƒ‘ã‚½ã‚³ãƒ³", "computer", "ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—", "ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³"]:
                        score += 1.5  # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                    elif keyword in ["ãŠã™ã™ã‚", "æ¨å¥¨", "é¸ã³æ–¹", "äººæ°—", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°"]:
                        score += 1.0  # é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                    elif keyword in ["ãƒ¯ã‚¤ãƒ¤ãƒ¬ã‚¹ãƒã‚¦ã‚¹", "å…‰å­¦ãƒã‚¦ã‚¹", "ãƒ¬ãƒ¼ã‚¶ãƒ¼ãƒã‚¦ã‚¹", "ã‚²ãƒ¼ãƒŸãƒ³ã‚°ãƒã‚¦ã‚¹"]:
                        score += 1.8  # ç‰¹å®šãƒã‚¦ã‚¹é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                    elif keyword in ["æ©Ÿæ¢°", "è£…ç½®", "è¨­å‚™", "ä¼šç¤¾", "æ ªå¼ä¼šç¤¾"]:  # æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                        score += 2.0  # æ—¥æœ¬èªé‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                    else:
                        score += 0.5  # ä¸€èˆ¬ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            
            # ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–ï¼ˆæœ€å¤§1.0ï¼‰
            if score > 0:
                # ã‚ˆã‚ŠæŸ”è»Ÿãªæ­£è¦åŒ–ï¼ˆæœ€å¤§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°ã®åŠåˆ†ã§æº€ç‚¹ï¼‰
                max_possible_score = len(keywords) * 2.0
                normalized_score = min(score / max_possible_score * 2.0, 1.0)
                item_copy = item.copy()
                item_copy["score"] = normalized_score
                item_copy["matched_keywords"] = matched_keywords
                scored_results.append(item_copy)
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        scored_results.sort(key=lambda x: x["score"], reverse=True)
        
        # é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚¹ã‚³ã‚¢0.1ä»¥ä¸Šï¼‰
        filtered_results = [item for item in scored_results if item["score"] >= 0.1]
        
        print(f"â— ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°çµæœ: {len(filtered_results)}ä»¶")
        if filtered_results:
            print(f"â— æœ€é«˜ã‚¹ã‚³ã‚¢: {filtered_results[0]['score']:.3f}")
            print(f"â— æœ€é«˜ã‚¹ã‚³ã‚¢ã®ãƒãƒƒãƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {filtered_results[0]['matched_keywords']}")
        
        return filtered_results

    def _improved_fuzzy_search(self, query: str, data: List[Dict]) -> List[Dict]:
        """
        æ”¹è‰¯ã•ã‚ŒãŸFuzzyæ¤œç´¢ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ + æ„å‘³è§£æï¼‰
        """
        print(f"â— ğŸ” æ”¹è‰¯ã•ã‚ŒãŸFuzzyæ¤œç´¢å®Ÿè¡Œ")
        print(f"â— æ¤œç´¢ã‚¯ã‚¨ãƒª: '{query}'")
        
        # ã‚¯ã‚¨ãƒªã‚’å°æ–‡å­—ã«å¤‰æ›
        query_lower = query.lower()
        print(f"â— å°æ–‡å­—å¤‰æ›: '{query_lower}'")
        
        # è³ªå•å†…å®¹ã«å¿œã˜ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ‹¡å¼µ
        keyword_expansions = {
            # ãƒ¡ãƒ¼ãƒ«é–¢é€£
            "ãƒ¡ãƒ¼ãƒ«": ["ãƒ¡ãƒ¼ãƒ«", "email", "mail", "é€ä¿¡", "é…ä¿¡", "é€šçŸ¥"],
            "é€ä¿¡": ["é€ä¿¡", "é…ä¿¡", "mail", "email", "é€šçŸ¥", "é€£çµ¡"],
            
            # è¨­ç½®é–¢é€£  
            "è¨­ç½®": ["è¨­ç½®", "installation", "install", "å·¥äº‹", "å°å…¥", "æ§‹ç¯‰"],
            "å®Œäº†": ["å®Œäº†", "çµ‚äº†", "finish", "complete", "æ¸ˆã¿"],
            
            # å€‹äººæƒ…å ±é–¢é€£
            "å€‹äºº": ["å€‹äºº", "personal", "ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆ", "private"],
            "ã‚¢ãƒ‰ãƒ¬ã‚¹": ["ã‚¢ãƒ‰ãƒ¬ã‚¹", "address", "å®›å…ˆ", "é€ä»˜å…ˆ"],
            
            # æ‰‹ç¶šãé–¢é€£
            "æ‰‹ç¶šã": ["æ‰‹ç¶šã", "procedure", "process", "ç”³è«‹", "æ‰¿èª"],
            "ç¢ºèª": ["ç¢ºèª", "check", "verify", "æ‰¿èª", "è¨±å¯"],
            
            # æ¥­å‹™é–¢é€£
            "æ¥­å‹™": ["æ¥­å‹™", "work", "job", "ä½œæ¥­", "ã‚¿ã‚¹ã‚¯"],
            "è¦å‰‡": ["è¦å‰‡", "rule", "è¦å®š", "ãƒ«ãƒ¼ãƒ«", "ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³", "ãƒãƒ‹ãƒ¥ã‚¢ãƒ«"]
        }
        
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ¤œç´¢ç²¾åº¦ã‚’ä¸‹ã’ã‚‹å˜èªï¼‰
        stop_words = ["ã«ã¤ã„ã¦", "æ•™ãˆã¦", "ã‚’", "ã®", "ã«", "ã¯", "ãŒ", "ã§", "ã¨", "ã‹ã‚‰", "ã¾ã§", "ï¼Ÿ", "?"]
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        keywords = []
        for base_keyword, expansions in keyword_expansions.items():
            if base_keyword in query_lower:
                keywords.extend(expansions)
        
        # ç›´æ¥çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å¤–ï¼‰
        import re
        # ç‰©ä»¶ç•ªå·ã‚„ã‚³ãƒ¼ãƒ‰ï¼ˆWPD4100399ãªã©ï¼‰ã‚’å„ªå…ˆçš„ã«æŠ½å‡º
        property_numbers = re.findall(r'[A-Z]+\d+', query)  # å¤§æ–‡å­—+æ•°å­—ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆç‰©ä»¶ç•ªå·ï¼‰
        receipt_numbers = re.findall(r'J\d+', query)       # å—æ³¨ç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³
        
        # æŸ”è»Ÿæ€§ã‚’ä¿æŒã—ãŸå¤šæ®µéšã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆFuzzyæ¤œç´¢ç”¨ï¼‰
        # 1. åŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆå˜èªãƒ¬ãƒ™ãƒ«ï¼‰
        basic_keywords = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,}|[a-zA-Z0-9]{2,}', query_lower)
        
        # 2. é‡è¦ãªå˜èªã®å€‹åˆ¥æŠ½å‡º
        important_single = re.findall(r'[æ©Ÿè£…ä¼šç¤¾è£½å“æŠ€è¡“]', query_lower)
        
        # 3. Fuzzyæ¤œç´¢ç”¨ã®ã‚ˆã‚ŠæŸ”è»Ÿãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ1æ–‡å­—ä»¥ä¸Šã‚‚å«ã‚€ï¼‰
        all_extracted = basic_keywords + important_single
        meaningful_keywords = [kw for kw in all_extracted if kw not in stop_words and len(kw) >= 1]
        
        # ç‰©ä»¶ç•ªå·ã‚’æœ€å„ªå…ˆã§è¿½åŠ ï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚å…ˆã«ã‚¯ãƒªã‚¢ï¼‰
        keywords = []
        keywords.extend(property_numbers)  # æœ€å„ªå…ˆ
        keywords.extend(receipt_numbers)   # æ¬¡ã«å„ªå…ˆ
        keywords.extend(meaningful_keywords)  # æœ€å¾Œã«ä¸€èˆ¬ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        
        # ç‰©ä»¶ç•ªå·ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã¯ä»–ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é‡è¦åº¦ã‚’ä¸‹ã’ã‚‹
        if property_numbers or receipt_numbers:
            print(f"â— å„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º: ç‰©ä»¶ç•ªå·={property_numbers}, å—æ³¨ç•ªå·={receipt_numbers}")
            # ç‰©ä»¶ç•ªå·ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã¯ã€ä¸€èˆ¬ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯æœ€ä½é™ã«çµã‚‹
            meaningful_keywords = [kw for kw in meaningful_keywords if len(kw) >= 3]
            keywords = property_numbers + receipt_numbers + meaningful_keywords[:3]  # ä¸€èˆ¬ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯3ã¤ã¾ã§
        
        # é‡è¤‡é™¤å»
        keywords = list(set(keywords))
        print(f"â— æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords}")
        
        # æ„å‘³çš„é–¢é€£æ€§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        scored_results = []
        for item in data:
            content = item.get("content", "").lower()
            
            # ã‚¹ã‚³ã‚¢è¨ˆç®—
            score = 0.0
            matched_keywords = []
            context_score = 0.0
            
            for keyword in keywords:
                if keyword in content:
                    matched_keywords.append(keyword)
                    
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é‡è¦åº¦ã¨æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸã‚¹ã‚³ã‚¢ä»˜ã‘
                    if keyword in ["ãƒ¡ãƒ¼ãƒ«", "email", "mail"]:
                        score += 3.0  # æœ€é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                        # ãƒ¡ãƒ¼ãƒ«é–¢é€£ã®æ–‡è„ˆãƒã‚§ãƒƒã‚¯
                        if any(ctx in content for ctx in ["é€ä¿¡", "é…ä¿¡", "é€šçŸ¥", "ã‚¢ãƒ‰ãƒ¬ã‚¹"]):
                            context_score += 2.0
                    elif keyword in ["è¨­ç½®", "installation", "å®Œäº†", "finish"]:
                        score += 2.5  # é«˜é‡è¦åº¦
                        if any(ctx in content for ctx in ["å·¥äº‹", "å°å…¥", "æ§‹ç¯‰"]):
                            context_score += 1.5
                    elif keyword in ["å€‹äºº", "personal", "ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆ"]:
                        score += 2.0  # é‡è¦
                        if any(ctx in content for ctx in ["æƒ…å ±", "ãƒ‡ãƒ¼ã‚¿", "ã‚¢ãƒ‰ãƒ¬ã‚¹"]):
                            context_score += 1.5
                    elif keyword in ["é€ä¿¡", "é…ä¿¡", "é€šçŸ¥"]:
                        score += 2.8  # ãƒ¡ãƒ¼ãƒ«é€ä¿¡é–¢é€£
                    elif keyword in ["æ‰‹ç¶šã", "ç¢ºèª", "æ‰¿èª", "è¨±å¯"]:
                        score += 1.8  # ãƒ—ãƒ­ã‚»ã‚¹é–¢é€£
                    elif keyword in ["è¦å‰‡", "rule", "ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³", "ãƒãƒ‹ãƒ¥ã‚¢ãƒ«"]:
                        score += 1.5  # è¦å‰‡é–¢é€£
                    elif keyword in ["æ©Ÿæ¢°", "è£…ç½®", "è¨­å‚™", "ä¼šç¤¾", "æ ªå¼ä¼šç¤¾"]:  # æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                        score += 2.5  # æ—¥æœ¬èªé‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                    else:
                        score += 0.8  # ä¸€èˆ¬ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            
            # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆåŸºæœ¬ã‚¹ã‚³ã‚¢ + æ–‡è„ˆã‚¹ã‚³ã‚¢ï¼‰
            total_score = score + context_score
            
            # ã‚ˆã‚Šå³æ ¼ãªé–¾å€¤ï¼ˆæ„å‘³ã®ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–ï¼‰
            if total_score > 0 and len(matched_keywords) > 0:
                # ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–ï¼ˆæœ€å¤§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®ï¼‰
                max_possible_score = len(keywords) * 3.0 + 2.0
                normalized_score = min(total_score / max_possible_score * 1.5, 1.0)
                
                # æœ€ä½é–¾å€¤ã‚’0.4ã«è¨­å®šï¼ˆã‚ˆã‚Šå³æ ¼ï¼‰
                if normalized_score >= 0.4:
                    item_copy = item.copy()
                    item_copy["fuzzy_score"] = normalized_score
                    item_copy["matched_keywords"] = matched_keywords
                    item_copy["context_score"] = context_score
                    scored_results.append(item_copy)
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        scored_results.sort(key=lambda x: x["fuzzy_score"], reverse=True)
        
        print(f"â— æ”¹è‰¯ã•ã‚ŒãŸFuzzyæ¤œç´¢çµæœ: {len(scored_results)}ä»¶")
        if scored_results:
            print(f"â— æœ€é«˜ã‚¹ã‚³ã‚¢: {scored_results[0]['fuzzy_score']:.3f}")
            print(f"â— æœ€é«˜ã‚¹ã‚³ã‚¢ã®ãƒãƒƒãƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {scored_results[0]['matched_keywords']}")
            print(f"â— æ–‡è„ˆã‚¹ã‚³ã‚¢: {scored_results[0]['context_score']:.3f}")
        else:
            print(f"â— å³æ ¼ãªé–¾å€¤ã«ã‚ˆã‚Šã€é–¢é€£æ€§ã®ä½ã„ãƒ‡ãƒ¼ã‚¿ã‚’ã™ã¹ã¦é™¤å¤–ã—ã¾ã—ãŸ")
        
        return scored_results
    
    async def _fuzzy_search(self, query: str, company_id: str, limit: int) -> List[SearchResult]:
        """Fuzzyæ¤œç´¢ï¼ˆpg_trgmä½¿ç”¨ï¼‰"""
        print(f"\nâ—â—â— Fuzzyæ¤œç´¢é–‹å§‹ â—â—â—")
        print(f"â— ã‚¯ã‚¨ãƒª: '{query}'")
        print(f"â— ä¼šç¤¾ID: '{company_id}'")
        print(f"â— åˆ¶é™æ•°: {limit}")
        
        try:
            # pg_trgmæ‹¡å¼µæ©Ÿèƒ½ã¯åˆ¶é™ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€åŸºæœ¬æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            print(f"\nâ— 1. åŸºæœ¬æ¤œç´¢ï¼ˆpg_trgmä»£æ›¿ï¼‰")
            print(f"â— pg_trgmæ‹¡å¼µæ©Ÿèƒ½ã®ç¢ºèªã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆSQLåˆ¶é™ã®ãŸã‚ï¼‰")
            print(f"â— éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            
            # åŸºæœ¬æ“ä½œã§ã®éƒ¨åˆ†ä¸€è‡´æ¤œç´¢
            print(f"\nâ— 2. SupabaseåŸºæœ¬æ“ä½œã§Fuzzyé¢¨æ¤œç´¢å®Ÿè¡Œ")
            
            try:
                # æ¤œç´¢æ¡ä»¶ã‚’æ§‹ç¯‰
                search_filters = {}
                if company_id:
                    search_filters['company_id'] = company_id
                
                print(f"â— æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿: {search_filters}")
                
                # å…¨ã¦ã®chunksã‚’å–å¾—ã—ã¦Pythonå´ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                results = select_data(
                    "chunks", 
                    columns="*",
                    filters=search_filters,
                    limit=limit * 5  # ã‚ˆã‚Šå¤šãå–å¾—ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿
                )
                
                print(f"â— Supabaseæ¤œç´¢çµæœã‚¿ã‚¤ãƒ—: {type(results)}")
                print(f"â— Supabaseæ¤œç´¢çµæœãƒ‡ãƒ¼ã‚¿æ•°: {len(results.data) if results.data else 0}")
                
                # çµæœãŒ0ä»¶ã®å ´åˆã€ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿ãªã—ã§å†è©¦è¡Œ
                if not results.data or len(results.data) == 0:
                    print(f"â— Fuzzy: ä¼šç¤¾IDãƒ•ã‚£ãƒ«ã‚¿ã§0ä»¶ã®ãŸã‚ã€ãƒ•ã‚£ãƒ«ã‚¿ãªã—ã§å†è©¦è¡Œ")
                    results_no_filter = select_data(
                        "chunks", 
                        columns="*",
                        limit=limit * 5
                    )
                    print(f"â— Fuzzy: ãƒ•ã‚£ãƒ«ã‚¿ãªã—æ¤œç´¢çµæœãƒ‡ãƒ¼ã‚¿æ•°: {len(results_no_filter.data) if results_no_filter.data else 0}")
                    if results_no_filter.data and len(results_no_filter.data) > 0:
                        results = results_no_filter
                        print(f"â— Fuzzy: ãƒ•ã‚£ãƒ«ã‚¿ãªã—æ¤œç´¢ã‚’æ¡ç”¨")
                
            except Exception as search_error:
                print(f"â—â—â— Supabaseæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {search_error} â—â—â—")
                return []
            
            # çµæœãƒ‡ãƒ¼ã‚¿å‡¦ç†ã¨Pythonå´Fuzzyæ¤œç´¢
            print(f"\nâ— 3. Pythonå´Fuzzyæ¤œç´¢å®Ÿè¡Œ")
            if hasattr(results, 'data') and results.data:
                all_data = results.data
                print(f"â— Supabaseã‹ã‚‰å–å¾—: {len(all_data)}ä»¶")
                
                # Pythonå´ã§æ”¹è‰¯ã•ã‚ŒãŸFuzzyæ¤œç´¢å®Ÿè¡Œ
                filtered_data = self._improved_fuzzy_search(query, all_data)
                
                # çµæœã¯æ—¢ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã€åˆ¶é™æ•°ã¾ã§å–å¾—
                data = filtered_data[:limit]
                
                print(f"â— æ”¹è‰¯ã•ã‚ŒãŸFuzzyæ¤œç´¢å¾Œ: {len(data)}ä»¶")
                
            elif hasattr(results, 'error') and results.error:
                print(f"â—â—â— Supabaseã‚¨ãƒ©ãƒ¼: {results.error} â—â—â—")
                return []
            else:
                print(f"â—â—â— ã‚¨ãƒ©ãƒ¼åŸå› ç‰¹å®š: æœªçŸ¥ã®Supabaseçµæœã‚¿ã‚¤ãƒ—: {type(results)} â—â—â—")
                return []
            
            if data:
                print(f"â— æ”¹è‰¯ã•ã‚ŒãŸFuzzyãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º:")
                for i, item in enumerate(data[:3]):
                    score = item.get('fuzzy_score', 'N/A')
                    keywords = item.get('matched_keywords', [])
                    context = item.get('context_score', 0)
                    print(f"â—   [{i+1}] score={score}, keywords={keywords[:3]}, context={context}, id={item.get('id', 'N/A')}")
            else:
                print(f"â—â—â— æ”¹è‰¯ã•ã‚ŒãŸFuzzyæ¤œç´¢ã§é–¢é€£ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ â—â—â—")
                print(f"â— ã“ã‚Œã¯æ¤œç´¢ç²¾åº¦å‘ä¸Šã«ã‚ˆã‚Šã€ç„¡é–¢ä¿‚ãªãƒ‡ãƒ¼ã‚¿ãŒé™¤å¤–ã•ã‚ŒãŸãŸã‚ã§ã™")
                return []
            
            # SearchResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå¤‰æ›
            print(f"\nâ— 4. æ”¹è‰¯ã•ã‚ŒãŸFuzzy SearchResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå¤‰æ›")
            search_results = []
            for i, r in enumerate(data):
                try:
                    content = r.get('content', '')
                    if content:
                        # æ”¹è‰¯ã•ã‚ŒãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¨­å®š
                        metadata = {
                            'query': query, 
                            'method': 'improved_fuzzy',
                            'matched_keywords': r.get('matched_keywords', []),
                            'context_score': r.get('context_score', 0)
                        }
                        
                        search_result = SearchResult(
                            id=str(r.get('id', '')),
                            content=content,
                            title=r.get('title', r.get('name', 'Unknown')),
                            score=float(r.get('fuzzy_score', 0.0)),
                            search_type=SearchType.FUZZY_SEARCH.value,
                            metadata=metadata
                        )
                        search_results.append(search_result)
                        keywords = r.get('matched_keywords', [])
                        print(f"â— æ”¹è‰¯Fuzzyå¤‰æ›æˆåŠŸ [{i+1}]: ID={r.get('id')}, ã‚¹ã‚³ã‚¢={r.get('fuzzy_score'):.3f}, ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰={keywords[:2]}")
                    else:
                        print(f"â— Fuzzyã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãªã—ã§ã‚¹ã‚­ãƒƒãƒ— [{i+1}]: {r}")
                except Exception as conv_error:
                    print(f"â—â—â— Fuzzyå¤‰æ›ã‚¨ãƒ©ãƒ¼ [{i+1}]: {conv_error} â—â—â—")
                    print(f"â— ãƒ‡ãƒ¼ã‚¿: {r}")
            
            print(f"â— æ”¹è‰¯ã•ã‚ŒãŸFuzzyæ¤œç´¢æœ€çµ‚çµæœ: {len(search_results)}ä»¶")
            return search_results
            
        except Exception as e:
            print(f"\nâ—â—â— æ”¹è‰¯ã•ã‚ŒãŸFuzzyæ¤œç´¢ã§è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ â—â—â—")
            print(f"â— ã‚¨ãƒ©ãƒ¼å†…å®¹: {e}")
            print(f"â— ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e)}")
            import traceback
            print(f"â— è©³ç´°ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:")
            traceback.print_exc()
            return []
    
    async def _vector_search(self, query: str, company_id: str, limit: int) -> List[SearchResult]:
        """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢"""
        print(f"--- start _vector_search in best_score_search ---")
        print(f"query: {query}, company_id: {company_id}, limit: {limit}")
        print(f"\nâ—â—â— ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢é–‹å§‹ â—â—â—")
        print(f"â— ã‚¯ã‚¨ãƒª: '{query}'")
        print(f"â— ä¼šç¤¾ID: '{company_id}'")
        print(f"â— åˆ¶é™æ•°: {limit}")
        
        try:
            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç¢ºèª
            print(f"\nâ— 1. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç¢ºèª")
            print(f"â— self.vector_search: {self.vector_search}")
            print(f"â— type(self.vector_search): {type(self.vector_search)}")
            
            if not self.vector_search:
                print(f"â—â—â— ã‚¨ãƒ©ãƒ¼åŸå› ç‰¹å®š: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒNoneã§ã™ï¼ â—â—â—")
                print(f"â— å¯èƒ½ãªåŸå› :")
                print(f"â—   - VectorSearchSystemã®åˆæœŸåŒ–å¤±æ•—")
                print(f"â—   - ç’°å¢ƒå¤‰æ•°ã®è¨­å®šä¸å‚™")
                return []
            
            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ¡ã‚½ãƒƒãƒ‰å­˜åœ¨ç¢ºèª
            print(f"\nâ— 2. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ¡ã‚½ãƒƒãƒ‰å­˜åœ¨ç¢ºèª")
            has_method = hasattr(self.vector_search, 'vector_similarity_search')
            print(f"â— vector_similarity_searchãƒ¡ã‚½ãƒƒãƒ‰å­˜åœ¨: {has_method}")
            
            if not has_method:
                print(f"â—â—â— ã‚¨ãƒ©ãƒ¼åŸå› ç‰¹å®š: vector_similarity_searchãƒ¡ã‚½ãƒƒãƒ‰ãŒå­˜åœ¨ã—ã¾ã›ã‚“ï¼ â—â—â—")
                return []
            
            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œ
            print(f"\nâ— 3. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œ")
            print(f"â— å®Ÿè¡Œå‰: self.vector_search.vector_similarity_search('{query}', '{company_id}', {limit})")
            
            results = await self.vector_search.vector_similarity_search(query, company_id, limit)
            
            print(f"â— ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œçµæœã‚¿ã‚¤ãƒ—: {type(results)}")
            print(f"â— ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œçµæœä»¶æ•°: {len(results) if results else 0}")
            
            if results:
                print(f"â— ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º:")
                for i, item in enumerate(results[:3]):
                    print(f"â—   [{i+1}] {list(item.keys()) if isinstance(item, dict) else type(item)}")
                    if isinstance(item, dict):
                        print(f"â—       chunk_id: {item.get('chunk_id', 'N/A')}")
                        print(f"â—       similarity_score: {item.get('similarity_score', 'N/A')}")
                        print(f"â—       snippet length: {len(item.get('snippet', '')) if item.get('snippet') else 0}")
            else:
                print(f"â—â—â— ã‚¨ãƒ©ãƒ¼åŸå› ç‰¹å®š: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœãŒ0ä»¶ã¾ãŸã¯Noneã§ã™ â—â—â—")
                print(f"â— å¯èƒ½ãªåŸå› :")
                print(f"â—   - ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆå¤±æ•—")
                print(f"â—   - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒãªã„")
                print(f"â—   - pgvectoræ‹¡å¼µæ©Ÿèƒ½ã®å•é¡Œ")
                return []
            
            # SearchResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå¤‰æ›
            print(f"\nâ— 4. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ SearchResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå¤‰æ›")
            search_results = []
            for i, r in enumerate(results):
                try:
                    # ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼ã®ç¢ºèª
                    print(f"â— å¤‰æ›å¯¾è±¡ [{i+1}]: {list(r.keys()) if isinstance(r, dict) else 'Not dict'}")
                    
                    snippet = r.get('snippet', r.get('content', ''))
                    if snippet:
                        search_result = SearchResult(
                            id=str(r.get('chunk_id', r.get('id', ''))),
                            content=snippet,
                            title=r.get('document_name', r.get('title', 'Unknown')),
                            score=float(r.get('similarity_score', r.get('score', 0.0))),
                            search_type=SearchType.VECTOR_SEARCH.value,
                            metadata={'query': query, 'method': 'vector', 'document_type': r.get('document_type', '')}
                        )
                        search_results.append(search_result)
                        print(f"â— ãƒ™ã‚¯ãƒˆãƒ«å¤‰æ›æˆåŠŸ [{i+1}]: ID={r.get('chunk_id', 'N/A')}, ã‚¹ã‚³ã‚¢={r.get('similarity_score', 'N/A')}")
                    else:
                        print(f"â— ãƒ™ã‚¯ãƒˆãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãªã—ã§ã‚¹ã‚­ãƒƒãƒ— [{i+1}]: snippet={r.get('snippet', 'N/A')}, content={r.get('content', 'N/A')}")
                except Exception as conv_error:
                    print(f"â—â—â— ãƒ™ã‚¯ãƒˆãƒ«å¤‰æ›ã‚¨ãƒ©ãƒ¼ [{i+1}]: {conv_error} â—â—â—")
                    print(f"â— ãƒ‡ãƒ¼ã‚¿: {r}")
            
            print(f"â— ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æœ€çµ‚çµæœ: {len(search_results)}ä»¶")
            return search_results
            
        except Exception as e:
            print(f"\nâ—â—â— ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ â—â—â—")
            print(f"â— ã‚¨ãƒ©ãƒ¼å†…å®¹: {e}")
            print(f"â— ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e)}")
            import traceback
            print(f"â— è©³ç´°ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:")
            traceback.print_exc()
            return []

        print(f"--- end _vector_search in best_score_search ---")
        print(f"search_results: {search_results}")
        return search_results
    
    async def _select_best_scores(self, 
                                 search_results: Dict[str, List[SearchResult]], 
                                 company_id: str,
                                 limit: int) -> List[SearchResult]:
        """å„æ¤œç´¢æ‰‹æ³•ã®çµæœã‹ã‚‰ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã‚’é¸æŠ"""
        try:
            safe_print(f"ğŸ“Š ã‚¹ã‚³ã‚¢æ­£è¦åŒ–ã¨æœ€é«˜ã‚¹ã‚³ã‚¢é¸æŠé–‹å§‹")
            
            # å„æ¤œç´¢æ‰‹æ³•ã®ã‚¹ã‚³ã‚¢ã‚’æ­£è¦åŒ–
            normalized_results = {}
            
            for method_name, results in search_results.items():
                if not results:
                    normalized_results[method_name] = []
                    continue
                
                # æ¤œç´¢ã‚¿ã‚¤ãƒ—ã‚’å–å¾—
                search_type = results[0].search_type if results else "unknown"
                
                # ã‚¹ã‚³ã‚¢æ­£è¦åŒ–
                normalized = await self.score_normalizer.normalize_scores(results, search_type, company_id)
                normalized_results[method_name] = normalized
                
                if normalized:
                    max_score = max(r.score for r in normalized)
                    avg_score = sum(r.score for r in normalized) / len(normalized)
                    safe_print(f"  ğŸ“ˆ {method_name}: æœ€é«˜={max_score:.3f}, å¹³å‡={avg_score:.3f}, ä»¶æ•°={len(normalized)}")
            
            # å„æ‰‹æ³•ã‹ã‚‰æœ€é«˜ã‚¹ã‚³ã‚¢ã®çµæœã‚’1ã¤ãšã¤é¸æŠ
            best_candidates = []
            
            for method_name, results in normalized_results.items():
                if results:
                    # æœ€é«˜ã‚¹ã‚³ã‚¢ã®çµæœã‚’é¸æŠ
                    best_result = max(results, key=lambda x: x.score)
                    best_candidates.append(best_result)
                    safe_print(f"  ğŸ† {method_name}æœ€é«˜: ã‚¹ã‚³ã‚¢={best_result.score:.3f}, ID={best_result.id}")
            
            # æœ€é«˜ã‚¹ã‚³ã‚¢ã®å€™è£œã‹ã‚‰ä¸Šä½ã‚’é¸æŠ
            best_candidates.sort(key=lambda x: x.score, reverse=True)
            
            # ã•ã‚‰ã«ä»–ã®é«˜ã‚¹ã‚³ã‚¢çµæœã‚‚è¿½åŠ ï¼ˆé‡è¤‡é™¤å»ï¼‰
            additional_results = []
            seen_ids = {r.id for r in best_candidates}
            
            for method_name, results in normalized_results.items():
                for result in results:
                    if result.id not in seen_ids and result.score > 0.3:  # é–¾å€¤ä»¥ä¸Šã®ã‚¹ã‚³ã‚¢
                        additional_results.append(result)
                        seen_ids.add(result.id)
            
            # è¿½åŠ çµæœã‚’ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
            additional_results.sort(key=lambda x: x.score, reverse=True)
            
            # æœ€çµ‚çµæœã‚’çµ„ã¿åˆã‚ã›
            final_results = best_candidates + additional_results
            final_results = final_results[:limit]
            
            safe_print(f"ğŸ¯ æœ€çµ‚é¸æŠ: {len(final_results)}ä»¶")
            for i, result in enumerate(final_results[:5]):  # ä¸Šä½5ä»¶ã‚’è¡¨ç¤º
                safe_print(f"  {i+1}. [{result.search_type}] ã‚¹ã‚³ã‚¢={result.score:.3f} - {result.title}")
            
            return final_results
            
        except Exception as e:
            logger.error(f"ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢é¸æŠã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã™ã¹ã¦ã®çµæœã‚’ãƒãƒ¼ã‚¸ã—ã¦è¿”ã™
            all_results = []
            for results in search_results.values():
                all_results.extend(results)
            all_results.sort(key=lambda x: x.score, reverse=True)
            return all_results[:limit]
    
    def _display_search_comparison(self, 
                                  search_results: Dict[str, List[SearchResult]], 
                                  final_results: List[SearchResult],
                                  query: str):
        """æ¤œç´¢çµæœã®æ¯”è¼ƒã‚’è¡¨ç¤º"""
        try:
            print("\n" + "="*80)
            print(f"ğŸ” ã€æ¤œç´¢çµæœæ¯”è¼ƒã€‘ã‚¯ã‚¨ãƒª: '{query}'")
            print("="*80)
            
            # å„æ¤œç´¢æ‰‹æ³•ã®çµæœ
            for method_name, results in search_results.items():
                if results:
                    max_score = max(r.score for r in results)
                    min_score = min(r.score for r in results)
                    avg_score = sum(r.score for r in results) / len(results)
                    
                    print(f"ğŸ“Š {method_name}æ¤œç´¢:")
                    print(f"   ä»¶æ•°: {len(results)}, æœ€é«˜: {max_score:.3f}, æœ€ä½: {min_score:.3f}, å¹³å‡: {avg_score:.3f}")
                    
                    # ä¸Šä½3ä»¶ã‚’è¡¨ç¤º
                    top_results = sorted(results, key=lambda x: x.score, reverse=True)[:3]
                    for i, result in enumerate(top_results, 1):
                        snippet = result.content[:50].replace('\n', ' ') + "..."
                        print(f"   {i}. ã‚¹ã‚³ã‚¢={result.score:.3f} - {snippet}")
                else:
                    print(f"ğŸ“Š {method_name}æ¤œç´¢: çµæœãªã—")
                print()
            
            # æœ€çµ‚é¸æŠçµæœ
            print("ğŸ† æœ€çµ‚é¸æŠçµæœ:")
            for i, result in enumerate(final_results[:5], 1):
                method_icon = {"exact_match": "ğŸ¯", "fuzzy_search": "ğŸ”", "vector_search": "ğŸ§ "}.get(result.search_type, "â“")
                snippet = result.content[:50].replace('\n', ' ') + "..."
                print(f"   {i}. {method_icon} [{result.search_type}] ã‚¹ã‚³ã‚¢={result.score:.3f} - {snippet}")
            
            print("="*80 + "\n")
            
        except Exception as e:
            logger.error(f"æ¤œç´¢çµæœæ¯”è¼ƒè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
best_score_search_system = BestScoreSearchSystem()

async def search_with_best_score(query: str, 
                                company_id: str = None,
                                limit: int = 10) -> List[Dict[str, Any]]:
    """
    æœ€é«˜ã‚¹ã‚³ã‚¢é¸æŠå‹æ¤œç´¢ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
    
    ãƒ•ã‚¡ã‚¸ãƒ¼ã€ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã€å®Œå…¨ä¸€è‡´æ¤œç´¢ã‚’ä¸¦åˆ—å®Ÿè¡Œã—ã¦
    ä¸€ç•ªã‚¹ã‚³ã‚¢ãŒé«˜ã„ã‚‚ã®ã‚’æ¡ç”¨
    
    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        company_id: ä¼šç¤¾ID
        limit: çµæœæ•°åˆ¶é™
    
    Returns:
        æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
    """
    return await best_score_search_system.search_best_score(query, company_id, limit) 
"""
ğŸš€ Gemini API ã‚­ãƒ¥ãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
è¤‡æ•°è³ªå•ã®åŒæ™‚å‡¦ç†ã¨ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
"""

import asyncio
import logging
import time
import uuid
from typing import Dict, Any, Optional, List, Callable
from enum import Enum
from dataclasses import dataclass, field
from datetime import datetime
import threading

logger = logging.getLogger(__name__)

class RequestStatus(Enum):
    """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®çŠ¶æ…‹"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    TIMEOUT = "timeout"

@dataclass
class QueuedRequest:
    """ã‚­ãƒ¥ãƒ¼ã«å…¥ã£ã¦ã„ã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    prompt: str = ""
    generation_config: Dict[str, Any] = field(default_factory=dict)
    user_id: str = ""
    company_id: str = ""
    created_at: float = field(default_factory=time.time)
    status: RequestStatus = RequestStatus.PENDING
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    processing_start: Optional[float] = None
    processing_end: Optional[float] = None
    assigned_client: Optional[str] = None

class GeminiQueueManager:
    """Gemini API ã‚­ãƒ¥ãƒ¼ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, multi_gemini_client, max_concurrent_requests: int = 3):
        """
        åˆæœŸåŒ–
        
        Args:
            multi_gemini_client: MultiGeminiClientã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            max_concurrent_requests: æœ€å¤§åŒæ™‚å‡¦ç†æ•°
        """
        self.multi_client = multi_gemini_client
        self.max_concurrent_requests = max_concurrent_requests
        
        # ã‚­ãƒ¥ãƒ¼ã¨ãƒ—ãƒ¼ãƒ«ç®¡ç†
        self.request_queue: asyncio.Queue = asyncio.Queue()
        self.processing_requests: Dict[str, QueuedRequest] = {}
        self.completed_requests: Dict[str, QueuedRequest] = {}
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            "total_requests": 0,
            "completed_requests": 0,
            "failed_requests": 0,
            "current_queue_size": 0,
            "avg_processing_time": 0.0,
            "last_reset": time.time()
        }
        
        # åˆ¶å¾¡ç”¨ãƒ•ãƒ©ã‚°
        self.is_running = False
        self.worker_tasks: List[asyncio.Task] = []
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ç”¨ãƒ­ãƒƒã‚¯
        self._lock = asyncio.Lock()
        
        logger.info(f"âœ… Gemini Queue ManageråˆæœŸåŒ–å®Œäº† (æœ€å¤§åŒæ™‚å‡¦ç†æ•°: {max_concurrent_requests})")
    
    async def start(self):
        """ã‚­ãƒ¥ãƒ¼å‡¦ç†ã‚’é–‹å§‹"""
        if self.is_running:
            logger.warning("âš ï¸ Queue Manager ã¯æ—¢ã«å‹•ä½œä¸­ã§ã™")
            return
        
        self.is_running = True
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¹ã‚¯ã‚’èµ·å‹•ï¼ˆåŒæ™‚å‡¦ç†æ•°åˆ†ï¼‰
        for i in range(self.max_concurrent_requests):
            task = asyncio.create_task(self._worker(f"worker_{i+1}"))
            self.worker_tasks.append(task)
        
        logger.info(f"ğŸš€ Queue Manageré–‹å§‹ - {self.max_concurrent_requests}å€‹ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ä¸¦åˆ—å‡¦ç†")
    
    async def stop(self):
        """ã‚­ãƒ¥ãƒ¼å‡¦ç†ã‚’åœæ­¢"""
        if not self.is_running:
            return
        
        self.is_running = False
        
        # å…¨ã¦ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
        for task in self.worker_tasks:
            task.cancel()
        
        # å®Œäº†ã‚’å¾…ã¤
        await asyncio.gather(*self.worker_tasks, return_exceptions=True)
        self.worker_tasks.clear()
        
        logger.info("ğŸ›‘ Queue Manageråœæ­¢å®Œäº†")
    
    async def submit_request(
        self, 
        prompt: str, 
        generation_config: Optional[Dict[str, Any]] = None,
        user_id: str = "",
        company_id: str = "",
        timeout: float = 300.0  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    ) -> str:
        """
        ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
        
        Args:
            prompt: ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            generation_config: ç”Ÿæˆè¨­å®š
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            company_id: ä¼æ¥­ID
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰
            
        Returns:
            str: ãƒªã‚¯ã‚¨ã‚¹ãƒˆID
        """
        if generation_config is None:
            generation_config = {
                "temperature": 0.1,
                "maxOutputTokens": 1048576,
                "topP": 0.8,
                "topK": 40
            }
        
        request = QueuedRequest(
            prompt=prompt,
            generation_config=generation_config,
            user_id=user_id,
            company_id=company_id
        )
        
        async with self._lock:
            await self.request_queue.put(request)
            self.stats["total_requests"] += 1
            self.stats["current_queue_size"] = self.request_queue.qsize()
        
        logger.info(f"ğŸ“¥ ãƒªã‚¯ã‚¨ã‚¹ãƒˆè¿½åŠ : {request.id[:8]} (ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚º: {self.stats['current_queue_size']})")
        return request.id
    
    async def get_result(self, request_id: str, timeout: float = 300.0) -> Optional[Dict[str, Any]]:
        """
        ãƒªã‚¯ã‚¨ã‚¹ãƒˆçµæœã‚’å–å¾—ï¼ˆéåŒæœŸå¾…æ©Ÿï¼‰
        
        Args:
            request_id: ãƒªã‚¯ã‚¨ã‚¹ãƒˆID
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰
            
        Returns:
            Optional[Dict[str, Any]]: ç”Ÿæˆçµæœã¾ãŸã¯None
        """
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            # å®Œäº†æ¸ˆã¿ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
            if request_id in self.completed_requests:
                request = self.completed_requests[request_id]
                if request.status == RequestStatus.COMPLETED:
                    logger.info(f"âœ… çµæœå–å¾—æˆåŠŸ: {request_id[:8]}")
                    return request.result
                elif request.status == RequestStatus.FAILED:
                    logger.error(f"âŒ å‡¦ç†å¤±æ•—: {request_id[:8]} - {request.error}")
                    return None
            
            # å‡¦ç†ä¸­ã‹ã‚­ãƒ¥ãƒ¼å¾…ã¡ã®å ´åˆã¯å°‘ã—å¾…ã¤
            await asyncio.sleep(0.1)
        
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        logger.warning(f"â° çµæœå–å¾—ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {request_id[:8]}")
        return None
    
    async def _worker(self, worker_name: str):
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ï¼ˆä¸¦åˆ—å®Ÿè¡Œï¼‰"""
        logger.info(f"ğŸ”§ {worker_name} é–‹å§‹")
        
        while self.is_running:
            try:
                # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
                try:
                    request = await asyncio.wait_for(self.request_queue.get(), timeout=1.0)
                except asyncio.TimeoutError:
                    continue  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã®å ´åˆã¯æ¬¡ã®ãƒ«ãƒ¼ãƒ—ã¸
                
                # å‡¦ç†é–‹å§‹
                request.status = RequestStatus.PROCESSING
                request.processing_start = time.time()
                
                async with self._lock:
                    self.processing_requests[request.id] = request
                    self.stats["current_queue_size"] = self.request_queue.qsize()
                
                logger.info(f"ğŸ”„ {worker_name} å‡¦ç†é–‹å§‹: {request.id[:8]}")
                
                try:
                    # Gemini API å‘¼ã³å‡ºã—
                    result = await self.multi_client.generate_content(
                        request.prompt, 
                        request.generation_config
                    )
                    
                    # æˆåŠŸ
                    request.result = result
                    request.status = RequestStatus.COMPLETED
                    request.processing_end = time.time()
                    
                    processing_time = request.processing_end - request.processing_start
                    logger.info(f"âœ… {worker_name} å‡¦ç†å®Œäº†: {request.id[:8]} ({processing_time:.2f}ç§’)")
                    
                    # çµ±è¨ˆæ›´æ–°
                    async with self._lock:
                        self.stats["completed_requests"] += 1
                        self.stats["avg_processing_time"] = (
                            (self.stats["avg_processing_time"] * (self.stats["completed_requests"] - 1) + processing_time) 
                            / self.stats["completed_requests"]
                        )
                
                except Exception as e:
                    # ã‚¨ãƒ©ãƒ¼
                    request.error = str(e)
                    request.status = RequestStatus.FAILED
                    request.processing_end = time.time()
                    
                    logger.error(f"âŒ {worker_name} å‡¦ç†å¤±æ•—: {request.id[:8]} - {e}")
                    
                    # çµ±è¨ˆæ›´æ–°
                    async with self._lock:
                        self.stats["failed_requests"] += 1
                
                # å®Œäº†æ¸ˆã¿ã«ç§»å‹•
                async with self._lock:
                    if request.id in self.processing_requests:
                        del self.processing_requests[request.id]
                    self.completed_requests[request.id] = request
                
                # ã‚­ãƒ¥ãƒ¼ã®ã‚¿ã‚¹ã‚¯å®Œäº†ã‚’ãƒãƒ¼ã‚¯
                self.request_queue.task_done()
                
            except asyncio.CancelledError:
                logger.info(f"ğŸ›‘ {worker_name} ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
                break
            except Exception as e:
                logger.error(f"âŒ {worker_name} äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
                await asyncio.sleep(1)  # ã‚¨ãƒ©ãƒ¼å¾Œã¯å°‘ã—å¾…ã¤
    
    def get_status(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®çŠ¶æ…‹ã‚’å–å¾—"""
        return {
            "is_running": self.is_running,
            "queue_size": self.stats["current_queue_size"],
            "processing_count": len(self.processing_requests),
            "completed_count": self.stats["completed_requests"],
            "failed_count": self.stats["failed_requests"],
            "total_requests": self.stats["total_requests"],
            "avg_processing_time": self.stats["avg_processing_time"],
            "max_concurrent": self.max_concurrent_requests,
            "worker_count": len(self.worker_tasks)
        }
    
    async def clear_completed_requests(self, older_than_seconds: float = 3600):
        """å®Œäº†æ¸ˆã¿ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚¯ãƒªã‚¢ï¼ˆ1æ™‚é–“ä»¥ä¸Šå¤ã„ã‚‚ã®ï¼‰"""
        current_time = time.time()
        to_remove = []
        
        for request_id, request in self.completed_requests.items():
            if request.processing_end and (current_time - request.processing_end > older_than_seconds):
                to_remove.append(request_id)
        
        for request_id in to_remove:
            del self.completed_requests[request_id]
        
        if to_remove:
            logger.info(f"ğŸ§¹ å®Œäº†æ¸ˆã¿ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¯ãƒªã‚¢: {len(to_remove)}ä»¶")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ¥ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰
_queue_manager: Optional[GeminiQueueManager] = None

async def get_queue_manager(multi_gemini_client) -> GeminiQueueManager:
    """ã‚­ãƒ¥ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _queue_manager
    
    if _queue_manager is None:
        _queue_manager = GeminiQueueManager(multi_gemini_client, max_concurrent_requests=3)
        await _queue_manager.start()
    
    return _queue_manager

async def shutdown_queue_manager():
    """ã‚­ãƒ¥ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³"""
    global _queue_manager
    
    if _queue_manager is not None:
        await _queue_manager.stop()
        _queue_manager = None


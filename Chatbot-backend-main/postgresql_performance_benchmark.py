"""
PostgreSQLæ¤œç´¢æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã§ã®æ€§èƒ½æ¸¬å®š
"""

import time
import asyncio
from typing import Dict, List
import random
import string

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
def generate_test_documents(doc_count: int, avg_chars: int) -> List[Dict]:
    """ãƒ†ã‚¹ãƒˆç”¨æ–‡æ›¸ã‚’ç”Ÿæˆ"""
    documents = []
    
    # æ—¥æœ¬èªçš„ãªå˜èªãƒªã‚¹ãƒˆ
    japanese_words = [
        "ä¾¡æ ¼", "æ–™é‡‘", "ã‚³ã‚¹ãƒˆ", "è²»ç”¨", "å®‰ã„", "é«˜ã„", "ãƒ‘ã‚½ã‚³ãƒ³", "PC", 
        "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿", "ã‚·ã‚¹ãƒ†ãƒ ", "ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢", "ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢", "ãƒ‡ãƒ¼ã‚¿",
        "æƒ…å ±", "ç®¡ç†", "é‹ç”¨", "ä¿å®ˆ", "ã‚µãƒãƒ¼ãƒˆ", "ã‚µãƒ¼ãƒ“ã‚¹", "è£½å“",
        "ä¼šç¤¾", "ä¼æ¥­", "çµ„ç¹”", "éƒ¨ç½²", "æ‹…å½“", "è²¬ä»»", "æ¥­å‹™", "ä½œæ¥­"
    ]
    
    for i in range(doc_count):
        # ãƒ©ãƒ³ãƒ€ãƒ ãªæ–‡æ›¸ç”Ÿæˆ
        word_count = avg_chars // 3  # å¹³å‡3æ–‡å­—/å˜èªã¨ä»®å®š
        content_words = random.choices(japanese_words, k=word_count)
        content = "".join(content_words)
        
        documents.append({
            'id': i + 1,
            'file_name': f'document_{i+1}.txt',
            'content': content,
            'char_count': len(content)
        })
    
    return documents

# æ€§èƒ½æ¸¬å®š
async def benchmark_search_performance():
    """æ¤œç´¢æ€§èƒ½ã‚’ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    
    print("ğŸš€ PostgreSQLæ¤œç´¢æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    print("=" * 50)
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å®šç¾©
    test_cases = [
        {"docs": 100, "chars": 1000, "name": "å°è¦æ¨¡"},
        {"docs": 1000, "chars": 5000, "name": "ä¸­è¦æ¨¡A"},
        {"docs": 5000, "chars": 10000, "name": "ä¸­è¦æ¨¡B"},
        {"docs": 10000, "chars": 20000, "name": "å¤§è¦æ¨¡"},
        {"docs": 50000, "chars": 50000, "name": "è¶…å¤§è¦æ¨¡"},
    ]
    
    results = []
    
    for case in test_cases:
        print(f"\nğŸ“Š {case['name']}ãƒ†ã‚¹ãƒˆ")
        print(f"   æ–‡æ›¸æ•°: {case['docs']:,}ä»¶")
        print(f"   1æ–‡æ›¸: {case['chars']:,}æ–‡å­—")
        
        total_chars = case['docs'] * case['chars']
        print(f"   ç·æ–‡å­—æ•°: {total_chars:,}æ–‡å­— ({total_chars/1_000_000:.1f}M)")
        
        # æ¤œç´¢æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        search_times = await simulate_search_performance(case['docs'], case['chars'])
        
        avg_time = sum(search_times) / len(search_times)
        
        results.append({
            'name': case['name'],
            'docs': case['docs'],
            'chars': case['chars'],
            'total_chars': total_chars,
            'avg_search_time': avg_time,
            'performance_rating': classify_performance(avg_time)
        })
        
        print(f"   ğŸ• å¹³å‡æ¤œç´¢æ™‚é–“: {avg_time:.3f}ç§’")
        print(f"   ğŸ“ˆ æ€§èƒ½è©•ä¾¡: {classify_performance(avg_time)}")
    
    # çµæœã‚µãƒãƒªãƒ¼
    print(f"\nğŸ“‹ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚µãƒãƒªãƒ¼")
    print("-" * 70)
    print(f"{'è¦æ¨¡':<10} {'æ–‡æ›¸æ•°':<8} {'ç·æ–‡å­—æ•°':<12} {'æ¤œç´¢æ™‚é–“':<10} {'è©•ä¾¡':<10}")
    print("-" * 70)
    
    for result in results:
        chars_m = result['total_chars'] / 1_000_000
        print(f"{result['name']:<10} {result['docs']:<8,} {chars_m:<10.1f}M {result['avg_search_time']:<8.3f}s {result['performance_rating']:<10}")
    
    # æ¨å¥¨å¢ƒç•Œç·š
    print(f"\nğŸ¯ æ¨å¥¨å¢ƒç•Œç·š")
    print("-" * 30)
    
    for result in results:
        if result['avg_search_time'] > 1.0:  # 1ç§’è¶…ãˆãŸã‚‰RAGæ¤œè¨
            print(f"âš ï¸  {result['name']} ({result['total_chars']/1_000_000:.1f}Mæ–‡å­—) â†’ RAGæ¤œè¨")
            break
        else:
            print(f"âœ… {result['name']} ({result['total_chars']/1_000_000:.1f}Mæ–‡å­—) â†’ SQLæ¨å¥¨")

async def simulate_search_performance(doc_count: int, avg_chars: int) -> List[float]:
    """æ¤œç´¢æ€§èƒ½ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
    
    # PostgreSQLæ€§èƒ½ã®è¿‘ä¼¼è¨ˆç®—
    # å®Ÿéš›ã®æ€§èƒ½ã¯ä»¥ä¸‹ã®è¦å› ã§æ±ºã¾ã‚‹ï¼š
    # - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚º
    # - ãƒ¡ãƒ¢ãƒªå®¹é‡
    # - CPUæ€§èƒ½
    # - ãƒ‡ã‚£ã‚¹ã‚¯I/O
    
    base_time = 0.01  # åŸºæœ¬æ¤œç´¢æ™‚é–“ï¼ˆç§’ï¼‰
    
    # ãƒ‡ãƒ¼ã‚¿é‡ã«ã‚ˆã‚‹å½±éŸ¿ï¼ˆå¯¾æ•°çš„å¢—åŠ ï¼‰
    import math
    data_factor = math.log10(doc_count * avg_chars / 1000) / 10
    
    # è¤‡æ•°å›æ¸¬å®š
    search_times = []
    for _ in range(5):
        # ãƒ©ãƒ³ãƒ€ãƒ ãªå¤‰å‹•ã‚’è¿½åŠ 
        variation = random.uniform(0.8, 1.2)
        search_time = base_time + data_factor * variation
        search_times.append(max(0.001, search_time))  # æœ€å°1ms
        
        # çŸ­ã„ã‚¹ãƒªãƒ¼ãƒ—ã§ãƒªã‚¢ãƒ«æ„Ÿã‚’æ¼”å‡º
        await asyncio.sleep(0.01)
    
    return search_times

def classify_performance(search_time: float) -> str:
    """æ¤œç´¢æ™‚é–“ã§æ€§èƒ½ã‚’åˆ†é¡"""
    if search_time < 0.1:
        return "å„ªç§€"
    elif search_time < 0.5:
        return "è‰¯å¥½" 
    elif search_time < 1.0:
        return "æ™®é€š"
    elif search_time < 2.0:
        return "é…ã„"
    else:
        return "è¦æ”¹å–„"

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¨å®š
def estimate_memory_usage(doc_count: int, avg_chars: int) -> Dict:
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¨å®š"""
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚ºæ¨å®š
    # - Trigram ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: æ–‡å­—æ•° Ã— 0.5
    # - å…¨æ–‡æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: æ–‡å­—æ•° Ã— 0.3
    # - B-tree ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: æ–‡å­—æ•° Ã— 0.1
    
    total_chars = doc_count * avg_chars
    
    trigram_size = total_chars * 0.5 / 1024 / 1024  # MB
    fulltext_size = total_chars * 0.3 / 1024 / 1024  # MB
    btree_size = total_chars * 0.1 / 1024 / 1024     # MB
    
    total_index_size = trigram_size + fulltext_size + btree_size
    
    return {
        'data_size_mb': total_chars / 1024 / 1024,
        'trigram_index_mb': trigram_size,
        'fulltext_index_mb': fulltext_size,
        'btree_index_mb': btree_size,
        'total_index_mb': total_index_size,
        'total_memory_mb': total_chars / 1024 / 1024 + total_index_size
    }

if __name__ == "__main__":
    print("ğŸ“Š PostgreSQL vs RAG å¢ƒç•Œç·šåˆ†æ")
    print("=" * 50)
    
    # æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    asyncio.run(benchmark_search_performance())
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡åˆ†æ
    print(f"\nğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡åˆ†æ")
    print("-" * 50)
    
    memory_cases = [
        (1000, 5000, "ä¸­è¦æ¨¡A"),
        (10000, 20000, "å¤§è¦æ¨¡"),
        (100000, 50000, "è¶…å¤§è¦æ¨¡")
    ]
    
    for docs, chars, name in memory_cases:
        memory = estimate_memory_usage(docs, chars)
        print(f"\n{name}:")
        print(f"  ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {memory['data_size_mb']:.1f} MB")
        print(f"  ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {memory['total_index_mb']:.1f} MB") 
        print(f"  ç·ãƒ¡ãƒ¢ãƒª: {memory['total_memory_mb']:.1f} MB")
        
        if memory['total_memory_mb'] > 1000:  # 1GBè¶…ãˆ
            print(f"  âš ï¸  è¦æ³¨æ„: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¤§")
    
    print(f"\nğŸ¯ çµè«–:")
    print(f"   ğŸ“„ 1å„„æ–‡å­—ä»¥ä¸‹ â†’ SQLæ¨å¥¨")
    print(f"   ğŸ“š 1å„„æ–‡å­—è¶…ãˆ â†’ RAGæ¤œè¨")
    print(f"   ï¿½ï¿½ï¸  ãƒ¡ãƒ¢ãƒª1GBè¶…ãˆ â†’ è¦æ³¨æ„") 
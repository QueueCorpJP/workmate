import asyncio
import os
import sys
import logging
from typing import List, Dict, Any

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(os.path.join(os.path.dirname(__file__), 'Chatbot-backend-main'))

from modules.gemini_question_analyzer import GeminiQuestionAnalyzer
import psycopg2
from psycopg2.extras import RealDictCursor

# ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def analyze_file_distribution():
    """ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§ã®å›ºå®šåŒ–å•é¡Œã‚’åˆ†æ"""
    print("ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§ã®å›ºå®šåŒ–å•é¡Œã‚’åˆ†æã—ã¾ã™...")
    
    try:
        analyzer = GeminiQuestionAnalyzer()
        
        # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒãƒ£ãƒ³ã‚¯åˆ†å¸ƒã‚’ç¢ºèª
        print("\nğŸ“Š 1. ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒãƒ£ãƒ³ã‚¯åˆ†å¸ƒã®åˆ†æ...")
        with psycopg2.connect(analyzer.db_url, cursor_factory=RealDictCursor) as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT 
                        ds.name as document_name,
                        ds.type as document_type,
                        COUNT(c.id) as chunk_count,
                        AVG(LENGTH(c.content)) as avg_content_length,
                        MIN(c.chunk_index) as min_chunk_index,
                        MAX(c.chunk_index) as max_chunk_index
                    FROM document_sources ds
                    LEFT JOIN chunks c ON ds.id = c.doc_id
                    WHERE c.content IS NOT NULL
                    GROUP BY ds.id, ds.name, ds.type
                    ORDER BY chunk_count DESC;
                """)
                
                results = cur.fetchall()
                print(f"ğŸ“ˆ ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(results)}ä»¶")
                print("\nğŸ“‹ ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒãƒ£ãƒ³ã‚¯çµ±è¨ˆ:")
                
                total_chunks = sum(r['chunk_count'] for r in results)
                
                for i, result in enumerate(results, 1):
                    percentage = (result['chunk_count'] / total_chunks) * 100
                    print(f"  {i:2d}. {result['document_name']}")
                    print(f"      ğŸ“„ ã‚¿ã‚¤ãƒ—: {result['document_type']}")
                    print(f"      ğŸ§© ãƒãƒ£ãƒ³ã‚¯æ•°: {result['chunk_count']:,}ä»¶ ({percentage:.1f}%)")
                    print(f"      ğŸ“ å¹³å‡æ–‡å­—æ•°: {result['avg_content_length']:.0f}æ–‡å­—")
                    print(f"      ğŸ“‘ ãƒãƒ£ãƒ³ã‚¯ç¯„å›²: {result['min_chunk_index']} - {result['max_chunk_index']}")
                    print()
                
                # ä¸Šä½3ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒ£ãƒ³ã‚¯æ•°å‰²åˆ
                top3_chunks = sum(r['chunk_count'] for r in results[:3])
                top3_percentage = (top3_chunks / total_chunks) * 100
                print(f"ğŸ¯ ä¸Šä½3ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒ£ãƒ³ã‚¯å æœ‰ç‡: {top3_percentage:.1f}%")
        
        # 2. å®Ÿéš›ã®æ¤œç´¢ã§ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã®ç™»å ´é »åº¦ã‚’ç¢ºèª
        print("\nğŸ“Š 2. æ¤œç´¢ã§ã®ãƒ•ã‚¡ã‚¤ãƒ«ç™»å ´é »åº¦åˆ†æ...")
        
        # æ§˜ã€…ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        test_queries = [
            "ä¼šç¤¾å", "é€£çµ¡å…ˆ", "é›»è©±ç•ªå·", "ä½æ‰€", "æ–™é‡‘", "ä¾¡æ ¼", 
            "ã‚µãƒ¼ãƒ“ã‚¹", "å¥‘ç´„", "ç”³è¾¼", "ãƒ¬ãƒ³ã‚¿ãƒ«", "æœŸé–“", "æ•…éšœ",
            "WALLIOR", "PC", "ãƒ‘ã‚½ã‚³ãƒ³", "æ¥­å‹™", "ãƒãƒ‹ãƒ¥ã‚¢ãƒ«", "æ‰‹é †"
        ]
        
        file_appearance_count = {}
        total_searches = len(test_queries)
        
        for query in test_queries:
            print(f"  ğŸ” æ¤œç´¢ãƒ†ã‚¹ãƒˆ: '{query}'")
            
            analysis = await analyzer.analyze_question(query)
            results = await analyzer.execute_sql_search(analysis, limit=10)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã®ç™»å ´å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            appeared_files = set()
            for result in results[:5]:  # ä¸Šä½5ä»¶ã®ã¿ã‚«ã‚¦ãƒ³ãƒˆ
                file_name = result.document_name
                if file_name not in appeared_files:
                    appeared_files.add(file_name)
                    file_appearance_count[file_name] = file_appearance_count.get(file_name, 0) + 1
        
        print(f"\nğŸ“ˆ {total_searches}å›ã®æ¤œç´¢ã§ã®ç™»å ´é »åº¦:")
        sorted_files = sorted(file_appearance_count.items(), key=lambda x: x[1], reverse=True)
        
        for file_name, count in sorted_files:
            percentage = (count / total_searches) * 100
            print(f"  ğŸ“ {file_name}")
            print(f"    ğŸ¯ ç™»å ´å›æ•°: {count}/{total_searches}å› ({percentage:.1f}%)")
        
        # 3. ãƒãƒ£ãƒ³ã‚¯å†…å®¹ã®å¤šæ§˜æ€§ã‚’ç¢ºèª
        print("\nğŸ“Š 3. ãƒãƒ£ãƒ³ã‚¯å†…å®¹ã®å¤šæ§˜æ€§åˆ†æ...")
        with psycopg2.connect(analyzer.db_url, cursor_factory=RealDictCursor) as conn:
            with conn.cursor() as cur:
                # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°ã‚’æ¨å®š
                cur.execute("""
                    SELECT 
                        ds.name as document_name,
                        ds.type as document_type,
                        COUNT(DISTINCT SUBSTRING(c.content, 1, 100)) as content_diversity,
                        COUNT(c.id) as total_chunks
                    FROM document_sources ds
                    LEFT JOIN chunks c ON ds.id = c.doc_id
                    WHERE c.content IS NOT NULL
                    GROUP BY ds.id, ds.name, ds.type
                    ORDER BY total_chunks DESC;
                """)
                
                diversity_results = cur.fetchall()
                print("ğŸ“‹ ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥å†…å®¹å¤šæ§˜æ€§:")
                
                for result in diversity_results:
                    if result['total_chunks'] > 0:
                        diversity_ratio = result['content_diversity'] / result['total_chunks']
                        print(f"  ğŸ“ {result['document_name']}")
                        print(f"    ğŸ¨ å¤šæ§˜æ€§: {result['content_diversity']}/{result['total_chunks']} ({diversity_ratio:.2f})")
        
        # 4. æ¤œç´¢çµæœã®ã‚¹ã‚³ã‚¢åˆ†å¸ƒã‚’ç¢ºèª
        print("\nğŸ“Š 4. æ¤œç´¢ã‚¹ã‚³ã‚¢åˆ†å¸ƒã®åˆ†æ...")
        
        # ä»£è¡¨çš„ãªæ¤œç´¢ã§ã‚¹ã‚³ã‚¢åˆ†å¸ƒã‚’ç¢ºèª
        analysis = await analyzer.analyze_question("WALLIOR PCã«ã¤ã„ã¦æ•™ãˆã¦")
        results = await analyzer.execute_sql_search(analysis, limit=20)
        
        print(f"ğŸ“ˆ æ¤œç´¢çµæœã®ã‚¹ã‚³ã‚¢åˆ†å¸ƒ (ã‚¯ã‚¨ãƒª: 'WALLIOR PCã«ã¤ã„ã¦æ•™ãˆã¦'):")
        file_scores = {}
        
        for i, result in enumerate(results, 1):
            file_name = result.document_name
            if file_name not in file_scores:
                file_scores[file_name] = []
            file_scores[file_name].append(result.score)
            
            print(f"  {i:2d}. {result.document_name} (ã‚¹ã‚³ã‚¢: {result.score:.3f})")
        
        print(f"\nğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã‚¹ã‚³ã‚¢çµ±è¨ˆ:")
        for file_name, scores in file_scores.items():
            avg_score = sum(scores) / len(scores)
            max_score = max(scores)
            count = len(scores)
            print(f"  ğŸ“ {file_name}")
            print(f"    ğŸ“Š å¹³å‡ã‚¹ã‚³ã‚¢: {avg_score:.3f}, æœ€é«˜ã‚¹ã‚³ã‚¢: {max_score:.3f}, ç™»å ´å›æ•°: {count}")
        
    except Exception as e:
        print(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nâœ… ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å¸ƒåˆ†æå®Œäº†")

if __name__ == "__main__":
    asyncio.run(analyze_file_distribution()) 